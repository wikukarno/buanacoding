<!doctype html><html lang=en data-theme=light><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><title>How to Build a Web Scraper in Python with BeautifulSoup and Requests | Buana Coding</title><meta name=description content="Learn how to build a complete web scraper in Python using BeautifulSoup and Requests. Step-by-step tutorial covering HTML parsing, data extraction, ethical scraping practices, and real-world projects."><meta name=keywords content="python web scraping,beautifulsoup tutorial,requests library,web scraper python,html parsing,data extraction,python automation,web scraping tutorial"><link rel=canonical href=https://www.buanacoding.com/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html><link rel=alternate hreflang=en href=https://www.buanacoding.com/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html><meta property="og:type" content="article"><meta property="og:title" content="How to Build a Web Scraper in Python with BeautifulSoup and Requests | Buana Coding"><meta property="og:description" content="Learn how to build a complete web scraper in Python using BeautifulSoup and Requests. Step-by-step tutorial covering HTML parsing, data extraction, ethical scraping practices, and real-world projects."><meta property="og:url" content="https://www.buanacoding.com/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html"><meta property="og:image" content="https://www.buanacoding.com/images/og-image.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="How to Build a Web Scraper in Python with BeautifulSoup and Requests | Buana Coding"><meta name=twitter:description content="Learn how to build a complete web scraper in Python using BeautifulSoup and Requests. Step-by-step tutorial covering HTML parsing, data extraction, ethical scraping practices, and real-world projects."><meta name=twitter:image content="https://www.buanacoding.com/images/og-image.jpg"><script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "How to Build a Web Scraper in Python with BeautifulSoup and Requests | Buana Coding",
    "description": "Learn how to build a complete web scraper in Python using BeautifulSoup and Requests. Step-by-step tutorial covering HTML parsing, data extraction, ethical scraping practices, and real-world projects.",
    "inLanguage": "en",
    "datePublished": "2025-10-27T10:00:00\u002b07:00",
    "dateModified": "2025-10-27T10:00:00\u002b07:00",
    "author": {
      "@type": "Person",
      "name": "Wiku Karno"
    },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/www.buanacoding.com\/2025\/10\/how-to-build-web-scraper-python-beautifulsoup-requests.html"
    },
    "image": "https:\/\/www.buanacoding.com\/images\/og-image.jpg",
    "publisher": {
      "@type": "Organization",
      "name": "Buana Coding"
    }
  }
  </script><link rel=icon href=/favicon.ico type=image/x-icon><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel=stylesheet><link rel=stylesheet href="/style.css?v=1761830464"><link rel=stylesheet href="/css/overrides.css?v=1761830464"><link rel=stylesheet href="/css/syntax.css?v=1761830464"><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("consent","default",{ad_storage:"granted",ad_user_data:"granted",ad_personalization:"granted",analytics_storage:"granted"})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-0EN9J73FXF"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-0EN9J73FXF")</script><script async src="https://fundingchoicesmessages.google.com/i/pub-3149036684216973?ers=1"></script><script>(function(){function e(){if(!window.frames.googlefcPresent)if(document.body){const e=document.createElement("iframe");e.style="width: 0; height: 0; border: none; z-index: -1000; left: -1000px; top: -1000px;",e.style.display="none",e.name="googlefcPresent",document.body.appendChild(e)}else setTimeout(e,0)}e()})()</script><script>(function(){"use strict";function at(e){var t=0;return function(){return t<e.length?{done:!1,value:e[t++]}:{done:!0}}}Z=typeof Object.defineProperties=="function"?Object.defineProperty:function(e,t,n){return e==Array.prototype||e==Object.prototype?e:(e[t]=n.value,e)};function Ge(e){e=["object"==typeof globalThis&&globalThis,e,"object"==typeof window&&window,"object"==typeof self&&self,"object"==typeof global&&global];for(var t,n=0;n<e.length;++n)if(t=e[n],t&&t.Math==Math)return t;throw Error("Cannot find global object")}Pe=Ge(this);function c(e,t){if(t)a:{var n,o,s=Pe;e=e.split(".");for(n=0;n<e.length-1;n++){if(o=e[n],!(o in s))break a;s=s[o]}e=e[e.length-1],n=s[e],t=t(n),t!=n&&t!=null&&Z(s,e,{configurable:!0,writable:!0,value:t})}}function G(e){return e.raw=e}function b(e){var t=typeof Symbol!="undefined"&&Symbol.iterator&&e[Symbol.iterator];if(t)return t.call(e);if(typeof e.length=="number")return{next:at(e)};throw Error(String(e)+" is not an iterable or ArrayLike")}function Xe(e){for(var t,n=[];!(t=e.next()).done;)n.push(t.value);return n}if(Se=typeof Object.create=="function"?Object.create:function(e){function t(){}return t.prototype=e,new t},typeof Object.setPrototypeOf=="function")R=Object.setPrototypeOf;else{a:{we={a:!0},V={};try{V.__proto__=we,P=V.a;break a}catch{}P=!1}R=P?function(e,t){if(e.__proto__=t,e.__proto__!==t)throw new TypeError(e+" is not extensible");return e}:null}W=R;function K(e,t){if(e.prototype=Se(t.prototype),e.prototype.constructor=e,W)W(e,t);else for(n in t)if(n!="prototype")if(Object.defineProperties){var n,s=Object.getOwnPropertyDescriptor(t,n);s&&Object.defineProperty(e,n,s)}else e[n]=t[n];e.A=t.prototype}function Q(){for(var t=Number(this),n=[],e=t;e<arguments.length;e++)n[e-t]=arguments[e];return n}c("Object.is",function(e){return e||function(e,t){return e===t?e!==0||1/e===1/t:e!==e&&t!==t}}),c("Array.prototype.includes",function(e){return e||function(e,t){var s,o,n=this;n instanceof String&&(n=String(n)),s=n.length,t=t||0;for(t<0&&(t=Math.max(t+s,0));t<s;t++)if(o=n[t],o===e||Object.is(o,e))return!0;return!1}}),c("String.prototype.includes",function(e){return e||function(e,t){if(this==null)throw new TypeError("The 'this' value for String.prototype.includes must not be null or undefined");if(e instanceof RegExp)throw new TypeError("First argument to String.prototype.includes must not be a regular expression");return this.indexOf(e,t||0)!==-1}}),c("Number.MAX_SAFE_INTEGER",function(){return 9007199254740991}),c("Number.isFinite",function(e){return e||function(e){return typeof e=="number"&&!isNaN(e)&&e!==1/0&&e!==-(1/0)}}),c("Number.isInteger",function(e){return e||function(e){return!!Number.isFinite(e)&&e===Math.floor(e)}}),c("Number.isSafeInteger",function(e){return e||function(e){return Number.isInteger(e)&&(e<0?-e:e)<=Number.MAX_SAFE_INTEGER}}),c("Math.trunc",function(e){return e||function(e){if(e=Number(e),isNaN(e)||e===1/0||e===-(1/0)||e===0)return e;var t=Math.floor(e<0?-e:e);return e<0?-t:t}}),e=this||self;function _(t,n){a:{for(var s=["CLOSURE_FLAGS"],o=e,i=0;i<s.length;i++)if(o=o[s[i]],o==null){s=null;break a}s=o}return t=s&&s[t],t??n}function q(e){return e}function ct(t){e.setTimeout(function(){throw t},0)}var ae=_(610401301,!1),ot=_(188588736,!0),et=_(645172343,_(1,!0)),Oe=e.navigator,O=Oe?Oe.userAgentData||null:null;function L(e){return!!ae&&!!O&&O.brands.some(function(t){return(t=t.brand)&&t.indexOf(e)!=-1})}function s(t){var n;a:{if((n=e.navigator)&&(n=n.userAgent))break a;n=""}return n.indexOf(t)!=-1}function r(){return!!ae&&!!O&&O.brands.length>0}function F(){return r()?L("Chromium"):(s("Chrome")||s("CriOS"))&&!(r()?0:s("Edge"))||s("Silk")}be=!r()&&(s("Trident")||s("MSIE")),!s("Android")||F(),F(),s("Safari")&&(F()||(r()?0:s("Coast"))||(r()?0:s("Opera"))||(r()?0:s("Edge"))||(r()?L("Microsoft Edge"):s("Edg/"))||r()&&L("Opera")),M={},v=null,T=typeof Uint8Array!="undefined",He=!be&&typeof btoa=="function";function N(){return typeof BigInt=="function"}g=typeof Symbol=="function"&&typeof Symbol()=="symbol";function xe(e){return typeof Symbol=="function"&&typeof Symbol()=="symbol"?Symbol():e}var Z,Pe,Se,t,R,P,we,e,n,O,V,W,be,v,M,T,He,Te,m,g,X,oe,C,E,Ae,j,Fe,le,re,ie,x=xe(),H=xe("2ex"),ye=g?function(e,t){e[x]|=t}:function(e,t){e.g!==0[0]?e.g|=t:Object.defineProperties(e,{g:{value:t,configurable:!0,writable:!0,enumerable:!1}})},h=g?function(e){return e[x]|0}:function(e){return e.g|0},d=g?function(e){return e[x]}:function(e){return e.g},l=g?function(e,t){e[x]=t}:function(e,t){e.g!==0[0]?e.g=t:Object.defineProperties(e,{g:{value:t,configurable:!0,writable:!0,enumerable:!1}})};function it(e,t){l(t,(e|0)&-14591)}function U(e,t){l(t,(e|34)&-14557)}m={},X={};function Ie(e){return!(!e||typeof e!="object"||e.g!==X)}function S(e){return e!==null&&typeof e=="object"&&!Array.isArray(e)&&e.constructor===Object}function y(e,t,n){if(!Array.isArray(e)||e.length)return!1;var s=h(e);return!!(s&1)||!!(t&&(Array.isArray(t)?t.includes(n):t.has(n)))&&(l(e,s|1),!0)}t=0,n=0;function J(e){var s=e>>>0;t=s,n=(e-s)/4294967296>>>0}function ee(e){if(e<0){J(-e);var s=b(A(t,n));e=s.next().value,s=s.next().value,t=e>>>0,n=s>>>0}else J(e)}function te(e,t){if(t>>>=0,e>>>=0,t<=2097151)var n=""+(4294967296*t+e);else N()?n=""+(BigInt(t)<<BigInt(32)|BigInt(e)):(n=(e>>>24|t<<8)&16777215,t=t>>16&65535,e=(e&16777215)+n*6777216+t*6710656,n+=t*8147497,t*=2,e>=1e7&&(n+=e/1e7>>>0,e%=1e7),n>=1e7&&(t+=n/1e7>>>0,n%=1e7),n=t+ne(n)+ne(e));return n}function ne(e){return e=String(e),"0000000".slice(e.length)+e}function A(e,t){return t=~t,e?e=~e+1:t+=1,[e,t]}oe=/^-?([1-9][0-9]*|0)(\.[0-9]+)?$/;function $(e,t){return E=t,e=new e(t),E=0[0],e}function k(e,t,n){if(e==null&&(e=E),E=0[0],e==null){var o,i,s=96;n?(e=[n],s|=512):e=[],t&&(s=s&-16760833|(t&1023)<<14)}else{if(!Array.isArray(e))throw Error("narr");if(s=h(e),s&2048)throw Error("farr");if(s&64)return e;if(s|=64,n&&(s|=512,n!==e[0]))throw Error("mid");a:{if(n=e,o=n.length,o&&(i=o-1,S(n[i]))){if(s|=256,t=i-(+!!(s&512)-1),t>=1024)throw Error("pvtlmt");s=s&-16760833|(t&1023)<<14;break a}if(t){if(t=Math.max(t,o-(+!!(s&512)-1)),t>1024)throw Error("spvt");s=s&-16760833|(t&1023)<<14}}}return l(e,s),e}function st(e){switch(typeof e){case"number":return isFinite(e)?e:String(e);case"boolean":return e?1:0;case"object":if(e)if(Array.isArray(e)){if(y(e,0[0],0))return}else if(T&&e!=null&&e instanceof Uint8Array){if(He){for(var n,o,i,a,c,l,t="",s=0,r=e.length-10240;s<r;)t+=String.fromCharCode.apply(null,e.subarray(s,s+=10240));t+=String.fromCharCode.apply(null,s?e.subarray(s):e),e=btoa(t)}else{if(t===0[0]&&(t=0),!v){v={},s="ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789".split(""),r=["+/=","+/","-_=","-_.","-_"];for(a=0;a<5;a++){n=s.concat(r[a].split("")),M[a]=n;for(o=0;o<n.length;o++)i=n[o],v[i]===0[0]&&(v[i]=o)}}t=M[t],s=Array(Math.floor(e.length/3)),r=t[64]||"";for(a=n=0;n<e.length-2;n+=3)c=e[n],l=e[n+1],i=e[n+2],o=t[c>>2],c=t[(c&3)<<4|l>>4],l=t[(l&15)<<2|i>>6],i=t[i&63],s[a++]=o+c+l+i;switch(o=0,i=r,e.length-n){case 2:o=e[n+1],i=t[(o&15)<<2]||r;case 1:e=e[n],s[a]=t[e>>2]+t[(e&3)<<4|o>>4]+i+r}e=s.join("")}return e}}return e}function nt(e,t,n){e=Array.prototype.slice.call(e);var i,o=e.length,s=t&256?e[o-1]:0[0];o+=s?-1:0;for(t=t&512?1:0;t<o;t++)e[t]=n(e[t]);if(s){t=e[t]={};for(i in s)Object.prototype.hasOwnProperty.call(s,i)&&(t[i]=n(s[i]))}return e}function de(e,t,n,s,o){if(e!=null){if(Array.isArray(e))e=y(e,0[0],0)?0[0]:o&&h(e)&2?e:B(e,t,n,s!==0[0],o);else if(S(e)){var i,a={};for(i in e)Object.prototype.hasOwnProperty.call(e,i)&&(a[i]=de(e[i],t,n,s,o));e=a}else e=t(e,s);return e}}function B(e,t,n,s,o){var i,a=s||n?h(e):0;s=s?!!(a&32):0[0],e=Array.prototype.slice.call(e);for(i=0;i<e.length;i++)e[i]=de(e[i],t,n,s,o);return n&&n(a,e),e}function We(e){return e.s===m?e.toJSON():st(e)}function me(e,t,n){if(n=n===0[0]?U:n,e!=null){if(T&&e instanceof Uint8Array)return t?e:new Uint8Array(e);if(Array.isArray(e)){var s=h(e);return s&2?e:(t&&(t=s===0||!!(s&32)&&!(s&64||!(s&16))),t?(l(e,(s|34)&-12293),e):B(e,me,s&4?U:n,!0,!0))}return e.s===m&&(n=e.h,s=d(n),e=s&2?e:$(e.constructor,fe(n,s,!0))),e}}function fe(e,t,n){var s=n||t&2?U:it,o=!!(t&32);return e=nt(e,t,function(e){return me(e,o,s)}),ye(e,32|(n?2:0)),e}function pe(e,t){return e=e.h,ve(e,d(e),t)}function ge(e,t,n,s){if(t=s+(+!!(t&512)-1),!(t<0||t>=e.length||t>=n))return e[t]}function ve(e,t,n,s){if(n===-1)return null;var o,a,i=t>>14&1023||536870912;if(n>=i){if(t&256)return e[e.length-1][n]}else return a=e.length,s&&t&256&&(s=e[a-1][n],s!=null)?(ge(e,t,i,n)&&H!=null&&(e=(o=Te)!=null?o:Te={},o=e[H]||0,o>=4||(e[H]=o+1,o=Error(),o.__closure__error__context__984382||(o.__closure__error__context__984382={}),o.__closure__error__context__984382.severity="incident",ct(o))),s):ge(e,t,i,n)}function Y(e,t,n,s,o){var i,a=t>>14&1023||536870912;if(n>=a||o&&!et){if(i=t,t&256)o=e[e.length-1];else{if(s==null)return;o=e[a+(+!!(t&512)-1)]={},i|=256}o[n]=s,n<a&&(e[n+(+!!(t&512)-1)]=0[0]),i!==t&&l(e,i)}else e[n+(+!!(t&512)-1)]=s,t&256&&(e=e[e.length-1],n in e&&delete e[n])}function je(e,t){var a,r,s=he,c=c!==0[0]&&c,o=e.h,i=d(o),n=ve(o,i,t,c);return n!=null&&typeof n=="object"&&n.s===m?s=n:Array.isArray(n)?(a=h(n),r=a,r===0&&(r|=i&32),r|=i&2,r!==a&&l(n,r),s=new s(n)):s=0[0],s!==n&&s!=null&&Y(o,i,t,s,c),o=s,o==null?o:(e=e.h,i=d(e),i&2||(n=o,s=n.h,a=d(s),n=a&2?$(n.constructor,fe(s,a,!1)):n,n!==o&&(o=n,Y(e,i,t,o,c))),o)}function I(e,t){return e=pe(e,t),e==null||typeof e=="string"?e:0[0]}function _e(e,s){if(i=i===0[0]?0:i,e=pe(e,s),e!=null)if(s=typeof e,s==="number"?Number.isFinite(e):s!=="string"?0:oe.test(e)){if(typeof e=="number")e=e|0,!Number.isSafeInteger(e)&&(ee(e),s=t,o=n,(e=o&2147483648)&&(s=~s+1>>>0,o=~o>>>0,s==0&&(o=o+1>>>0)),s=o*4294967296+(s>>>0),e=e?-s:s);else if(s=Number(e)|0,Number.isSafeInteger(s))e=String(s);else if(s=e.indexOf("."),s!==-1&&(e=e.substring(0,s)),!(e[0]==="-"?e.length<20||e.length===20&&Number(e.substring(0,7))>-922337:e.length<19||e.length===19&&Number(e.substring(0,6))<922337)){if(e.length<16)ee(Number(e));else if(N())e=BigInt(e),t=Number(e&BigInt(4294967295))>>>0,n=Number(e>>BigInt(32)&BigInt(4294967295));else{s=+(e[0]==="-"),n=t=0,o=e.length;for(var o,i,a=s,r=(o-s)%6+s;r<=o;a=r,r+=6)a=Number(e.slice(a,r)),n*=1e6,t=t*1e6+a,t>=4294967296&&(n+=t/4294967296|0,n>>>=0,t>>>=0);s&&(s=b(A(t,n)),e=s.next().value,s=s.next().value,t=e,n=s)}e=t,s=n,s&2147483648?N()?e=""+(BigInt(s|0)<<BigInt(32)|BigInt(e>>>0)):(s=b(A(e,s)),e=s.next().value,s=s.next().value,e="-"+te(e,s)):e=te(e,s)}}else e=0[0];return e??i}function a(e,t){var n=n===0[0]?"":n;return e=I(e,t),e??n}function u(e,t,n){this.h=k(e,t,n)}u.prototype.toJSON=function(){return Ce(this)},u.prototype.s=m,u.prototype.toString=function(){try{return C=!0,Ce(this).toString()}finally{C=!1}};function Ce(e){if(a=C?e.h:B(e.h,We,0[0],0[0],!1),m=!C,h=ot?0[0]:e.constructor.v,l=d(m?e.h:a),e=a.length){var t,n,s,o,i,a,l,u,h,m,f,c=a[e-1],r=S(c);if(r?e--:c=0[0],l=+!!(l&512)-1,i=a,r){b:{if(s=c,n={},r=!1,s)for(t in s)Object.prototype.hasOwnProperty.call(s,t)&&(isNaN(+t)?n[t]=s[t]:(o=s[t],Array.isArray(o)&&(y(o,h,+t)||Ie(o)&&o.size===0)&&(o=null),o==null&&(r=!0),o!=null&&(n[t]=o)));if(r){for(u in n)break b;n=null}else n=s}s=n==null?c!=null:n!==c}for(;e>0;e--){if(u=e-1,t=i[u],u-=l,!(t==null||y(t,h,u)||Ie(t)&&t.size===0))break;f=!0}(i!==a||s||f)&&(m?(f||s||n)&&(i.length=e):i=Array.prototype.slice.call(i,0,e),n&&i.push(n)),a=i}return a}function Ee(e){return function(t){if(t==null||t=="")t=new e;else{if(t=JSON.parse(t),!Array.isArray(t))throw Error("dnarr");ye(t,32),t=$(e,t)}return t}}function ke(e){this.h=k(e)}K(ke,u),Ae=Ee(ke);function w(e){this.g=e}w.prototype.toString=function(){return this.g+""},Fe={};function D(t){if(j===0[0]){var n=null,s=e.trustedTypes;if(s&&s.createPolicy){try{n=s.createPolicy("goog#html",{createHTML:q,createScript:q,createScriptURL:q})}catch(t){e.console&&e.console.error(t.message)}j=n}else j=n}return t=(n=j)?n.createScriptURL(t):t,new w(t,Fe)}function ze(e){if(t=Q.apply(1,arguments),t.length===0)return D(e[0]);for(var t,s=e[0],n=0;n<t.length;n++)s+=encodeURIComponent(t[n])+e[n+1];return D(s)}function Ye(e,t){e.src=t instanceof w&&t.constructor===w?t.g:"type_error:TrustedResourceUrl";var n,s;(n=(t=(s=(n=(e.ownerDocument&&e.ownerDocument.defaultView||window).document).querySelector)==null?0[0]:s.call(n,"script[nonce]"))?t.nonce||t.getAttribute("nonce")||"":"")&&e.setAttribute("nonce",n)}function Ne(){return Math.floor(Math.random()*2147483648).toString(36)+Math.abs(Math.floor(Math.random()*2147483648)^Date.now()).toString(36)}function Le(e,t){return t=String(t),e.contentType==="application/xhtml+xml"&&(t=t.toLowerCase()),e.createElement(t)}function Re(t){this.g=t||e.document||document}function qe(e){return e=e===0[0]?document:e,e.createElement("script")}function z(e,t,n,s,o,i){try{var r=e.g,a=qe(r);a.async=!0,Ye(a,t),r.head.appendChild(a),a.addEventListener("load",function(){o(),s&&r.head.removeChild(a)}),a.addEventListener("error",function(){n>0?z(e,t,n-1,s,o,i):(s&&r.head.removeChild(a),i())})}catch{i()}}var Ke=e.atob("aHR0cHM6Ly93d3cuZ3N0YXRpYy5jb20vaW1hZ2VzL2ljb25zL21hdGVyaWFsL3N5c3RlbS8xeC93YXJuaW5nX2FtYmVyXzI0ZHAucG5n"),Ue=e.atob("WW91IGFyZSBzZWVpbmcgdGhpcyBtZXNzYWdlIGJlY2F1c2UgYWQgb3Igc2NyaXB0IGJsb2NraW5nIHNvZnR3YXJlIGlzIGludGVyZmVyaW5nIHdpdGggdGhpcyBwYWdlLg=="),Je=e.atob("RGlzYWJsZSBhbnkgYWQgb3Igc2NyaXB0IGJsb2NraW5nIHNvZnR3YXJlLCB0aGVuIHJlbG9hZCB0aGlzIHBhZ2Uu");function tt(e,t,n){this.i=e,this.u=t,this.o=n,this.g=null,this.j=[],this.m=!1,this.l=new Re(this.i)}function Ve(t){if(t.i.body&&!t.m){var n=function(){Be(t),e.setTimeout(function(){Me(t,3)},50)};z(t.l,t.u,2,!0,function(){e[t.o]||n()},n),t.m=!0}}function Be(e){for(var s,a,r,c,t=o(1,5),n=0;n<t;n++)s=i(e),e.i.body.appendChild(s),e.j.push(s);t=i(e),t.style.bottom="0",t.style.left="0",t.style.position="fixed",t.style.width=o(100,110).toString()+"%",t.style.zIndex=o(2147483544,2147483644).toString(),t.style.backgroundColor=De(249,259,242,252,219,229),t.style.boxShadow="0 0 12px #888",t.style.color=De(0,10,0,10,0,10),t.style.display="flex",t.style.justifyContent="center",t.style.fontFamily="Roboto, Arial",n=i(e),n.style.width=o(80,85).toString()+"%",n.style.maxWidth=o(750,775).toString()+"px",n.style.margin="24px",n.style.display="flex",n.style.alignItems="flex-start",n.style.justifyContent="center",s=Le(e.l.g,"IMG"),s.className=Ne(),s.src=Ke,s.alt="Warning icon",s.style.height="24px",s.style.width="24px",s.style.paddingRight="16px",a=i(e),r=i(e),r.style.fontWeight="bold",r.textContent=Ue,c=i(e),c.textContent=Je,p(e,a,r),p(e,a,c),p(e,n,s),p(e,n,a),p(e,t,n),e.g=t,e.i.body.appendChild(e.g),t=o(1,5);for(n=0;n<t;n++)s=i(e),e.i.body.appendChild(s),e.j.push(s)}function p(e,t,n){for(var r,s=o(1,5),a=0;a<s;a++)r=i(e),t.appendChild(r);t.appendChild(n),n=o(1,5);for(s=0;s<n;s++)a=i(e),t.appendChild(a)}function o(e,t){return Math.floor(e+Math.random()*(t-e))}function De(e,t,n,s,i,a){return"rgb("+o(Math.max(e,0),Math.min(t,255)).toString()+","+o(Math.max(n,0),Math.min(s,255)).toString()+","+o(Math.max(i,0),Math.min(a,255)).toString()+")"}function i(e){return e=Le(e.l.g,"DIV"),e.className=Ne(),e}function Me(t,n){n<=0||t.g!=null&&t.g.offsetHeight!==0&&t.g.offsetWidth!==0||(Qe(t),Be(t),e.setTimeout(function(){Me(t,n-1)},50))}function Qe(e){for(var n=b(e.j),t=n.next();!t.done;t=n.next())(t=t.value)&&t.parentNode&&t.parentNode.removeChild(t);e.j=[],(n=e.g)&&n.parentNode&&n.parentNode.removeChild(n),e.g=null}function Ze(t,n,s,o,i){function r(t){document.body?c(document.body):t>0?e.setTimeout(function(){r(t-1)},i):n()}function c(s){s.appendChild(a),e.setTimeout(function(){a?(a.offsetHeight!==0&&a.offsetWidth!==0?n():t(),a.parentNode&&a.parentNode.removeChild(a)):t()},o)}var a=$e(s);r(3)}function $e(e){var t=document.createElement("div");return t.className=e,t.style.width="1px",t.style.height="1px",t.style.position="absolute",t.style.left="-10000px",t.style.top="-10000px",t.style.zIndex="-10000",t}function he(e){this.h=k(e)}K(he,u);function ue(e){this.h=k(e)}K(ue,u),le=Ee(ue);function ce(e){if(!e)return null;e=I(e,4);var t;return e==null?t=null:t=D(e),t}re=G([""]),ie=G([""]);function se(e,t){this.m=e,this.o=new Re(e.document),this.g=t,this.j=a(this.g,1),this.u=ce(je(this.g,2))||ze(re),this.i=!1,t=ce(je(this.g,13))||ze(ie),this.l=new tt(e.document,t,a(this.g,12))}se.prototype.start=function(){rt(this)};function rt(t){lt(t),z(t.o,t.u,3,!1,function(){a:{var o,n=t.j,s=e.btoa(n);if(s=e[s]){try{o=Ae(e.atob(s))}catch{n=!1;break a}n=n===I(o,1)}else n=!1}n?f(t,a(t.g,14)):(f(t,a(t.g,8)),Ve(t.l))},function(){Ze(function(){f(t,a(t.g,7)),Ve(t.l)},function(){return f(t,a(t.g,6))},a(t.g,9),_e(t.g,10),_e(t.g,11))})}function f(e,t){e.i||(e.i=!0,e=new e.m.XMLHttpRequest,e.open("GET",t,!0),e.send())}function lt(t){var n=e.btoa(t.j);t.m[n]&&f(t,a(t.g,5))}(function(t,n){e[t]=function(){var s=Q.apply(0,arguments);e[t]=function(){},n.call.apply(n,[null].concat(s instanceof Array?s:Xe(b(s))))}})("__h82AlnkH6D91__",function(e){typeof window.atob=="function"&&new se(window,le(window.atob(e))).start()})}).call(this),window.__h82AlnkH6D91__("WyJwdWItMzE0OTAzNjY4NDIxNjk3MyIsW251bGwsbnVsbCxudWxsLCJodHRwczovL2Z1bmRpbmdjaG9pY2VzbWVzc2FnZXMuZ29vZ2xlLmNvbS9iL3B1Yi0zMTQ5MDM2Njg0MjE2OTczIl0sbnVsbCxudWxsLCJodHRwczovL2Z1bmRpbmdjaG9pY2VzbWVzc2FnZXMuZ29vZ2xlLmNvbS9lbC9BR1NLV3hXRW1tb2ZrZGEwZHlSNkcxMkdNSnNEZDF5UFF4a3FSYS11R2NVem9LZHh6NGFxNndzZm5DN3VPekxfWmljWUZvSW5iYnNKRGtFUUU1a1RUNFFGb25MMF9nXHUwMDNkXHUwMDNkP3RlXHUwMDNkVE9LRU5fRVhQT1NFRCIsImh0dHBzOi8vZnVuZGluZ2Nob2ljZXNtZXNzYWdlcy5nb29nbGUuY29tL2VsL0FHU0tXeFZVeXNCQ3NSc2llS3ExOENZVDNibVVqbFREbTVKM01FR1BSdUVoRjFsV3c3UjVNdGRwWnhNM2hJQm1tMnZhbjNmVWxZcVNLSnoycE5xMWNSTUJZYUxfLXdcdTAwM2RcdTAwM2Q/YWJcdTAwM2QxXHUwMDI2c2JmXHUwMDNkMSIsImh0dHBzOi8vZnVuZGluZ2Nob2ljZXNtZXNzYWdlcy5nb29nbGUuY29tL2VsL0FHU0tXeFVvQ2JlQlRkS2gzZmU0NnB1VERhek9PdHN0b0p3NjhKUU5SOGQ1OFgwWm9QRlV0LUZPUm1HU1ZJajM2WXQwQjU3c0t1bUFmcjFNWU5JV3U3U3JXNi1EQkFcdTAwM2RcdTAwM2Q/YWJcdTAwM2QyXHUwMDI2c2JmXHUwMDNkMSIsImh0dHBzOi8vZnVuZGluZ2Nob2ljZXNtZXNzYWdlcy5nb29nbGUuY29tL2VsL0FHU0tXeFdORXhuRXA0STJueGlBNXdiejVRMDZzT0Njek53bFltRXo4cWhOY0sxZzd2N1UwR2ZVZ0hJOFlPeWk1Qms2NGljWTNzU3BMRE9IZ3pSNTg0VlA5ak5sZmdcdTAwM2RcdTAwM2Q/c2JmXHUwMDNkMiIsImRpdi1ncHQtYWQiLDIwLDEwMCwiY0hWaUxUTXhORGt3TXpZMk9EUXlNVFk1TnpNXHUwMDNkIixbbnVsbCxudWxsLG51bGwsImh0dHBzOi8vd3d3LmdzdGF0aWMuY29tLzBlbW4vZi9wL3B1Yi0zMTQ5MDM2Njg0MjE2OTczLmpzP3VzcXBcdTAwM2RDQWMiXSwiaHR0cHM6Ly9mdW5kaW5nY2hvaWNlc21lc3NhZ2VzLmdvb2dsZS5jb20vZWwvQUdTS1d4VmpzRmdxZHhwME1yZFNEMjF6Q2l2TkR3cjZvM0FfNEQyYVlaRUJJZXZNSmN1eUVvNmRmX29XMUNiZFVRdVR1WW52cGZUdWhteS1NTXZfcGlqVlB0NUQxZ1x1MDAzZFx1MDAzZCJd")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3149036684216973" crossorigin=anonymous></script></head><body class="bg-gray-50 dark:bg-gray-900 text-gray-900 dark:text-white min-h-screen flex flex-col"><header class="bg-[#0f7ea9] dark:bg-[#094d66] text-white shadow"><div class="max-w-7xl mx-auto px-4 py-4 flex justify-between items-center"><h1 class="text-2xl font-extrabold tracking-wide"><a href=/ class=hover:opacity-90>BUANACODING</a></h1><nav class="hidden md:flex items-center space-x-6 font-semibold uppercase tracking-wide"><a href=/tags/general class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">General
</a><a href=/tags/go class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Go
</a><a href=/tags/laravel class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Laravel
</a><a href=/tags/linux class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Linux
</a><a href=/tags/python class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Python
</a><a href=/tags/security class="no-underline hover:text-cyan-300 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Security</a><div class="hidden md:flex items-center"><div class="flex border border-white/30 rounded overflow-hidden text-xs font-medium"><a href=/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html data-lang=en data-has-trans=true class="js-lang-link px-2 py-1 bg-white/20 text-white">EN
</a><a href=/id/ data-lang=id data-has-trans=false class="js-lang-link px-2 py-1 bg-transparent hover:bg-white/10 text-white/80">ID</a></div></div><button id=openSearchModal class="text-white transition duration-300 cursor-pointer focus:outline-none dark:hover:text-[#0f7ea9] hover:text-cyan-300" aria-label=Search>
<i data-feather=search class="w-5 h-5"></i>
</button>
<button id=theme-toggle class="ml-2 text-white hover:text-yellow-300 transition duration-300 focus:outline-none cursor-pointer" aria-label="Toggle Dark Mode">
<i id=icon-sun data-feather=sun class="w-5 h-5 hidden"></i>
<i id=icon-moon data-feather=moon class="w-5 h-5"></i></button></nav><button id=menu-toggle class="md:hidden text-2xl focus:outline-none">
☰</button></div><div id=mobile-menu class="hidden md:hidden px-4 pb-4 space-y-2"><a href=/tags/general class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">General
</a><a href=/tags/go class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Go
</a><a href=/tags/laravel class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Laravel
</a><a href=/tags/linux class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Linux
</a><a href=/tags/python class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Python
</a><a href=/tags/security class="block no-underline hover:text-cyan-500 dark:hover:text-[#0f7ea9] transition font-medium text-sm capitalize">Security</a><div class="mt-8 space-y-6"><button id=openSearchModalMobile class="w-full text-left text-white hover:text-cyan-300 transition duration-300 flex items-center gap-4 py-2" aria-label=Search>
<i data-feather=search class="w-5 h-5"></i>
<span class="text-sm font-medium">Search</span></button><div class="flex items-center justify-between py-2"><div class="flex items-center gap-4"><i data-feather=globe class="w-5 h-5 text-white/80"></i>
<span class="text-sm font-medium text-white">Language</span></div><div class="flex text-xs"><a href=/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html data-lang=en data-has-trans=true class="js-lang-link px-3 py-1.5 text-white font-medium">EN
</a><a href=/id/ data-lang=id data-has-trans=false class="js-lang-link px-3 py-1.5 text-white/60 hover:text-white">ID</a></div></div><button id=theme-toggle-mobile class="w-full text-left text-white hover:text-cyan-300 transition duration-300 flex items-center gap-4 py-2" aria-label="Toggle Dark Mode">
<i id=icon-sun-mobile data-feather=sun class="w-5 h-5 hidden"></i>
<i id=icon-moon-mobile data-feather=moon class="w-5 h-5"></i>
<span class="text-sm font-medium">Theme</span></button></div></div></header><div class="bg-gray-50 dark:bg-gray-900 border-b border-gray-200 dark:border-gray-700 py-2"><div class="max-w-7xl mx-auto px-4"><div class="text-center mb-2"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><div class="flex justify-center"><ins class=adsbygoogle style=display:block data-ad-client=ca-pub-3149036684216973 data-ad-slot=9616560861 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div><div id=searchModal class="fixed inset-0 z-50 hidden flex justify-center items-center bg-black/30 dark:bg-black/60 backdrop-blur-md p-4"><div class="bg-white dark:bg-[#1f2937] rounded-xl shadow-2xl w-full max-w-xl p-6 relative"><button id=closeSearchModal class="absolute -top-4 -right-4 bg-white dark:bg-[#374151] rounded-full p-1 shadow hover:bg-gray-100 dark:hover:bg-gray-600 transition duration-200 ease-in-out transform hover:scale-110">
<i data-feather=x class="w-5 h-5 text-gray-600 dark:text-white hover:text-red-500"></i>
</button>
<input type=text id=searchBox placeholder="Search articles..." class="w-full px-4 py-3 mb-4 border border-cyan-400 text-gray-800 dark:text-white dark:bg-[#374151] placeholder-gray-400 dark:placeholder-gray-400 rounded-md focus:outline-none focus:ring-2 focus:ring-cyan-500"><div id=searchAdBanner class="mt-4 mb-8 hidden"><div class="text-center mb-2"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><ins class="adsbygoogle search-results-ad" style=display:block;min-height:100px;max-width:100% data-ad-client=ca-pub-3149036684216973 data-ad-slot=3879195288 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><style>#searchAdBanner{transition:opacity .3s ease-in-out,transform .3s ease-in-out}#searchAdBanner.show{display:block!important;opacity:1;transform:translateY(0)}#searchAdBanner.hide{opacity:0;transform:translateY(-10px)}.search-results-ad{max-height:100px;overflow:hidden}</style><ul id=results class="bg-white dark:bg-[#1f2937] rounded-lg shadow max-h-60 overflow-y-auto divide-y divide-gray-200 dark:divide-gray-600"></ul></div></div><main class="flex-1 w-full"><div class="max-w-7xl mx-auto py-8 px-4"><div class="max-w-7xl mx-auto grid grid-cols-1 lg:grid-cols-3 gap-8 py-8"><article class="prose prose-blue dark:prose-invert lg:col-span-2"><nav class="text-sm text-gray-600 dark:text-gray-300 mb-4"><a href=/ class="text-gray-600 dark:text-gray-300 hover:text-[#0f7ea9] no-underline">Home</a>
<span class="mx-2 text-gray-400 dark:text-gray-500">/</span>
<a href=/tags/python class="text-gray-600 dark:text-gray-300 hover:text-[#0f7ea9] no-underline">#Python</a></nav><h1>How to Build a Web Scraper in Python with BeautifulSoup and Requests</h1><div class="flex items-center gap-4 my-4 py-3 border-b border-gray-200 dark:border-gray-700"><span class="text-sm text-gray-500 dark:text-gray-400 font-medium mr-2">Share:</span>
<a href="https://twitter.com/intent/tweet?text=How+to+Build+a+Web+Scraper+in+Python+with+BeautifulSoup+and+Requests&url=https%3A%2F%2Fwww.buanacoding.com%2F2025%2F10%2Fhow-to-build-web-scraper-python-beautifulsoup-requests.html&via=buanacoding" target=_blank rel=noopener class="p-2 text-gray-500 dark:text-gray-400 hover:text-black hover:bg-gray-100 dark:hover:text-white dark:hover:bg-gray-700 rounded-full transition-all duration-200" title="Share on Twitter"><svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
</a><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fwww.buanacoding.com%2F2025%2F10%2Fhow-to-build-web-scraper-python-beautifulsoup-requests.html" target=_blank rel=noopener class="p-2 text-gray-500 dark:text-gray-400 hover:text-blue-600 hover:bg-blue-50 dark:hover:text-blue-400 dark:hover:bg-blue-900/20 rounded-full transition-all duration-200" title="Share on Facebook"><svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M24 12.073c0-6.627-5.373-12-12-12s-12 5.373-12 12c0 5.99 4.388 10.954 10.125 11.854v-8.385H7.078v-3.47h3.047V9.43c0-3.007 1.792-4.669 4.533-4.669 1.312.0 2.686.235 2.686.235v2.953H15.83c-1.491.0-1.956.925-1.956 1.874v2.25h3.328l-.532 3.47h-2.796v8.385C19.612 23.027 24 18.062 24 12.073z"/></svg>
</a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fwww.buanacoding.com%2F2025%2F10%2Fhow-to-build-web-scraper-python-beautifulsoup-requests.html" target=_blank rel=noopener class="p-2 text-gray-500 dark:text-gray-400 hover:text-blue-700 hover:bg-blue-50 dark:hover:text-blue-400 dark:hover:bg-blue-900/20 rounded-full transition-all duration-200" title="Share on LinkedIn"><svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853.0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601.0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144.0-2.063-.926-2.063-2.065.0-1.138.92-2.063 2.063-2.063 1.14.0 2.064.925 2.064 2.063.0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225.0H1.771C.792.0.0.774.0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2.0 22.222.0h.003z"/></svg>
</a><a href="https://wa.me/?text=How+to+Build+a+Web+Scraper+in+Python+with+BeautifulSoup+and+Requests%20https%3A%2F%2Fwww.buanacoding.com%2F2025%2F10%2Fhow-to-build-web-scraper-python-beautifulsoup-requests.html" target=_blank rel=noopener class="p-2 text-gray-500 dark:text-gray-400 hover:text-green-600 hover:bg-green-50 dark:hover:text-green-400 dark:hover:bg-green-900/20 rounded-full transition-all duration-200" title="Share on WhatsApp"><svg class="w-4 h-4" fill="currentColor" viewBox="0 0 24 24"><path d="M17.472 14.382c-.297-.149-1.758-.867-2.03-.967-.273-.099-.471-.148-.67.15-.197.297-.767.966-.94 1.164-.173.199-.347.223-.644.075-.297-.15-1.255-.463-2.39-1.475-.883-.788-1.48-1.761-1.653-2.059-.173-.297-.018-.458.13-.606.134-.133.298-.347.446-.52.149-.174.198-.298.298-.497.099-.198.05-.371-.025-.52-.075-.149-.669-1.612-.916-2.207-.242-.579-.487-.5-.669-.51-.173-.008-.371-.01-.57-.01-.198.0-.52.074-.792.372-.272.297-1.04 1.016-1.04 2.479.0 1.462 1.065 2.875 1.213 3.074.149.198 2.096 3.2 5.077 4.487.709.306 1.262.489 1.694.625.712.227 1.36.195 1.871.118.571-.085 1.758-.719 2.006-1.413.248-.694.248-1.289.173-1.413-.074-.124-.272-.198-.57-.347m-5.421 7.403h-.004a9.87 9.87.0 01-5.031-1.378l-.361-.214-3.741.982.998-3.648-.235-.374a9.86 9.86.0 01-1.51-5.26c.001-5.45 4.436-9.884 9.888-9.884 2.64.0 5.122 1.03 6.988 2.898a9.825 9.825.0 012.893 6.994c-.003 5.45-4.437 9.884-9.885 9.884m8.413-18.297A11.815 11.815.0 0012.05.0C5.495.0.16 5.335.157 11.892c0 2.096.547 4.142 1.588 5.945L.057 24l6.305-1.654a11.882 11.882.0 005.683 1.448h.005c6.554.0 11.89-5.335 11.893-11.893A11.821 11.821.0 0020.484 3.488"/></svg>
</a><button onclick='copyLinkHeader("https://www.buanacoding.com/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html")' class="p-2 text-gray-500 dark:text-gray-400 hover:text-gray-800 hover:bg-gray-100 dark:hover:text-gray-200 dark:hover:bg-gray-700 rounded-full transition-all duration-200" title="Copy Link">
<svg class="w-4 h-4" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M8 16H6a2 2 0 01-2-2V6a2 2 0 012-2h8a2 2 0 012 2v2m-6 12h8a2 2 0 002-2v-8a2 2 0 00-2-2h-8a2 2 0 00-2 2v8a2 2 0 002 2z"/></svg>
</button>
<span id=copy-header-success class="hidden text-xs text-green-600 dark:text-green-400 font-medium">Copied!</span></div><script>function copyLinkHeader(e){navigator.clipboard.writeText(e).then(function(){showHeaderSuccess()}).catch(function(){const n=document.createElement("textarea");n.value=e,document.body.appendChild(n),n.select(),document.execCommand("copy"),document.body.removeChild(n),showHeaderSuccess()})}function showHeaderSuccess(){const e=document.getElementById("copy-header-success");e.classList.remove("hidden"),setTimeout(function(){e.classList.add("hidden")},2e3)}</script><p>I spent three weeks last month manually copying product prices from competitor websites into spreadsheets for a client&rsquo;s market analysis. Three. Weeks. Every day, opening dozens of browser tabs, copying prices, checking specifications, pasting into Excel. My eyes hurt, my wrists hurt, and I kept making mistakes because humans aren&rsquo;t meant to do repetitive tasks for hours.</p><p>Then I learned web scraping. That same job that took three weeks? Now runs automatically in twenty minutes while I grab coffee. The data is cleaner, more accurate, and I can run it daily instead of monthly. Web scraping literally gave me my life back.</p><p>If you&rsquo;ve ever found yourself manually copying data from websites, this tutorial is for you. We&rsquo;re going to build real web scrapers from scratch using Python&rsquo;s BeautifulSoup and Requests libraries&ndash;no fluff, just practical code you can actually use. By the end, you&rsquo;ll be automating data collection like a pro.</p><div class="my-8 flex justify-center"><div class="w-full max-w-2xl"><div class="text-center mb-2"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-3149036684216973 data-ad-slot=2492895769></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><p>Once you&rsquo;ve got web scraping down, you can level up by <a href=/2025/08/fastapi-tutorial-build-rest-api-from-scratch-beginner-guide.html>building APIs to serve your scraped data with FastAPI
</a>, or <a href=/2025/10/how-to-automate-tasks-cron-jobs-shell-scripts-linux.html>automate your scrapers to run on schedules using Linux cron jobs
</a>.</p><h2 id=why-web-scraping-is-a-superpower-you-need>Why Web Scraping Is a Superpower You Need</h2><p>Look, I get it&ndash;web scraping sounds technical and maybe a little intimidating. But here&rsquo;s the thing: it&rsquo;s just automating what you already do manually. When you visit a website and copy information, you&rsquo;re reading HTML, finding the data you want, and recording it somewhere. Web scraping is teaching your computer to do exactly that, but thousands of times faster and without mistakes.</p><p>Think about all the use cases: price monitoring for e-commerce, collecting job postings for market research, gathering news articles for sentiment analysis, tracking real estate listings, monitoring competitor products, aggregating reviews, building datasets for machine learning. The list goes on. Any time you need data from websites and the site doesn&rsquo;t provide an API, web scraping is your answer.</p><p>The best part? BeautifulSoup and Requests make it genuinely easy. We&rsquo;re not talking about complex browser automation or reverse-engineering APIs (though we&rsquo;ll touch on that later). We&rsquo;re talking about straightforward Python code that fetches HTML and extracts data. If you can write a for loop and understand basic HTML structure, you can build scrapers.</p><h2 id=prerequisites-and-what-youll-learn>Prerequisites and What You&rsquo;ll Learn</h2><p>Before we dive in, here&rsquo;s what you need:</p><ul><li>Python 3.8 or higher installed (<a href=/2025/08/fastapi-tutorial-build-rest-api-from-scratch-beginner-guide.html>
check our Python installation guides
</a>if you need help)</li><li>Basic Python knowledge (variables, functions, lists, dictionaries)</li><li>Understanding of HTML structure (tags, attributes, classes) - don&rsquo;t worry, I&rsquo;ll explain as we go</li><li>A code editor (VS Code, PyCharm, or even a simple text editor)</li><li>Internet connection for installing libraries and testing scrapers</li></ul><p>What we&rsquo;ll build together:</p><ol><li>A simple scraper to extract article titles and links from a blog</li><li>A product scraper that collects names, prices, and images from an e-commerce site</li><li>A news aggregator that scrapes headlines from multiple sources</li><li>Handling pagination to scrape multiple pages automatically</li><li>Dealing with common anti-scraping measures (politely and ethically)</li></ol><h2 id=setting-up-your-scraping-environment>Setting Up Your Scraping Environment</h2><p>Let&rsquo;s get your environment ready. I always create a separate virtual environment for scraping projects&ndash;it keeps dependencies isolated and prevents version conflicts.</p><p><strong>Create a new project directory:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>mkdir web-scraping-tutorial
</span></span><span class=line><span class=cl><span class=nb>cd</span> web-scraping-tutorial
</span></span></code></pre></div><p><strong>Set up a virtual environment:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># On macOS/Linux</span>
</span></span><span class=line><span class=cl>python3 -m venv venv
</span></span><span class=line><span class=cl><span class=nb>source</span> venv/bin/activate
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># On Windows</span>
</span></span><span class=line><span class=cl>python -m venv venv
</span></span><span class=line><span class=cl>venv<span class=se>\S</span>cripts<span class=se>\a</span>ctivate
</span></span></code></pre></div><p>You should see <code>(venv)</code> at the beginning of your command prompt, indicating the virtual environment is active.</p><p><strong>Install the required libraries:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install requests beautifulsoup4 lxml
</span></span></code></pre></div><p>Here&rsquo;s what each library does:</p><ul><li><strong>requests</strong>: Fetches web pages by sending HTTP requests (it&rsquo;s like your browser, but for Python)</li><li><strong>beautifulsoup4</strong>: Parses HTML and lets you extract data using simple Python code</li><li><strong>lxml</strong>: Fast HTML parser that BeautifulSoup uses under the hood (faster than Python&rsquo;s built-in parser)</li></ul><p><strong>Optional but recommended:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install fake-useragent  <span class=c1># Rotate user agents to avoid detection</span>
</span></span><span class=line><span class=cl>pip install pandas          <span class=c1># For exporting scraped data to CSV/Excel</span>
</span></span></code></pre></div><p><strong>Verify your installation:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Requests version: </span><span class=si>{</span><span class=n>requests</span><span class=o>.</span><span class=n>__version__</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;BeautifulSoup imported successfully&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Setup complete!&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>If this runs without errors, you&rsquo;re ready to start scraping.</p><h2 id=understanding-how-web-scraping-actually-works>Understanding How Web Scraping Actually Works</h2><p>Before writing code, let&rsquo;s understand the process at a high level. When you visit a website in your browser, here&rsquo;s what happens:</p><ol><li>Your browser sends an HTTP request to the server</li><li>The server responds with HTML, CSS, and JavaScript</li><li>Your browser parses the HTML and renders it visually</li><li>JavaScript might load additional content dynamically</li></ol><p>Web scraping replicates steps 1 and 2, but instead of rendering visually, we parse the HTML programmatically to extract data. Here&rsquo;s the basic flow:</p><pre tabindex=0><code>Your Python Script
      ?
Send HTTP Request (requests library)
      ?
Receive HTML Response
      ?
Parse HTML (BeautifulSoup)
      ?
Extract Desired Data
      ?
Store or Process Data
</code></pre><p>The key insight: websites are just text files (HTML) with a structure. BeautifulSoup lets you navigate that structure like a tree of nested tags. Once you understand how to locate elements in HTML, extracting data is straightforward.</p><p><strong>Quick HTML refresher:</strong></p><p>HTML is made up of tags that nest inside each other:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-html data-lang=html><span class=line><span class=cl><span class=p>&lt;</span><span class=nt>div</span> <span class=na>class</span><span class=o>=</span><span class=s>&#34;product&#34;</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=p>&lt;</span><span class=nt>h2</span> <span class=na>class</span><span class=o>=</span><span class=s>&#34;product-title&#34;</span><span class=p>&gt;</span>Laptop<span class=p>&lt;/</span><span class=nt>h2</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=p>&lt;</span><span class=nt>span</span> <span class=na>class</span><span class=o>=</span><span class=s>&#34;price&#34;</span><span class=p>&gt;</span>$999<span class=p>&lt;/</span><span class=nt>span</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=p>&lt;</span><span class=nt>a</span> <span class=na>href</span><span class=o>=</span><span class=s>&#34;/product/123&#34;</span><span class=p>&gt;</span>View Details<span class=p>&lt;/</span><span class=nt>a</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=p>&lt;/</span><span class=nt>div</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Each element has:</p><ul><li><strong>Tag name</strong>: <code>div</code>, <code>h2</code>, <code>span</code>, <code>a</code></li><li><strong>Attributes</strong>: <code>class="product"</code>, <code>href="/product/123"</code></li><li><strong>Text content</strong>: &ldquo;Laptop&rdquo;, &ldquo;$999&rdquo;, &ldquo;View Details&rdquo;</li></ul><p>BeautifulSoup lets you find elements by tag name, class, ID, or any attribute, then extract text or attributes.</p><h2 id=your-first-web-scraper-fetching-and-parsing-html>Your First Web Scraper: Fetching and Parsing HTML</h2><p>Let&rsquo;s build the simplest possible scraper. We&rsquo;ll fetch a web page and extract its title&ndash;just to see the process end-to-end.</p><p><strong>Create a file called <code>first_scraper.py</code>:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scrape_page_title</span><span class=p>(</span><span class=n>url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Fetch a web page and extract its title.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Send HTTP GET request</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Check if request was successful</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>response</span><span class=o>.</span><span class=n>status_code</span> <span class=o>==</span> <span class=mi>200</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Successfully fetched </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Failed to fetch page. Status code: </span><span class=si>{</span><span class=n>response</span><span class=o>.</span><span class=n>status_code</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Parse HTML content</span>
</span></span><span class=line><span class=cl>    <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=s1>&#39;lxml&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Extract the page title</span>
</span></span><span class=line><span class=cl>    <span class=n>title</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;title&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>title</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>title</span><span class=o>.</span><span class=n>get_text</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=s2>&#34;No title found&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test the scraper</span>
</span></span><span class=line><span class=cl><span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;https://www.buanacoding.com&#34;</span>
</span></span><span class=line><span class=cl><span class=n>page_title</span> <span class=o>=</span> <span class=n>scrape_page_title</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Page Title: </span><span class=si>{</span><span class=n>page_title</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>Run this with <code>python first_scraper.py</code>. You should see the page title printed.</p><p><strong>Let&rsquo;s break down what&rsquo;s happening:</strong></p><ol><li><code>requests.get(url)</code> sends an HTTP GET request and returns a Response object</li><li><code>response.status_code</code> tells us if the request succeeded (200 means success)</li><li><code>response.content</code> contains the raw HTML as bytes</li><li><code>BeautifulSoup(response.content, 'lxml')</code> parses the HTML into a navigable tree structure</li><li><code>soup.find('title')</code> searches for the first <code>&lt;title></code> tag</li><li><code>title.get_text()</code> extracts the text content inside the tag</li></ol><p>This is the foundation of every scraper: fetch HTML, parse it, extract data. Everything else is variations on this theme.</p><h2 id=extracting-data-finding-elements-with-beautifulsoup>Extracting Data: Finding Elements with BeautifulSoup</h2><p>BeautifulSoup provides multiple ways to find elements. Let&rsquo;s explore the most useful methods:</p><p><strong>Find a single element:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Find by tag name</span>
</span></span><span class=line><span class=cl><span class=n>h1</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;h1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find by class</span>
</span></span><span class=line><span class=cl><span class=n>product</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product-card&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find by ID</span>
</span></span><span class=line><span class=cl><span class=n>header</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=nb>id</span><span class=o>=</span><span class=s1>&#39;main-header&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find by multiple attributes</span>
</span></span><span class=line><span class=cl><span class=n>link</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;class&#39;</span><span class=p>:</span> <span class=s1>&#39;btn&#39;</span><span class=p>,</span> <span class=s1>&#39;data-id&#39;</span><span class=p>:</span> <span class=s1>&#39;123&#39;</span><span class=p>})</span>
</span></span></code></pre></div><p><strong>Find all matching elements:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Find all paragraphs</span>
</span></span><span class=line><span class=cl><span class=n>paragraphs</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;p&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find all elements with a specific class</span>
</span></span><span class=line><span class=cl><span class=n>products</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find all links</span>
</span></span><span class=line><span class=cl><span class=n>links</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Limit results</span>
</span></span><span class=line><span class=cl><span class=n>first_five_links</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=n>limit</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Extracting text and attributes:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>element</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product-link&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get text content</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>get_text</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># Or: text = element.text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get attribute value</span>
</span></span><span class=line><span class=cl><span class=n>href</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;href&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Or: href = element[&#39;href&#39;]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get all attributes as dictionary</span>
</span></span><span class=line><span class=cl><span class=n>all_attrs</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>attrs</span>
</span></span></code></pre></div><p><strong>Navigating the tree:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Get parent element</span>
</span></span><span class=line><span class=cl><span class=n>parent</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>parent</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get all children</span>
</span></span><span class=line><span class=cl><span class=n>children</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>children</span>  <span class=c1># Returns iterator</span>
</span></span><span class=line><span class=cl><span class=n>child_list</span> <span class=o>=</span> <span class=nb>list</span><span class=p>(</span><span class=n>element</span><span class=o>.</span><span class=n>children</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get next sibling</span>
</span></span><span class=line><span class=cl><span class=n>next_elem</span> <span class=o>=</span> <span class=n>element</span><span class=o>.</span><span class=n>next_sibling</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Find within a parent (scoped search)</span>
</span></span><span class=line><span class=cl><span class=n>product_div</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>price_within_product</span> <span class=o>=</span> <span class=n>product_div</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;span&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;price&#39;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=building-a-real-scraper-blog-article-extractor>Building a Real Scraper: Blog Article Extractor</h2><p>Let&rsquo;s build something actually useful: a scraper that extracts article titles, publication dates, and links from a blog.</p><p><strong>Create <code>blog_scraper.py</code>:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scrape_blog_articles</span><span class=p>(</span><span class=n>url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Scrape article information from a blog homepage.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Set a custom user-agent to be polite</span>
</span></span><span class=line><span class=cl>    <span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;User-Agent&#39;</span><span class=p>:</span> <span class=s1>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&#39;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span>  <span class=c1># Raise exception for bad status codes</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error fetching </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=s1>&#39;lxml&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Find all article containers (adjust selectors for your target site)</span>
</span></span><span class=line><span class=cl>    <span class=n>articles</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;article&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;post&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scraped_data</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>article</span> <span class=ow>in</span> <span class=n>articles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Extract title</span>
</span></span><span class=line><span class=cl>        <span class=n>title_elem</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;h2&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;post-title&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>title</span> <span class=o>=</span> <span class=n>title_elem</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=k>if</span> <span class=n>title_elem</span> <span class=k>else</span> <span class=s1>&#39;No title&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract link</span>
</span></span><span class=line><span class=cl>        <span class=n>link_elem</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>link</span> <span class=o>=</span> <span class=n>link_elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;href&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>link_elem</span> <span class=k>else</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract date</span>
</span></span><span class=line><span class=cl>        <span class=n>date_elem</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;time&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;post-date&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>date</span> <span class=o>=</span> <span class=n>date_elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;datetime&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>date_elem</span> <span class=k>else</span> <span class=s1>&#39;No date&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract excerpt</span>
</span></span><span class=line><span class=cl>        <span class=n>excerpt_elem</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;p&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;excerpt&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>excerpt</span> <span class=o>=</span> <span class=n>excerpt_elem</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=k>if</span> <span class=n>excerpt_elem</span> <span class=k>else</span> <span class=s1>&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>scraped_data</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;title&#39;</span><span class=p>:</span> <span class=n>title</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;url&#39;</span><span class=p>:</span> <span class=n>link</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;date&#39;</span><span class=p>:</span> <span class=n>date</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;excerpt&#39;</span><span class=p>:</span> <span class=n>excerpt</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>scraped_data</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>save_to_file</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>filename</span><span class=o>=</span><span class=s1>&#39;articles.txt&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Save scraped articles to a text file.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>idx</span><span class=p>,</span> <span class=n>article</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>idx</span><span class=si>}</span><span class=s2>. </span><span class=si>{</span><span class=n>article</span><span class=p>[</span><span class=s1>&#39;title&#39;</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;   URL: </span><span class=si>{</span><span class=n>article</span><span class=p>[</span><span class=s1>&#39;url&#39;</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;   Date: </span><span class=si>{</span><span class=n>article</span><span class=p>[</span><span class=s1>&#39;date&#39;</span><span class=p>]</span><span class=si>}</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;   Excerpt: </span><span class=si>{</span><span class=n>article</span><span class=p>[</span><span class=s1>&#39;excerpt&#39;</span><span class=p>][:</span><span class=mi>100</span><span class=p>]</span><span class=si>}</span><span class=s2>...</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Saved </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>data</span><span class=p>)</span><span class=si>}</span><span class=s2> articles to </span><span class=si>{</span><span class=n>filename</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Test the scraper</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>blog_url</span> <span class=o>=</span> <span class=s2>&#34;https://www.buanacoding.com/blog&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Scraping </span><span class=si>{</span><span class=n>blog_url</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>articles</span> <span class=o>=</span> <span class=n>scrape_blog_articles</span><span class=p>(</span><span class=n>blog_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>articles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Found </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>articles</span><span class=p>)</span><span class=si>}</span><span class=s2> articles&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>save_to_file</span><span class=p>(</span><span class=n>articles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Print first article as sample</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Sample article:&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Title: </span><span class=si>{</span><span class=n>articles</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;title&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;URL: </span><span class=si>{</span><span class=n>articles</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=s1>&#39;url&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;No articles found. Check your selectors.&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Important notes:</strong></p><ol><li><p><strong>Selectors are site-specific</strong>: The classes like <code>post-title</code> and <code>excerpt</code> need to match the actual HTML structure of your target site. Inspect the page source to find the correct selectors.</p></li><li><p><strong>User-Agent header</strong>: We&rsquo;re identifying ourselves as a browser. The default <code>python-requests/x.x.x</code> user-agent gets blocked by many sites.</p></li><li><p><strong>Error handling</strong>: Always wrap requests in try-except blocks. Networks fail, servers go down, and your scraper should handle that gracefully.</p></li><li><p><strong>Being polite</strong>: We&rsquo;ll add delays between requests soon&ndash;don&rsquo;t hammer servers.</p></li></ol><h2 id=handling-pagination-scraping-multiple-pages>Handling Pagination: Scraping Multiple Pages</h2><p>Most blogs and e-commerce sites split content across pages. Let&rsquo;s scrape all pages automatically:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>scrape_multiple_pages</span><span class=p>(</span><span class=n>base_url</span><span class=p>,</span> <span class=n>num_pages</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Scrape articles from multiple pages.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>all_articles</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>page_num</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>num_pages</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Construct URL for current page</span>
</span></span><span class=line><span class=cl>        <span class=c1># Common patterns:</span>
</span></span><span class=line><span class=cl>        <span class=c1># - https://site.com/blog/page/2</span>
</span></span><span class=line><span class=cl>        <span class=c1># - https://site.com/blog?page=2</span>
</span></span><span class=line><span class=cl>        <span class=c1># - https://site.com/blog/2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>url</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>base_url</span><span class=si>}</span><span class=s2>/page/</span><span class=si>{</span><span class=n>page_num</span><span class=si>}</span><span class=s2>&#34;</span>  <span class=c1># Adjust pattern as needed</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Scraping page </span><span class=si>{</span><span class=n>page_num</span><span class=si>}</span><span class=s2>...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>articles</span> <span class=o>=</span> <span class=n>scrape_blog_articles</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>all_articles</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>articles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Be polite: add delay between requests</span>
</span></span><span class=line><span class=cl>        <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span>  <span class=c1># 2 second delay</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Stop if page returned no articles (we&#39;ve hit the end)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>articles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;No more articles found at page </span><span class=si>{</span><span class=n>page_num</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>all_articles</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Usage</span>
</span></span><span class=line><span class=cl><span class=n>all_articles</span> <span class=o>=</span> <span class=n>scrape_multiple_pages</span><span class=p>(</span><span class=s2>&#34;https://www.buanacoding.com/blog&#34;</span><span class=p>,</span> <span class=n>num_pages</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total articles scraped: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>all_articles</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>The key here is identifying the pagination pattern. Open your target site, click through a few pages, and watch how the URL changes. Then replicate that pattern in your code.</p><h2 id=extracting-product-data-e-commerce-scraper>Extracting Product Data: E-commerce Scraper</h2><p>Let&rsquo;s build a more complex scraper for e-commerce data&ndash;products with names, prices, ratings, and images:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scrape_products</span><span class=p>(</span><span class=n>url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Scrape product information from an e-commerce page.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;User-Agent&#39;</span><span class=p>:</span> <span class=s1>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&#39;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=s1>&#39;lxml&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Find all product cards</span>
</span></span><span class=line><span class=cl>    <span class=n>products</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product-card&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>scraped_products</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>product</span> <span class=ow>in</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=c1># Extract product name</span>
</span></span><span class=line><span class=cl>        <span class=n>name_elem</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;h3&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product-name&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span> <span class=o>=</span> <span class=n>name_elem</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=k>if</span> <span class=n>name_elem</span> <span class=k>else</span> <span class=s1>&#39;N/A&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract price</span>
</span></span><span class=line><span class=cl>        <span class=n>price_elem</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;span&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;price&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>price_text</span> <span class=o>=</span> <span class=n>price_elem</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> <span class=k>if</span> <span class=n>price_elem</span> <span class=k>else</span> <span class=s1>&#39;N/A&#39;</span>
</span></span><span class=line><span class=cl>        <span class=c1># Clean price: remove currency symbols and convert to float</span>
</span></span><span class=line><span class=cl>        <span class=n>price</span> <span class=o>=</span> <span class=n>clean_price</span><span class=p>(</span><span class=n>price_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract rating</span>
</span></span><span class=line><span class=cl>        <span class=n>rating_elem</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;rating&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rating</span> <span class=o>=</span> <span class=n>rating_elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;data-rating&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>rating_elem</span> <span class=k>else</span> <span class=s1>&#39;N/A&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract image URL</span>
</span></span><span class=line><span class=cl>        <span class=n>img_elem</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;img&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>image_url</span> <span class=o>=</span> <span class=n>img_elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;src&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>img_elem</span> <span class=k>else</span> <span class=s1>&#39;N/A&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Extract product URL</span>
</span></span><span class=line><span class=cl>        <span class=n>link_elem</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;a&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product-link&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>product_url</span> <span class=o>=</span> <span class=n>link_elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;href&#39;</span><span class=p>)</span> <span class=k>if</span> <span class=n>link_elem</span> <span class=k>else</span> <span class=s1>&#39;N/A&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Make relative URLs absolute</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>product_url</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>product_url</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s1>&#39;http&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>product_url</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;https://example.com</span><span class=si>{</span><span class=n>product_url</span><span class=si>}</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>scraped_products</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;name&#39;</span><span class=p>:</span> <span class=n>name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;price&#39;</span><span class=p>:</span> <span class=n>price</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;rating&#39;</span><span class=p>:</span> <span class=n>rating</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;image&#39;</span><span class=p>:</span> <span class=n>image_url</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;url&#39;</span><span class=p>:</span> <span class=n>product_url</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>scraped_products</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>clean_price</span><span class=p>(</span><span class=n>price_text</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Convert price text to float.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=kn>import</span> <span class=nn>re</span>
</span></span><span class=line><span class=cl>    <span class=c1># Remove currency symbols and commas</span>
</span></span><span class=line><span class=cl>    <span class=n>price_clean</span> <span class=o>=</span> <span class=n>re</span><span class=o>.</span><span class=n>sub</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;[^\d.]&#39;</span><span class=p>,</span> <span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>price_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=nb>float</span><span class=p>(</span><span class=n>price_clean</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=ne>ValueError</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>save_to_csv</span><span class=p>(</span><span class=n>products</span><span class=p>,</span> <span class=n>filename</span><span class=o>=</span><span class=s1>&#39;products.csv&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Save products to CSV file.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;No products to save&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>keys</span> <span class=o>=</span> <span class=n>products</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>keys</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>newline</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>DictWriter</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>fieldnames</span><span class=o>=</span><span class=n>keys</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span><span class=o>.</span><span class=n>writeheader</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span><span class=o>.</span><span class=n>writerows</span><span class=p>(</span><span class=n>products</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Saved </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>products</span><span class=p>)</span><span class=si>}</span><span class=s2> products to </span><span class=si>{</span><span class=n>filename</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Usage</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>url</span> <span class=o>=</span> <span class=s2>&#34;https://example-ecommerce.com/products&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>products</span> <span class=o>=</span> <span class=n>scrape_products</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>save_to_csv</span><span class=p>(</span><span class=n>products</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Scraped </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>products</span><span class=p>)</span><span class=si>}</span><span class=s2> products successfully&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;No products found&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>This scraper demonstrates several important techniques:</p><ul><li>Cleaning extracted data (removing currency symbols from prices)</li><li>Converting relative URLs to absolute URLs</li><li>Exporting data to CSV for easy analysis</li><li>Using regex to extract numbers from messy text</li></ul><h2 id=being-ethical-respecting-robotstxt-and-rate-limits>Being Ethical: Respecting Robots.txt and Rate Limits</h2><p>Here&rsquo;s the thing nobody likes to talk about: scraping can be rude if done carelessly. Websites cost money to run, and aggressive scrapers can cause real harm. Let&rsquo;s be responsible.</p><p><strong>Always check robots.txt:</strong></p><p>Every site has a <code>robots.txt</code> file at the root (e.g., <code>https://example.com/robots.txt</code>) that tells crawlers which paths are allowed:</p><pre tabindex=0><code>User-agent: *
Disallow: /admin/
Disallow: /api/
Crawl-delay: 10
</code></pre><p>This means: don&rsquo;t scrape <code>/admin/</code> or <code>/api/</code>, and wait 10 seconds between requests.</p><p><strong>Check robots.txt programmatically:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>check_robots_txt</span><span class=p>(</span><span class=n>base_url</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Fetch and display robots.txt content.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>robots_url</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>base_url</span><span class=si>}</span><span class=s2>/robots.txt&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>robots_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>response</span><span class=o>.</span><span class=n>status_code</span> <span class=o>==</span> <span class=mi>200</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;No robots.txt found (status </span><span class=si>{</span><span class=n>response</span><span class=o>.</span><span class=n>status_code</span><span class=si>}</span><span class=s2>)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error fetching robots.txt: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>check_robots_txt</span><span class=p>(</span><span class=s2>&#34;https://www.buanacoding.com&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Implement rate limiting:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>RateLimiter</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Simple rate limiter to control request frequency.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>requests_per_second</span><span class=o>=</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>delay</span> <span class=o>=</span> <span class=mf>1.0</span> <span class=o>/</span> <span class=n>requests_per_second</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>last_request</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>wait</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Wait if necessary to maintain rate limit.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>last_request</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>elapsed</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span> <span class=o>-</span> <span class=bp>self</span><span class=o>.</span><span class=n>last_request</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>elapsed</span> <span class=o>&lt;</span> <span class=bp>self</span><span class=o>.</span><span class=n>delay</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>delay</span> <span class=o>-</span> <span class=n>elapsed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>last_request</span> <span class=o>=</span> <span class=n>time</span><span class=o>.</span><span class=n>time</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Usage</span>
</span></span><span class=line><span class=cl><span class=n>rate_limiter</span> <span class=o>=</span> <span class=n>RateLimiter</span><span class=p>(</span><span class=n>requests_per_second</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>  <span class=c1># 1 request every 2 seconds</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>url</span> <span class=ow>in</span> <span class=n>urls_to_scrape</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>rate_limiter</span><span class=o>.</span><span class=n>wait</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ... process response</span>
</span></span></code></pre></div><p><strong>Best practices summary:</strong></p><ul><li>Start conservatively: 1 request every 10-15 seconds</li><li>Respect robots.txt directives</li><li>Use realistic User-Agent headers</li><li>Implement retries with exponential backoff</li><li>Cache responses to avoid repeat requests</li><li>Scrape during off-peak hours if possible</li><li>Include contact info in your User-Agent so webmasters can reach you</li></ul><h2 id=handling-common-challenges-and-anti-scraping-measures>Handling Common Challenges and Anti-Scraping Measures</h2><p>Real-world scraping is messier than tutorials let on. Here&rsquo;s how to handle common issues:</p><p><strong>1. Websites that block the default User-Agent:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>fake_useragent</span> <span class=kn>import</span> <span class=n>UserAgent</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ua</span> <span class=o>=</span> <span class=n>UserAgent</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>headers</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;User-Agent&#39;</span><span class=p>:</span> <span class=n>ua</span><span class=o>.</span><span class=n>random</span><span class=p>,</span>  <span class=c1># Randomize user agent</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Accept&#39;</span><span class=p>:</span> <span class=s1>&#39;text/html,application/xhtml+xml&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Accept-Language&#39;</span><span class=p>:</span> <span class=s1>&#39;en-US,en;q=0.9&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Referer&#39;</span><span class=p>:</span> <span class=s1>&#39;https://google.com&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>headers</span><span class=o>=</span><span class=n>headers</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>2. Session cookies and persistent connections:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>session</span><span class=o>.</span><span class=n>headers</span><span class=o>.</span><span class=n>update</span><span class=p>(</span><span class=n>headers</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Cookies persist across requests</span>
</span></span><span class=line><span class=cl><span class=n>response1</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>response2</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url2</span><span class=p>)</span>  <span class=c1># Cookies from response1 are sent</span>
</span></span></code></pre></div><p><strong>3. Handling timeouts and retries:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>requests.adapters</span> <span class=kn>import</span> <span class=n>HTTPAdapter</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>requests.packages.urllib3.util.retry</span> <span class=kn>import</span> <span class=n>Retry</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>create_session</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Create a session with retry logic.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>retry</span> <span class=o>=</span> <span class=n>Retry</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>total</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>backoff_factor</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>status_forcelist</span><span class=o>=</span><span class=p>[</span><span class=mi>500</span><span class=p>,</span> <span class=mi>502</span><span class=p>,</span> <span class=mi>503</span><span class=p>,</span> <span class=mi>504</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>adapter</span> <span class=o>=</span> <span class=n>HTTPAdapter</span><span class=p>(</span><span class=n>max_retries</span><span class=o>=</span><span class=n>retry</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span><span class=o>.</span><span class=n>mount</span><span class=p>(</span><span class=s1>&#39;http://&#39;</span><span class=p>,</span> <span class=n>adapter</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>session</span><span class=o>.</span><span class=n>mount</span><span class=p>(</span><span class=s1>&#39;https://&#39;</span><span class=p>,</span> <span class=n>adapter</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>session</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>session</span> <span class=o>=</span> <span class=n>create_session</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>session</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>4. Dealing with missing or inconsistent data:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>safe_extract</span><span class=p>(</span><span class=n>soup</span><span class=p>,</span> <span class=n>selector</span><span class=p>,</span> <span class=n>attribute</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=s1>&#39;N/A&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Safely extract data with fallback.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>elem</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=o>*</span><span class=n>selector</span><span class=p>)</span> <span class=k>if</span> <span class=nb>isinstance</span><span class=p>(</span><span class=n>selector</span><span class=p>,</span> <span class=nb>tuple</span><span class=p>)</span> <span class=k>else</span> <span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=n>selector</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>elem</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>attribute</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>elem</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>attribute</span><span class=p>,</span> <span class=n>default</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>elem</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>default</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Usage</span>
</span></span><span class=line><span class=cl><span class=n>title</span> <span class=o>=</span> <span class=n>safe_extract</span><span class=p>(</span><span class=n>soup</span><span class=p>,</span> <span class=p>(</span><span class=s1>&#39;h1&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;class&#39;</span><span class=p>:</span> <span class=s1>&#39;title&#39;</span><span class=p>}))</span>
</span></span><span class=line><span class=cl><span class=n>price</span> <span class=o>=</span> <span class=n>safe_extract</span><span class=p>(</span><span class=n>soup</span><span class=p>,</span> <span class=p>(</span><span class=s1>&#39;span&#39;</span><span class=p>,</span> <span class=p>{</span><span class=s1>&#39;class&#39;</span><span class=p>:</span> <span class=s1>&#39;price&#39;</span><span class=p>}),</span> <span class=n>default</span><span class=o>=</span><span class=s1>&#39;0.00&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>5. Parsing relative URLs correctly:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>urllib.parse</span> <span class=kn>import</span> <span class=n>urljoin</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>base_url</span> <span class=o>=</span> <span class=s2>&#34;https://example.com/products/&#34;</span>
</span></span><span class=line><span class=cl><span class=n>relative_url</span> <span class=o>=</span> <span class=s2>&#34;../images/product.jpg&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>absolute_url</span> <span class=o>=</span> <span class=n>urljoin</span><span class=p>(</span><span class=n>base_url</span><span class=p>,</span> <span class=n>relative_url</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Result: https://example.com/images/product.jpg</span>
</span></span></code></pre></div><h2 id=storing-your-scraped-data-effectively>Storing Your Scraped Data Effectively</h2><p>You&rsquo;ve scraped data&ndash;now what? Let&rsquo;s look at storage options:</p><p><strong>CSV export (best for tabular data):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>export_to_csv</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>filename</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Export list of dictionaries to CSV.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>data</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>newline</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>DictWriter</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>fieldnames</span><span class=o>=</span><span class=n>data</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span><span class=o>.</span><span class=n>writeheader</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>writer</span><span class=o>.</span><span class=n>writerows</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>export_to_csv</span><span class=p>(</span><span class=n>scraped_products</span><span class=p>,</span> <span class=s1>&#39;products.csv&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>JSON export (preserves nested structures):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>export_to_json</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>filename</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Export data to JSON file.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>json</span><span class=o>.</span><span class=n>dump</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=n>f</span><span class=p>,</span> <span class=n>indent</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>ensure_ascii</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>export_to_json</span><span class=p>(</span><span class=n>scraped_products</span><span class=p>,</span> <span class=s1>&#39;products.json&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>SQLite database (queryable, efficient):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>sqlite3</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>store_in_database</span><span class=p>(</span><span class=n>products</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Store products in SQLite database.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>conn</span> <span class=o>=</span> <span class=n>sqlite3</span><span class=o>.</span><span class=n>connect</span><span class=p>(</span><span class=s1>&#39;products.db&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>cursor</span> <span class=o>=</span> <span class=n>conn</span><span class=o>.</span><span class=n>cursor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Create table</span>
</span></span><span class=line><span class=cl>    <span class=n>cursor</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        CREATE TABLE IF NOT EXISTS products (
</span></span></span><span class=line><span class=cl><span class=s1>            id INTEGER PRIMARY KEY AUTOINCREMENT,
</span></span></span><span class=line><span class=cl><span class=s1>            name TEXT,
</span></span></span><span class=line><span class=cl><span class=s1>            price REAL,
</span></span></span><span class=line><span class=cl><span class=s1>            rating TEXT,
</span></span></span><span class=line><span class=cl><span class=s1>            url TEXT UNIQUE,
</span></span></span><span class=line><span class=cl><span class=s1>            scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
</span></span></span><span class=line><span class=cl><span class=s1>        )
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Insert products</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>product</span> <span class=ow>in</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>cursor</span><span class=o>.</span><span class=n>execute</span><span class=p>(</span><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>                INSERT OR REPLACE INTO products (name, price, rating, url)
</span></span></span><span class=line><span class=cl><span class=s1>                VALUES (?, ?, ?, ?)
</span></span></span><span class=line><span class=cl><span class=s1>            &#39;&#39;&#39;</span><span class=p>,</span> <span class=p>(</span><span class=n>product</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>],</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>],</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;rating&#39;</span><span class=p>],</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;url&#39;</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=n>sqlite3</span><span class=o>.</span><span class=n>Error</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error inserting </span><span class=si>{</span><span class=n>product</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>]</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>conn</span><span class=o>.</span><span class=n>commit</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>conn</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>store_in_database</span><span class=p>(</span><span class=n>scraped_products</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>Using pandas for data manipulation:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert to DataFrame</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>DataFrame</span><span class=p>(</span><span class=n>scraped_products</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Clean and analyze</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>to_numeric</span><span class=p>(</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>],</span> <span class=n>errors</span><span class=o>=</span><span class=s1>&#39;coerce&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span> <span class=o>=</span> <span class=n>df</span><span class=o>.</span><span class=n>dropna</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>])</span>  <span class=c1># Remove rows with missing prices</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate statistics</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Average price: $</span><span class=si>{</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Price range: $</span><span class=si>{</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2> - $</span><span class=si>{</span><span class=n>df</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Export</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>to_csv</span><span class=p>(</span><span class=s1>&#39;products_cleaned.csv&#39;</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>df</span><span class=o>.</span><span class=n>to_excel</span><span class=p>(</span><span class=s1>&#39;products.xlsx&#39;</span><span class=p>,</span> <span class=n>index</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=building-a-complete-news-aggregator>Building a Complete News Aggregator</h2><p>Let&rsquo;s tie everything together with a production-ready news scraper:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>requests</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>csv</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configure logging</span>
</span></span><span class=line><span class=cl><span class=n>logging</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>level</span><span class=o>=</span><span class=n>logging</span><span class=o>.</span><span class=n>INFO</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nb>format</span><span class=o>=</span><span class=s1>&#39;</span><span class=si>%(asctime)s</span><span class=s1> - </span><span class=si>%(levelname)s</span><span class=s1> - </span><span class=si>%(message)s</span><span class=s1>&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>handlers</span><span class=o>=</span><span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>FileHandler</span><span class=p>(</span><span class=s1>&#39;scraper.log&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>StreamHandler</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>NewsAggregator</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Scrape news headlines from multiple sources.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>session</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>Session</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>session</span><span class=o>.</span><span class=n>headers</span><span class=o>.</span><span class=n>update</span><span class=p>({</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;User-Agent&#39;</span><span class=p>:</span> <span class=s1>&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36&#39;</span>
</span></span><span class=line><span class=cl>        <span class=p>})</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>articles</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>scrape_source</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>url</span><span class=p>,</span> <span class=n>selectors</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Scrape a single news source.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Scraping </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>response</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>session</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>,</span> <span class=n>timeout</span><span class=o>=</span><span class=mi>15</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>response</span><span class=o>.</span><span class=n>raise_for_status</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>content</span><span class=p>,</span> <span class=s1>&#39;lxml&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>articles</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>select</span><span class=p>(</span><span class=n>selectors</span><span class=p>[</span><span class=s1>&#39;article_container&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>article</span> <span class=ow>in</span> <span class=n>articles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>headline</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>select_one</span><span class=p>(</span><span class=n>selectors</span><span class=p>[</span><span class=s1>&#39;headline&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                <span class=n>link</span> <span class=o>=</span> <span class=n>article</span><span class=o>.</span><span class=n>select_one</span><span class=p>(</span><span class=n>selectors</span><span class=p>[</span><span class=s1>&#39;link&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>headline</span> <span class=ow>and</span> <span class=n>link</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=bp>self</span><span class=o>.</span><span class=n>articles</span><span class=o>.</span><span class=n>append</span><span class=p>({</span>
</span></span><span class=line><span class=cl>                        <span class=s1>&#39;headline&#39;</span><span class=p>:</span> <span class=n>headline</span><span class=o>.</span><span class=n>get_text</span><span class=p>(</span><span class=n>strip</span><span class=o>=</span><span class=kc>True</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                        <span class=s1>&#39;url&#39;</span><span class=p>:</span> <span class=n>link</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;href&#39;</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                        <span class=s1>&#39;source&#39;</span><span class=p>:</span> <span class=n>url</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=s1>&#39;scraped_at&#39;</span><span class=p>:</span> <span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=o>.</span><span class=n>isoformat</span><span class=p>()</span>
</span></span><span class=line><span class=cl>                    <span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Scraped </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>articles</span><span class=p>)</span><span class=si>}</span><span class=s2> articles from </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span>  <span class=c1># Be polite</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=n>requests</span><span class=o>.</span><span class=n>exceptions</span><span class=o>.</span><span class=n>RequestException</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logging</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Error scraping </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>except</span> <span class=ne>Exception</span> <span class=k>as</span> <span class=n>e</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logging</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Unexpected error: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>scrape_all_sources</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>sources</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Scrape multiple news sources.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>source</span> <span class=ow>in</span> <span class=n>sources</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>scrape_source</span><span class=p>(</span><span class=n>source</span><span class=p>[</span><span class=s1>&#39;url&#39;</span><span class=p>],</span> <span class=n>source</span><span class=p>[</span><span class=s1>&#39;selectors&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>export_results</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>filename</span><span class=o>=</span><span class=s1>&#39;news.csv&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Export scraped articles to CSV.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>articles</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>logging</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=s2>&#34;No articles to export&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>filename</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>newline</span><span class=o>=</span><span class=s1>&#39;&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>writer</span> <span class=o>=</span> <span class=n>csv</span><span class=o>.</span><span class=n>DictWriter</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>fieldnames</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>articles</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=n>writer</span><span class=o>.</span><span class=n>writeheader</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>writer</span><span class=o>.</span><span class=n>writerows</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>articles</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Exported </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>articles</span><span class=p>)</span><span class=si>}</span><span class=s2> articles to </span><span class=si>{</span><span class=n>filename</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Configuration for different news sources</span>
</span></span><span class=line><span class=cl><span class=n>news_sources</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;url&#39;</span><span class=p>:</span> <span class=s1>&#39;https://news.ycombinator.com&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;selectors&#39;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;article_container&#39;</span><span class=p>:</span> <span class=s1>&#39;.athing&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;headline&#39;</span><span class=p>:</span> <span class=s1>&#39;.titleline &gt; a&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s1>&#39;link&#39;</span><span class=p>:</span> <span class=s1>&#39;.titleline &gt; a&#39;</span>
</span></span><span class=line><span class=cl>        <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=c1># Add more sources here</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run the aggregator</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>aggregator</span> <span class=o>=</span> <span class=n>NewsAggregator</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>aggregator</span><span class=o>.</span><span class=n>scrape_all_sources</span><span class=p>(</span><span class=n>news_sources</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>aggregator</span><span class=o>.</span><span class=n>export_results</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Successfully scraped </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>aggregator</span><span class=o>.</span><span class=n>articles</span><span class=p>)</span><span class=si>}</span><span class=s2> articles&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>This scraper demonstrates production patterns:</p><ul><li>Logging for debugging and monitoring</li><li>Session management for efficiency</li><li>Graceful error handling</li><li>Configurable selectors for multiple sources</li><li>Structured output with timestamps</li></ul><h2 id=when-beautifulsoup-isnt-enough-javascript-heavy-sites>When BeautifulSoup Isn&rsquo;t Enough: JavaScript-Heavy Sites</h2><p>BeautifulSoup only sees the HTML the server sends&ndash;it doesn&rsquo;t execute JavaScript. If a site loads content dynamically (most modern single-page apps do), you&rsquo;ll need different tools.</p><p><strong>Check if you actually need JavaScript rendering:</strong></p><p>Open your browser&rsquo;s DevTools (F12), go to the Network tab, and look for XHR/Fetch requests. Often sites load data via JSON APIs that you can call directly&ndash;this is faster and more reliable than rendering JavaScript.</p><p><strong>Example: Calling an API directly instead of scraping:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Instead of scraping the rendered HTML...</span>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;https://site.com/products?page=1&amp;limit=100&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>data</span> <span class=o>=</span> <span class=n>response</span><span class=o>.</span><span class=n>json</span><span class=p>()</span>  <span class=c1># If the endpoint returns JSON</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># You get structured data directly</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>product</span> <span class=ow>in</span> <span class=n>data</span><span class=p>[</span><span class=s1>&#39;results&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>product</span><span class=p>[</span><span class=s1>&#39;name&#39;</span><span class=p>],</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>])</span>
</span></span></code></pre></div><p><strong>When you do need JavaScript rendering, use Selenium:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>selenium</span> <span class=kn>import</span> <span class=n>webdriver</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>selenium.webdriver.common.by</span> <span class=kn>import</span> <span class=n>By</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>selenium.webdriver.support.ui</span> <span class=kn>import</span> <span class=n>WebDriverWait</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>selenium.webdriver.support</span> <span class=kn>import</span> <span class=n>expected_conditions</span> <span class=k>as</span> <span class=n>EC</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Setup (install: pip install selenium)</span>
</span></span><span class=line><span class=cl><span class=n>driver</span> <span class=o>=</span> <span class=n>webdriver</span><span class=o>.</span><span class=n>Chrome</span><span class=p>()</span>  <span class=c1># Or Firefox, Edge, etc.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>driver</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;https://example.com&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Wait for elements to load</span>
</span></span><span class=line><span class=cl>    <span class=n>wait</span> <span class=o>=</span> <span class=n>WebDriverWait</span><span class=p>(</span><span class=n>driver</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>products</span> <span class=o>=</span> <span class=n>wait</span><span class=o>.</span><span class=n>until</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>EC</span><span class=o>.</span><span class=n>presence_of_all_elements_located</span><span class=p>((</span><span class=n>By</span><span class=o>.</span><span class=n>CLASS_NAME</span><span class=p>,</span> <span class=s1>&#39;product-card&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Extract data</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>product</span> <span class=ow>in</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>name</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find_element</span><span class=p>(</span><span class=n>By</span><span class=o>.</span><span class=n>CLASS_NAME</span><span class=p>,</span> <span class=s1>&#39;name&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>        <span class=n>price</span> <span class=o>=</span> <span class=n>product</span><span class=o>.</span><span class=n>find_element</span><span class=p>(</span><span class=n>By</span><span class=o>.</span><span class=n>CLASS_NAME</span><span class=p>,</span> <span class=s1>&#39;price&#39;</span><span class=p>)</span><span class=o>.</span><span class=n>text</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=n>name</span><span class=p>,</span> <span class=n>price</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>finally</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>driver</span><span class=o>.</span><span class=n>quit</span><span class=p>()</span>
</span></span></code></pre></div><p>Selenium is powerful but much slower than Requests+BeautifulSoup. Use it sparingly, and consider <a href=/2025/08/install-docker-on-ubuntu-24-04-with-compose-v2-and-rootless.html>deploying your scrapers with Docker
</a>if they need browser automation in production.</p><h2 id=debugging-your-scrapers-when-they-break>Debugging Your Scrapers When They Break</h2><p>Scrapers break constantly&ndash;websites change their HTML structure without warning. Here&rsquo;s how to debug effectively:</p><p><strong>1. Save the HTML for offline testing:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>requests</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=n>url</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;debug.html&#39;</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=n>response</span><span class=o>.</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Then test parsing locally without hitting the website</span>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=s1>&#39;debug.html&#39;</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>f</span><span class=o>.</span><span class=n>read</span><span class=p>(),</span> <span class=s1>&#39;lxml&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>2. Print the actual HTML you&rsquo;re parsing:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>soup</span><span class=o>.</span><span class=n>prettify</span><span class=p>())</span>  <span class=c1># Formatted HTML</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>soup</span><span class=o>.</span><span class=n>find</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product&#39;</span><span class=p>))</span>  <span class=c1># Specific element</span>
</span></span></code></pre></div><p><strong>3. Check what your selectors actually find:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>products</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;product&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Found </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>products</span><span class=p>)</span><span class=si>}</span><span class=s2> products&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=ow>not</span> <span class=n>products</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=c1># Selector might be wrong</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;No products found. Checking alternative selectors...&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>alt_products</span> <span class=o>=</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>(</span><span class=s1>&#39;div&#39;</span><span class=p>,</span> <span class=n>class_</span><span class=o>=</span><span class=s1>&#39;item&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Found </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>alt_products</span><span class=p>)</span><span class=si>}</span><span class=s2> items with class &#39;item&#39;&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>4. Validate extracted data immediately:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>validate_product</span><span class=p>(</span><span class=n>product</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Check if extracted product data is valid.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>issues</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>product</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;name&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>issues</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34;Missing name&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=ow>not</span> <span class=n>product</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;price&#39;</span><span class=p>)</span> <span class=ow>or</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;price&#39;</span><span class=p>]</span> <span class=o>&lt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>issues</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34;Invalid price&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>product</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;url&#39;</span><span class=p>)</span> <span class=ow>and</span> <span class=ow>not</span> <span class=n>product</span><span class=p>[</span><span class=s1>&#39;url&#39;</span><span class=p>]</span><span class=o>.</span><span class=n>startswith</span><span class=p>(</span><span class=s1>&#39;http&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>issues</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s2>&#34;Invalid URL&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>issues</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>logging</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Product validation failed: </span><span class=si>{</span><span class=n>issues</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>False</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=kc>True</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Use validation</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=n>validate_product</span><span class=p>(</span><span class=n>scraped_product</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>products</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>scraped_product</span><span class=p>)</span>
</span></span></code></pre></div><p><strong>5. Use logging extensively:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>logging</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>logging</span><span class=o>.</span><span class=n>basicConfig</span><span class=p>(</span><span class=n>level</span><span class=o>=</span><span class=n>logging</span><span class=o>.</span><span class=n>DEBUG</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span> <span class=o>=</span> <span class=n>logging</span><span class=o>.</span><span class=n>getLogger</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>debug</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Fetching </span><span class=si>{</span><span class=n>url</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>info</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Found </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>products</span><span class=p>)</span><span class=si>}</span><span class=s2> products&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>warning</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Missing price for product: </span><span class=si>{</span><span class=n>product_name</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>logger</span><span class=o>.</span><span class=n>error</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Failed to parse page: </span><span class=si>{</span><span class=n>e</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=scheduling-your-scrapers-to-run-automatically>Scheduling Your Scrapers to Run Automatically</h2><p>Once your scraper works, you&rsquo;ll want to run it regularly without manual intervention. If you&rsquo;re on Linux, <a href=/2025/10/how-to-automate-tasks-cron-jobs-shell-scripts-linux.html>cron jobs are perfect for scheduling automated tasks
</a>.</p><p><strong>Basic cron job (runs daily at 3 AM):</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Open crontab editor</span>
</span></span><span class=line><span class=cl>crontab -e
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add this line (adjust paths)</span>
</span></span><span class=line><span class=cl><span class=m>0</span> <span class=m>3</span> * * * /usr/bin/python3 /path/to/your/scraper.py &gt;&gt; /path/to/scraper.log 2&gt;<span class=p>&amp;</span><span class=m>1</span>
</span></span></code></pre></div><p><strong>Or use Python&rsquo;s schedule library:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>schedule</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>time</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>job</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Run the scraper.&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Starting scraper at </span><span class=si>{</span><span class=n>datetime</span><span class=o>.</span><span class=n>now</span><span class=p>()</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Your scraping code here</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Schedule job</span>
</span></span><span class=line><span class=cl><span class=n>schedule</span><span class=o>.</span><span class=n>every</span><span class=p>()</span><span class=o>.</span><span class=n>day</span><span class=o>.</span><span class=n>at</span><span class=p>(</span><span class=s2>&#34;03:00&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>do</span><span class=p>(</span><span class=n>job</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>schedule</span><span class=o>.</span><span class=n>every</span><span class=p>()</span><span class=o>.</span><span class=n>hour</span><span class=o>.</span><span class=n>do</span><span class=p>(</span><span class=n>job</span><span class=p>)</span>  <span class=c1># Or every hour</span>
</span></span><span class=line><span class=cl><span class=n>schedule</span><span class=o>.</span><span class=n>every</span><span class=p>(</span><span class=mi>30</span><span class=p>)</span><span class=o>.</span><span class=n>minutes</span><span class=o>.</span><span class=n>do</span><span class=p>(</span><span class=n>job</span><span class=p>)</span>  <span class=c1># Or every 30 minutes</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Keep script running</span>
</span></span><span class=line><span class=cl><span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>schedule</span><span class=o>.</span><span class=n>run_pending</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>time</span><span class=o>.</span><span class=n>sleep</span><span class=p>(</span><span class=mi>60</span><span class=p>)</span>  <span class=c1># Check every minute</span>
</span></span></code></pre></div><p>For production deployments, containerize with Docker and use proper job schedulers like cron, systemd timers, or cloud services like AWS Lambda for serverless scraping.</p><h2 id=what-to-do-next-taking-your-scraping-further>What To Do Next: Taking Your Scraping Further</h2><p>You&rsquo;ve learned the fundamentals&ndash;here&rsquo;s how to level up:</p><p><strong>1. Build a data pipeline:</strong> Scrape data, clean it, store it in a database, and <a href=/2025/08/fastapi-tutorial-build-rest-api-from-scratch-beginner-guide.html>serve it via an API using FastAPI
</a>.</p><p><strong>2. Add monitoring and alerts:</strong> Track scraper success rates and get notified when things break. Use tools like Sentry for error tracking.</p><p><strong>3. Scale up with Scrapy:</strong> For large-scale scraping (thousands of pages), the Scrapy framework provides concurrency, middleware, and built-in best practices.</p><p><strong>4. Learn about browser automation:</strong> Selenium and Playwright let you scrape JavaScript-heavy sites, fill forms, and interact with pages.</p><p><strong>5. Explore data analysis:</strong> Use pandas to analyze your scraped data, create visualizations, and extract insights.</p><p><strong>6. Deploy to the cloud:</strong> Run your scrapers on VPS servers with <a href=/2025/08/nginx-certbot-ubuntu-24-04-free-https.html>proper HTTPS setup
</a>for production reliability.</p><p><strong>7. Stay secure:</strong> If you&rsquo;re handling scraped data containing sensitive information, review <a href=/2025/08/phishing-signs-fake-email-examples-how-to-avoid.html>security best practices
</a>to keep data safe.</p><h2 id=the-bottom-line>The Bottom Line</h2><p>Web scraping transformed how I work, and I hope this guide does the same for you. The ability to programmatically gather data opens up countless possibilities&ndash;market research, price monitoring, content aggregation, dataset building for machine learning, and so much more.</p><p>Remember: scrape responsibly, respect website owners, and always consider whether an official API exists before scraping. When done ethically, web scraping is an incredibly powerful skill that will serve you throughout your career.</p><p>Start with simple projects and gradually increase complexity. Scrape sites you&rsquo;re genuinely interested in&ndash;the best way to learn is by solving real problems. And when your scrapers break (they will), don&rsquo;t get discouraged. Websites change, and adapting scrapers is part of the game.</p><p>Now go automate some tedious data collection and reclaim your time. What&rsquo;s the first thing you&rsquo;re going to scrape?</p></p><div class="bg-gray-100 dark:bg-gray-800 p-4 my-8 rounded"><h2 class="text-lg font-semibold mb-2">📌 Read Also</h2><ul class="list-disc list-outside ml-5"><li><a href=/2025/10/how-to-build-rest-api-django-rest-framework-production-ready.html class="text-gray-900 dark:text-white hover:text-[#0f7ea9] no-underline">How to Build a REST API with Django REST Framework (Production Ready)</a></li><li><a href=/2025/10/python-automation-scripts-every-developer-should-know.html class="text-gray-900 dark:text-white hover:text-[#0f7ea9] no-underline">Python Automation Scripts Every Developer Should Know (Save Hours Weekly)</a></li><li><a href=/2025/10/how-to-analyze-data-python-pandas-from-zero-to-insights.html class="text-gray-900 dark:text-white hover:text-[#0f7ea9] no-underline">How to Analyze Data with Python Pandas: From Zero to Data Insights</a></li></ul></div><div class="hidden lg:block"><div class="my-8 py-6 border-t border-b border-gray-200 dark:border-gray-700"><div class="max-w-4xl mx-auto"><div class="text-center mb-3"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><div class="flex justify-center"><ins class=adsbygoogle style=display:block;min-height:90px;max-width:100% data-ad-client=ca-pub-3149036684216973 data-ad-slot=3879195288 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div></div><div class="block lg:hidden my-8 px-2 mb-10"><div class="text-center mb-2"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><div class="flex justify-center"><ins class=adsbygoogle style=display:block;min-height:50px;max-width:100% data-ad-client=ca-pub-3149036684216973 data-ad-slot=3879195288 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div><div class="lg:hidden mt-8 pt-8 border-t border-gray-200 dark:border-gray-700"><h3 class="text-xl font-bold mb-4 text-gray-900 dark:text-white">Popular Posts</h3><div class="grid gap-3"><a href=/2025/10/how-to-build-rest-api-django-rest-framework-production-ready.html class="block py-2 hover:text-[#0f7ea9] transition no-underline"><h4 class="font-medium text-gray-900 dark:text-white text-sm mb-1">How to Build a REST API with Django REST Framework (Production Ready)</h4><p class="text-xs text-gray-500 dark:text-gray-400">29 Oct 2025</p></a><a href=/2025/10/python-automation-scripts-every-developer-should-know.html class="block py-2 hover:text-[#0f7ea9] transition no-underline"><h4 class="font-medium text-gray-900 dark:text-white text-sm mb-1">Python Automation Scripts Every Developer Should Know (Save Hours Weekly)</h4><p class="text-xs text-gray-500 dark:text-gray-400">28 Oct 2025</p></a><a href=/2025/10/how-to-analyze-data-python-pandas-from-zero-to-insights.html class="block py-2 hover:text-[#0f7ea9] transition no-underline"><h4 class="font-medium text-gray-900 dark:text-white text-sm mb-1">How to Analyze Data with Python Pandas: From Zero to Data Insights</h4><p class="text-xs text-gray-500 dark:text-gray-400">28 Oct 2025</p></a><a href=/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html class="block py-2 hover:text-[#0f7ea9] transition no-underline"><h4 class="font-medium text-gray-900 dark:text-white text-sm mb-1">How to Build a Web Scraper in Python with BeautifulSoup and Requests</h4><p class="text-xs text-gray-500 dark:text-gray-400">27 Oct 2025</p></a><a href=/2025/10/how-to-automate-tasks-cron-jobs-shell-scripts-linux.html class="block py-2 hover:text-[#0f7ea9] transition no-underline"><h4 class="font-medium text-gray-900 dark:text-white text-sm mb-1">How to Automate Tasks with Cron Jobs and Shell Scripts on Linux</h4><p class="text-xs text-gray-500 dark:text-gray-400">26 Oct 2025</p></a></div></div><div class="my-8 py-6"><div class="max-w-4xl mx-auto"><div class="text-center mb-3"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><div class="flex justify-center"><ins class=adsbygoogle style=display:block data-ad-format=autorelaxed data-ad-client=ca-pub-3149036684216973 data-ad-slot=7106179106></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div><div class="not-prose my-12 -mx-4 sm:-mx-6 lg:mx-0"><div class=max-w-full><div class=mb-8><h2 class="text-2xl sm:text-3xl font-bold text-gray-900 dark:text-white mb-2">Frequently Asked Questions</h2><p class="text-gray-600 dark:text-gray-400">Find quick answers to common questions</p></div><ul class="faq-list space-y-0 list-none p-0 m-0"><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">Is web scraping legal and how can I scrape ethically without getting blocked?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>Web scraping is legal when you&rsquo;re collecting publicly available data, but legality depends on what you scrape, how you access it, and your jurisdiction. Always check robots.txt (located at website.com/robots.txt) to see which paths are allowed or disallowed for crawlers. Respect rate limits by implementing delays between requests&ndash;a conservative approach is one request every 10-15 seconds. Use a realistic User-Agent header instead of the default Python-requests identifier which screams &lsquo;bot&rsquo; to websites. Never scrape data behind authentication without permission, avoid collecting personal identifiable information (PII), and never bypass CAPTCHAs or security measures. The GDPR in Europe now treats ignoring robots.txt as a factor against your legitimate interest claim. Recent court cases like Meta vs Bright Data have sided with public web scraping, but you must act in good faith. Always honor Terms of Service where reasonable, implement exponential backoff on errors, and consider reaching out to sites for API access first. Ethical scraping respects server resources&ndash;if your scraper causes performance issues or costs, you&rsquo;ve crossed the line. Add delays with time.sleep(), randomize request intervals, cache results to avoid repeat requests, and monitor HTTP status codes (429 means rate limited, 503 means server overloaded). Include contact info in your User-Agent so webmasters can reach you if needed. For production scrapers, implement proper logging, rotate user agents and IPs if scraping at scale, and always assume the website owner can see your activity. The golden rule: scrape websites the way you&rsquo;d want others to scrape yours&ndash;respectfully, transparently, and without causing harm.</div></div></details></li><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">What's the difference between BeautifulSoup parsers (html.parser, lxml, html5lib) and which should I use?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>BeautifulSoup supports three main parsers and each has trade-offs for speed, accuracy, and dependencies. html.parser is Python&rsquo;s built-in parser&ndash;it requires no external dependencies, works everywhere Python runs, and handles most websites fine. It&rsquo;s moderately fast and lenient with broken HTML but sometimes fails on severely malformed markup. Use it for simple scrapers, learning, or when you can&rsquo;t install dependencies. lxml is the fastest parser available, often 5-10x quicker than html.parser on large documents. It requires the lxml C library to be installed (pip install lxml), which can be tricky on some systems. It&rsquo;s strict about HTML structure but includes good error recovery. Choose lxml for production scrapers processing many pages, large HTML documents, or when performance matters. It&rsquo;s the default choice for most professional scrapers. html5lib is the most lenient parser&ndash;it parses HTML exactly like a web browser would, handling even horribly broken HTML gracefully. It requires the html5lib package and is significantly slower than both alternatives, often 10-50x slower than lxml. Use it only when html.parser and lxml both fail on broken markup, or when you absolutely need browser-identical parsing behavior. The parser affects how BeautifulSoup builds the parse tree and handles edge cases. Switching parsers can change results on malformed HTML&ndash;text might appear in different places in the tree or be missing entirely. In practice: start with html.parser for learning and simple scripts, upgrade to lxml when you need speed or are scraping many pages, and only use html5lib as a last resort for problem websites. When specifying: BeautifulSoup(html, &lsquo;html.parser&rsquo;) vs BeautifulSoup(html, &rsquo;lxml&rsquo;) vs BeautifulSoup(html, &lsquo;html5lib&rsquo;). If you don&rsquo;t specify, BeautifulSoup picks automatically based on what&rsquo;s installed, preferring lxml if available. For production code, always explicitly specify the parser so behavior is consistent across environments.</div></div></details></li><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">How do I handle dynamic content loaded by JavaScript when BeautifulSoup only sees the initial HTML?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>BeautifulSoup and Requests only fetch the raw HTML response from the server&ndash;they don&rsquo;t execute JavaScript, so any content loaded dynamically won&rsquo;t appear in your parsed data. There are several solutions depending on your needs. First, check if the site has an API&ndash;open browser DevTools (F12), go to the Network tab, and reload the page while filtering by XHR/Fetch requests. Many sites load data via JSON APIs that JavaScript then renders. Scraping the API directly is faster, more reliable, and less likely to break than parsing HTML. Look for requests to endpoints returning JSON&ndash;you can often call these directly with requests.get() and parse JSON instead of HTML. Second, use Selenium or Playwright for full browser automation&ndash;these tools control real browsers (Chrome, Firefox) and wait for JavaScript to execute before extracting content. Selenium is mature and widely used: from selenium import webdriver; driver = webdriver.Chrome(); driver.get(url). Playwright is newer, faster, and has better APIs. Both let you wait for elements to appear, scroll pages to trigger infinite scroll, click buttons, and interact like a human. The downside is they&rsquo;re much slower (seconds vs milliseconds) and more resource-intensive than Requests. Use them only when necessary. Third, analyze the JavaScript source code to find data sources&ndash;sometimes data is embedded in script tags as JSON, or the JS builds API URLs you can replicate. Search the page source for JSON-looking data structures. Fourth, use requests-html library which includes a simple JavaScript renderer based on Chromium. Fifth, consider using a headless browser service like Splash or paid APIs like ScraperAPI that handle JavaScript rendering for you. For dynamic content patterns like infinite scroll, Selenium is usually required&ndash;you&rsquo;ll scroll gradually (driver.execute_script(&lsquo;window.scrollTo(0, document.body.scrollHeight)&rsquo;)) and wait for content to load between scrolls. For sites requiring interaction (clicking &rsquo;load more&rsquo;, filling forms), browser automation is your only option. However, check robots.txt and Terms of Service extra carefully when using browser automation&ndash;it&rsquo;s more detectable and resource-intensive than simple HTTP requests. A hybrid approach works well: use Requests + BeautifulSoup for static content and Selenium only for pages requiring JavaScript. This keeps most scraping fast while handling edge cases.</div></div></details></li><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">My scraper worked yesterday but returns different HTML or errors today--how do I make it reliable and maintainable?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>Websites change constantly and scrapers break&ndash;it&rsquo;s the nature of web scraping. Building reliable scrapers requires defensive programming, monitoring, and maintenance strategies. First, expect and handle failures gracefully. Wrap all requests in try-except blocks catching requests.exceptions.RequestException for network issues, and check response.status_code before parsing (200 means success, 404 is not found, 429 is rate limited, 503 is server error). Implement retry logic with exponential backoff&ndash;if a request fails, wait 1 second and retry, then 2 seconds, then 4, up to a maximum. Use the retrying or tenacity libraries to automate this. Second, websites change their HTML structure. Avoid brittle selectors like soup.find(&lsquo;div&rsquo;).find(&lsquo;div&rsquo;).find(&lsquo;span&rsquo;)&ndash;the nth div might change. Instead, use semantic selectors targeting IDs, classes with clear names, or data attributes that are less likely to change. soup.find(&lsquo;article&rsquo;, class_=&lsquo;product-card&rsquo;) is more stable than soup.find_all(&lsquo;div&rsquo;)[7]. Add fallback selectors: try multiple ways to find the same data. If soup.find(id=&lsquo;price&rsquo;) fails, try soup.find(class_=&lsquo;price-tag&rsquo;), then regex on text. Third, validate extracted data immediately. Check types, ranges, and required fields: if price is None or not price.strip() or float(price) &lt; 0: log_error(). This catches problems early before bad data enters your pipeline. Fourth, implement comprehensive logging. Log every request URL, response status, parsing success/failure, and extracted data samples. Use Python&rsquo;s logging module with timestamps and levels (DEBUG, INFO, WARNING, ERROR). When things break, logs tell you exactly what changed. Fifth, separate scraping logic from parsing logic. Write functions like fetch_page(url) and parse_product(soup) separately. This makes testing easier and lets you cache HTML for testing parsing logic without hitting the website repeatedly. Sixth, add monitoring and alerts. For production scrapers, track metrics like success rate, average response time, and data quality. Alert when success rate drops below 90% or no data extracted for X hours. Tools like Sentry or simple email alerts work. Seventh, version control your scrapers in git and document which HTML structure they expect. When updating, keep old parsing code for a while to compare outputs. Eighth, for important scrapers, save raw HTML periodically so you can reprocess if parsing logic improves. Finally, accept that maintenance is unavoidable&ndash;websites redesign, add anti-bot protection, or go offline. Plan for regular updates and monitoring. Build scrapers knowing they&rsquo;ll need attention every few months.</div></div></details></li><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">How do I scrape multiple pages efficiently--should I use threading, asyncio, or multiprocessing?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>When scraping multiple pages, the right concurrency approach depends on your bottleneck. Web scraping is IO-bound (waiting for network responses) not CPU-bound, so threading or asyncio is almost always better than multiprocessing. Sequential scraping (one request at a time) is simplest but slow&ndash;if each page takes 2 seconds and you need 1000 pages, that&rsquo;s 33 minutes. With concurrency, you can get 10-50x speedup depending on the website. Threading with concurrent.futures.ThreadPoolExecutor is the easiest concurrency approach. Threads share memory, work well with blocking libraries like Requests, and Python&rsquo;s ThreadPoolExecutor handles the complexity. Example: with ThreadPoolExecutor(max_workers=5) as executor: results = executor.map(scrape_page, urls). Start with 5-10 workers and increase gradually while monitoring for errors or rate limiting. Too many concurrent requests will get you blocked&ndash;respect the website. Threading has overhead from thread creation and the GIL (Global Interpreter Lock) limits true parallelism, but for IO-bound scraping this doesn&rsquo;t matter. asyncio with aiohttp is faster than threading for large-scale scraping. It uses cooperative multitasking&ndash;one thread interleaves many requests during IO waits. asyncio scales to hundreds or thousands of concurrent requests with lower overhead than threads. However, it requires async/await syntax throughout your code (async def scrape(), await response.text()) and you can&rsquo;t use synchronous libraries like Requests&ndash;you must use aiohttp. BeautifulSoup works fine in async code since parsing is synchronous. Use asyncio when scraping thousands of pages and you&rsquo;re comfortable with async Python. multiprocessing spawns separate Python processes, bypassing the GIL for true parallelism. Use it only when parsing HTML is the bottleneck (CPU-intensive regex, large documents) not network requests. Processes have high overhead and don&rsquo;t share memory easily. Rarely needed for web scraping. Best practices for concurrent scraping: implement rate limiting even with concurrency&ndash;add delays between requests or use a token bucket algorithm. Handle errors per-request, not per-batch&ndash;one failure shouldn&rsquo;t crash your whole scraper. Use a queue or job system for large scrape jobs so you can pause/resume and track progress. Save results incrementally, not at the end&ndash;if your scraper crashes after 6 hours, you don&rsquo;t want to lose everything. Consider using a task queue like Celery for production scrapers that need reliability, monitoring, and retries. For small to medium scrapers (under 10,000 pages), threading with ThreadPoolExecutor and 5-10 workers is the sweet spot&ndash;simple, fast enough, and compatible with all libraries. For large-scale scraping, invest time learning asyncio or use a framework like Scrapy that handles concurrency for you. Always test concurrency levels gradually and monitor website response&ndash;getting blocked wastes more time than slow scraping.</div></div></details></li><li class="faq-item border-b border-gray-200 dark:border-gray-700" itemscope itemprop=mainEntity itemtype=https://schema.org/Question><details class=faq-details><summary class="cursor-pointer list-none py-5 md:py-6 flex items-start justify-between gap-6 select-none focus:outline-none hover:bg-gray-50 dark:hover:bg-gray-800/50 transition-colors duration-150" itemprop=name><h3 class="font-semibold text-base md:text-lg text-gray-900 dark:text-white leading-[1.6] m-0 flex-1">How should I store and export scraped data, and what's the best way to handle large datasets?</h3><span class="faq-arrow flex-shrink-0 mt-1"><svg class="w-5 h-5 text-gray-600 dark:text-gray-400 transition-transform duration-200 ease-in-out" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2"><path stroke-linecap="round" stroke-linejoin="round" d="M19 9l-7 7-7-7"/></svg></span></summary><div class="pb-6 pt-2" itemscope itemprop=acceptedAnswer itemtype=https://schema.org/Answer><div class="text-gray-700 dark:text-gray-300 leading-[1.7] text-[15px] md:text-base prose prose-gray dark:prose-invert max-w-none prose-p:my-2.5 prose-p:leading-[1.7] prose-code:bg-gray-100 dark:prose-code:bg-gray-700 prose-code:px-2 prose-code:py-1 prose-code:rounded prose-code:text-sm prose-code:font-mono" itemprop=text>Data storage depends on your data volume, structure, and intended use. For small scraping jobs (hundreds to a few thousand records), CSV files are simplest: use Python&rsquo;s csv module or pandas.to_csv(). CSV is universal, opens in Excel, and easy to share. However, CSV has limitations&ndash;no standard for nested data, encoding issues with special characters, and no data types (everything becomes text). For structured data with relationships, use JSON: json.dump(data, file) or pandas.to_json(). JSON preserves data types, handles nested structures naturally, and is widely supported. But JSON files can be huge and aren&rsquo;t efficient for querying. For medium datasets (thousands to millions of records) or when you need to query data, use SQLite&ndash;it&rsquo;s a file-based SQL database requiring no server setup. Use Python&rsquo;s sqlite3 module or SQLAlchemy ORM. SQLite lets you query, index, and join data efficiently. Create a database schema matching your scraped data structure, insert records as you scrape (commit every 100-1000 records for performance), and query results later. For large-scale or production scraping, use a proper database server like PostgreSQL or MySQL. They handle concurrent access, large datasets efficiently, and integrate with analytics tools. For high-volume scraping, consider these strategies: stream data to disk incrementally rather than loading everything in memory&ndash;scrape one page, write results, clear memory, repeat. Use batch inserts for databases (100-1000 records at once) instead of inserting one at a time. If scraping millions of records, partition data by date, category, or chunks to avoid giant files. For huge datasets, consider data warehouses like BigQuery or data lakes. File format matters for large data: Parquet is a columnar format that compresses well and queries fast&ndash;used widely in data engineering. Consider pandas.to_parquet() for analytical datasets. JSONL (newline-delimited JSON) is better than JSON for large files&ndash;each line is a valid JSON object, so you can stream-process without loading the entire file. Best practices for data export: validate data immediately after scraping&ndash;check for required fields, data types, and reasonable values. Store raw scraped data separately from cleaned data so you can reprocess if cleaning logic improves. Include metadata like scrape timestamp, source URL, and scraper version. Implement deduplication&ndash;use unique IDs or hashes to avoid scraping the same item twice. For production systems, consider this pipeline: scrape -> store raw data in database or files -> transform/clean data -> export to final format. Use a tool like Apache Airflow to orchestrate this pipeline. Handle incremental scraping for sites that update&ndash;only scrape new or changed items by tracking last-scrape timestamps or comparing hashes. For sharing data, CSV or Excel for business users, JSON or Parquet for developers, SQL database dumps for technical users. Always document your data schema, field meanings, and any data quality issues. Include a README with scrape date, methodology, and known limitations. Most importantly, secure your data appropriately&ndash;scraped data might contain PII or sensitive information, so encrypt at rest, limit access, and comply with data protection regulations like GDPR. Choose storage based on this decision tree: prototype or one-time scrape -> CSV or JSON. Repeated scraping or need to query -> SQLite. Production system or team access -> PostgreSQL. Big data or analytics -> Parquet or data warehouse.</div></div></details></li></ul></div></div><script type=application/ld+json>
{
  "@context": "https://schema.org",
  "@type": "FAQPage",
  "mainEntity": [
    
    
    {
      "@type": "Question",
      "name": "\"Is web scraping legal and how can I scrape ethically without getting blocked?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"Web scraping is legal when you're collecting publicly available data, but legality depends on what you scrape, how you access it, and your jurisdiction. Always check robots.txt (located at website.com/robots.txt) to see which paths are allowed or disallowed for crawlers. Respect rate limits by implementing delays between requests--a conservative approach is one request every 10-15 seconds. Use a realistic User-Agent header instead of the default Python-requests identifier which screams 'bot' to websites. Never scrape data behind authentication without permission, avoid collecting personal identifiable information (PII), and never bypass CAPTCHAs or security measures. The GDPR in Europe now treats ignoring robots.txt as a factor against your legitimate interest claim. Recent court cases like Meta vs Bright Data have sided with public web scraping, but you must act in good faith. Always honor Terms of Service where reasonable, implement exponential backoff on errors, and consider reaching out to sites for API access first. Ethical scraping respects server resources--if your scraper causes performance issues or costs, you've crossed the line. Add delays with time.sleep(), randomize request intervals, cache results to avoid repeat requests, and monitor HTTP status codes (429 means rate limited, 503 means server overloaded). Include contact info in your User-Agent so webmasters can reach you if needed. For production scrapers, implement proper logging, rotate user agents and IPs if scraping at scale, and always assume the website owner can see your activity. The golden rule: scrape websites the way you'd want others to scrape yours--respectfully, transparently, and without causing harm.\""
      }
    }
    
    ,
    {
      "@type": "Question",
      "name": "\"What's the difference between BeautifulSoup parsers (html.parser, lxml, html5lib) and which should I use?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"BeautifulSoup supports three main parsers and each has trade-offs for speed, accuracy, and dependencies. html.parser is Python's built-in parser--it requires no external dependencies, works everywhere Python runs, and handles most websites fine. It's moderately fast and lenient with broken HTML but sometimes fails on severely malformed markup. Use it for simple scrapers, learning, or when you can't install dependencies. lxml is the fastest parser available, often 5-10x quicker than html.parser on large documents. It requires the lxml C library to be installed (pip install lxml), which can be tricky on some systems. It's strict about HTML structure but includes good error recovery. Choose lxml for production scrapers processing many pages, large HTML documents, or when performance matters. It's the default choice for most professional scrapers. html5lib is the most lenient parser--it parses HTML exactly like a web browser would, handling even horribly broken HTML gracefully. It requires the html5lib package and is significantly slower than both alternatives, often 10-50x slower than lxml. Use it only when html.parser and lxml both fail on broken markup, or when you absolutely need browser-identical parsing behavior. The parser affects how BeautifulSoup builds the parse tree and handles edge cases. Switching parsers can change results on malformed HTML--text might appear in different places in the tree or be missing entirely. In practice: start with html.parser for learning and simple scripts, upgrade to lxml when you need speed or are scraping many pages, and only use html5lib as a last resort for problem websites. When specifying: BeautifulSoup(html, 'html.parser') vs BeautifulSoup(html, 'lxml') vs BeautifulSoup(html, 'html5lib'). If you don't specify, BeautifulSoup picks automatically based on what's installed, preferring lxml if available. For production code, always explicitly specify the parser so behavior is consistent across environments.\""
      }
    }
    
    ,
    {
      "@type": "Question",
      "name": "\"How do I handle dynamic content loaded by JavaScript when BeautifulSoup only sees the initial HTML?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"BeautifulSoup and Requests only fetch the raw HTML response from the server--they don't execute JavaScript, so any content loaded dynamically won't appear in your parsed data. There are several solutions depending on your needs. First, check if the site has an API--open browser DevTools (F12), go to the Network tab, and reload the page while filtering by XHR/Fetch requests. Many sites load data via JSON APIs that JavaScript then renders. Scraping the API directly is faster, more reliable, and less likely to break than parsing HTML. Look for requests to endpoints returning JSON--you can often call these directly with requests.get() and parse JSON instead of HTML. Second, use Selenium or Playwright for full browser automation--these tools control real browsers (Chrome, Firefox) and wait for JavaScript to execute before extracting content. Selenium is mature and widely used: from selenium import webdriver; driver = webdriver.Chrome(); driver.get(url). Playwright is newer, faster, and has better APIs. Both let you wait for elements to appear, scroll pages to trigger infinite scroll, click buttons, and interact like a human. The downside is they're much slower (seconds vs milliseconds) and more resource-intensive than Requests. Use them only when necessary. Third, analyze the JavaScript source code to find data sources--sometimes data is embedded in script tags as JSON, or the JS builds API URLs you can replicate. Search the page source for JSON-looking data structures. Fourth, use requests-html library which includes a simple JavaScript renderer based on Chromium. Fifth, consider using a headless browser service like Splash or paid APIs like ScraperAPI that handle JavaScript rendering for you. For dynamic content patterns like infinite scroll, Selenium is usually required--you'll scroll gradually (driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')) and wait for content to load between scrolls. For sites requiring interaction (clicking 'load more', filling forms), browser automation is your only option. However, check robots.txt and Terms of Service extra carefully when using browser automation--it's more detectable and resource-intensive than simple HTTP requests. A hybrid approach works well: use Requests + BeautifulSoup for static content and Selenium only for pages requiring JavaScript. This keeps most scraping fast while handling edge cases.\""
      }
    }
    
    ,
    {
      "@type": "Question",
      "name": "\"My scraper worked yesterday but returns different HTML or errors today--how do I make it reliable and maintainable?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"Websites change constantly and scrapers break--it's the nature of web scraping. Building reliable scrapers requires defensive programming, monitoring, and maintenance strategies. First, expect and handle failures gracefully. Wrap all requests in try-except blocks catching requests.exceptions.RequestException for network issues, and check response.status_code before parsing (200 means success, 404 is not found, 429 is rate limited, 503 is server error). Implement retry logic with exponential backoff--if a request fails, wait 1 second and retry, then 2 seconds, then 4, up to a maximum. Use the retrying or tenacity libraries to automate this. Second, websites change their HTML structure. Avoid brittle selectors like soup.find('div').find('div').find('span')--the nth div might change. Instead, use semantic selectors targeting IDs, classes with clear names, or data attributes that are less likely to change. soup.find('article', class_='product-card') is more stable than soup.find_all('div')[7]. Add fallback selectors: try multiple ways to find the same data. If soup.find(id='price') fails, try soup.find(class_='price-tag'), then regex on text. Third, validate extracted data immediately. Check types, ranges, and required fields: if price is None or not price.strip() or float(price) \\u003c 0: log_error(). This catches problems early before bad data enters your pipeline. Fourth, implement comprehensive logging. Log every request URL, response status, parsing success/failure, and extracted data samples. Use Python's logging module with timestamps and levels (DEBUG, INFO, WARNING, ERROR). When things break, logs tell you exactly what changed. Fifth, separate scraping logic from parsing logic. Write functions like fetch_page(url) and parse_product(soup) separately. This makes testing easier and lets you cache HTML for testing parsing logic without hitting the website repeatedly. Sixth, add monitoring and alerts. For production scrapers, track metrics like success rate, average response time, and data quality. Alert when success rate drops below 90% or no data extracted for X hours. Tools like Sentry or simple email alerts work. Seventh, version control your scrapers in git and document which HTML structure they expect. When updating, keep old parsing code for a while to compare outputs. Eighth, for important scrapers, save raw HTML periodically so you can reprocess if parsing logic improves. Finally, accept that maintenance is unavoidable--websites redesign, add anti-bot protection, or go offline. Plan for regular updates and monitoring. Build scrapers knowing they'll need attention every few months.\""
      }
    }
    
    ,
    {
      "@type": "Question",
      "name": "\"How do I scrape multiple pages efficiently--should I use threading, asyncio, or multiprocessing?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"When scraping multiple pages, the right concurrency approach depends on your bottleneck. Web scraping is IO-bound (waiting for network responses) not CPU-bound, so threading or asyncio is almost always better than multiprocessing. Sequential scraping (one request at a time) is simplest but slow--if each page takes 2 seconds and you need 1000 pages, that's 33 minutes. With concurrency, you can get 10-50x speedup depending on the website. Threading with concurrent.futures.ThreadPoolExecutor is the easiest concurrency approach. Threads share memory, work well with blocking libraries like Requests, and Python's ThreadPoolExecutor handles the complexity. Example: with ThreadPoolExecutor(max_workers=5) as executor: results = executor.map(scrape_page, urls). Start with 5-10 workers and increase gradually while monitoring for errors or rate limiting. Too many concurrent requests will get you blocked--respect the website. Threading has overhead from thread creation and the GIL (Global Interpreter Lock) limits true parallelism, but for IO-bound scraping this doesn't matter. asyncio with aiohttp is faster than threading for large-scale scraping. It uses cooperative multitasking--one thread interleaves many requests during IO waits. asyncio scales to hundreds or thousands of concurrent requests with lower overhead than threads. However, it requires async/await syntax throughout your code (async def scrape(), await response.text()) and you can't use synchronous libraries like Requests--you must use aiohttp. BeautifulSoup works fine in async code since parsing is synchronous. Use asyncio when scraping thousands of pages and you're comfortable with async Python. multiprocessing spawns separate Python processes, bypassing the GIL for true parallelism. Use it only when parsing HTML is the bottleneck (CPU-intensive regex, large documents) not network requests. Processes have high overhead and don't share memory easily. Rarely needed for web scraping. Best practices for concurrent scraping: implement rate limiting even with concurrency--add delays between requests or use a token bucket algorithm. Handle errors per-request, not per-batch--one failure shouldn't crash your whole scraper. Use a queue or job system for large scrape jobs so you can pause/resume and track progress. Save results incrementally, not at the end--if your scraper crashes after 6 hours, you don't want to lose everything. Consider using a task queue like Celery for production scrapers that need reliability, monitoring, and retries. For small to medium scrapers (under 10,000 pages), threading with ThreadPoolExecutor and 5-10 workers is the sweet spot--simple, fast enough, and compatible with all libraries. For large-scale scraping, invest time learning asyncio or use a framework like Scrapy that handles concurrency for you. Always test concurrency levels gradually and monitor website response--getting blocked wastes more time than slow scraping.\""
      }
    }
    
    ,
    {
      "@type": "Question",
      "name": "\"How should I store and export scraped data, and what's the best way to handle large datasets?\"",
      "acceptedAnswer": {
        "@type": "Answer",
        "text": "\"Data storage depends on your data volume, structure, and intended use. For small scraping jobs (hundreds to a few thousand records), CSV files are simplest: use Python's csv module or pandas.to_csv(). CSV is universal, opens in Excel, and easy to share. However, CSV has limitations--no standard for nested data, encoding issues with special characters, and no data types (everything becomes text). For structured data with relationships, use JSON: json.dump(data, file) or pandas.to_json(). JSON preserves data types, handles nested structures naturally, and is widely supported. But JSON files can be huge and aren't efficient for querying. For medium datasets (thousands to millions of records) or when you need to query data, use SQLite--it's a file-based SQL database requiring no server setup. Use Python's sqlite3 module or SQLAlchemy ORM. SQLite lets you query, index, and join data efficiently. Create a database schema matching your scraped data structure, insert records as you scrape (commit every 100-1000 records for performance), and query results later. For large-scale or production scraping, use a proper database server like PostgreSQL or MySQL. They handle concurrent access, large datasets efficiently, and integrate with analytics tools. For high-volume scraping, consider these strategies: stream data to disk incrementally rather than loading everything in memory--scrape one page, write results, clear memory, repeat. Use batch inserts for databases (100-1000 records at once) instead of inserting one at a time. If scraping millions of records, partition data by date, category, or chunks to avoid giant files. For huge datasets, consider data warehouses like BigQuery or data lakes. File format matters for large data: Parquet is a columnar format that compresses well and queries fast--used widely in data engineering. Consider pandas.to_parquet() for analytical datasets. JSONL (newline-delimited JSON) is better than JSON for large files--each line is a valid JSON object, so you can stream-process without loading the entire file. Best practices for data export: validate data immediately after scraping--check for required fields, data types, and reasonable values. Store raw scraped data separately from cleaned data so you can reprocess if cleaning logic improves. Include metadata like scrape timestamp, source URL, and scraper version. Implement deduplication--use unique IDs or hashes to avoid scraping the same item twice. For production systems, consider this pipeline: scrape -\\u003e store raw data in database or files -\\u003e transform/clean data -\\u003e export to final format. Use a tool like Apache Airflow to orchestrate this pipeline. Handle incremental scraping for sites that update--only scrape new or changed items by tracking last-scrape timestamps or comparing hashes. For sharing data, CSV or Excel for business users, JSON or Parquet for developers, SQL database dumps for technical users. Always document your data schema, field meanings, and any data quality issues. Include a README with scrape date, methodology, and known limitations. Most importantly, secure your data appropriately--scraped data might contain PII or sensitive information, so encrypt at rest, limit access, and comply with data protection regulations like GDPR. Choose storage based on this decision tree: prototype or one-time scrape -\\u003e CSV or JSON. Repeated scraping or need to query -\\u003e SQLite. Production system or team access -\\u003e PostgreSQL. Big data or analytics -\\u003e Parquet or data warehouse.\""
      }
    }
    
  ]
}
</script><style>details.faq-details[open] .faq-arrow svg{transform:rotate(180deg)}</style><script>document.addEventListener("DOMContentLoaded",function(){const e=document.querySelectorAll(".faq-item");e.forEach(function(t){t.addEventListener("toggle",function(){this.open&&e.forEach(function(e){e!==t&&e.open&&(e.open=!1)})})})})</script></article><aside class="space-y-10 hidden lg:block"><div><h3 class="text-xl font-bold mb-4 text-gray-900 dark:text-white">Popular Posts</h3><ul class="space-y-3 list-disc list-outside pl-5 text-sm text-gray-800 dark:text-gray-200"><li><a href=/2025/10/how-to-build-rest-api-django-rest-framework-production-ready.html class="hover:text-[#0f7ea9] transition">How to Build a REST API with Django REST Framework (Production Ready)</a></li><li><a href=/2025/10/python-automation-scripts-every-developer-should-know.html class="hover:text-[#0f7ea9] transition">Python Automation Scripts Every Developer Should Know (Save Hours Weekly)</a></li><li><a href=/2025/10/how-to-analyze-data-python-pandas-from-zero-to-insights.html class="hover:text-[#0f7ea9] transition">How to Analyze Data with Python Pandas: From Zero to Data Insights</a></li><li><a href=/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html class="hover:text-[#0f7ea9] transition">How to Build a Web Scraper in Python with BeautifulSoup and Requests</a></li><li><a href=/2025/10/how-to-automate-tasks-cron-jobs-shell-scripts-linux.html class="hover:text-[#0f7ea9] transition">How to Automate Tasks with Cron Jobs and Shell Scripts on Linux</a></li></ul></div></aside></div><section id=comments class=mt-12><script src=https://giscus.app/client.js data-repo=wikukarno/buanacoding data-repo-id=R_kgD0Oj4jAw data-category=General data-category-id=DIC_kwD0Oj4jA84Cr_fx data-mapping=pathname data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></section></div></main><div class=py-6><div class="max-w-7xl mx-auto px-4"><div class="text-center mb-3"><span class="text-xs text-gray-400 dark:text-gray-500 uppercase tracking-wide font-medium">Advertisement</span></div><div class="flex justify-center"><ins class=adsbygoogle style=display:block;min-height:90px;max-width:100% data-ad-client=ca-pub-3149036684216973 data-ad-slot=3879195288 data-ad-format=auto data-full-width-responsive=true></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div></div></div><div class="bg-gray-100 dark:bg-gray-900 text-gray-800 dark:text-white py-4 transition-colors duration-300"><div class="text-center space-x-4 text-sm font-medium"><a href=/about class="hover:text-cyan-500 dark:hover:text-cyan-400">About</a>
<span>-</span>
<a href=/contact class="hover:text-cyan-500 dark:hover:text-cyan-400">Contact</a>
<span>-</span>
<a href=/privacy-policy class="hover:text-cyan-500 dark:hover:text-cyan-400">Privacy Policy</a>
<span>-</span>
<a href=/disclaimer class="hover:text-cyan-500 dark:hover:text-cyan-400">Disclaimer</a></div></div><footer class="text-center text-sm text-gray-500 dark:text-gray-400 py-4 border-t border-gray-200 dark:border-gray-700">&copy; 2025 buanacoding.com</footer><button id=scrollToTop class="fixed bottom-6 right-6 lg:bottom-6 lg:right-6 bg-[#0f7ea9] hover:bg-cyan-600 text-white p-3 rounded-full shadow transition duration-300 z-50" aria-label="Scroll to top">
<svg fill="none" class="w-4 h-4" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 15l7-7 7 7"/></svg></button><style>body.floating-ad-active #scrollToTop{bottom:100px!important}@media(min-width:1024px){body.floating-ad-active #scrollToTop{bottom:1.5rem!important}}</style><div id=google_translate_element style=display:none></div><script src=https://unpkg.com/feather-icons></script><script src=https://cdn.jsdelivr.net/npm/fuse.js@6.4.6></script><script>feather.replace();const btn=document.getElementById("scrollToTop"),html=document.documentElement,toggleBtn=document.getElementById("theme-toggle"),toggleBtnMobile=document.getElementById("theme-toggle-mobile"),iconSun=document.getElementById("icon-sun"),iconMoon=document.getElementById("icon-moon"),iconSunMobile=document.getElementById("icon-sun-mobile"),iconMoonMobile=document.getElementById("icon-moon-mobile");document.getElementById("menu-toggle")?.addEventListener("click",()=>{document.getElementById("mobile-menu")?.classList.toggle("hidden")}),btn.style.display="none",window.addEventListener("scroll",()=>{window.scrollY>300?btn.style.display="flex":btn.style.display="none"}),btn.addEventListener("click",()=>{window.scrollTo({top:0,behavior:"smooth"})});function updateIcons(e){iconSun.classList.toggle("hidden",!e),iconMoon.classList.toggle("hidden",e),iconSunMobile.classList.toggle("hidden",!e),iconMoonMobile.classList.toggle("hidden",e)}function setDarkMode(e){html.setAttribute("data-theme",e?"dark":"light"),localStorage.setItem("theme",e?"dark":"light"),updateIcons(e)}const userPref=localStorage.getItem("theme"),systemPref=window.matchMedia("(prefers-color-scheme: dark)").matches,isDark=userPref==="dark"||!userPref&&systemPref;setDarkMode(isDark),toggleBtn?.addEventListener("click",()=>{const e=html.getAttribute("data-theme")==="dark";setDarkMode(!e)}),toggleBtnMobile?.addEventListener("click",()=>{const e=html.getAttribute("data-theme")==="dark";setDarkMode(!e)}),document.querySelectorAll("pre").forEach(e=>{const t=document.createElement("button");t.innerHTML=`
      <span>Copy</span>
    `,t.className=`
    copy-btn absolute top-2 right-2
    text-xs px-2 py-1 rounded flex items-center gap-1
    transition-colors duration-200
    bg-gray-200 text-gray-800 hover:bg-gray-300
    dark:bg-white/10 dark:text-white dark:hover:bg-white/20
  `.trim();const n=document.createElement("div");n.className="relative",e.parentNode.insertBefore(n,e),n.appendChild(e),n.appendChild(t),t.addEventListener("click",()=>{const n=e.innerText;navigator.clipboard.writeText(n).then(()=>{t.querySelector("span").innerText="Copied!",setTimeout(()=>{t.querySelector("span").innerText="Copy"},2e3)})})});const openBtn=document.getElementById("openSearchModal"),closeBtn=document.getElementById("closeSearchModal"),modal=document.getElementById("searchModal");openBtn.addEventListener("click",()=>{console.log("Search modal opened"),modal.classList.remove("hidden"),document.getElementById("searchBox").focus()}),closeBtn.addEventListener("click",()=>{modal.classList.add("hidden")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&modal.classList.add("hidden")}),fetch("/index.json").then(e=>e.json()).then(e=>{const n=new Fuse(e,{keys:["title","content"],threshold:.3}),s=document.getElementById("searchBox"),t=document.getElementById("results");s.addEventListener("input",function(e){const o=e.target.value.trim(),s=document.getElementById("searchAdBanner");if(o.length<2){t.innerHTML="",s&&(s.classList.remove("show"),s.classList.add("hide"),setTimeout(()=>s.style.display="none",300));return}s&&(s.style.display="block",s.classList.remove("hide"),s.classList.add("show"));const i=n.search(o);if(i.length===0)t.innerHTML=`
            <li class='px-4 py-3 text-center text-gray-500 dark:text-gray-400'>
              <div class="mb-2">📄 No results found for "${o}"</div>
              <div class="text-xs">Try different keywords or browse categories below</div>
            </li>
          `;else{let e="";i.forEach((t,n)=>{e+=`
              <li class="px-4 py-3 hover:bg-gray-100 dark:hover:bg-gray-700 transition">
                <a href="${t.item.href}" class="block font-semibold text-gray-900 dark:text-white no-underline">${t.item.title}</a>
                <p class="text-xs text-gray-500 dark:text-gray-400 mt-1">${t.item.content.substring(0,100)}...</p>
              </li>
            `,(n+1)%3===0&&n<i.length-1&&(e+=`
                <li class="px-4 py-2 bg-blue-50 dark:bg-gray-800 border-l-4 border-blue-400">
                  <div class="text-xs text-blue-600 dark:text-blue-400 font-medium">📚 More related articles</div>
                </li>
              `)}),t.innerHTML=e}})});const openBtnMobile=document.getElementById("openSearchModalMobile"),closeBtnMobile=document.getElementById("closeSearchModal"),modalMobile=document.getElementById("searchModal");openBtnMobile.addEventListener("click",()=>{console.log("Search modal opened (mobile)"),modalMobile.classList.remove("hidden"),document.getElementById("searchBox").focus()}),closeBtnMobile.addEventListener("click",()=>{modalMobile.classList.add("hidden")}),document.addEventListener("keydown",e=>{e.key==="Escape"&&modalMobile.classList.add("hidden")}),function(){function t(e,t){if(window.google&&window.google.translate&&window.google.translate.TranslateElement){e&&e();return}if(window.__gtReadyCallbacks=window.__gtReadyCallbacks||[],e&&window.__gtReadyCallbacks.push(e),window.__gtLoading)return;window.__gtLoading=!0,window.initGoogleTranslate=function(){try{new window.google.translate.TranslateElement({pageLanguage:"en",includedLanguages:"id",autoDisplay:!1},"google_translate_element")}catch{}(window.__gtReadyCallbacks||[]).forEach(function(e){try{e()}catch{}}),window.__gtReadyCallbacks=[]};var s,n=document.createElement("script");n.src="https://translate.google.com/translate_a/element.js?cb=initGoogleTranslate",n.defer=!0,s=!1,n.onerror=function(){s=!0,t&&t()},setTimeout(function(){(!window.google||!window.google.translate)&&(s=!0,t&&t())},4e3),document.head.appendChild(n)}function n(e,t){document.cookie=e+"="+t+"; path=/; expires="+new Date(Date.now()+365*24*3600*1e3).toUTCString()}function e(e){document.cookie=e+"=; expires=Thu, 01 Jan 1970 00:00:00 GMT; path=/"}function s(){if(document.getElementById("translate-fallback"))return;var e=document.createElement("div");e.id="translate-fallback",e.style.cssText="position:fixed;bottom:12px;left:50%;transform:translateX(-50%);z-index:9999;background:#111827;color:#fff;padding:10px 14px;border-radius:10px;box-shadow:0 4px 16px rgba(0,0,0,0.25);display:flex;gap:10px;align-items:center;",e.innerHTML='<span style="font-size:13px">Auto-translate tidak tersedia di perangkat ini.</span><button id="btnTryId" style="background:#0ea5e9;color:#fff;border:none;padding:6px 10px;border-radius:8px;font-size:12px">Coba versi Indonesia</button><button id="btnCloseTf" style="background:transparent;color:#fff;border:1px solid #fff3;padding:6px 10px;border-radius:8px;font-size:12px">Tutup</button>',document.body.appendChild(e),document.getElementById("btnCloseTf").onclick=function(){e.remove()},document.getElementById("btnTryId").onclick=function(){var t=new URL(location.href),e=location.pathname;e.startsWith("/id/")||(t.pathname="/id"+(e.startsWith("/")?e:"/"+e)),fetch(t,{method:"HEAD"}).then(function(e){e.ok?location.href=t:location.href="/id/"}).catch(function(){location.href="/id/"})}}function o(e){t(function(){var t=document.querySelector(".goog-te-combo");t?(t.value=e,t.dispatchEvent(new Event("change"))):(n("googtrans","/en/"+e),location.reload())},s)}function i(){e("googtrans"),e("GOOGTRANS")}document.querySelectorAll(".js-lang-link").forEach(function(e){e.addEventListener("click",function(t){var n=e.getAttribute("data-lang"),s=e.getAttribute("data-has-trans")==="true";if(n==="en"){i();return}s||(t.preventDefault(),o(n))})})}()</script></body></html>