<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open WebUI on Buana Coding</title><link>https://www.buanacoding.com/tags/open-webui/</link><description>Recent content in Open WebUI on Buana Coding</description><generator>Hugo</generator><language>en</language><lastBuildDate>Tue, 19 Aug 2025 10:00:00 +0700</lastBuildDate><atom:link href="https://www.buanacoding.com/tags/open-webui/index.xml" rel="self" type="application/rss+xml"/><item><title>Install Ollama and Open WebUI on Ubuntu 24.04: Local AI (CPU/GPU)</title><link>https://www.buanacoding.com/2025/08/install-ollama-openwebui-ubuntu-24-04.html</link><pubDate>Tue, 19 Aug 2025 10:00:00 +0700</pubDate><guid>https://www.buanacoding.com/2025/08/install-ollama-openwebui-ubuntu-24-04.html</guid><description>&lt;p&gt;If you want to run AI models locally on Ubuntu 24.04 with a clean web UI, this guide is for you. We’ll install &lt;a href="https://ollama.com" target="_blank" rel="nofollow noopener noreferrer"&gt;
 Ollama
&lt;/a&gt;
, pull a model, and use &lt;a href="https://github.com/open-webui/open-webui" target="_blank" rel="nofollow noopener noreferrer"&gt;
 Open WebUI
&lt;/a&gt;
 for a modern chat interface. The steps cover CPU‑only and NVIDIA GPU notes, optional systemd services, and practical troubleshooting.&lt;/p&gt;
&lt;p&gt;What you&amp;rsquo;ll do&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Install Ollama on Ubuntu 24.04 (Noble)&lt;/li&gt;
&lt;li&gt;Pull and run a starter model (e.g., &lt;code&gt;llama3.1&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Run Open WebUI (Docker) and connect to Ollama&lt;/li&gt;
&lt;li&gt;Optionally enable NVIDIA GPU acceleration (CUDA)&lt;/li&gt;
&lt;li&gt;Set up systemd services and basic hardening tips&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Prerequisites&lt;/p&gt;</description></item></channel></rss>