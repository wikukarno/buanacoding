[{"content":"The way we write code is changing dramatically. AI coding assistants have moved from experimental tools to essential companions that can genuinely transform your development workflow. I\u0026rsquo;ve been testing AI coding tools since they first emerged, and honestly, the progress in 2025 has been mind-blowing.\nWhat used to take hours of debugging, researching documentation, or writing boilerplate code can now be done in minutes. But with so many AI coding assistants flooding the market, choosing the right one feels overwhelming. That\u0026rsquo;s why I\u0026rsquo;ve spent months testing every major AI coding tool to bring you this comprehensive guide.\nWhether you\u0026rsquo;re a seasoned developer looking to boost productivity or a beginner wanting to accelerate your learning, these AI coding assistants will revolutionize how you approach programming. Let\u0026rsquo;s dive into the 10 best options that are actually worth your time and money in 2025.\nWhy AI Coding Assistants Matter in 2025 Before we jump into the tools, let\u0026rsquo;s talk about why this matters. According to recent studies, developers using AI coding assistants report 55% faster task completion and significantly reduced debugging time. It\u0026rsquo;s not about replacing developers – it\u0026rsquo;s about eliminating the tedious parts so you can focus on creative problem-solving and architecture decisions.\nThink of it this way: instead of spending 20 minutes writing repetitive CRUD operations or searching Stack Overflow for that regex pattern you always forget, you can focus on building features that actually matter to your users.\n1. GitHub Copilot - The Industry Standard Website: github.com/features/copilot Price: $10/month (Individual), $19/month (Business)\nBest For: All-around coding assistance across multiple languages\nGitHub Copilot remains the gold standard for AI coding assistance, and for good reason. Built on OpenAI\u0026rsquo;s Codex, it understands context incredibly well and provides suggestions that feel almost telepathic.\nWhat makes Copilot special is its deep integration with your development environment. It doesn\u0026rsquo;t just complete lines – it understands your project structure, follows your coding patterns, and even generates entire functions based on comments. I\u0026rsquo;ve found it particularly brilliant for writing REST API endpoints and database queries.\nThe chat feature introduced in 2024 has been a game-changer. You can ask questions about your code, request explanations, or even get help with debugging directly in your IDE. It\u0026rsquo;s like having a senior developer looking over your shoulder, ready to help whenever you\u0026rsquo;re stuck.\nPros: Excellent context awareness, seamless IDE integration, strong community\nCons: Subscription cost, occasional overcomplicated suggestions\n2. Amazon CodeWhisperer - AWS Integration Champion Website: aws.amazon.com/codewhisperer Price: Free tier available, $19/month for Professional\nBest For: AWS development, cloud-native applications\nAmazon\u0026rsquo;s entry into the AI coding space focuses heavily on cloud development, and it shows. CodeWhisperer excels at generating AWS-optimized code and identifying security vulnerabilities in real-time.\nWhat impressed me most is its security scanning capabilities. It automatically flags potential security issues and suggests fixes, which is incredibly valuable when deploying applications to production . The integration with AWS services is seamless – it understands Lambda functions, DynamoDB operations, and S3 interactions better than any other AI assistant.\nThe free tier is genuinely useful, making it accessible for individual developers and small teams. If you\u0026rsquo;re working primarily with AWS, this should be your first choice.\nPros: Strong AWS integration, security focus, generous free tier\nCons: Less effective outside AWS ecosystem, newer to market\n3. Tabnine - The Privacy-Focused Choice Website: tabnine.com Price: Free basic version, $12/month for Pro\nBest For: Enterprise environments, privacy-conscious developers\nTabnine takes a different approach by focusing on privacy and customization. Unlike cloud-based solutions, it can run entirely on your local machine, which is crucial for enterprise environments with strict data policies.\nWhat sets Tabnine apart is its ability to learn from your specific codebase. It adapts to your team\u0026rsquo;s coding standards, naming conventions, and architectural patterns. This personalization makes its suggestions feel more relevant and consistent with your project\u0026rsquo;s style.\nThe AI models are trained on permissively licensed code only, addressing copyright concerns that some developers have with other tools. This ethical approach has made it popular in corporate environments.\nPros: Privacy-focused, learns team patterns, ethical training data\nCons: Smaller suggestion database, requires setup time\n4. Codeium - The Free Powerhouse Website: codeium.com Price: Free for individuals, Enterprise pricing available\nBest For: Budget-conscious developers, students\nCodeium might be the most impressive free AI coding assistant available. It offers features that rival paid solutions without the subscription cost. I\u0026rsquo;ve been consistently surprised by the quality of its suggestions, especially for JavaScript and Python development .\nThe tool supports over 70 programming languages and integrates with all major IDEs. Its chat feature helps explain code, suggest improvements, and even refactor existing functions. For students or developers just starting with AI assistance, this is an excellent entry point.\nThe company\u0026rsquo;s business model focuses on enterprise features rather than limiting individual use, which means the free tier remains genuinely useful rather than a limited trial.\nPros: Completely free for individuals, wide language support, no usage limits\nCons: Enterprise features limited, newer company\n5. Cursor - The AI-First IDE Website: cursor.sh Price: $20/month for Pro features\nBest For: Developers wanting an AI-native editing experience\nCursor takes a revolutionary approach by building AI assistance directly into the editor rather than as a plugin. This isn\u0026rsquo;t just another code completion tool – it\u0026rsquo;s an entire IDE designed around AI collaboration.\nThe standout feature is its ability to understand and modify entire codebases. You can ask it to implement features across multiple files, refactor large sections of code, or explain complex system interactions. It\u0026rsquo;s particularly powerful for understanding unfamiliar codebases quickly.\nThe AI can also generate commit messages, write tests, and even help with code reviews. It feels like the future of software development, where AI is a true collaborative partner rather than just a smart autocomplete.\nPros: Revolutionary AI integration, codebase understanding, innovative features\nCons: Requires learning new IDE, higher price point\n6. Claude Dev - The Reasoning Expert Website: claude.ai Price: Various pricing tiers based on usage\nBest For: Complex problem solving, architectural decisions\nClaude Dev, built on Anthropic\u0026rsquo;s Claude AI, excels at understanding context and providing thoughtful, well-reasoned coding suggestions. It\u0026rsquo;s particularly strong at explaining complex concepts and helping with architectural decisions.\nWhat I appreciate most about Claude Dev is its ability to engage in detailed technical discussions. You can ask about design patterns, performance implications, or security considerations, and get responses that feel like consulting with a senior architect.\nThe tool is excellent for code reviews, suggesting not just syntax improvements but also discussing the broader implications of different implementation approaches.\nPros: Excellent reasoning abilities, architectural insights, detailed explanations\nCons: Higher cost for heavy usage, focus on consultation over completion\n7. Replit Ghostwriter - The Collaborative Coder Website: replit.com/site/ghostwriter Price: $10/month as part of Replit Core\nBest For: Learning, prototyping, collaborative development\nReplit\u0026rsquo;s Ghostwriter shines in collaborative and educational environments. Integrated into the Replit platform, it provides contextual suggestions while you code in the browser, making it perfect for rapid prototyping and learning.\nThe tool excels at explaining code concepts and helping beginners understand programming patterns. It can generate complete applications from descriptions, making it excellent for proof-of-concepts and learning new technologies .\nThe collaborative features allow teams to work together with AI assistance, making it valuable for code reviews and pair programming sessions.\nPros: Excellent for learning, collaborative features, browser-based\nCons: Limited to Replit platform, less suitable for complex projects\n8. Sourcegraph Cody - The Enterprise Solution Website: sourcegraph.com/cody Price: Free tier, $9/month for Pro, Enterprise pricing\nBest For: Large codebases, enterprise development\nCody by Sourcegraph is designed for enterprise environments with massive codebases. It understands code across entire repositories and can help navigate complex system architectures.\nThe tool\u0026rsquo;s strength lies in its ability to understand relationships between different parts of large applications. It can suggest changes that maintain consistency across your entire codebase and help identify potential breaking changes.\nFor teams working on microservices or large monolithic applications, Cody\u0026rsquo;s repository-wide understanding is invaluable for maintaining code quality and consistency.\nPros: Enterprise-focused, large codebase understanding, team features\nCons: Overkill for small projects, enterprise-focused pricing\n9. CodeT5 - The Open Source Alternative Website: github.com/salesforce/CodeT5 Price: Free (open source)\nBest For: Researchers, privacy advocates, custom implementations\nCodeT5 represents the open-source approach to AI coding assistance. Based on the T5 transformer architecture, it provides transparency and customizability that proprietary solutions can\u0026rsquo;t match.\nWhile it requires more technical setup than commercial alternatives, CodeT5 offers complete control over your AI coding assistant. You can train it on your specific domain, modify its behavior, and integrate it into custom workflows.\nThis tool is perfect for organizations with strict privacy requirements or researchers wanting to experiment with AI-assisted development.\nPros: Open source, customizable, transparent\nCons: Requires technical setup, less polish than commercial tools\n10. JetBrains AI Assistant - The IDE Native Website: jetbrains.com/ai Price: Included with JetBrains IDEs subscription\nBest For: JetBrains IDE users, integrated development workflows\nJetBrains\u0026rsquo; AI Assistant integrates seamlessly with their popular IDE suite, providing context-aware suggestions that understand your project structure and dependencies.\nThe tool excels at code generation within the JetBrains ecosystem, understanding project templates, frameworks, and coding standards. It\u0026rsquo;s particularly strong for Java and Kotlin development , leveraging JetBrains\u0026rsquo; deep understanding of these languages.\nFor developers already using IntelliJ IDEA, PyCharm, or other JetBrains IDEs, this provides seamless AI assistance without changing your workflow.\nPros: Deep IDE integration, framework awareness, familiar interface\nCons: Limited to JetBrains IDEs, part of larger subscription\nChoosing the Right AI Coding Assistant The best AI coding assistant depends on your specific needs:\nFor Beginners: Start with Codeium (free) or GitHub Copilot (industry standard)\nFor AWS Development: Amazon CodeWhisperer is unmatched\nFor Privacy: Tabnine offers local processing and ethical training\nFor Innovation: Cursor provides a glimpse into the future of AI-assisted development\nFor Enterprise: Sourcegraph Cody handles large codebases effectively\nThe Future of AI-Assisted Development These tools are just the beginning. As AI models become more sophisticated and context-aware, we\u0026rsquo;ll see even more intelligent assistance. The key is starting now, learning how to effectively collaborate with AI, and staying updated with new developments.\nAI coding assistants aren\u0026rsquo;t about replacing developers – they\u0026rsquo;re about amplifying human creativity and problem-solving abilities. By handling routine tasks, they free us to focus on architecture, user experience, and innovative solutions.\nGetting Started My recommendation? Try the free tiers of 2-3 different tools to see which fits your workflow best. Most developers end up using different AI assistants for different tasks – GitHub Copilot for general development, CodeWhisperer for AWS projects, and Cursor for complex refactoring.\nRemember, these tools are most effective when you understand what you\u0026rsquo;re trying to build. They accelerate development but don\u0026rsquo;t replace the need to understand programming fundamentals and system design principles.\nThe AI coding revolution is here, and these 10 assistants represent the best tools available today. Choose the ones that fit your workflow, budget, and privacy requirements, then start building the future of software development.\nWhat\u0026rsquo;s your experience with AI coding assistants? Have you tried any of these tools? Let me know in the comments which ones have been most helpful in your development workflow!\n","href":"/2025/08/10-best-ai-coding-assistants-every-developer-should-try-2025.html","title":"10 Best AI Coding Assistants Every Developer Should Try in 2025"},{"content":"Whether you\u0026rsquo;re building web applications, managing servers, or working in DevOps, mastering Linux commands is absolutely essential for any developer in 2025. I\u0026rsquo;ve been working with Linux systems for years, and I can tell you that knowing the right commands at the right time can save you hours of work and make you incredibly productive.\nLinux dominates the server world, powers most cloud infrastructure, and is the backbone of modern development environments. From managing Docker containers to setting up secure web servers with HTTPS , these commands will be your daily companions.\nIn this comprehensive guide, I\u0026rsquo;ll walk you through the most essential Linux commands that every developer should master in 2025. These aren\u0026rsquo;t just theoretical examples - every command here has been tested and works in real-world scenarios.\nNavigation and File System Commands 1. pwd - Print Working Directory Before you can navigate anywhere, you need to know where you are. The pwd command shows your current directory location.\npwd Example output:\n/home/username/projects/myapp This is incredibly useful when you\u0026rsquo;re deep in a project structure and need to orient yourself quickly.\n2. ls - List Directory Contents The ls command is probably the most used command in Linux. It shows you what\u0026rsquo;s in your current directory.\n# Basic listing ls # Detailed listing with permissions and timestamps ls -la # List only directories ls -d */ # Sort by modification time (newest first) ls -lt # Show file sizes in human readable format ls -lh The -la flag is particularly useful as it shows hidden files (those starting with a dot), file permissions, ownership, and timestamps. Perfect for debugging permission issues or finding configuration files.\n3. cd - Change Directory Moving around the file system is fundamental. The cd command lets you navigate to different directories.\n# Go to a specific directory cd /var/log # Go to home directory cd ~ cd # Go back to previous directory cd - # Go up one directory level cd .. # Go up two directory levels cd ../.. Pro tip: cd - is a lifesaver when you\u0026rsquo;re switching between two directories frequently during development.\n4. find - Search for Files and Directories The find command is incredibly powerful for locating files based on various criteria.\n# Find files by name find . -name \u0026#34;*.js\u0026#34; # Find files modified in the last 7 days find . -mtime -7 # Find files larger than 100MB find . -size +100M # Find and execute command on results find . -name \u0026#34;*.log\u0026#34; -exec rm {} \\; # Find directories only find . -type d -name \u0026#34;node_modules\u0026#34; # Find files with specific permissions find . -perm 755 This is essential when working with large codebases or trying to clean up old files and dependencies.\nFile Operations and Management 5. cp - Copy Files and Directories Copying files and directories is a daily task for developers, whether backing up configurations or duplicating project structures.\n# Copy a file cp source.txt destination.txt # Copy with preserving timestamps and permissions cp -p config.json config.backup.json # Copy directory recursively cp -r project/ project-backup/ # Copy multiple files to directory cp *.txt backup/ # Interactive copy (asks before overwriting) cp -i important.conf important.conf.new 6. mv - Move/Rename Files The mv command both moves and renames files - it\u0026rsquo;s the same operation in Linux.\n# Rename a file mv old_name.txt new_name.txt # Move file to different directory mv myfile.txt /home/username/documents/ # Move and rename simultaneously mv temp.log /var/log/application.log # Move multiple files mv *.txt documents/ 7. rm - Remove Files and Directories Use with caution! The rm command permanently deletes files.\n# Remove a file rm unwanted.txt # Remove multiple files rm file1.txt file2.txt # Remove directory and all contents rm -rf old_project/ # Interactive removal (asks for confirmation) rm -i suspicious_file.txt # Remove all .log files in current directory rm *.log Warning: rm -rf is powerful but dangerous. Double-check your path before running it, especially with sudo privileges.\n8. mkdir - Create Directories Creating directories is straightforward with mkdir.\n# Create a single directory mkdir new_project # Create nested directories mkdir -p project/src/components # Create multiple directories at once mkdir backend frontend database # Create directory with specific permissions mkdir -m 755 public_folder The -p flag is particularly useful in development when you need to create entire directory structures in one command.\nFile Content Operations 9. cat - Display File Contents The cat command displays the entire content of a file.\n# Display file content cat package.json # Display multiple files cat file1.txt file2.txt # Display with line numbers cat -n app.js # Display non-printing characters cat -A config.txt 10. less and more - Page Through Files For large files, less and more allow you to scroll through content page by page.\n# View large log files less /var/log/syslog # Search within less (press / then type search term) less application.log # View file with more (simpler than less) more large_file.txt In less, use:\nSpace bar to go forward one page b to go back one page q to quit /search_term to search n to find next occurrence 11. head and tail - Show Beginning or End of Files Perfect for checking log files or large datasets.\n# Show first 10 lines (default) head error.log # Show first 20 lines head -n 20 access.log # Show last 10 lines tail error.log # Show last 20 lines and follow new additions (great for logs) tail -f -n 20 /var/log/nginx/access.log # Show last 50 lines tail -n 50 application.log The tail -f command is invaluable for monitoring log files in real-time during development and debugging.\n12. grep - Search Text Patterns grep is one of the most powerful tools for searching text patterns in files.\n# Search for text in a file grep \u0026#34;error\u0026#34; application.log # Case insensitive search grep -i \u0026#34;warning\u0026#34; system.log # Search recursively in all files grep -r \u0026#34;TODO\u0026#34; src/ # Show line numbers with matches grep -n \u0026#34;function\u0026#34; app.js # Search for exact word grep -w \u0026#34;user\u0026#34; database.log # Invert match (show lines that don\u0026#39;t contain pattern) grep -v \u0026#34;debug\u0026#34; error.log # Count matching lines grep -c \u0026#34;success\u0026#34; access.log # Show context (2 lines before and after match) grep -C 2 \u0026#34;exception\u0026#34; error.log 13. sed - Stream Editor sed is perfect for quick text replacements and file modifications.\n# Replace first occurrence in each line sed \u0026#39;s/old/new/\u0026#39; file.txt # Replace all occurrences sed \u0026#39;s/old/new/g\u0026#39; file.txt # Edit file in place sed -i \u0026#39;s/localhost/production.server.com/g\u0026#39; config.txt # Delete lines containing pattern sed \u0026#39;/debug/d\u0026#39; log.txt # Print specific line numbers sed -n \u0026#39;10,20p\u0026#39; large_file.txt Process and System Management 14. ps - Display Running Processes Understanding what\u0026rsquo;s running on your system is crucial for debugging and performance monitoring.\n# Show processes for current user ps # Show all processes with detailed info ps aux # Show processes in tree format ps auxf # Show processes for specific user ps -u username # Find specific process ps aux | grep nginx 15. top and htop - Real-time Process Monitoring Monitor system resources and running processes in real-time.\n# Basic system monitor top # Better alternative (if installed) htop # Show processes by CPU usage top -o %CPU # Show processes by memory usage top -o %MEM In top:\nPress q to quit Press k to kill a process Press M to sort by memory Press P to sort by CPU 16. kill and killall - Terminate Processes Stop problematic or unnecessary processes.\n# Kill process by PID kill 1234 # Force kill process kill -9 1234 # Kill process by name killall node # Kill all processes matching pattern pkill -f \u0026#34;python.*myapp\u0026#34; # List signals available kill -l 17. jobs, bg, and fg - Job Control Manage background and foreground processes.\n# List active jobs jobs # Put current process in background # (Press Ctrl+Z to suspend, then:) bg # Bring job to foreground fg # Run command in background from start nohup python long_running_script.py \u0026amp; # Bring specific job to foreground fg %1 Network and System Information 18. ping - Test Network Connectivity Test if you can reach other systems or websites.\n# Basic ping ping google.com # Ping with limited count ping -c 4 8.8.8.8 # Ping with specific interval ping -i 2 localhost # Ping with larger packet size ping -s 1000 server.com 19. wget and curl - Download Files and Test APIs Essential for downloading files and testing web services.\n# Download file with wget wget https://example.com/file.zip # Download and save with different name wget -O myfile.zip https://example.com/file.zip # Download recursively (be careful!) wget -r -np https://example.com/directory/ # Basic curl request curl https://api.example.com/users # POST request with data curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;}\u0026#39; https://api.example.com/users # Save response to file curl -o response.json https://api.example.com/data # Follow redirects curl -L https://bit.ly/shortened-url # Include headers in output curl -i https://api.example.com/status 20. netstat and ss - Network Statistics Monitor network connections and ports.\n# Show all connections netstat -a # Show listening ports netstat -l # Show TCP connections netstat -t # Show which process is using which port netstat -tulpn # Modern alternative to netstat ss -tulpn # Check specific port ss -tulpn | grep :80 File Permissions and Ownership 21. chmod - Change File Permissions Managing file permissions is critical for security and functionality.\n# Make file executable chmod +x script.sh # Set specific permissions (rwxr-xr-x) chmod 755 myfile.txt # Make file readable/writable for owner only chmod 600 private.key # Remove execute permission for group and others chmod go-x sensitive_script.sh # Recursively change permissions chmod -R 644 web_content/ Permission numbers:\n7 = rwx (read, write, execute) 6 = rw- (read, write) 5 = r-x (read, execute) 4 = r\u0026ndash; (read only) 22. chown - Change File Ownership Change who owns files and directories.\n# Change owner chown username file.txt # Change owner and group chown username:groupname file.txt # Recursively change ownership chown -R www-data:www-data /var/www/html/ # Change only group chown :developers project/ Archive and Compression 23. tar - Archive Files tar is essential for creating backups and distributing code.\n# Create archive tar -czf backup.tar.gz project/ # Extract archive tar -xzf backup.tar.gz # List archive contents tar -tzf backup.tar.gz # Extract to specific directory tar -xzf backup.tar.gz -C /tmp/ # Create archive excluding certain files tar --exclude=\u0026#39;*.log\u0026#39; -czf clean_backup.tar.gz project/ 24. zip and unzip - Create and Extract ZIP Files Sometimes ZIP format is more convenient, especially for sharing with non-Linux users.\n# Create zip archive zip -r project.zip project/ # Extract zip file unzip project.zip # List zip contents unzip -l project.zip # Extract to specific directory unzip project.zip -d /tmp/extracted/ # Create zip excluding certain files zip -r project.zip project/ -x \u0026#34;*.log\u0026#34; \u0026#34;*/node_modules/*\u0026#34; Text Processing and Data Manipulation 25. sort - Sort Lines of Text Sorting data is frequently needed in development and analysis.\n# Sort file contents sort names.txt # Sort numerically sort -n numbers.txt # Reverse sort sort -r file.txt # Sort by specific column (space-separated) sort -k2 data.txt # Remove duplicates while sorting sort -u duplicated.txt # Sort by file size ls -l | sort -k5 -n 26. uniq - Report or Filter Unique Lines Work with unique lines in files (usually used after sort).\n# Show unique lines only sort file.txt | uniq # Count occurrences of each line sort file.txt | uniq -c # Show only duplicated lines sort file.txt | uniq -d # Show only unique lines (no duplicates) sort file.txt | uniq -u 27. wc - Word, Line, Character, and Byte Count Count various aspects of file contents.\n# Count lines, words, and characters wc file.txt # Count only lines wc -l file.txt # Count only words wc -w file.txt # Count only characters wc -c file.txt # Count files in directory ls | wc -l System Monitoring and Disk Usage 28. df - Display Filesystem Disk Usage Monitor disk space usage across mounted filesystems.\n# Show disk usage for all filesystems df # Show in human readable format df -h # Show specific filesystem df -h /var # Show inode usage df -i 29. du - Display Directory Space Usage Check how much space directories and files are using.\n# Show directory sizes du -h # Show only directory totals du -sh */ # Show largest directories first du -h | sort -rh # Show size of specific directory du -sh project/ # Exclude certain file types du -h --exclude=\u0026#34;*.log\u0026#34; project/ 30. free - Display Memory Usage Monitor system memory usage.\n# Show memory usage free # Show in human readable format free -h # Update every 2 seconds free -h -s 2 # Show memory usage in MB free -m Environment and Variables 31. env and export - Environment Variables Manage environment variables for applications and scripts.\n# Show all environment variables env # Set environment variable for current session export DATABASE_URL=\u0026#34;postgresql://localhost:5432/mydb\u0026#34; # Show specific variable echo $PATH # Set variable for single command DATABASE_URL=\u0026#34;test://localhost\u0026#34; node app.js # Make variable available to child processes export NODE_ENV=production 32. which and whereis - Locate Commands Find where commands and programs are located.\n# Find command location which python # Find multiple locations and info whereis python # Check if command exists which docker || echo \u0026#34;Docker not installed\u0026#34; # Show all locations in PATH which -a python Advanced Tips and Combinations Command Chaining and Pipes Linux\u0026rsquo;s real power comes from combining commands:\n# Chain commands with pipes ps aux | grep node | awk \u0026#39;{print $2}\u0026#39; | xargs kill # Find large files and show details find . -size +100M | xargs ls -lh # Count unique IP addresses in log grep \u0026#34;GET\u0026#34; access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr # Monitor log file for errors tail -f error.log | grep -i \u0026#34;critical\u0026#34; Using History and Shortcuts Make your terminal work more efficiently:\n# Show command history history # Re-run last command !! # Re-run command from history by number !123 # Search history interactively # Press Ctrl+R and type search term # Clear history history -c Useful Keyboard Shortcuts Ctrl+C: Kill current process Ctrl+Z: Suspend current process Ctrl+D: Exit current shell Ctrl+L: Clear screen Ctrl+A: Go to beginning of line Ctrl+E: Go to end of line Ctrl+U: Clear line before cursor Ctrl+K: Clear line after cursor Real-World Development Scenarios Debugging Web Applications When your web application isn\u0026rsquo;t working properly:\n# Check if service is running ps aux | grep nginx # Check what\u0026#39;s listening on port 80 ss -tulpn | grep :80 # Monitor error logs tail -f /var/log/nginx/error.log # Check disk space (common cause of issues) df -h # Find large log files eating disk space find /var/log -name \u0026#34;*.log\u0026#34; -size +100M Project Cleanup and Management Keeping your development environment clean:\n# Find and remove node_modules directories find . -name \u0026#34;node_modules\u0026#34; -type d -exec rm -rf {} + # Clean up old log files find . -name \u0026#34;*.log\u0026#34; -mtime +30 -delete # Find duplicate files by name find . -name \u0026#34;*.js\u0026#34; | sort | uniq -d # Check project size du -sh . \u0026amp;\u0026amp; du -sh */ | sort -rh Server Maintenance When managing Docker containers or web servers:\n# Monitor system resources top -u www-data # Check network connectivity ping -c 3 database.server.com # Verify SSL certificates (if using HTTPS setup) openssl s_client -connect domain.com:443 \u0026lt; /dev/null # Check service status systemctl status nginx # View recent system messages journalctl -n 50 Performance and Security Considerations Security Best Practices When working with these commands, especially on production servers:\nUse sudo carefully: Only when necessary, and always double-check commands Verify paths: Especially with rm -rf commands Check permissions: Before modifying files, understand their current permissions Monitor logs: Regularly check system and application logs for anomalies Backup before changes: Always backup important files before modifications Performance Tips Use specific paths: Instead of searching entire filesystem, limit searches to relevant directories Combine commands efficiently: Use pipes to avoid creating temporary files Monitor resource usage: Keep an eye on CPU and memory usage with top or htop Clean up regularly: Remove old logs, temporary files, and unused packages Conclusion Mastering these essential Linux commands will significantly boost your productivity as a developer in 2025. Whether you\u0026rsquo;re setting up secure HTTPS servers , managing containerized applications, or debugging complex systems, these commands form the foundation of effective Linux administration.\nThe key to becoming proficient is practice. Start incorporating these commands into your daily workflow, and soon they\u0026rsquo;ll become second nature. Remember, Linux command mastery isn\u0026rsquo;t about memorizing every flag and option - it\u0026rsquo;s about understanding the core functionality and knowing how to combine commands to solve real problems efficiently.\nAs development environments become increasingly complex with microservices, containers, and cloud infrastructure, these fundamental Linux skills become even more valuable. They\u0026rsquo;re the building blocks that will help you troubleshoot issues, automate tasks, and manage systems effectively throughout your development career.\nKeep this guide handy, practice regularly, and don\u0026rsquo;t hesitate to use the man command (e.g., man grep) to explore additional options and flags for each command. The Linux terminal is incredibly powerful, and these commands are your gateway to unlocking that power.\n","href":"/2025/08/essential-linux-commands-every-developer-must-know-2025.html","title":"Essential Linux Commands Every Developer Must Know in 2025"},{"content":"Building modern distributed systems is tricky business - you need services that can talk to each other quickly and reliably. That\u0026rsquo;s where gRPC comes in and absolutely crushes it. I\u0026rsquo;ve been building REST APIs for years, but when I first tried gRPC, it was like switching from a bicycle to a sports car. The speed difference is insane, plus you get type safety and can use it with practically any programming language.\nIf you\u0026rsquo;ve been building REST APIs in Go and wondering whether there\u0026rsquo;s a better approach for service-to-service communication, you\u0026rsquo;re in the right place. Today, we\u0026rsquo;ll explore gRPC from the ground up, building a complete user management service that you can actually use in production.\nWhat Makes gRPC Special? Before we dive into the code, let me tell you why I made the switch from REST to gRPC for service-to-service communication. Don\u0026rsquo;t get me wrong, REST APIs are fantastic for public APIs, but when you have a bunch of microservices that need to chat with each other all day long, REST starts showing its limitations.\ngRPC (Google Remote Procedure Call) runs on HTTP/2, so you automatically get all the cool stuff like multiplexing, server push, and binary serialization without any extra work. Instead of parsing JSON all the time (which gets expensive), gRPC uses Protocol Buffers for serialization. It\u0026rsquo;s way faster and takes up less space.\nBut here\u0026rsquo;s the real kicker - type safety. When you define your service contract with protobuf, it literally generates all your client and server code for you. Pretty sweet, right? No more of those annoying bugs where you spend 2 hours debugging only to find out someone changed a field name or used a string instead of an integer.\nSetting Up Your Go gRPC Environment First things first - let\u0026rsquo;s get everything set up. First, make sure you have Go installed (if not, check out our guide on installing Go on Linux ).\nYou\u0026rsquo;ll need to install the Protocol Buffer compiler and the Go plugins:\n# Install protoc compiler # On macOS brew install protobuf # On Ubuntu/Debian sudo apt update \u0026amp;\u0026amp; sudo apt install -y protobuf-compiler # Install Go plugins go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest Create a new Go module for our project:\nmkdir grpc-user-service cd grpc-user-service go mod init grpc-user-service Install the required Go dependencies:\ngo get google.golang.org/grpc go get google.golang.org/protobuf/reflect/protoreflect go get google.golang.org/protobuf/runtime/protoimpl Defining Your Service Contract with Protocol Buffers Here\u0026rsquo;s where gRPC gets really cool - everything starts with defining your service contract. Create a proto directory and add our user service definition:\nmkdir proto Create proto/user.proto:\nsyntax = \u0026#34;proto3\u0026#34;; package user; option go_package = \u0026#34;./proto\u0026#34;; // User message definition message User { int32 id = 1; string name = 2; string email = 3; int32 age = 4; bool active = 5; } // Request messages message CreateUserRequest { string name = 1; string email = 2; int32 age = 3; } message GetUserRequest { int32 id = 1; } message UpdateUserRequest { int32 id = 1; string name = 2; string email = 3; int32 age = 4; bool active = 5; } message DeleteUserRequest { int32 id = 1; } message ListUsersRequest { int32 page = 1; int32 page_size = 2; } // Response messages message CreateUserResponse { User user = 1; string message = 2; } message GetUserResponse { User user = 1; } message UpdateUserResponse { User user = 1; string message = 2; } message DeleteUserResponse { string message = 1; } message ListUsersResponse { repeated User users = 1; int32 total = 2; } // UserService definition service UserService { rpc CreateUser(CreateUserRequest) returns (CreateUserResponse); rpc GetUser(GetUserRequest) returns (GetUserResponse); rpc UpdateUser(UpdateUserRequest) returns (UpdateUserResponse); rpc DeleteUser(DeleteUserRequest) returns (DeleteUserResponse); rpc ListUsers(ListUsersRequest) returns (ListUsersResponse); } Now generate the Go code from our protobuf definition:\nprotoc --go_out=. --go-grpc_out=. proto/user.proto This creates proto/user.pb.go and proto/user_grpc.pb.go with all the generated code we need.\nImportant: Make sure to add the generated files to your project structure. Your directory should look like this:\ngrpc-user-service/ ├── go.mod ├── go.sum ├── main.go ├── proto/ │ ├── user.proto │ ├── user.pb.go # Generated │ └── user_grpc.pb.go # Generated ├── server/ │ └── user_server.go └── client/ └── main.go Implementing the gRPC Server Now we\u0026rsquo;re getting to the fun part. Unlike handling HTTP requests manually , gRPC generates most of the boilerplate for us. We just need to implement the business logic.\nCreate server/user_server.go:\npackage server import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; ) type UserServer struct { pb.UnimplementedUserServiceServer users map[int32]*pb.User nextID int32 mu sync.RWMutex } func NewUserServer() *UserServer { return \u0026amp;UserServer{ users: make(map[int32]*pb.User), nextID: 1, } } func (s *UserServer) CreateUser(ctx context.Context, req *pb.CreateUserRequest) (*pb.CreateUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() // Basic validation if req.Name == \u0026#34;\u0026#34; { return nil, status.Error(codes.InvalidArgument, \u0026#34;name cannot be empty\u0026#34;) } if req.Email == \u0026#34;\u0026#34; { return nil, status.Error(codes.InvalidArgument, \u0026#34;email cannot be empty\u0026#34;) } if req.Age \u0026lt; 0 { return nil, status.Error(codes.InvalidArgument, \u0026#34;age must be positive\u0026#34;) } // Create new user user := \u0026amp;pb.User{ Id: s.nextID, Name: req.Name, Email: req.Email, Age: req.Age, Active: true, } s.users[s.nextID] = user s.nextID++ return \u0026amp;pb.CreateUserResponse{ User: user, Message: \u0026#34;User created successfully\u0026#34;, }, nil } func (s *UserServer) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.GetUserResponse, error) { s.mu.RLock() defer s.mu.RUnlock() user, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } return \u0026amp;pb.GetUserResponse{User: user}, nil } func (s *UserServer) UpdateUser(ctx context.Context, req *pb.UpdateUserRequest) (*pb.UpdateUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() user, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } // Update fields if provided if req.Name != \u0026#34;\u0026#34; { user.Name = req.Name } if req.Email != \u0026#34;\u0026#34; { user.Email = req.Email } if req.Age \u0026gt; 0 { user.Age = req.Age } user.Active = req.Active s.users[req.Id] = user return \u0026amp;pb.UpdateUserResponse{ User: user, Message: \u0026#34;User updated successfully\u0026#34;, }, nil } func (s *UserServer) DeleteUser(ctx context.Context, req *pb.DeleteUserRequest) (*pb.DeleteUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() _, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } delete(s.users, req.Id) return \u0026amp;pb.DeleteUserResponse{ Message: \u0026#34;User deleted successfully\u0026#34;, }, nil } func (s *UserServer) ListUsers(ctx context.Context, req *pb.ListUsersRequest) (*pb.ListUsersResponse, error) { s.mu.RLock() defer s.mu.RUnlock() var users []*pb.User for _, user := range s.users { users = append(users, user) } // Simple pagination pageSize := req.PageSize if pageSize \u0026lt;= 0 { pageSize = 10 } page := req.Page if page \u0026lt;= 0 { page = 1 } start := (page - 1) * pageSize end := start + pageSize if start \u0026gt;= int32(len(users)) { users = []*pb.User{} } else if end \u0026gt; int32(len(users)) { users = users[start:] } else { users = users[start:end] } return \u0026amp;pb.ListUsersResponse{ Users: users, Total: int32(len(s.users)), }, nil } See that mutex stuff? That\u0026rsquo;s to keep things thread-safe when multiple requests come in at once. Obviously in real production apps, you\u0026rsquo;d swap out this in-memory storage for a proper database - but this keeps things simple for learning.\nRunning the gRPC Server Create main.go to start our server:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;grpc-user-service/server\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/reflection\u0026#34; ) func main() { // Listen on port 50051 lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50051\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to listen: %v\u0026#34;, err) } // Create gRPC server s := grpc.NewServer() // Register our service userServer := server.NewUserServer() pb.RegisterUserServiceServer(s, userServer) // Enable reflection for debugging with tools like grpcurl reflection.Register(s) log.Println(\u0026#34;gRPC server starting on :50051\u0026#34;) if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } Run the server:\ngo run main.go Building a gRPC Client Now let\u0026rsquo;s create a client to interact with our service. Create client/main.go:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/credentials/insecure\u0026#34; ) func main() { // Connect to the gRPC server conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() client := pb.NewUserServiceClient(conn) // Create a user ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() createResp, err := client.CreateUser(ctx, \u0026amp;pb.CreateUserRequest{ Name: \u0026#34;John Doe\u0026#34;, Email: \u0026#34;john@example.com\u0026#34;, Age: 30, }) if err != nil { log.Fatalf(\u0026#34;Could not create user: %v\u0026#34;, err) } log.Printf(\u0026#34;Created user: %v\u0026#34;, createResp.User) // Get the user getResp, err := client.GetUser(ctx, \u0026amp;pb.GetUserRequest{ Id: createResp.User.Id, }) if err != nil { log.Fatalf(\u0026#34;Could not get user: %v\u0026#34;, err) } log.Printf(\u0026#34;Retrieved user: %v\u0026#34;, getResp.User) // Update the user updateResp, err := client.UpdateUser(ctx, \u0026amp;pb.UpdateUserRequest{ Id: createResp.User.Id, Name: \u0026#34;John Smith\u0026#34;, Email: \u0026#34;johnsmith@example.com\u0026#34;, Age: 31, Active: true, }) if err != nil { log.Fatalf(\u0026#34;Could not update user: %v\u0026#34;, err) } log.Printf(\u0026#34;Updated user: %v\u0026#34;, updateResp.User) // List users listResp, err := client.ListUsers(ctx, \u0026amp;pb.ListUsersRequest{ Page: 1, PageSize: 10, }) if err != nil { log.Fatalf(\u0026#34;Could not list users: %v\u0026#34;, err) } log.Printf(\u0026#34;Total users: %d\u0026#34;, listResp.Total) for _, user := range listResp.Users { log.Printf(\u0026#34;User: %v\u0026#34;, user) } // Delete the user deleteResp, err := client.DeleteUser(ctx, \u0026amp;pb.DeleteUserRequest{ Id: createResp.User.Id, }) if err != nil { log.Fatalf(\u0026#34;Could not delete user: %v\u0026#34;, err) } log.Printf(\u0026#34;Delete response: %s\u0026#34;, deleteResp.Message) } Test the client in a new terminal:\ngo run client/main.go Production Considerations Alright, so when you want to actually deploy this thing to production, there\u0026rsquo;s some stuff you need to think about. Unlike deploying a simple REST API , gRPC services need a bit more thought around load balancing and TLS setup.\nFirst off, make sure you\u0026rsquo;ve got solid error handling throughout your service. gRPC gives you a bunch of useful status codes so your clients know exactly what went wrong.\nFor auth, you\u0026rsquo;ll probably want JWT token validation or mutual TLS. Interceptors are your friend here - you can use them to handle auth, logging, and metrics for all your RPC methods in one place.\nObviously, you\u0026rsquo;ll need to hook up a real database for production. Swap out that in-memory storage for a real database connection. Check out our PostgreSQL guide if you\u0026rsquo;re going the SQL route, or look into NoSQL depending on what you\u0026rsquo;re building.\nPerformance Benefits and Testing What really blew my mind about gRPC was just how much faster it is compared to REST APIs. Protocol Buffers\u0026rsquo; binary serialization absolutely destroys JSON in terms of speed, and HTTP/2 lets you handle tons of requests over one connection without breaking a sweat.\nTesting gRPC services is actually pretty straightforward - you can write unit tests with mock clients and servers. All that generated code makes testing way easier than dealing with REST endpoints.\nWrapping Up gRPC is honestly a game changer for building fast, reliable distributed systems in Go. Sure, there\u0026rsquo;s a bit of a learning curve if you\u0026rsquo;re coming from REST, but trust me - once you see the performance gains and never have to deal with JSON parsing bugs again, you\u0026rsquo;ll wonder why you waited so long.\nWhat we built today is just basic CRUD stuff, but you can go crazy with streaming, fancy auth, and integrate it with your existing Go project setup .\nNext time you\u0026rsquo;re working on microservices, seriously give gRPC a shot. I guarantee you\u0026rsquo;ll be kicking yourself for not trying it sooner.\nGot questions about getting gRPC working in your Go projects? Hit me up in the comments - I\u0026rsquo;m always down to chat about different approaches and the weird edge cases you run into in production.\n","href":"/2025/08/grpc-in-go-complete-guide-basics-production.html","title":"gRPC in Go: Complete Guide from Basics to Production Ready Services"},{"content":" Let\u0026rsquo;s be honest – vanilla Visual Studio Code is good, but it\u0026rsquo;s not amazing. What makes this popular code editor truly shine are the extensions that turn it into a powerhouse IDE. After years of coding and trying countless extensions, I\u0026rsquo;ve narrowed down the absolute essentials that every developer should have installed in 2025.\nWhether you\u0026rsquo;re a seasoned developer or just starting your coding journey, these extensions will save you hours of work, catch bugs before they happen, and make your coding experience so much smoother. Let\u0026rsquo;s dive into the tools that have become indispensable in modern development.\n1. GitHub Copilot – Your AI Coding Companion If you\u0026rsquo;re not using an AI coding assistant in 2025, you\u0026rsquo;re missing out on a massive productivity boost. GitHub Copilot has evolved into something truly remarkable – it\u0026rsquo;s like having a senior developer sitting next to you, offering suggestions and writing boilerplate code.\nWhat makes it special:\nGenerates entire functions from comments Suggests code completions in real-time Understands context from your existing codebase Supports almost every programming language I\u0026rsquo;ve found Copilot particularly useful when working with APIs or writing repetitive code patterns. Instead of googling \u0026ldquo;how to make HTTP request in Python\u0026rdquo; for the hundredth time, Copilot just knows what you want to do.\nPro tip: Don\u0026rsquo;t just accept every suggestion blindly. Copilot is smart, but it\u0026rsquo;s not perfect. Always review the generated code and understand what it does.\nInstall: GitHub Copilot 2. Prettier – Code Formatting Made Effortless Arguing about code formatting is so 2015. Prettier solves this problem once and for all by automatically formatting your code according to consistent rules. No more debates about tabs vs spaces or where to put your brackets.\nWhy it\u0026rsquo;s essential:\nFormats code on save automatically Maintains consistent style across your team Supports JavaScript, TypeScript, CSS, HTML, JSON, and more Reduces cognitive load during code reviews The beauty of Prettier is that you set it up once and forget about it. Your code always looks clean and professional, which makes it easier to read and maintain. When you\u0026rsquo;re building REST APIs or working on complex projects, consistent formatting becomes even more crucial.\nInstall: Prettier - Code formatter 3. ESLint – Your JavaScript Guardian Angel ESLint is like having a vigilant code reviewer who never gets tired of pointing out potential issues. It catches common mistakes, enforces coding standards, and helps you write better JavaScript and TypeScript.\nKey benefits:\nCatches syntax errors before runtime Enforces coding best practices Customizable rules for your project needs Integrates perfectly with Prettier I can\u0026rsquo;t count how many times ESLint has saved me from shipping buggy code. It\u0026rsquo;s particularly helpful when working with frameworks like React or Vue, where certain patterns can lead to performance issues or bugs.\nInstall: ESLint 4. Live Server – Instant Development Server Testing your web applications locally used to require setting up complex development servers. Live Server changes that by providing a one-click solution to run your HTML, CSS, and JavaScript projects with hot reload.\nWhat it offers:\nInstant local development server Auto-reload when files change Works with any static website Perfect for front-end development This extension is a lifesaver when you\u0026rsquo;re working on static sites or testing your front-end code. It\u0026rsquo;s especially useful if you\u0026rsquo;re following tutorials or building portfolio projects.\nInstall: Live Server 5. GitLens – Git Supercharged Git is powerful, but it\u0026rsquo;s not always the most user-friendly. GitLens transforms your IDE\u0026rsquo;s Git integration into something intuitive and informative. You can see who changed what, when, and why – all without leaving your editor.\nStandout features:\nBlame annotations show code authorship Rich commit information and history File and line history visualization Seamless GitHub/GitLab integration GitLens is particularly valuable when working on team projects or maintaining legacy code. Understanding the history and context of code changes becomes effortless.\nInstall: GitLens — Git supercharged 6. Bracket Pair Colorizer – Navigate Complex Code When you\u0026rsquo;re dealing with deeply nested code structures, matching brackets can become a nightmare. Bracket Pair Colorizer solves this by giving matching brackets the same color, making it easy to see code structure at a glance.\nWhy it\u0026rsquo;s helpful:\nColor-codes matching brackets Reduces syntax errors Makes complex nested structures readable Supports multiple bracket types This might seem like a small thing, but it makes a huge difference when you\u0026rsquo;re working with complex data structures or deeply nested functions. Your eyes will thank you.\nInstall: Bracket Pair Colorizer 2 7. Auto Rename Tag – HTML/XML Editing Made Simple If you\u0026rsquo;ve ever spent time manually renaming HTML or XML tags, you know how tedious it can be. Auto Rename Tag automatically renames the paired tag when you change one, keeping your markup consistent.\nKey features:\nAutomatically renames paired HTML/XML tags Works with React JSX Prevents mismatched tag errors Saves time during refactoring This extension is a must-have if you\u0026rsquo;re doing any web development. It\u0026rsquo;s one of those tools that you don\u0026rsquo;t realize you need until you have it, and then you can\u0026rsquo;t live without it.\nInstall: Auto Rename Tag 8. Thunder Client – API Testing Inside VS Code Postman is great, but do you really want to switch between applications just to test an API? Thunder Client brings API testing directly into your code editor, making it seamless to test your endpoints while you develop.\nWhat makes it awesome:\nTest REST APIs without leaving VS Code Clean, intuitive interface Environment variables support Request collections and organization This is particularly useful when you\u0026rsquo;re building FastAPI applications or working on backend services. You can write code, test it, and debug issues all in one place.\nInstall: Thunder Client 9. Error Lens – Inline Error Highlighting Error Lens takes your programming environment\u0026rsquo;s error reporting and makes it impossible to ignore. Instead of having to hover over squiggly lines or check the problems panel, errors and warnings appear right in your editor as inline messages.\nBenefits:\nDisplays errors and warnings inline Highlights entire error lines Customizable styling and behavior Works with all language extensions This extension has completely changed how I deal with errors. Instead of missing warnings or ignoring small issues, they\u0026rsquo;re right there in your face, encouraging you to fix them immediately.\nInstall: Error Lens 10. Material Icon Theme – Beautiful File Icons This might seem superficial, but good visual organization actually impacts productivity. Material Icon Theme provides beautiful, recognizable icons for different file types, making it easier to navigate your project structure.\nWhy it matters:\nInstantly recognizable file type icons Consistent, professional appearance Easier project navigation Customizable icon themes When you\u0026rsquo;re jumping between different files and folders constantly, having clear visual indicators for file types makes everything faster and more pleasant to work with.\nInstall: Material Icon Theme Setting Up Your Perfect Development Environment Installing these programming tools is just the first step. Here\u0026rsquo;s how to get the most out of your development setup:\nConfigure Prettier and ESLint together – They work beautifully as a team when properly configured Set up keyboard shortcuts – Learn the shortcuts for your most-used extensions Customize settings – Each extension has settings that can be tuned to your preferences Keep them updated – Extension updates often bring performance improvements and new features If you\u0026rsquo;re working with specific technologies, you might also want to explore language-specific developer tools. For example, if you\u0026rsquo;re into Go development , there are excellent Go extensions. Similarly, Laravel developers have their own set of must-have tools.\nThe Impact on Your Development Workflow These programming extensions don\u0026rsquo;t just add features – they totally change how you code. With AI assistance from Copilot, automatic formatting from Prettier, and inline error detection from Error Lens, you spend less time on mundane tasks and more time solving interesting problems.\nThe combination of these coding tools creates an IDE where:\nCode quality improves automatically Common mistakes are caught early Repetitive tasks are automated Navigation and organization are effortless Avoiding Extension Bloat While developer extensions are powerful, it\u0026rsquo;s easy to go overboard. I recommend starting with these 10 programming essentials and only adding more if you have a specific need. Too many extensions can slow down your development environment and create conflicts.\nPro tip: Regularly review your installed extensions and disable or uninstall ones you\u0026rsquo;re not actively using. Your code editor performs better with fewer active programming tools.\nLooking Ahead: The Future of VS Code Extensions The Visual Studio Code extension ecosystem keeps getting better fast. AI-powered extensions like Copilot are just the beginning. We\u0026rsquo;re starting to see programming tools that can refactor code, generate tests, and even help with code reviews.\nAs we move further into 2025, I expect to see more developer tools that integrate machine learning, provide better collaborative features, and offer deeper integration with cloud services. The key is to stay updated with the programming ecosystem while not getting caught up in every new coding utility.\nConclusion These 10 developer tools have become indispensable in my daily coding workflow, and I\u0026rsquo;m confident they\u0026rsquo;ll improve yours too. They represent the best of what makes this IDE great – a lightweight programming environment that can be transformed into exactly what you need.\nThe beauty of this setup is that it works whether you\u0026rsquo;re building secure web applications , working on deployment automation , or just learning the basics of programming. These tools scale with your needs and grow with your skills.\nStart with installing a few of these extensions and gradually add the rest as you see their value. Your future self will thank you for the time saved and bugs prevented. Happy coding!\nLooking for more development tips and tutorials? Check out our guides on FastAPI development and Linux server management to level up your backend skills.\n","href":"/2025/08/10-essential-vscode-extensions-developers-2025.html","title":"10 Essential VS Code Extensions Every Developer Must Have in 2025"},{"content":"Building APIs used to scare me when I first started programming. There\u0026rsquo;s so much to learn - databases, HTTP methods, authentication, error handling. But FastAPI changed everything for me. It\u0026rsquo;s like having training wheels that actually make you faster, not slower.\nWe\u0026rsquo;re going to build a real Book Library API from the ground up. No fluff, no complicated setups - just practical, working code that you can understand and expand on. By the end of this guide, you\u0026rsquo;ll have a fully functional REST API that can handle creating, reading, updating, and deleting books.\nOnce you master the basics here, you can take your FastAPI skills further with JWT authentication and OAuth2 security , or learn how to deploy your FastAPI application to production .\nI\u0026rsquo;m not going to throw a bunch of code at you and hope it sticks. We\u0026rsquo;ll walk through each piece together, and I\u0026rsquo;ll explain why we\u0026rsquo;re doing things a certain way. Think of it as pair programming through an article. Everything runs locally too - no cloud accounts or credit cards needed.\nWhat you\u0026rsquo;ll build:\nA complete Book Library REST API CRUD operations (Create, Read, Update, Delete) Data validation with Pydantic SQLite database integration Interactive API documentation Error handling and responses Testing with real HTTP requests Prerequisites Before we dive in, make sure you have:\nPython 3.8 or higher installed Basic Python knowledge (variables, functions, classes) A code editor (VS Code, PyCharm, or any text editor) Command line familiarity Don\u0026rsquo;t worry if you\u0026rsquo;re not an expert in any of these - we\u0026rsquo;ll explain everything as we go.\nWhy FastAPI? Why FastAPI and not Django or Flask? Good question. I\u0026rsquo;ve used all three in production, and here\u0026rsquo;s my take: Django feels like driving a truck when you need a motorcycle. Flask is that motorcycle, but you end up building the truck yourself anyway. FastAPI? It\u0026rsquo;s like a sports car that comes with GPS, heated seats, and a great sound system right out of the box.\nFastAPI automatically generates interactive documentation for your API, validates request data, and handles serialization. These features alone save hours of manual work. Plus, it\u0026rsquo;s built on modern Python features like type hints, making your code more readable and less bug-prone.\nSetting Up the Development Environment First things first - let\u0026rsquo;s set up a proper workspace. Open your terminal and create a new directory:\nmkdir book-library-api cd book-library-api Now create a virtual environment. This keeps our project dependencies separate from other Python projects on your system:\npython -m venv venv Activate the virtual environment:\nOn Windows:\nvenv\\Scripts\\activate On macOS/Linux:\nsource venv/bin/activate You should see (venv) at the beginning of your command prompt, indicating the virtual environment is active.\nInstalling Dependencies We need just a few packages to get started:\npip install fastapi uvicorn sqlalchemy Here\u0026rsquo;s what each package does:\nFastAPI: The web framework itself Uvicorn: ASGI server to run our application SQLAlchemy: Database ORM (Object-Relational Mapping) Let\u0026rsquo;s also create a requirements.txt file to track our dependencies:\npip freeze \u0026gt; requirements.txt Project Structure Good organization makes your code easier to understand and maintain. Create this folder structure:\nmkdir app mkdir app/models mkdir app/schemas mkdir app/database touch app/__init__.py touch app/main.py touch app/models/__init__.py touch app/models/book.py touch app/schemas/__init__.py touch app/schemas/book.py touch app/database/__init__.py touch app/database/database.py Your project should now look like this:\nbook-library-api/ ├── venv/ ├── app/ │ ├── __init__.py │ ├── main.py │ ├── models/ │ │ ├── __init__.py │ │ └── book.py │ ├── schemas/ │ │ ├── __init__.py │ │ └── book.py │ └── database/ │ ├── __init__.py │ └── database.py └── requirements.txt This structure separates different parts of our application, making it easier to find and modify code later.\nDatabase Setup Let\u0026rsquo;s start by setting up our database connection. We\u0026rsquo;ll use SQLite because it\u0026rsquo;s simple and doesn\u0026rsquo;t require a separate database server.\nCreate the database configuration:\n# app/database/database.py from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker # SQLite database file SQLALCHEMY_DATABASE_URL = \u0026#34;sqlite:///./books.db\u0026#34; # Create engine engine = create_engine( SQLALCHEMY_DATABASE_URL, connect_args={\u0026#34;check_same_thread\u0026#34;: False} ) # Create session SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) # Base class for models Base = declarative_base() # Dependency to get database session def get_db(): db = SessionLocal() try: yield db finally: db.close() This code sets up our database connection. The get_db() function is a dependency that FastAPI will use to provide database sessions to our API endpoints.\nCreating the Database Model Now let\u0026rsquo;s define what a book looks like in our database:\n# app/models/book.py from sqlalchemy import Column, Integer, String, Text from app.database.database import Base class Book(Base): __tablename__ = \u0026#34;books\u0026#34; id = Column(Integer, primary_key=True, index=True) title = Column(String(255), nullable=False) author = Column(String(255), nullable=False) description = Column(Text, nullable=True) published_year = Column(Integer, nullable=True) isbn = Column(String(20), unique=True, nullable=True) This model defines our book structure with fields for title, author, description, publication year, and ISBN. The __tablename__ tells SQLAlchemy what to name the table in the database.\nPydantic Schemas Pydantic schemas define how data should look when it comes into or goes out of our API. Think of them as contracts that ensure data consistency:\n# app/schemas/book.py from pydantic import BaseModel from typing import Optional, List, Any class BookBase(BaseModel): title: str author: str description: Optional[str] = None published_year: Optional[int] = None isbn: Optional[str] = None class BookCreate(BookBase): pass class BookUpdate(BaseModel): title: Optional[str] = None author: Optional[str] = None description: Optional[str] = None published_year: Optional[int] = None isbn: Optional[str] = None class BookResponse(BookBase): id: int class Config: from_attributes = True # Standard API Response Schemas class Meta(BaseModel): success: bool message: str total: Optional[int] = None page: Optional[int] = None limit: Optional[int] = None total_pages: Optional[int] = None class StandardResponse(BaseModel): meta: Meta data: Any class BookListResponse(BaseModel): meta: Meta data: List[BookResponse] class SingleBookResponse(BaseModel): meta: Meta data: BookResponse We have different schemas for different purposes:\nBookBase: Common fields for all book operations BookCreate: For creating new books (inherits from BookBase) BookUpdate: For updating existing books (all fields optional) BookResponse: For returning book data (includes the ID) Meta: Metadata for API responses (success status, pagination info) StandardResponse: Generic response wrapper with meta and data BookListResponse: Specific response for book lists SingleBookResponse: Specific response for single book operations This standard response format makes your API more consistent and easier to consume by frontend applications or other services.\nBuilding the FastAPI Application Now for the main event - creating our FastAPI application:\n# app/main.py from fastapi import FastAPI, HTTPException, Depends, status from sqlalchemy.orm import Session from typing import List import math from app.database.database import engine, get_db from app.models import book as book_models from app.schemas import book as book_schemas # Create database tables book_models.Base.metadata.create_all(bind=engine) # Initialize FastAPI app app = FastAPI( title=\u0026#34;Book Library API\u0026#34;, description=\u0026#34;A simple REST API for managing books with standardized responses\u0026#34;, version=\u0026#34;1.0.0\u0026#34; ) # Helper function to create standard responses def create_response(success: bool, message: str, data=None, total=None, page=None, limit=None): meta = book_schemas.Meta( success=success, message=message, total=total, page=page, limit=limit, total_pages=math.ceil(total / limit) if total and limit else None ) return book_schemas.StandardResponse(meta=meta, data=data) # Root endpoint @app.get(\u0026#34;/\u0026#34;) def read_root(): return create_response( success=True, message=\u0026#34;Welcome to Book Library API\u0026#34;, data={\u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;} ) # Health check endpoint @app.get(\u0026#34;/health\u0026#34;) def health_check(): return create_response( success=True, message=\u0026#34;API is healthy\u0026#34;, data={\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-01T00:00:00Z\u0026#34;} ) # Get all books @app.get(\u0026#34;/books\u0026#34;, response_model=book_schemas.BookListResponse) def get_books(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)): # Get total count for pagination total_books = db.query(book_models.Book).count() # Get books with pagination books = db.query(book_models.Book).offset(skip).limit(limit).all() current_page = (skip // limit) + 1 meta = book_schemas.Meta( success=True, message=\u0026#34;Books retrieved successfully\u0026#34;, total=total_books, page=current_page, limit=limit, total_pages=math.ceil(total_books / limit) if total_books \u0026gt; 0 else 0 ) return book_schemas.BookListResponse(meta=meta, data=books) # Get single book by ID @app.get(\u0026#34;/books/{book_id}\u0026#34;, response_model=book_schemas.SingleBookResponse) def get_book(book_id: int, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) meta = book_schemas.Meta( success=True, message=\u0026#34;Book retrieved successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=book) # Create new book @app.post(\u0026#34;/books\u0026#34;, response_model=book_schemas.SingleBookResponse, status_code=status.HTTP_201_CREATED) def create_book(book: book_schemas.BookCreate, db: Session = Depends(get_db)): # Check if book with same ISBN already exists if book.isbn: existing_book = db.query(book_models.Book).filter(book_models.Book.isbn == book.isbn).first() if existing_book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with ISBN {book.isbn} already exists\u0026#34; ) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) db_book = book_models.Book(**book.dict()) db.add(db_book) db.commit() db.refresh(db_book) meta = book_schemas.Meta( success=True, message=\u0026#34;Book created successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=db_book) # Update existing book @app.put(\u0026#34;/books/{book_id}\u0026#34;, response_model=book_schemas.SingleBookResponse) def update_book(book_id: int, book_update: book_schemas.BookUpdate, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) # Update only provided fields update_data = book_update.dict(exclude_unset=True) if not update_data: meta = book_schemas.Meta( success=False, message=\u0026#34;No fields provided for update\u0026#34; ) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) for field, value in update_data.items(): setattr(book, field, value) db.commit() db.refresh(book) meta = book_schemas.Meta( success=True, message=\u0026#34;Book updated successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=book) # Delete book @app.delete(\u0026#34;/books/{book_id}\u0026#34;) def delete_book(book_id: int, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) db.delete(book) db.commit() return create_response( success=True, message=f\u0026#34;Book with id {book_id} deleted successfully\u0026#34;, data={\u0026#34;deleted_book_id\u0026#34;: book_id} ) This is the heart of our API. Let\u0026rsquo;s break down what each endpoint does:\nGET /: Welcome message with standardized response GET /health: Simple health check with status information GET /books: Get all books with pagination and metadata GET /books/{book_id}: Get a specific book by ID POST /books: Create a new book with validation PUT /books/{book_id}: Update an existing book DELETE /books/{book_id}: Delete a book Standard Response Format Notice how all our responses now follow a consistent structure with meta and data fields:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Books retrieved successfully\u0026#34;, \u0026#34;total\u0026#34;: 25, \u0026#34;page\u0026#34;: 1, \u0026#34;limit\u0026#34;: 10, \u0026#34;total_pages\u0026#34;: 3 }, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } ] } This format provides several benefits:\nConsistent structure across all endpoints Success/failure indication in every response Helpful messages for debugging and user feedback Pagination metadata for list endpoints Easy parsing for frontend applications Running the Application Let\u0026rsquo;s see our API in action! Run this command from your project root:\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000 The --reload flag automatically restarts the server when you change code, making development much smoother.\nOpen your browser and go to http://127.0.0.1:8000. You should see:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Welcome to Book Library API\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34; } } Interactive API Documentation Now for the coolest part. Navigate to http://127.0.0.1:8000/docs and prepare to be impressed. FastAPI automatically created interactive documentation for your entire API. You can test every endpoint right in the browser - no Postman needed.\nTry creating a book:\nClick on the POST /books endpoint Click \u0026ldquo;Try it out\u0026rdquo; Enter this sample data: { \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } Click \u0026ldquo;Execute\u0026rdquo; You should get a successful response with your newly created book in the standard format:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Book created successfully\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } } Testing Your API Let\u0026rsquo;s test all our endpoints to make sure everything works. You can use the interactive docs at http://127.0.0.1:8000/docs, or test via command line with these examples:\nMethod 1: Copy-Paste Ready Commands Create a book (single line - just copy and paste):\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34;}\u0026#39; Get all books:\ncurl \u0026#34;http://127.0.0.1:8000/books\u0026#34; Get a specific book:\ncurl \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; Update a book:\ncurl -X PUT \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners - Updated Edition\u0026#34;}\u0026#39; Delete a book:\ncurl -X DELETE \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; Method 2: Using JSON Files (Recommended for Complex Data) For easier testing with complex data, create JSON files:\nCreate book.json:\ncat \u0026gt; book.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } EOF Then use the file:\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @book.json Create update.json:\ncat \u0026gt; update.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners - Updated Edition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step with the latest updates\u0026#34; } EOF Update using file:\ncurl -X PUT \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @update.json Expected Response Examples Successful book creation:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Book created successfully\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } } Get all books response:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Books retrieved successfully\u0026#34;, \u0026#34;total\u0026#34;: 1, \u0026#34;page\u0026#34;: 1, \u0026#34;limit\u0026#34;: 100, \u0026#34;total_pages\u0026#34;: 1 }, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } ] } Testing with Pagination Get books with pagination:\ncurl \u0026#34;http://127.0.0.1:8000/books?skip=0\u0026amp;limit=5\u0026#34; Get second page:\ncurl \u0026#34;http://127.0.0.1:8000/books?skip=5\u0026amp;limit=5\u0026#34; Understanding the Code Before we move on, let me explain a few important concepts that might not be obvious:\nDependency Injection: The Depends(get_db) parameter in our endpoints is dependency injection. FastAPI automatically calls get_db() and provides the database session to your function.\nType Hints: Notice how we specify types like book_id: int and response_model=List[book_schemas.BookResponse]. This isn\u0026rsquo;t just for documentation - FastAPI uses these to validate data and provide better error messages.\nHTTP Status Codes: We use appropriate status codes like 201 for created resources and 404 for not found. This makes our API more professional and easier to integrate with.\nError Handling: When something goes wrong (like trying to access a non-existent book), we raise HTTPException with appropriate status codes and messages.\nAdding More Features Want to extend your API? Here are some ideas:\nSearch functionality:\n@app.get(\u0026#34;/books/search\u0026#34;, response_model=List[book_schemas.BookResponse]) def search_books(q: str, db: Session = Depends(get_db)): books = db.query(book_models.Book).filter( book_models.Book.title.contains(q) | book_models.Book.author.contains(q) ).all() return books Filtering by author:\n@app.get(\u0026#34;/books/by-author/{author}\u0026#34;, response_model=List[book_schemas.BookResponse]) def get_books_by_author(author: str, db: Session = Depends(get_db)): books = db.query(book_models.Book).filter(book_models.Book.author == author).all() return books Common Issues and Solutions Import Errors: Make sure all your __init__.py files exist and you\u0026rsquo;re running commands from the project root.\nDatabase Errors: If you get database-related errors, delete the books.db file and restart the application to recreate it.\nPort Already in Use: If port 8000 is busy, use a different port: uvicorn app.main:app --reload --port 8001\ncurl Command Issues: If you get \u0026ldquo;command not found\u0026rdquo; errors when copying multiline curl commands, use the single-line versions provided above, or create JSON files as shown in Method 2.\nJSON Parsing Errors: Make sure your JSON is valid. If you get \u0026ldquo;Field required\u0026rdquo; errors, check that your JSON structure matches the expected schema. Use the interactive docs at /docs to see the exact format needed.\nPermission Errors: On some systems, you might need to escape quotes differently. If single quotes don\u0026rsquo;t work, try double quotes with escaped inner quotes:\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;FastAPI for Beginners\\\u0026#34;, \\\u0026#34;author\\\u0026#34;: \\\u0026#34;Jane Developer\\\u0026#34;}\u0026#34; Next Steps Congratulations! You\u0026rsquo;ve built a complete REST API from scratch. Here\u0026rsquo;s what you can do next:\nAdd Authentication: Protect your endpoints with JWT authentication and OAuth2 password flow Deploy to Production: Learn how to deploy FastAPI on Ubuntu 24.04 with Nginx and HTTPS Add Validation: Implement more complex validation rules with Pydantic Add Tests: Write unit tests for your endpoints Add a Frontend: Build a web interface to interact with your API Scale with Docker: Containerize your application for easier deployment Wrapping Up Look at what you just built - a real REST API that handles data, validates input, and documents itself. That\u0026rsquo;s not trivial stuff. You went from zero to having something that could actually power a web app or mobile app.\nWhat I love about this setup is how easy it is to extend. Need user accounts? Add a User model. Want to track book reviews? Create a Review endpoint. The foundation is solid, and FastAPI handles the boring stuff so you can focus on the interesting problems.\nKeep experimenting with different endpoints and features. The best way to learn API development is by building real projects and solving real problems. Your Book Library API is just the beginning - imagine what you\u0026rsquo;ll build next!\n","href":"/2025/08/fastapi-tutorial-build-rest-api-from-scratch-beginner-guide.html","title":"FastAPI Tutorial: Build REST API from Scratch (Beginner Guide)"},{"content":"Deploying a Laravel application to a VPS (Virtual Private Server) with Nginx gives you complete control over your hosting environment and superior performance compared to shared hosting. This comprehensive guide will walk you through the entire process, from server setup to production optimization.\nWhat You\u0026rsquo;ll Learn Set up a VPS for Laravel deployment Configure Nginx for optimal Laravel performance Secure your application with SSL certificates Implement production best practices Set up automated deployments Monitor and maintain your application Prerequisites Before starting, ensure you have:\nA VPS running Ubuntu 20.04/22.04 LTS (DigitalOcean, Linode, AWS EC2, etc.) SSH access to your server A domain name pointing to your VPS IP Basic terminal/command line knowledge A Laravel application ready for deployment Step 1: Initial Server Setup Connect to Your VPS ssh root@your-server-ip Update System Packages apt update \u0026amp;\u0026amp; apt upgrade -y Create a Non-Root User # Create new user adduser deploy # Add to sudo group usermod -aG sudo deploy # Switch to new user su - deploy Configure SSH Key Authentication # On your local machine, copy your public key ssh-copy-id deploy@your-server-ip # Or manually add your key mkdir -p ~/.ssh chmod 700 ~/.ssh nano ~/.ssh/authorized_keys # Paste your public key and save chmod 600 ~/.ssh/authorized_keys Step 2: Install Required Software Install Nginx sudo apt install nginx -y sudo systemctl start nginx sudo systemctl enable nginx Install PHP 8.2 and Extensions # Add PHP repository sudo apt install software-properties-common -y sudo add-apt-repository ppa:ondrej/php -y sudo apt update # Install PHP and required extensions sudo apt install php8.2-fpm php8.2-common php8.2-mysql php8.2-xml php8.2-xmlrpc php8.2-curl php8.2-gd php8.2-imagick php8.2-cli php8.2-dev php8.2-imap php8.2-mbstring php8.2-opcache php8.2-soap php8.2-zip php8.2-intl php8.2-bcmath -y Install Composer cd ~ curl -sS https://getcomposer.org/installer -o composer-setup.php sudo php composer-setup.php --install-dir=/usr/local/bin --filename=composer rm composer-setup.php Install MySQL sudo apt install mysql-server -y sudo mysql_secure_installation Create Database and User sudo mysql -u root -p CREATE DATABASE laravel_app; CREATE USER \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;strong_password_here\u0026#39;; GRANT ALL PRIVILEGES ON laravel_app.* TO \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39;; FLUSH PRIVILEGES; EXIT; Install Git sudo apt install git -y Step 3: Deploy Your Laravel Application Clone Your Repository cd /var/www sudo git clone https://github.com/username/your-laravel-app.git sudo chown -R deploy:deploy your-laravel-app cd your-laravel-app Install Dependencies composer install --optimize-autoloader --no-dev Configure Environment cp .env.example .env nano .env Update your .env file:\nAPP_NAME=\u0026#34;Your Laravel App\u0026#34; APP_ENV=production APP_DEBUG=false APP_URL=https://yourdomain.com DB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=laravel_app DB_USERNAME=laravel_user DB_PASSWORD=strong_password_here CACHE_DRIVER=file QUEUE_CONNECTION=sync SESSION_DRIVER=file SESSION_LIFETIME=120 Generate Application Key php artisan key:generate Run Database Migrations php artisan migrate --force Optimize for Production php artisan config:cache php artisan route:cache php artisan view:cache php artisan storage:link Set File Permissions sudo chown -R www-data:www-data /var/www/your-laravel-app sudo chmod -R 755 /var/www/your-laravel-app sudo chmod -R 775 /var/www/your-laravel-app/storage sudo chmod -R 775 /var/www/your-laravel-app/bootstrap/cache Step 4: Configure Nginx Create Nginx Configuration sudo nano /etc/nginx/sites-available/your-laravel-app Add the following configuration:\nserver { listen 80; server_name yourdomain.com www.yourdomain.com; root /var/www/your-laravel-app/public; add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34;; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; add_header X-Content-Type-Options \u0026#34;nosniff\u0026#34;; index index.php; charset utf-8; # Security headers add_header Referrer-Policy \u0026#34;no-referrer-when-downgrade\u0026#34;; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains\u0026#34;; location / { try_files $uri $uri/ /index.php?$query_string; } location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } error_page 404 /index.php; location ~ \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; include fastcgi_params; fastcgi_hide_header X-Powered-By; } location ~ /\\.(?!well-known).* { deny all; } # Asset caching location ~* \\.(js|css|png|jpg|jpeg|gif|ico|svg)$ { expires 1y; add_header Cache-Control \u0026#34;public, immutable\u0026#34;; } # Deny access to sensitive files location ~ /\\.(htaccess|htpasswd|env) { deny all; } # Client max body size (for file uploads) client_max_body_size 20M; } Enable the Site sudo ln -s /etc/nginx/sites-available/your-laravel-app /etc/nginx/sites-enabled/ sudo nginx -t sudo systemctl reload nginx Remove Default Nginx Site sudo rm /etc/nginx/sites-enabled/default Step 5: Configure PHP-FPM Optimize PHP-FPM Settings sudo nano /etc/php/8.2/fpm/pool.d/www.conf Update these settings:\nuser = www-data group = www-data listen.owner = www-data listen.group = www-data pm = dynamic pm.max_children = 50 pm.start_servers = 5 pm.min_spare_servers = 5 pm.max_spare_servers = 35 pm.max_requests = 500 PHP Configuration sudo nano /etc/php/8.2/fpm/php.ini Update these settings:\nupload_max_filesize = 20M post_max_size = 25M memory_limit = 256M max_execution_time = 300 max_input_vars = 3000 opcache.enable=1 opcache.memory_consumption=128 opcache.max_accelerated_files=10000 Restart PHP-FPM sudo systemctl restart php8.2-fpm Step 6: SSL Certificate with Let\u0026rsquo;s Encrypt Install Certbot sudo apt install certbot python3-certbot-nginx -y Obtain SSL Certificate sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com Auto-renewal Setup sudo crontab -e Add this line:\n0 12 * * * /usr/bin/certbot renew --quiet Step 7: Firewall Configuration Configure UFW sudo ufw allow ssh sudo ufw allow \u0026#39;Nginx Full\u0026#39; sudo ufw --force enable sudo ufw status Step 8: Production Optimization Configure Queue Processing Create a supervisor configuration:\nsudo apt install supervisor -y sudo nano /etc/supervisor/conf.d/laravel-worker.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/your-laravel-app/artisan queue:work autostart=true autorestart=true stopasgroup=true killasgroup=true user=www-data numprocs=2 redirect_stderr=true stdout_logfile=/var/www/your-laravel-app/storage/logs/worker.log stopwaitsecs=3600 sudo supervisorctl reread sudo supervisorctl update sudo supervisorctl start laravel-worker:* Setup Log Rotation sudo nano /etc/logrotate.d/laravel /var/www/your-laravel-app/storage/logs/*.log { daily rotate 14 missingok notifempty compress delaycompress copytruncate } Configure Redis (Optional) sudo apt install redis-server -y sudo systemctl enable redis-server Update .env:\nCACHE_DRIVER=redis SESSION_DRIVER=redis QUEUE_CONNECTION=redis Step 9: Automated Deployment Script Create a deployment script:\nnano ~/deploy.sh #!/bin/bash APP_DIR=\u0026#34;/var/www/your-laravel-app\u0026#34; BRANCH=\u0026#34;main\u0026#34; echo \u0026#34;Starting deployment...\u0026#34; # Navigate to app directory cd $APP_DIR # Enable maintenance mode sudo -u www-data php artisan down # Pull latest changes git pull origin $BRANCH # Install/update composer dependencies sudo -u www-data composer install --optimize-autoloader --no-dev # Clear and cache config sudo -u www-data php artisan config:clear sudo -u www-data php artisan config:cache # Clear and cache routes sudo -u www-data php artisan route:clear sudo -u www-data php artisan route:cache # Clear and cache views sudo -u www-data php artisan view:clear sudo -u www-data php artisan view:cache # Run database migrations sudo -u www-data php artisan migrate --force # Restart PHP-FPM and queue workers sudo systemctl reload php8.2-fpm sudo supervisorctl restart laravel-worker:* # Disable maintenance mode sudo -u www-data php artisan up echo \u0026#34;Deployment completed successfully!\u0026#34; Make it executable:\nchmod +x ~/deploy.sh Step 10: Monitoring and Security Install Fail2Ban sudo apt install fail2ban -y sudo systemctl enable fail2ban Configure Nginx Rate Limiting Add to your Nginx configuration:\n# Add to http block in /etc/nginx/nginx.conf limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m; # Add to your server block location /login { limit_req zone=login burst=5 nodelay; try_files $uri $uri/ /index.php?$query_string; } Monitor Logs # View Nginx logs sudo tail -f /var/log/nginx/access.log sudo tail -f /var/log/nginx/error.log # View Laravel logs tail -f /var/www/your-laravel-app/storage/logs/laravel.log # View PHP-FPM logs sudo tail -f /var/log/php8.2-fpm.log Step 11: Backup Strategy Database Backup Script nano ~/backup-db.sh #!/bin/bash BACKUP_DIR=\u0026#34;/home/deploy/backups\u0026#34; DATE=$(date +%Y%m%d_%H%M%S) DB_NAME=\u0026#34;laravel_app\u0026#34; DB_USER=\u0026#34;laravel_user\u0026#34; DB_PASS=\u0026#34;your_password\u0026#34; mkdir -p $BACKUP_DIR # Create database backup mysqldump -u $DB_USER -p$DB_PASS $DB_NAME \u0026gt; $BACKUP_DIR/db_backup_$DATE.sql # Keep only last 7 days of backups find $BACKUP_DIR -name \u0026#34;db_backup_*.sql\u0026#34; -mtime +7 -delete echo \u0026#34;Database backup completed: $BACKUP_DIR/db_backup_$DATE.sql\u0026#34; Schedule Daily Backups crontab -e 0 2 * * * /home/deploy/backup-db.sh Troubleshooting Common Issues 1. 502 Bad Gateway # Check PHP-FPM status sudo systemctl status php8.2-fpm # Check PHP-FPM socket sudo ls -la /var/run/php/ # Restart services sudo systemctl restart php8.2-fpm nginx 2. Permission Issues sudo chown -R www-data:www-data /var/www/your-laravel-app sudo chmod -R 755 /var/www/your-laravel-app sudo chmod -R 775 /var/www/your-laravel-app/storage sudo chmod -R 775 /var/www/your-laravel-app/bootstrap/cache 3. Storage Link Issues php artisan storage:link sudo chown -R www-data:www-data /var/www/your-laravel-app/public/storage 4. Memory Issues # Increase PHP memory limit sudo nano /etc/php/8.2/fpm/php.ini # Set: memory_limit = 512M # Restart PHP-FPM sudo systemctl restart php8.2-fpm Performance Optimization Tips 1. Enable OPcache Ensure these settings in /etc/php/8.2/fpm/php.ini:\nopcache.enable=1 opcache.memory_consumption=256 opcache.max_accelerated_files=20000 opcache.validate_timestamps=0 opcache.save_comments=1 opcache.fast_shutdown=0 2. Optimize MySQL sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf [mysqld] innodb_buffer_pool_size = 256M innodb_log_file_size = 64M query_cache_type = 1 query_cache_size = 32M 3. Use CDN for Assets Consider using a CDN service like Cloudflare for static assets to improve loading times globally.\nSecurity Best Practices Regular Updates: Keep your server and applications updated Strong Passwords: Use complex passwords and consider key-based authentication Firewall: Configure UFW properly SSL/TLS: Always use HTTPS in production Hide Server Info: Remove server version information from headers Regular Backups: Implement automated backup strategies Monitor Logs: Regularly check access and error logs Rate Limiting: Implement rate limiting for sensitive endpoints Conclusion You now have a robust Laravel application running on a VPS with Nginx, complete with SSL certificates, optimization, and security measures. This setup provides excellent performance and gives you full control over your hosting environment.\nKey benefits of this setup:\nPerformance: Nginx + PHP-FPM provides excellent performance Security: SSL certificates and security headers protect your application Scalability: Easy to scale as your application grows Control: Full control over server configuration and optimization Cost-effective: VPS hosting is often more cost-effective than managed hosting Remember to:\nMonitor your application regularly Keep everything updated Implement proper backup strategies Test your deployment process in a staging environment first Happy deploying!\n","href":"/2025/08/deploy-laravel-to-vps-with-nginx-complete-guide.html","title":"Deploy Laravel Application to VPS with Nginx: Complete Production Guide"},{"content":"Need to remove Docker from Ubuntu 24.04 (Noble) cleanly? This guide shows a safe, step‑by‑step removal that gets rid of the Engine, Compose v2 plugin, configs, and data — plus optional rootless Docker cleanup. If you plan to reinstall after this, see: Install Docker on Ubuntu 24.04: Post‑Install, Rootless, and Compose v2 . For HTTPS and reverse proxy, see: Nginx + Certbot on Ubuntu 24.04: Free HTTPS with Let’s Encrypt .\nWarning: The steps below can remove containers, images, volumes, and networks. Back up anything important before continuing.\nWhat you’ll do\nStop and disable Docker services (Engine and containerd) Optionally remove all containers, images, volumes, and networks Purge Docker packages and the Compose v2 plugin Delete configuration and data directories (Engine and containerd) Optionally uninstall rootless Docker Verify that Docker is completely gone Prerequisites\nUbuntu 24.04 LTS (Noble) with sudo access Terminal access to the machine (SSH or local) (Optional) Remove containers, images, volumes, networks If you want a fully clean state, remove runtime data first. If you prefer to keep data, skip this step. # Remove all containers (running and stopped) sudo docker ps -aq | xargs -r sudo docker rm -f # Remove all images sudo docker image prune -a -f # Remove all volumes sudo docker volume prune -f # Remove unused networks sudo docker network prune -f Stop Docker services sudo systemctl disable --now docker docker.socket containerd || true Purge Docker packages Remove Engine, CLI, Buildx, and Compose v2 plugin (installed as apt plugins on Ubuntu 24.04 per official repo). Also cover legacy packages. sudo apt update sudo apt purge -y \\ docker-ce docker-ce-cli containerd.io \\ docker-buildx-plugin docker-compose-plugin \\ docker-ce-rootless-extras || true # In case older/alternative packages were installed sudo apt purge -y docker.io docker-doc podman-docker containerd runc || true sudo apt autoremove -y sudo apt clean Remove configuration, data, and repo files # Engine \u0026amp; containerd data/config sudo rm -rf /var/lib/docker /var/lib/containerd sudo rm -rf /etc/docker /etc/containerd 2\u0026gt;/dev/null || true sudo rm -rf /etc/systemd/system/docker.service.d 2\u0026gt;/dev/null || true # Socket leftovers sudo rm -f /var/run/docker.sock # Apt repository and key (official Docker repo) sudo rm -f /etc/apt/sources.list.d/docker.list sudo rm -f /etc/apt/keyrings/docker.gpg sudo apt update # Per-user Docker config (CLI) rm -rf ~/.docker (Optional) Uninstall rootless Docker (if you enabled it) Rootless Docker runs as a user service under systemd. If you used it, clean it up as well. # Stop/disable user service if present systemctl --user stop docker 2\u0026gt;/dev/null || true systemctl --user disable docker 2\u0026gt;/dev/null || true systemctl --user daemon-reload || true # If you installed via the helper tool, uninstall it command -v dockerd-rootless-setuptool.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; \\ dockerd-rootless-setuptool.sh uninstall || true # Remove user data/config rm -rf ~/.local/share/docker ~/.config/docker rm -f ~/.config/systemd/user/docker.service # Optional: disable lingering if you previously enabled it for rootless sudo loginctl disable-linger \u0026#34;$USER\u0026#34; 2\u0026gt;/dev/null || true Verify removal # Docker CLI should be missing if command -v docker; then echo \u0026#34;Docker still present\u0026#34;; else echo \u0026#34;Docker CLI not found ✔\u0026#34;; fi # No Docker or containerd packages dpkg -l | grep -E \u0026#34;^(ii|rc)\\s+(docker|containerd)\u0026#34; || echo \u0026#34;No docker/containerd packages found ✔\u0026#34; # Services should be inactive systemctl status docker 2\u0026gt;/dev/null | grep -q running \u0026amp;\u0026amp; echo \u0026#34;docker running\u0026#34; || echo \u0026#34;docker not running ✔\u0026#34; systemctl status containerd 2\u0026gt;/dev/null | grep -q running \u0026amp;\u0026amp; echo \u0026#34;containerd running\u0026#34; || echo \u0026#34;containerd not running ✔\u0026#34; Common troubleshooting\nStuck socket at /var/run/docker.sock: remove it with sudo rm -f /var/run/docker.sock and re‑check. Packages reappear after purge: run sudo apt purge ... again, then sudo apt autoremove -y \u0026amp;\u0026amp; sudo apt clean. Rootless processes still around: ps -u \u0026quot;$USER\u0026quot; | grep -E 'dockerd|containerd' then kill the PIDs, re‑run step 5. WSL2 on Windows: make sure you uninstall Docker Desktop WSL integration separately; this guide targets native Ubuntu 24.04. Reinstall later? When you’re ready to install again, follow the fresh 24.04 guide (official repo, Compose v2 plugin, optional rootless): Install Docker on Ubuntu 24.04: Post‑Install, Rootless, and Compose v2 .\n","href":"/2025/08/uninstall-docker-ubuntu-24-04-clean-removal.html","title":"Uninstall Docker on Ubuntu 24.04: Complete Clean Removal"},{"content":"Passkeys are increasingly supported across major platforms. They enable fast, convenient logins without passwords and are resistant to phishing. No more weak passwords or OTP codes hijacked via SIM swaps. This guide explains how passkeys work, compares them with legacy 2FA, and shows how to enable them on Google and Apple or use them with password managers like 1Password and Bitwarden.\nSummary: What Is a Passkey? A passkey is a passwordless credential based on FIDO2/WebAuthn. Instead of typing a shared secret, you prove possession of a private cryptographic key securely stored on your device (or in a compatible password manager). When you log in, the site/app sends a challenge that only your private key can sign. The server verifies the signature with the public key you registered. No shared secret travels over the network.\nIn practice:\nNo password transmission—only per‑site cryptographic signatures. Phishing‑resistant—signatures are bound to the real origin. More convenient—use Face/Touch ID or your device PIN to approve. Why It’s Safer Than Passwords and SMS 2FA Phishing resistance: Traditional passwords/managers can be tricked by look‑alike domains; passkeys can’t. The challenge only works on the correct origin. No SIM‑swap risk: SMS codes can be intercepted or diverted; passkeys don’t rely on SMS. Not guessable/brute‑forceable: They’re cryptographic keys, not words. Lower breach impact: Sites store public keys, not secrets. A database leak alone won’t let attackers sign in. Caveat: Passkeys aren’t magic. A compromised device still puts you at risk. Keep your OS, browser, and extensions clean.\nHow to Enable Passkeys Menus vary by OS/browser version; the flow is similar everywhere.\nGoogle (Android/Chrome/Google Password Manager) Go to myaccount.google.com \u0026gt; Security \u0026gt; Passkeys. Click “Create a passkey” and approve with biometrics. On other websites that support passkeys, choose “Use passkey” to register/login. Sync: Your passkeys are stored in Google Password Manager and available on devices signed in to your Google account (protected by local biometrics/PIN). Apple (iCloud Keychain on iOS/macOS/Safari) Ensure iCloud Keychain is enabled: Settings \u0026gt; iCloud \u0026gt; Passwords and Keychain. When a site offers passkeys, Safari will prompt to save a passkey. Next logins are approved with Face ID/Touch ID. End‑to‑end encrypted sync via iCloud across your Apple devices. 1Password Update to the latest 1Password and enable passkey support in the app/extension. When a site offers passkeys, choose to save it in 1Password. Future logins can be approved via 1Password—no password required. Benefits: cross‑platform, secure sharing for families/teams, admin policies for orgs. Bitwarden Update Bitwarden and enable passkey support in the extension/app. Save passkeys when registering/enabling them on supported sites. Approve future logins using Bitwarden with local biometrics/PIN. Benefits: open‑source, cost‑effective, organization features. Tip: If “Create/Use a passkey” doesn’t appear, check the site’s account security settings. Support is expanding—banks, email providers, marketplaces, and developer platforms are rolling it out.\nHow It Works (The Short Version) On registration, your device creates a public/private key pair and registers the public key with the site. On login, the site sends a challenge that your device signs with the private key after local verification (biometrics/PIN). The browser enforces origin binding so signatures don’t work on fake domains. That property provides phishing resistance.\nLimitations and How to Mitigate Them Lost/replaced device: Ensure sync is enabled (iCloud/Google/manager) and keep recovery methods (backup codes) for critical accounts. Compatibility: Some sites don’t support passkeys yet—keep a strong password + app‑based 2FA or a security key as fallback. Mixed ecosystems: If you use Apple + Windows + Android, a passkey‑capable manager (1Password/Bitwarden/Proton Pass) often provides the smoothest experience. Travel/emergency access: Keep at least one hardware security key as a break‑glass option for email, domain registrar, banking, and cloud. Migration Strategy: Practical Priorities Prioritize high‑value accounts first—the ones attackers target most and the ones that would most harm your brand/SEO if compromised.\nSecure critical accounts first:\nPrimary email (Gmail/iCloud/Outlook) Cloud storage (Google Drive/iCloud/OneDrive) Banking/fintech Developer, domain/DNS, and hosting control panels Enable passkeys and keep app‑based 2FA (TOTP) as backup\nAvoid SMS where possible. Use a FIDO2 hardware key for mission‑critical accounts. Hygiene and audits\nRemove weak/duplicate passwords. Run your manager’s vault health check. Revoke unknown sessions/devices and retire risky recovery methods (old SMS). Team education (for orgs)\nStandardize on passkeys + authenticator + security keys. Teach staff to spot look‑alike domains, OAuth consent scams, and QR phishing. Quick FAQ Do I still need passwords? For many sites, yes—as fallback. Increasingly, services allow passkey‑only. Keep a unique, strong fallback where required.\nAre passkeys safe if my phone is stolen? Passkeys are protected behind device biometrics/PIN. Enable remote wipe and rotate critical credentials if a device is lost.\nHow are passkeys different from TOTP? TOTP sits on top of passwords and can be entered on phishing sites. Passkeys remove passwords and bind authentication to the real domain.\nDo I need a hardware key? Highly recommended for critical accounts as a robust backup, but not mandatory for every account.\nGetting Started Enable passkeys on your Google/Apple account. Turn on passkey support in 1Password or Bitwarden (if you use them). Add passkeys to your primary email, domain registrar, and work platforms. Store recovery codes offline. Add one hardware key if possible. Phase out SMS 2FA where a stronger alternative exists (auth app/security key). Key Takeaways Passkeys provide a practical improvement: fast, convenient, and phishing‑resistant logins. Start with your most important accounts, enable trustworthy sync, set up recovery paths, and keep strong 2FA as backup. You get shorter logins, lower risk, and less password‑management overhead—without the weak links of traditional passwords.\n","href":"/2025/08/what-are-passkeys-how-to-enable-google-apple-password-managers.html","title":"What Are Passkeys? How to Enable Them on Google, Apple, and Password Managers (2025 Guide)"},{"content":"Staying safe online is getting harder. Scammers use convincing emails, text messages, websites, and even mobile apps to trick people into giving away passwords, banking details, or installing malware. This plain-English guide explains the most common phishing signs, shows realistic (safe) examples, and gives you clear steps to protect yourself.\nWhat Is Phishing? Phishing is a social-engineering attack where criminals pretend to be a trusted brand, coworker, or service (bank, delivery company, marketplace, government agency) to make you click a link, open a file, or share sensitive information. Modern phishing blends good design with urgency (“Your account will be closed in 24 hours!”) so you act before thinking.\nQuick Warning: Dangerous Links and Apps Suspicious links can install malware or steal logins. Avoid clicking links from unexpected messages, even if they look official. Malicious apps (especially outside official stores) can steal SMS codes, read notifications, or take over your device. Shortened links (e.g., bit.ly), QR codes, and fake update pop-ups are common traps. Always verify the destination before proceeding. Common Signs of Phishing Emails Look for several red flags at the same time, not just one:\nMismatch sender and domain: The display name says “YourBank”, but the email is from notice@account-security.yourbank-support.example.com. Urgent or threatening tone: “Immediate action required”, “We detected unusual activity”, “Final warning”. Generic greeting: “Dear user” or “Dear customer” instead of your real name. Unexpected attachments: ZIP, PDF, HTML, or Office files asking to “enable content/macros”. Login links that don’t match the real domain: yourbank.secure-login.example.net instead of yourbank.com. Spelling or design inconsistencies: Wrong logo spacing, odd grammar, off-brand colors, or low-quality images. Requests for sensitive info: Passwords, OTP codes, card PIN, recovery codes—legitimate companies won’t ask these by email/DM. Fake Email Examples (Safe Text-Only) Example 1 — Delivery scam:\nSubject: Action required: Package on hold\n“We attempted to deliver your parcel. Confirm address and pay a small fee to release your package: hxxps://post-track-confirm[.]info/your-id”\nWhy it’s phishing: Delivery firms don’t ask for card details via generic links. The domain is unrelated to the real company.\nExample 2 — Bank alert:\nSubject: Suspicious sign-in blocked\n“Your account will be suspended. Verify now: hxxps://yourbank-login[.]secure-check[.]net”\nWhy it’s phishing: Real banks use their exact domain (e.g., yourbank.com) and don’t threaten suspension via email links.\nExample 3 — Workplace spear-phish:\nSubject: Updated payroll calendar Q3\n“See attached ‘Payroll_Q3.html’ and log in with your company email to view.”\nWhy it’s phishing: HTML attachments that ask you to log in are often credential harvesters.\nLink-Based Scams You’ll See Right Now Smishing (SMS) and messaging apps: Short texts with urgent links (“Your package fee is unpaid”) that open fake payment pages. QR phishing (QRishing): A QR code placed on posters or emails leading to a fake login portal. Treat QR codes like links—verify before scanning. Link shorteners: Hide destinations. Use a URL expander or long-press/hover to preview before opening. Punycode lookalikes: Domains that visually mimic real brands (e.g., rn vs m, or accented characters) but are different under the hood. Fake invoice or payment request: “See invoice” buttons leading to a login capture page. OAuth consent scams: “This app wants access to your email/drive.” If approved, attackers don’t need your password. Only grant access to verified apps. Malicious Apps and Fake Updates Android sideloading (APK): Installing apps from links or unofficial stores can grant malware broad permissions (SMS, accessibility, overlay) to intercept OTP codes or control the screen. iOS test builds and profiles: Attackers may push TestFlight invites or configuration profiles that enable risky settings. Only install from known developers. Browser extensions: Fake “coupon”, “PDF”, or “security” extensions can read every page you visit. Only use well-reviewed, publisher-verified extensions. Fake update pop-ups: “Your browser/Flash needs an update” banners that download malware. Update via system settings or official stores only. How to Stay Safe (Practical Checklist) Verify the domain before you click. Manually type the website or use your saved bookmark. Check for subtle typos or extra words (e.g., -secure, -verify, or unusual subdomains). Use a password manager. It auto-fills only on the correct domain, acting as a built-in phishing detector. Turn on 2FA—prefer authenticator apps or security keys over SMS. Security keys (FIDO2) block many phishing attempts by design. Never share OTP codes, recovery codes, or PINs—no legitimate support will ask for them. Preview links. On desktop, hover to see the full URL. On mobile, long-press to preview. Expand shortened links before opening. Install apps only from official stores. Disable “install unknown apps”. Review requested permissions—deny anything that looks excessive. Keep devices updated. Apply OS and app updates from official sources. Enable automatic updates. Use built-in protections: spam filters, Safe Browsing/SmartScreen, and device encryption. Consider enabling DNS filtering for families. Separate email addresses. Use one for banking/critical accounts, another for newsletters/shops to reduce exposure. Educate family and coworkers. Share examples, run quick simulations, and agree on a “call to verify” habit for money or data requests. What To Do If You Clicked Don’t panic—act methodically. If you entered a password, change it immediately on the real site and any other site where you reused it. Then enable 2FA. If you approved a suspicious app/extension, remove it and revoke access: check your account’s “connected apps” or “security” page. Scan your device with a trusted security tool. On mobile, uninstall unknown apps and review permissions (Accessibility, Device Admin). Watch your accounts for unusual activity (login alerts, forwarding rules, payment changes). Set up alerts if available. Report the phish: mark as spam/phishing in your email app. If it impersonates your bank or employer, notify them through official channels. For financial or identity risk, contact your bank, freeze cards if needed, and consider credit monitoring. For Website and Email Owners (Quick Wins) Email authentication: Set up SPF, DKIM, and DMARC with a “quarantine/reject” policy to reduce spoofing of your domain. Enforce MFA for admin panels, hosting, and email accounts. Prefer security keys for critical roles. Use a WAF/CDN with bot and phishing page detection; enable rate limits for login endpoints. Educate staff about spear-phishing and CEO fraud. Use out-of-band verification for payment or credential requests. Key Takeaways Phishing is about pressure and imitation. Slow down and verify. Links and apps can be dangerous—stick to official sources and check domains carefully. Password managers and security keys dramatically reduce risk. If you slip, reset credentials, revoke access, and monitor activity quickly. Stay cautious, share this guide with friends and family, and help others pause before they click.\n","href":"/2025/08/phishing-signs-fake-email-examples-how-to-avoid.html","title":"Phishing: Signs, Fake Email Examples, and How to Avoid Them (2025 Guide)"},{"content":"Looking to add login to your FastAPI app without pulling in a full auth service? Here’s a small, production‑friendly setup. We’ll build username/password authentication with the OAuth2 Password flow and JSON Web Tokens (JWTs) for stateless access. It uses Pydantic v2 for validation and SQLAlchemy 2.0 for persistence. You’ll hash passwords properly, create/verify tokens, protect routes, and test everything end‑to‑end.\nIf you’re deploying the finished app on Ubuntu with HTTPS, check the deployment guide: Deploy FastAPI on Ubuntu 24.04: Gunicorn + Nginx + Certbot .\nWhat you’ll build A minimal user model backed by SQLAlchemy 2.0 Password hashing using passlib[bcrypt] JWT access token creation and verification with python-jose OAuth2 Password flow login endpoint (/token) Protected routes using OAuth2PasswordBearer A simple current‑user dependency that decodes JWTs Prerequisites Python 3.10+ Basic FastAPI experience SQLite for demo (swap with PostgreSQL/MySQL in production) Best‑practice project structure Use a small but clear layout so your imports stay tidy as the app grows:\napp/ main.py core/ security.py db/ base.py session.py models/ user.py schemas/ user.py api/ deps.py routes/ auth.py users.py health.py Install dependencies Create and activate a virtual environment, then install dependencies:\npython3 -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install fastapi uvicorn sqlalchemy pydantic passlib[bcrypt] python-jose[cryptography] python-dotenv python-multipart Optional but recommended: manage secrets via a .env file during development.\nCreate a .env file in your project root:\ncat \u0026gt; .env \u0026lt;\u0026lt;\u0026#39;ENV\u0026#39; SECRET_KEY=$(openssl rand -hex 32) ACCESS_TOKEN_EXPIRE_MINUTES=30 ENV Where should .env live? Put .env in the project root (same level as app/). Run the app from the root so load_dotenv() finds it: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Add .env to .gitignore so it doesn’t get committed: .env If you sometimes run the app from a different working directory, you can load .env with an explicit path:\n# app/core/security.py (alternative) from pathlib import Path from dotenv import load_dotenv load_dotenv(Path(__file__).resolve().parents[2] / \u0026#34;.env\u0026#34;) .env.example and requirements.txt placement Keep both files at the project root for clarity and portability:\n. ├─ .env # not committed ├─ .env.example # committed, template for teammates/CI ├─ requirements.txt # pinned or curated dependencies └─ app/ ├─ core/ ├─ db/ ├─ models/ ├─ schemas/ ├─ api/ └─ main.py Suggested .env.example:\n# .env.example # Copy this file to .env and change the values as needed. SECRET_KEY=change-me-to-a-strong-random-value ACCESS_TOKEN_EXPIRE_MINUTES=30 # DATABASE_URL is optional here because the demo uses SQLite via app/db/session.py # For Postgres, uncomment and use your DSN: # DATABASE_URL=postgresql+psycopg://user:password@localhost:5432/mydb Pin dependencies with a requirements.txt (recommended):\nOption A — write a curated requirements.txt with compatible ranges:\nfastapi\u0026gt;=0.110,\u0026lt;1 uvicorn[standard]\u0026gt;=0.29,\u0026lt;1 sqlalchemy\u0026gt;=2.0,\u0026lt;3 pydantic\u0026gt;=2.5,\u0026lt;3 passlib[bcrypt]\u0026gt;=1.7,\u0026lt;2 python-jose[cryptography]\u0026gt;=3.3,\u0026lt;4 python-dotenv\u0026gt;=1.0,\u0026lt;2 python-multipart\u0026gt;=0.0.9,\u0026lt;1 Option B — pin exact versions from your current env:\npip freeze \u0026gt; requirements.txt Later, reproduce the env with:\npython3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt Database setup (SQLAlchemy 2.0) Create two files for database plumbing.\n# app/db/base.py from sqlalchemy.orm import DeclarativeBase class Base(DeclarativeBase): pass # app/db/session.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker SQLALCHEMY_DATABASE_URL = \u0026#34;sqlite:///./app.db\u0026#34; engine = create_engine( SQLALCHEMY_DATABASE_URL, connect_args={\u0026#34;check_same_thread\u0026#34;: False} ) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) def get_db(): db = SessionLocal() try: yield db finally: db.close() Models and schemas We’ll store users with username and a hashed password (never store plain passwords).\n# app/models/user.py from sqlalchemy import Integer, String from sqlalchemy.orm import Mapped, mapped_column from app.db.base import Base class User(Base): __tablename__ = \u0026#34;users\u0026#34; id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True) username: Mapped[str] = mapped_column(String, unique=True, index=True) hashed_password: Mapped[str] = mapped_column(String) Pydantic v2 schemas for reading/creating users:\n# app/schemas/user.py from pydantic import BaseModel class UserCreate(BaseModel): username: str password: str class UserRead(BaseModel): id: int username: str model_config = { \u0026#34;from_attributes\u0026#34;: True } class Token(BaseModel): access_token: str token_type: str = \u0026#34;bearer\u0026#34; Security helpers: hashing and JWT # app/core/security.py import os from datetime import datetime, timedelta, timezone from typing import Optional from jose import jwt from passlib.context import CryptContext from dotenv import load_dotenv load_dotenv() # load variables from .env if present pwd_context = CryptContext(schemes=[\u0026#34;bcrypt\u0026#34;], deprecated=\u0026#34;auto\u0026#34;) SECRET_KEY = os.getenv(\u0026#34;SECRET_KEY\u0026#34;, \u0026#34;change-this-in-env\u0026#34;) # override in production ALGORITHM = \u0026#34;HS256\u0026#34; ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv(\u0026#34;ACCESS_TOKEN_EXPIRE_MINUTES\u0026#34;, \u0026#34;30\u0026#34;)) def verify_password(plain_password: str, hashed_password: str) -\u0026gt; bool: return pwd_context.verify(plain_password, hashed_password) def hash_password(password: str) -\u0026gt; str: return pwd_context.hash(password) def create_access_token(subject: str, expires_delta: Optional[timedelta] = None) -\u0026gt; str: expire = datetime.now(timezone.utc) + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)) to_encode = {\u0026#34;sub\u0026#34;: subject, \u0026#34;exp\u0026#34;: expire} return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM) def decode_token(token: str) -\u0026gt; dict: return jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM]) API dependencies and routes Dependencies (current user) and small helper functions:\n# app/api/deps.py from fastapi import Depends, HTTPException, status from fastapi.security import OAuth2PasswordBearer from sqlalchemy.orm import Session from jose import JWTError from app.db.session import get_db from app.models.user import User from app.core.security import verify_password, decode_token oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\u0026#34;token\u0026#34;) def get_user_by_username(db: Session, username: str) -\u0026gt; User | None: return db.query(User).filter(User.username == username).first() def authenticate_user(db: Session, username: str, password: str) -\u0026gt; User | None: user = get_user_by_username(db, username) if not user or not verify_password(password, user.hashed_password): return None return user def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)) -\u0026gt; User: try: payload = decode_token(token) username: str | None = payload.get(\u0026#34;sub\u0026#34;) if username is None: raise HTTPException(status_code=401, detail=\u0026#34;Invalid token payload\u0026#34;) except JWTError: raise HTTPException(status_code=401, detail=\u0026#34;Invalid or expired token\u0026#34;) user = get_user_by_username(db, username) if not user: raise HTTPException(status_code=404, detail=\u0026#34;User not found\u0026#34;) return user Auth and user routes:\n# app/api/routes/auth.py from datetime import timedelta from fastapi import APIRouter, Depends, HTTPException, status from fastapi.security import OAuth2PasswordRequestForm from sqlalchemy.orm import Session from app.api.deps import authenticate_user from app.db.session import get_db from app.schemas.user import Token from app.core.security import create_access_token, ACCESS_TOKEN_EXPIRE_MINUTES router = APIRouter() @router.post(\u0026#34;/token\u0026#34;, response_model=Token) def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)): user = authenticate_user(db, form_data.username, form_data.password) if not user: raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Incorrect username or password\u0026#34;, headers={\u0026#34;WWW-Authenticate\u0026#34;: \u0026#34;Bearer\u0026#34;}, ) access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES) token = create_access_token(subject=user.username, expires_delta=access_token_expires) return {\u0026#34;access_token\u0026#34;: token, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;} # app/api/routes/users.py from fastapi import APIRouter, Depends, HTTPException from sqlalchemy.orm import Session from app.api.deps import get_current_user, get_user_by_username from app.db.session import get_db from app.models.user import User from app.schemas.user import UserCreate, UserRead from app.core.security import hash_password router = APIRouter() @router.post(\u0026#34;/users\u0026#34;, response_model=UserRead, status_code=201) def create_user(payload: UserCreate, db: Session = Depends(get_db)): exists = get_user_by_username(db, payload.username) if exists: raise HTTPException(status_code=400, detail=\u0026#34;Username already taken\u0026#34;) user = User(username=payload.username, hashed_password=hash_password(payload.password)) db.add(user) db.commit() db.refresh(user) return user @router.get(\u0026#34;/me\u0026#34;, response_model=UserRead) def read_me(current_user: User = Depends(get_current_user)): return current_user # app/api/routes/health.py from fastapi import APIRouter router = APIRouter() @router.get(\u0026#34;/healthz\u0026#34;) def healthz(): return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} FastAPI application entrypoint # app/main.py from fastapi import FastAPI from app.db.base import Base from app.db.session import engine from app.api.routes import auth, users, health app = FastAPI() # Create tables Base.metadata.create_all(bind=engine) # Mount routers app.include_router(auth.router, tags=[\u0026#34;auth\u0026#34;]) app.include_router(users.router, tags=[\u0026#34;users\u0026#34;]) app.include_router(health.router, tags=[\u0026#34;health\u0026#34;]) Try it out Run the app (from the project root):\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Option A — Swagger UI (easiest)\nOpen http://127.0.0.1:8000/docs POST /users to register a user (username + password) POST /token to get an access token Click “Authorize”, paste Bearer \u0026lt;the_token\u0026gt; GET /me to verify it returns your user Option B — curl (robust, copy‑paste safe) To avoid shell quoting/wrapping issues, send JSON from a file and use urlencoded helpers:\n# 1) Register user echo \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;alice\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;S3curePass!\u0026#34;}\u0026#39; \u0026gt; user.json curl -sS -i -X POST http://127.0.0.1:8000/users \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-binary @user.json # 2) Get token (form-url-encoded) TOKEN=$(curl -sS -X POST http://127.0.0.1:8000/token \\ -H \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ --data-urlencode \u0026#39;username=alice\u0026#39; \\ --data-urlencode \u0026#39;password=S3curePass!\u0026#39; \\ | jq -r .access_token) # 3) Call protected route curl -sS -i http://127.0.0.1:8000/me -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Notes\nIf you see “Invalid HTTP request received”, your curl command likely broke across lines or used smart quotes. Use the file + --data-binary approach above. If username is taken, register a different one (e.g., alice2). If you don’t have jq, you can copy the token manually from the JSON response, or extract it with Python: python -c \u0026quot;import sys,json;print(json.load(sys.stdin)['access_token'])\u0026quot;. Make sure python-multipart is installed; it’s required for the /token form endpoint. Option C — Postman (GUI)\nRegister (POST /users): Body: raw → JSON Content-Type: application/json Payload: { \u0026quot;username\u0026quot;: \u0026quot;alice\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;S3curePass!\u0026quot; } Login (POST /token): Body: x-www-form-urlencoded (not raw JSON) Keys: username=alice, password=S3curePass! Protected (GET /me): Authorization tab → Type: Bearer Token → paste the token (no quotes) Optional: import the Postman collection and use it directly: /postman/fastapi-jwt-auth.postman_collection.json.\nOption D — HTTPie (nice DX)\n# Register http POST :8000/users username=alice password=S3curePass! # Login http -f POST :8000/token username=alice password=S3curePass! | jq # Me TOKEN=$(http -f POST :8000/token username=alice password=S3curePass! | jq -r .access_token) http GET :8000/me \u0026#34;Authorization:Bearer $TOKEN\u0026#34; Production notes Secrets: Never hardcode SECRET_KEY. Read it from environment variables or a secret manager. Token lifetime: Adjust ACCESS_TOKEN_EXPIRE_MINUTES based on risk. Consider short‑lived access tokens with refresh tokens. HTTPS and reverse proxy: Put FastAPI behind Nginx/Traefik and enforce HTTPS. See the deployment guide: /2025/08/deploy-fastapi-ubuntu-24-04-gunicorn-nginx-certbot.html. Password policy: Enforce minimum length and complexity. Consider rate‑limiting login attempts. Database: For PostgreSQL, change the SQLALCHEMY_DATABASE_URL (e.g., postgresql+psycopg://user:pass@host/db). Use Alembic for migrations. CORS/SPA: If used from a browser SPA, configure CORS properly and store tokens securely. For cookie‑based auth, consider OAuth2PasswordBearer alternatives with httponly cookies and CSRF protection. Scopes/roles: FastAPI supports OAuth2 scopes; add them to tokens and check in dependencies. Testing: Use httpx.AsyncClient and pytest to cover login and protected routes. Systemd tip (prod): set env vars in the unit file instead of .env:\n[Service] Environment=\u0026#34;SECRET_KEY=your-strong-secret\u0026#34; Environment=\u0026#34;ACCESS_TOKEN_EXPIRE_MINUTES=30\u0026#34; Wrap‑up You now have a working JWT‑based login using the OAuth2 Password flow in FastAPI with Pydantic v2 and SQLAlchemy 2.0. The example is deliberately small but production‑leaning: it hashes passwords, issues signed tokens, and protects endpoints with a simple dependency. From here, add what you need—refresh tokens, roles/scopes, social logins, and migrations—then deploy behind Nginx with HTTPS.\n","href":"/2025/08/fastapi-jwt-auth-oauth2-password-flow-pydantic-v2-sqlalchemy-2.html","title":"FastAPI JWT Auth with OAuth2 Password Flow (Pydantic v2 + SQLAlchemy 2.0)"},{"content":"If you want to run AI models locally on Ubuntu 24.04 with a clean web UI, this guide is for you. We’ll install Ollama , pull a model, and use Open WebUI for a modern chat interface. The steps cover CPU‑only and NVIDIA GPU notes, optional systemd services, and practical troubleshooting.\nWhat you\u0026rsquo;ll do\nInstall Ollama on Ubuntu 24.04 (Noble) Pull and run a starter model (e.g., llama3.1) Run Open WebUI (Docker) and connect to Ollama Optionally enable NVIDIA GPU acceleration (CUDA) Set up systemd services and basic hardening tips Prerequisites\nUbuntu 24.04 LTS (Noble), sudo user 4GB RAM minimum (8GB+ recommended) Optional: NVIDIA GPU with recent drivers for acceleration Step 1: Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh Start (or restart) the service:\nsudo systemctl enable --now ollama sudo systemctl status ollama --no-pager Step 2: Pull a model and test Examples:\nollama pull llama3.1 ollama run llama3.1 In the REPL, type a prompt and press Enter. Exit with Ctrl+C.\nStep 3 (optional): NVIDIA GPU acceleration If you have an NVIDIA GPU, ensure drivers and CUDA libraries are present. A common path is to install the official NVIDIA driver from Ubuntu’s Additional Drivers tool, then add CUDA if needed. Minimal CLI install:\nsudo apt update sudo apt install -y ubuntu-drivers-common ubuntu-drivers devices # see recommended driver sudo ubuntu-drivers install # installs the recommended driver sudo reboot After reboot, verify:\nnvidia-smi Ollama will detect CUDA automatically when available.\nStep 4: Run Open WebUI (Docker) Open WebUI connects to Ollama via its API (default http://127.0.0.1:11434).\ndocker run -d \\ --name open-webui \\ -p 3000:8080 \\ -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\ -v open-webui:/app/backend/data \\ --restart unless-stopped \\ ghcr.io/open-webui/open-webui:latest Notes:\nOn Linux, host.docker.internal works on recent Docker. If it doesn\u0026rsquo;t, you can either: Add host gateway mapping: --add-host=host.docker.internal:host-gateway, or Use host networking: --network host and set -e OLLAMA_BASE_URL=http://127.0.0.1:11434. Visit http://SERVER_IP:3000 to access the UI. Step 5 (optional): Make Ollama listen on LAN By default, Ollama binds to localhost. To make it reachable (e.g., from other machines or containers without host network), create an override:\nsudo systemctl edit ollama Paste the following (then save):\n[Service] Environment=\u0026#34;OLLAMA_HOST=0.0.0.0:11434\u0026#34; Apply the change:\nsudo systemctl daemon-reload sudo systemctl restart ollama Secure with a firewall (UFW) and reverse proxy auth if exposing publicly. For example, allow only your management IP and HTTPS:\nsudo ufw allow 22/tcp sudo ufw allow 443/tcp sudo ufw allow from YOUR_IP to any port 11434 proto tcp # optional, management only sudo ufw enable Step 6: Persist and manage with systemd (Open WebUI option) If you prefer systemd over docker run, create a simple unit that uses Docker Compose or a raw Docker command. Example raw Docker service:\nsudo tee /etc/systemd/system/open-webui.service \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; [Unit] Description=Open WebUI (Docker) After=network-online.target docker.service Wants=network-online.target [Service] Restart=always TimeoutStartSec=0 ExecStartPre=/usr/bin/docker rm -f open-webui || true ExecStart=/usr/bin/docker run --name open-webui \\ -p 3000:8080 \\ -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\ -v open-webui:/app/backend/data \\ --restart unless-stopped \\ ghcr.io/open-webui/open-webui:latest ExecStop=/usr/bin/docker stop open-webui [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable --now open-webui Step 7 (optional): Reverse proxy (Nginx) If you want https://ai.example.com, set up an Nginx proxy and a Let’s Encrypt cert. See this guide for TLS issuance and hardening: Nginx + Certbot on Ubuntu 24.04 Then proxy ai.example.com → 127.0.0.1:3000.\nTroubleshooting\nPort 11434 in use: sudo lsof -i :11434 to find the process. Restart Ollama: sudo systemctl restart ollama. nvidia-smi missing or fails: ensure proper NVIDIA driver install; consider purging and reinstalling drivers. Open WebUI can’t reach Ollama: verify OLLAMA_BASE_URL, container networking, and that curl http://127.0.0.1:11434/api/tags returns JSON. Low RAM: try smaller models (e.g., phi3, qwen2:0.5b, or quantized variants) and keep a single model loaded. Uninstall Ollama:\nsudo systemctl disable --now ollama sudo rm -f /etc/systemd/system/ollama.service sudo rm -rf /usr/local/bin/ollama ~/.ollama sudo systemctl daemon-reload Open WebUI:\nsudo systemctl disable --now open-webui || true sudo rm -f /etc/systemd/system/open-webui.service sudo systemctl daemon-reload docker rm -f open-webui || true docker volume rm open-webui || true That’s it — you now have a local AI stack on Ubuntu 24.04 with Ollama and Open WebUI. Start lightweight models first, then scale up as your hardware allows.\n","href":"/2025/08/install-ollama-openwebui-ubuntu-24-04.html","title":"Install Ollama and Open WebUI on Ubuntu 24.04: Local AI (CPU/GPU)"},{"content":"If you reuse passwords, the internet is quietly stacking odds against you. One small site gets breached, your email and password leak, and attackers try the same combo on your email, banking, cloud storage—everywhere. That “I’ll remember it” system works right up until it doesn’t. The fix isn’t superhuman memory; it’s outsourcing the problem to a tool designed for it: a password manager.\nWhat a password manager actually does\nGenerates strong, unique passwords for every account Stores them encrypted, synced across your devices Auto‑fills only on the correct websites/apps Audits your vault for weak/reused/compromised passwords Holds secure notes, TOTP codes (in some apps), and sometimes passkeys The goal is simple: every account gets its own high‑entropy secret, and you never type or remember it again.\nRecommended apps (pick one that fits you)\nBitwarden (Free + Premium): Open‑source, great value, works on all platforms and browsers, supports organizations/families, and has excellent import/export. Paid tier adds TOTP, vault health, and more. A strong “default choice” for most people. 1Password (Paid): Polished UX, excellent security model (Secret Key + Master Password), great families features, best‑in‑class browser integration. If you want something that “just feels nice” and you’re okay paying, it’s hard to beat. Proton Pass (Free + Paid): From the Proton team (Mail/Drive/VPN). Simple, privacy‑centric, integrated with Proton ecosystem, passkey support. Good if you already live in Proton land. KeePassXC (Free, local): No cloud, full control. Great for people who want local files + their own sync (e.g., iCloud Drive, Syncthing). More hands‑on, but beloved by power users. Quick decision guide\nI want the best free cross‑platform option: Bitwarden I want the smoothest family experience: 1Password Families I want privacy + Proton ecosystem: Proton Pass I want local/no cloud: KeePassXC (plus a sync method you trust) How to migrate in a weekend (no overwhelm)\nChoose your manager and install across your devices Install the desktop app and browser extension (Chrome/Firefox/Safari/Edge). Install the mobile app. Enable biometrics for convenience (your face/fingerprint is only a local unlock—your master password still matters). Create a strong master password Use a long passphrase (5–6 random words, with separators). Length beats cleverness. Don’t reuse this anywhere else. Write it down once and store it in a safe or lockbox until you’ve memorized it. Turn on 2FA for your password manager Use an authenticator app (or hardware key) to protect your vault login. Import existing logins Export from your browser’s saved passwords (Chrome/Edge/Firefox/Safari) or your old manager. Import into the new vault. Then disable the browser’s built‑in password saving to avoid duplicates/confusion. Set your generator defaults 20+ characters, random, include symbols, avoid ambiguous characters. For sites that reject long passwords (it happens), drop to 16—never reuse an old one. Fix the crown jewels first Email, primary phone account, banking, cloud storage, Apple/Google/Microsoft IDs, domain registrars, developer platforms (GitHub, GitLab). Rotate these passwords immediately and enable 2FA. Enable passkeys where available Many sites now support passkeys (phishing‑resistant, no password to steal). Your manager or platform (iCloud Keychain, Google Password Manager) can store them. Use passkeys when you can; keep a password fallback when you must. Clean up and audit Run the vault health check (Bitwarden/1Password/Proton Pass) to spot reused/weak/compromised passwords. Replace a handful each day until the list is clean. Back up recovery options Save recovery codes for critical accounts (email, cloud, banks). Store them offline. If your manager offers an emergency kit (1Password), print it and keep it safe. New habit: let the manager do the typing On sign‑up screens, use “Generate password” and save. On login, auto‑fill from the extension or app. If you ever type a password by hand, it’s a smell. Simple rules that keep you safe long‑term\nOne master password to rule them all—never reuse it. 2FA everywhere it matters (email first, then banks, then social/dev tools). Unique passwords for every account, no exceptions. Don’t store 2FA codes in the same place as passwords for high‑value targets (email, banking). Split risk—use a separate authenticator or a security key. Treat SMS 2FA as the last resort; prefer authenticator apps or hardware keys. Be picky about browser auto‑fill prompts. If your manager doesn’t light up on a page, double‑check the URL. Phishing relies on rushed clicks. What about “my browser already saves passwords”?\nBrowsers have improved, but dedicated managers still win on cross‑platform support, breach monitoring, secure sharing, granular vaults, and recovery workflows. If you’re deep in one platform (e.g., only Apple devices), iCloud Keychain + passkeys is fine—but for most mixed setups, Bitwarden/1Password/Proton Pass give you fewer sharp edges.\nThreats this actually addresses\nCredential stuffing: Unique passwords stop attackers from reusing a leaked password elsewhere. Phishing: Managers auto‑fill only on the right domain; passkeys resist phishing by design. Weak/guessable passwords: Generators create high‑entropy secrets that aren’t in any wordlist. Things this does not solve (and what to do)\nMalware on your device: Keep OS and browser updated, don’t install sketchy extensions, and scan if anything feels off. Public Wi‑Fi interception: Use HTTPS (default) and a reputable VPN if you must use untrusted networks. Account recovery traps: Keep recovery emails/phones current; store backup codes offline. Quick Action Steps\nInstall a manager on desktop + phone Set a long master passphrase and enable 2FA on the vault Import your browser’s saved passwords Rotate the password on your email + cloud + bank Disable browser password saving, keep only the manager You don’t need to fix your entire digital life in one night—just stop the worst risk: reuse. Move your important accounts now, chip away at the rest, and let the tool do the heavy lifting. In a week, you’ll wonder how you ever lived without the “Generate” button.\n","href":"/2025/08/stop-reusing-passwords-practical-password-manager-guide.html","title":"Stop Reusing Passwords: A Practical Guide to Password Managers"},{"content":"Want to deploy FastAPI on Ubuntu 24.04 with a clean, secure, and maintainable setup? This guide walks you through running Gunicorn (ASGI server), Nginx (reverse proxy), and free HTTPS from Let’s Encrypt using Certbot. We’ll also use systemd so your service starts on boot and is easy to restart after updates.\nWhat you’ll build:\nA minimal FastAPI project structure Running the app with Gunicorn (Uvicorn worker) A systemd service for start/stop/restart Nginx reverse proxy to Gunicorn HTTPS (Certbot) with auto‑renewal UFW firewall (open 80/443), logs, and troubleshooting tips Prerequisites Ubuntu 24.04 server (sudo access) A domain pointing to the server (A/AAAA records) Python 3.10+ (Ubuntu 24.04 default is fine) Prepare the project structure on the server A tidy layout makes automation easier.\nsudo mkdir -p /opt/fastapi/app sudo adduser --system --group --home /opt/fastapi fastapi sudo chown -R fastapi:fastapi /opt/fastapi Create a virtualenv and install dependencies sudo apt update sudo apt install -y python3-venv sudo -u fastapi python3 -m venv /opt/fastapi/venv sudo -u fastapi /opt/fastapi/venv/bin/pip install --upgrade pip sudo -u fastapi /opt/fastapi/venv/bin/pip install fastapi uvicorn gunicorn Create a requirements.txt for easier dependency management:\nsudo -u fastapi tee /opt/fastapi/requirements.txt \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;REQS\u0026#39; fastapi==0.104.1 uvicorn[standard]==0.24.0 gunicorn==21.2.0 pydantic==2.5.0 REQS sudo -u fastapi /opt/fastapi/venv/bin/pip install -r /opt/fastapi/requirements.txt Create a minimal FastAPI app sudo -u fastapi tee /opt/fastapi/app/main.py \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;PY\u0026#39; from fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/healthz\u0026#34;) def healthz(): return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} @app.get(\u0026#34;/\u0026#34;) def root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from FastAPI on Ubuntu 24.04!\u0026#34;} PY Optional: quick local test\n# IMPORTANT: Change to app directory first to avoid permission errors cd /opt/fastapi sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Visit http://SERVER_IP:8000 to verify it works.\nCommon Error Fix: If you get PermissionError: Permission denied (os error 13) about [\u0026quot;/root\u0026quot;], it means uvicorn is trying to watch the wrong directory. Always cd /opt/fastapi first before running the command.\nRun with Gunicorn (ASGI) manually for testing sudo -u fastapi /opt/fastapi/venv/bin/gunicorn \\ -k uvicorn.workers.UvicornWorker \\ -w 2 \\ -b 0.0.0.0:8000 \\ app.main:app If logs look healthy and port 8000 serves requests (try curl http://SERVER_IP:8000/healthz or curl 127.0.0.1:8000/healthz), proceed to the service setup.\nChoose your process manager (pick one) =========================================== You need to choose how to run your FastAPI app as a service. Pick either Option A (systemd) or Option B (PM2):\nOption A: Create a systemd service for Gunicorn (Recommended) sudo tee /etc/systemd/system/fastapi.service \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;SERVICE\u0026#39; [Unit] Description=FastAPI app with Gunicorn After=network.target [Service] User=fastapi Group=fastapi WorkingDirectory=/opt/fastapi Environment=\u0026#34;PATH=/opt/fastapi/venv/bin\u0026#34; ExecStart=/opt/fastapi/venv/bin/gunicorn -k uvicorn.workers.UvicornWorker -w 2 -b 0.0.0.0:8000 app.main:app Restart=always RestartSec=5 [Install] WantedBy=multi-user.target SERVICE sudo systemctl daemon-reload sudo systemctl enable --now fastapi sudo systemctl status fastapi --no-pager Option B: Using PM2 (Alternative Process Manager) PM2 is great for Node.js but also works excellently with Python apps. It provides easy clustering, monitoring, and log management.\nInstall PM2:\n# Install Node.js and PM2 curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g pm2 Create PM2 ecosystem config:\nsudo -u fastapi tee /opt/fastapi/ecosystem.config.js \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;JS\u0026#39; module.exports = { apps: [{ name: \u0026#39;fastapi-app\u0026#39;, script: \u0026#39;/opt/fastapi/venv/bin/gunicorn\u0026#39;, args: \u0026#39;-k uvicorn.workers.UvicornWorker -w 2 -b 127.0.0.1:8000 app.main:app\u0026#39;, cwd: \u0026#39;/opt/fastapi\u0026#39;, instances: 1, autorestart: true, watch: false, max_memory_restart: \u0026#39;1G\u0026#39;, env: { NODE_ENV: \u0026#39;production\u0026#39; }, error_file: \u0026#39;/opt/fastapi/logs/err.log\u0026#39;, out_file: \u0026#39;/opt/fastapi/logs/out.log\u0026#39;, log_file: \u0026#39;/opt/fastapi/logs/combined.log\u0026#39;, time: true }] } JS # Create logs directory sudo -u fastapi mkdir -p /opt/fastapi/logs Start with PM2:\n# Start the application sudo -u fastapi pm2 start /opt/fastapi/ecosystem.config.js # Save PM2 process list sudo -u fastapi pm2 save # Setup PM2 to start on boot sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u fastapi --hp /opt/fastapi # Check status sudo -u fastapi pm2 status sudo -u fastapi pm2 logs fastapi-app PM2 Management Commands:\n# Restart app sudo -u fastapi pm2 restart fastapi-app # Stop app sudo -u fastapi pm2 stop fastapi-app # Monitor in real-time sudo -u fastapi pm2 monit # View logs sudo -u fastapi pm2 logs fastapi-app --lines 50 Systemd vs PM2 Comparison:\nFeature Systemd PM2 Built-in Ubuntu Yes Requires Node.js Memory usage Lower Higher (Node.js overhead) Monitoring UI Command line only pm2 monit dashboard Log management journalctl Built-in log rotation Clustering Manual setup Easy clustering Learning curve Moderate Easier Production ready Enterprise grade Battle tested Choose systemd if: You want minimal overhead and native Ubuntu integration. Choose PM2 if: You want easier monitoring, log management, and plan to scale horizontally.\nIMPORTANT: You must complete either Option A or Option B above before proceeding to Nginx setup!\nInstall and configure Nginx (reverse proxy) sudo apt install -y nginx sudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 60s; } } NGINX sudo ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Open the firewall (UFW) for HTTP/HTTPS sudo ufw allow \u0026#39;Nginx Full\u0026#39; # opens 80/tcp and 443/tcp sudo ufw allow 8000 # allow direct access to FastAPI for testing sudo ufw status Issue free HTTPS with Certbot sudo apt install -y certbot python3-certbot-nginx sudo certbot --nginx -d example.com -d www.example.com Certbot will configure the 443 server block and set up auto‑renewal. You can test renewal with:\nsudo certbot renew --dry-run Checks and monitoring Try: curl -I https://example.com/healthz App logs: journalctl -u fastapi -f Nginx logs: /var/log/nginx/access.log and error.log Production optimizations Add some production-ready configurations:\nGunicorn production config:\nsudo -u fastapi tee /opt/fastapi/gunicorn.conf.py \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;GUNICORN\u0026#39; # Gunicorn configuration file bind = \u0026#34;0.0.0.0:8000\u0026#34; worker_class = \u0026#34;uvicorn.workers.UvicornWorker\u0026#34; workers = 2 worker_connections = 1000 max_requests = 1000 max_requests_jitter = 100 preload_app = True keepalive = 2 timeout = 30 graceful_timeout = 30 GUNICORN # Update systemd service to use config file sudo sed -i \u0026#39;s|ExecStart=.*|ExecStart=/opt/fastapi/venv/bin/gunicorn -c /opt/fastapi/gunicorn.conf.py app.main:app|\u0026#39; /etc/systemd/system/fastapi.service sudo systemctl daemon-reload sudo systemctl restart fastapi Enhanced Nginx config with security headers:\nsudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name example.com www.example.com; # SSL configuration (handled by Certbot) # Security headers add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains\u0026#34; always; # Gzip compression gzip on; gzip_vary on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 60s; proxy_connect_timeout 60s; proxy_send_timeout 60s; # Buffer settings proxy_buffering on; proxy_buffer_size 4k; proxy_buffers 8 4k; } # Health check endpoint (no logging) location /healthz { proxy_pass http://127.0.0.1:8000; access_log off; } } NGINX sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Update and deployment strategies For systemd deployments:\n# Create deployment script sudo tee /opt/fastapi/deploy.sh \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;DEPLOY\u0026#39; #!/bin/bash set -e echo \u0026#34;Starting deployment...\u0026#34; # Pull latest code (if using git) cd /opt/fastapi sudo -u fastapi git pull origin main # Update dependencies sudo -u fastapi /opt/fastapi/venv/bin/pip install -r requirements.txt # Run any migrations or setup scripts here # sudo -u fastapi /opt/fastapi/venv/bin/python manage.py migrate # Test the app syntax sudo -u fastapi /opt/fastapi/venv/bin/python -c \u0026#34;import app.main\u0026#34; # Restart the service sudo systemctl restart fastapi # Wait a moment and check if it\u0026#39;s running sleep 5 sudo systemctl is-active --quiet fastapi \u0026amp;\u0026amp; echo \u0026#34;Deployment successful!\u0026#34; || echo \u0026#34;Deployment failed!\u0026#34; echo \u0026#34;Checking app health...\u0026#34; curl -f http://127.0.0.1:8000/healthz || echo \u0026#34;Health check failed\u0026#34; DEPLOY sudo chmod +x /opt/fastapi/deploy.sh For PM2 deployments:\n# PM2 deployment sudo -u fastapi pm2 stop fastapi-app cd /opt/fastapi sudo -u fastapi git pull origin main sudo -u fastapi /opt/fastapi/venv/bin/pip install -r requirements.txt sudo -u fastapi pm2 restart fastapi-app sudo -u fastapi pm2 save Zero-downtime deployment with PM2:\n# Update ecosystem.config.js for zero-downtime sudo -u fastapi tee /opt/fastapi/ecosystem.config.js \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;JS\u0026#39; module.exports = { apps: [{ name: \u0026#39;fastapi-app\u0026#39;, script: \u0026#39;/opt/fastapi/venv/bin/gunicorn\u0026#39;, args: \u0026#39;-c /opt/fastapi/gunicorn.conf.py app.main:app\u0026#39;, cwd: \u0026#39;/opt/fastapi\u0026#39;, instances: 2, // Multiple instances for zero-downtime exec_mode: \u0026#39;fork\u0026#39;, autorestart: true, watch: false, max_memory_restart: \u0026#39;1G\u0026#39;, kill_timeout: 5000, wait_ready: true, listen_timeout: 10000, env: { NODE_ENV: \u0026#39;production\u0026#39; } }] } JS # Reload with zero downtime sudo -u fastapi pm2 reload fastapi-app Monitoring and logging Basic monitoring with systemd:\n# Check service status sudo systemctl status fastapi # View logs in real-time sudo journalctl -u fastapi -f # Check resource usage sudo systemctl show fastapi --property=MainPID ps aux | grep $(sudo systemctl show fastapi --property=MainPID --value) Basic monitoring with PM2:\n# Real-time monitoring dashboard sudo -u fastapi pm2 monit # Check memory and CPU usage sudo -u fastapi pm2 list # View detailed process info sudo -u fastapi pm2 describe fastapi-app Log rotation setup:\n# For systemd logs sudo tee /etc/logrotate.d/fastapi \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;LOGROTATE\u0026#39; /var/log/nginx/access.log { daily missingok rotate 30 compress delaycompress notifempty create 644 www-data www-data postrotate sudo systemctl reload nginx \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 endscript } LOGROTATE Tips \u0026amp; troubleshooting Common issues and solutions:\nPermission denied error when testing uvicorn:\n# Wrong: This will cause permission error if run from /root sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload # Correct: Always change directory first cd /opt/fastapi sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 # Or run without reload flag for testing sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 Root cause: Uvicorn with --reload tries to watch the current working directory. If you run from /root, the fastapi user cannot access it.\n502 Bad Gateway:\n# Check if FastAPI service is running sudo systemctl status fastapi # or for PM2 sudo -u fastapi pm2 status # Check application logs sudo journalctl -u fastapi -f --since \u0026#34;10 minutes ago\u0026#34; # or for PM2 sudo -u fastapi pm2 logs fastapi-app --lines 50 # Test direct connection to Gunicorn curl -I http://127.0.0.1:8000/healthz # Or test from outside curl -I http://SERVER_IP:8000/healthz High memory usage:\n# Check memory consumption sudo systemctl show fastapi --property=MemoryCurrent # Restart if memory is too high sudo systemctl restart fastapi # For PM2 - automatic restart on high memory # Already configured with max_memory_restart: \u0026#39;1G\u0026#39; Performance tuning:\n# Adjust workers based on CPU cores # Rule of thumb: (2 x CPU cores) + 1 nproc # Check CPU cores # Update gunicorn workers in config sudo sed -i \u0026#39;s/workers = 2/workers = 3/\u0026#39; /opt/fastapi/gunicorn.conf.py sudo systemctl restart fastapi SSL certificate issues:\n# Test certificate renewal sudo certbot renew --dry-run # Check certificate expiry sudo certbot certificates # Manual renewal if needed sudo certbot renew --force-renewal -d example.com Security Best Practices:\nNon-root user (fastapi) Firewall (UFW) configured SSL/TLS encryption Security headers in Nginx No direct access to Gunicorn port Consider: fail2ban, regular security updates Consider: database connection encryption Consider: rate limiting in Nginx Recommended next steps Monitoring: Set up Prometheus + Grafana for advanced metrics Backup: Database backups, SSL certificate backups CI/CD: GitHub Actions for automated testing and deployment Load balancing: Multiple app servers behind Nginx for high availability Caching: Redis for session storage and caching Database: PostgreSQL with connection pooling (SQLAlchemy + asyncpg) Related articles:\nNginx + Certbot on Ubuntu 24.04 - SSL setup guide Install Docker on Ubuntu 24.04 - Containerized deployment option That\u0026rsquo;s it! You now have a production-ready FastAPI deployment on Ubuntu 24.04 with multiple process management options (systemd vs PM2), HTTPS encryption, and comprehensive monitoring. Choose the approach that best fits your infrastructure and scaling needs. Happy coding!\n","href":"/2025/08/deploy-fastapi-ubuntu-24-04-gunicorn-nginx-certbot.html","title":"Deploy FastAPI on Ubuntu 24.04: Gunicorn + Nginx + Certbot (HTTPS)"},{"content":"Want a free, trusted HTTPS certificate for your site on Ubuntu 24.04? This guide walks you through installing Nginx, opening the right firewall ports, issuing a free Let’s Encrypt certificate with Certbot, enabling automatic renewal, forcing HTTP→HTTPS redirects, and applying sane TLS settings. You’ll also see common troubleshooting steps and how to test your configuration. If you need to containerize your apps first, set up Docker here: Install Docker on Ubuntu 24.04: Post-Install, Rootless, and Compose v2 What you’ll do\nPoint your domain to your server via DNS (A/AAAA records) Install Nginx from Ubuntu repositories Allow HTTP/HTTPS through the firewall Install Certbot and issue a Let’s Encrypt certificate Auto-renew the certificate and verify renewal Redirect HTTP to HTTPS and harden TLS settings Test, troubleshoot, and (optionally) revoke/uninstall Prerequisites\nUbuntu 24.04 LTS (Noble) with sudo access A domain name (e.g., example.com) you control DNS A/AAAA records pointing to your server’s public IP Configure DNS Make sure your domain points to your server. At your DNS provider, set: A record: example.com → YOUR_IPV4 AAAA record: example.com → YOUR_IPV6 (optional) Optional: wildcard or subdomain records (e.g., www.example.com ) Propagation can take minutes to hours. You can check resolution with:\ndig +short example.com dig +short www.example.com Install Nginx sudo apt update sudo apt install -y nginx Validate Nginx is running:\nsystemctl status nginx --no-pager Open your server’s IP in a browser; you should see the default Nginx welcome page.\nOpen the firewall (UFW) If UFW is enabled, allow Nginx traffic: sudo ufw allow \u0026#39;Nginx Full\u0026#39; # opens 80/tcp and 443/tcp sudo ufw status If UFW is disabled, you can skip this step. For cloud providers, also ensure security groups allow ports 80 and 443.\nCreate a basic server block (optional but recommended) By default, Nginx serves the default site. Create a server block for your domain to keep things organized: sudo mkdir -p /var/www/example.com/html echo \u0026#39;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026#39; | sudo tee /var/www/example.com/html/index.html \u0026gt; /dev/null sudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; root /var/www/example.com/html; index index.html; location / { try_files $uri $uri/ =404; } } NGINX sudo ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Visit http://example.com to confirm it serves your content.\nInstall Certbot (recommended via snap) The Certbot team recommends snap for the latest version. sudo apt install -y snapd sudo snap install core; sudo snap refresh core sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot || true Obtain and install a certificate (Nginx plugin) Use the Nginx plugin to edit config and reload automatically: sudo certbot --nginx -d example.com -d www.example.com Follow the prompts (email, ToS). Choose the redirect option when asked so HTTP automatically redirects to HTTPS.\nAlternative: Webroot method (if you prefer manual control)\nsudo certbot certonly --webroot -w /var/www/example.com/html -d example.com -d www.example.com If you used webroot, add SSL directives to your server block and reload Nginx (see step 8 for TLS settings).\nAuto-renewal Snap installs a systemd timer for Certbot. Verify it: systemctl list-timers | grep certbot sudo certbot renew --dry-run Dry-run should complete without errors. Certificates renew automatically ~30 days before expiry.\nForce HTTP→HTTPS and apply TLS best practices If you didn’t choose the redirect option during Certbot run or you used webroot, update your Nginx config. A sane baseline (based on Mozilla’s “intermediate” profile) is: server { listen 80; listen [::]:80; server_name example.com www.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name example.com www.example.com; root /var/www/example.com/html; index index.html; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; ssl_session_timeout 1d; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers \u0026#39;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305\u0026#39;; ssl_prefer_server_ciphers off; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains; preload\u0026#34; always; add_header X-Content-Type-Options nosniff; add_header X-Frame-Options DENY; add_header Referrer-Policy no-referrer-when-downgrade; location / { try_files $uri $uri/ =404; } } Then test and reload:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Use SSL Labs (Qualys) to analyze: https://www.ssllabs.com/ssltest/ Canonical redirect (optional) If you want to force a single hostname (e.g., redirect www→apex), add a dedicated server block:\nserver { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.example.com; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; return 301 https://example.com$request_uri; } OCSP stapling (recommended) Reduce TLS handshake latency and improve scores with OCSP stapling:\nssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate /etc/letsencrypt/live/example.com/chain.pem; resolver 1.1.1.1 1.0.0.1 valid=300s; resolver_timeout 5s; Place these inside the TLS server block (port 443) after your ssl_certificate lines.\nCompression (performance) Enable gzip (widely available) for text assets:\ngzip on; gzip_comp_level 5; gzip_min_length 256; gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml; gzip_vary on; Note: Brotli offers better compression but may not be compiled by default in Ubuntu’s Nginx. If you install a Brotli-enabled build, you can use:\n# brotli on; # brotli_comp_level 5; # brotli_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml; Test your HTTPS setup Browser: go to https://example.com and inspect the lock icon CLI: curl -I https://example.com should return HTTP/2 200 (or 301 → 200 if redirecting from www) Check Nginx logs: /var/log/nginx/access.log and /var/log/nginx/error.log Troubleshooting\nDNS/Challenge failed: Ensure your A/AAAA records point to this server and port 80 is reachable from the internet. Temporarily disable any reverse proxy or CDN during issuance. Firewall blocks: Open ports 80 and 443 in UFW/security groups. nc -vz your.ip 80 from an external host can help verify reachability. Nginx conflicts: Run sudo nginx -t to find syntax errors or duplicated server_name blocks. Rate limits: Let’s Encrypt enforces rate limits. Use --dry-run for testing or wait before re-issuing. Webroot path mismatch: If using --webroot, ensure the -w path matches your server root and that Nginx serves /.well-known/acme-challenge/. Apt update/upgrade errors when installing snap/certbot Lihat: How to fix broken update error in Linux (Terminal) → /2023/11/how-to-fix-broken-update-error-in-linux.html Multiple sites tip\nUntuk beberapa domain, buat satu file di sites-available/ per domain. Hindari overlap server_name agar Certbot dan Nginx bisa memilih blok yang tepat. Renewal and maintenance tips\nCertificates renew automatically; review logs in /var/log/letsencrypt/. After major Nginx changes, run sudo certbot renew --dry-run to confirm hooks still work. Consider enabling OCSP stapling and caching for further optimization if you terminate high traffic. Revoke or uninstall (if needed) Revoke a cert (compromised key or domain transfer):\nsudo certbot revoke --cert-path /etc/letsencrypt/live/example.com/fullchain.pem Remove cert files:\nsudo certbot delete --cert-name example.com Remove Certbot (snap) and Nginx:\nsudo snap remove certbot sudo apt purge -y nginx* \u0026amp;\u0026amp; sudo apt autoremove -y That’s it—your site now serves a trusted HTTPS certificate with automatic renewal on Ubuntu 24.04. Enjoy the speed and security of Nginx + Let’s Encrypt!\n","href":"/2025/08/nginx-certbot-ubuntu-24-04-free-https.html","title":"Nginx + Certbot on Ubuntu 24.04: Free HTTPS with Let’s Encrypt"},{"content":"This guide shows how to install Docker Engine on Ubuntu 24.04 LTS (Noble Numbat), configure it for non-root use, enable optional rootless mode, and use Docker Compose v2. It also includes test commands, common troubleshooting tips, and how to uninstall cleanly. For securing your site with HTTPS, see: Nginx + Certbot on Ubuntu 24.04 What you’ll do\nAdd the official Docker repository for Ubuntu 24.04 (Noble) Install Docker Engine, Buildx, and Compose v2 plugins Run Docker as your regular user (without sudo) Optionally enable rootless Docker Verify with test containers and fix common errors Prerequisites\nFresh or updated Ubuntu 24.04 LTS (Noble) A user with sudo privileges Remove old Docker packages (if any) sudo apt remove -y docker docker-engine docker.io containerd runc || true Set up the Docker repository sudo apt update sudo apt install -y ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\ https://download.docker.com/linux/ubuntu $(. /etc/os-release; echo $VERSION_CODENAME) stable\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update Install Docker Engine, Buildx, and Compose v2 sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo systemctl enable --now docker Test Docker (root) sudo docker run --rm hello-world You should see a confirmation message.\nPost-install: run Docker without sudo sudo usermod -aG docker $USER newgrp docker # reload group membership for current shell docker run --rm hello-world If the second command works without sudo, your user is set up correctly.\nDocker Compose v2 docker compose is included as a plugin. Check the version: docker compose version Example usage:\ncat \u0026gt; compose.yaml \u0026lt;\u0026lt;\u0026#39;YAML\u0026#39; services: web: image: nginx:alpine ports: - \u0026#34;8080:80\u0026#34; YAML docker compose up -d docker compose ps docker compose down 6a) Verify Buildx docker buildx is the modern builder with multi-platform support and advanced caching.\ndocker buildx version You should see a version string. Optionally, try a quick build to confirm the builder is healthy:\ndocker buildx bake --print 2\u0026gt;/dev/null || echo \u0026#34;Buildx is installed and ready.\u0026#34; Optional: Rootless Docker Rootless mode runs the Docker daemon and containers without root privileges. Good for tighter isolation (with some feature limitations). Install requirements and set up:\nsudo apt install -y uidmap dbus-user-session dockerd-rootless-setuptool.sh install Start and enable the user service:\nsystemctl --user start docker systemctl --user enable docker # Keep user services running after logout sudo loginctl enable-linger $USER Use the rootless daemon by pointing the client to your user socket (usually done automatically by the setup tool):\nexport DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock docker info | grep -i rootless Notes on rootless mode\nSome features (e.g., privileged containers, low ports \u0026lt;1024) are restricted. For Kubernetes-in-Docker or system-wide networking, classic (rootful) Docker is recommended. Troubleshooting Permission denied on /var/run/docker.sock Run: groups and ensure docker is listed. If not, run sudo usermod -aG docker $USER then re-login or newgrp docker. Network issues pulling images Check DNS and proxy settings. Try docker pull alpine and ping registry-1.docker.io (may be blocked by firewall). Cannot connect to the Docker daemon Check service: systemctl status docker (rootful) or systemctl --user status docker (rootless). Compose command not found Ensure you installed docker-compose-plugin and run docker compose (space), not docker-compose. Apt update/upgrade errors during install Lihat: How to fix broken update error in Linux (Terminal) → /2023/11/how-to-fix-broken-update-error-in-linux.html 8a) Maintenance \u0026amp; Cleanup (disk usage) Over time, images/layers can consume disk space. Inspect usage and prune carefully:\ndocker system df docker image prune -f # remove unused images (dangling) docker container prune -f # remove stopped containers docker volume prune -f # remove unused volumes docker builder prune -f # remove unused build cache Tip: omit -f to get a prompt before deleting. Review before pruning on production hosts.\nUninstall Docker completely sudo apt purge -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras sudo rm -rf /var/lib/docker /var/lib/containerd sudo rm -f /etc/apt/sources.list.d/docker.list /etc/apt/keyrings/docker.gpg sudo apt autoremove -y Security note\nMembers of the docker group can effectively escalate privileges on the host (they can start containers with access to the filesystem). Only add trusted users to the docker group. That’s it! You now have Docker Engine, Compose v2, and (optionally) rootless mode on Ubuntu 24.04.\n","href":"/2025/08/install-docker-on-ubuntu-24-04-compose-v2-rootless.html","title":"Install Docker on Ubuntu 24.04: Post-Install, Rootless, and Compose v2"},{"content":"In modern web applications, storing and retrieving data from a database is a fundamental requirement. Go provides a low-level database/sql package, but using it directly can be verbose and repetitive. Thankfully, sqlx extends database/sql by adding useful features like struct scanning and named queries, making database operations in Go much easier.\nIn this article, we’ll walk through how to connect a Go application to a PostgreSQL database using sqlx, and how to perform basic CRUD operations.\nWhat is sqlx? sqlx is a Go library that enhances the standard database/sql by making it easier to work with structs and common query patterns. It\u0026rsquo;s widely used for developers who want more control and performance without jumping into full ORMs.\nInstall sqlx with:\ngo get github.com/jmoiron/sqlx You also need the PostgreSQL driver:\ngo get github.com/lib/pq Connect to PostgreSQL To connect to a PostgreSQL database, you need to provide a connection string that includes the database name, user, password, host, and port. Here’s how to set up a basic connection using sqlx:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) var db *sqlx.DB func main() { dsn := \u0026#34;user=postgres password=yourpassword dbname=mydb sslmode=disable\u0026#34; var err error db, err = sqlx.Connect(\u0026#34;postgres\u0026#34;, dsn) if err != nil { log.Fatalln(err) } fmt.Println(\u0026#34;Connected to PostgreSQL!\u0026#34;) } Make sure to replace yourpassword and mydb with your actual PostgreSQL credentials and database name.\nCreate a Struct and Table Create a table in PostgreSQL:\nCREATE TABLE users ( id SERIAL PRIMARY KEY, name TEXT NOT NULL, age INT NOT NULL ); Next, define a Go struct that matches the table schema:\ntype User struct { ID int `db:\u0026#34;id\u0026#34;` Name string `db:\u0026#34;name\u0026#34;` Age int `db:\u0026#34;age\u0026#34;` } Insert Data To insert data into the users table, you can use the NamedExec method provided by sqlx, which allows you to use named parameters in your SQL queries:\nfunc createUser(name string, age int) error { user := User{Name: name, Age: age} query := `INSERT INTO users (name, age) VALUES (:name, :age)` _, err := db.NamedExec(query, user) return err } Query Data To retrieve data from the users table, you can use the Select method, which scans the results into a slice of structs:\nfunc getUsers() ([]User, error) { var users []User query := `SELECT * FROM users` err := db.Select(\u0026amp;users, query) return users, err } Update Data To update a user\u0026rsquo;s information, you can use the NamedExec method again:\nfunc updateUser(id int, name string, age int) error { user := User{ID: id, Name: name, Age: age} query := `UPDATE users SET name = :name, age = :age WHERE id = :id` _, err := db.NamedExec(query, user) return err } Delete Data To delete a user from the users table, you can use the Exec method:\nfunc deleteUser(id int) error { query := `DELETE FROM users WHERE id = $1` _, err := db.Exec(query, id) return err } Putting It All Together Here’s a complete example that includes connecting to the database, creating a user, retrieving users, updating a user, and deleting a user:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) type User struct { ID int `db:\u0026#34;id\u0026#34;` Name string `db:\u0026#34;name\u0026#34;` Age int `db:\u0026#34;age\u0026#34;` } var db *sqlx.DB func main() { dsn := \u0026#34;user=postgres password=yourpassword dbname=mydb sslmode=disable\u0026#34; var err error db, err = sqlx.Connect(\u0026#34;postgres\u0026#34;, dsn) if err != nil { log.Fatalln(err) } fmt.Println(\u0026#34;Connected to PostgreSQL!\u0026#34;) // Create a user if err := createUser(\u0026#34;Alice\u0026#34;, 30); err != nil { log.Println(\u0026#34;Error creating user:\u0026#34;, err) } // Get users users, err := getUsers() if err != nil { log.Println(\u0026#34;Error getting users:\u0026#34;, err) } else { fmt.Println(\u0026#34;Users:\u0026#34;, users) } // Update a user if err := updateUser(1, \u0026#34;Alice Smith\u0026#34;, 31); err != nil { log.Println(\u0026#34;Error updating user:\u0026#34;, err) } // Delete a user if err := deleteUser(1); err != nil { log.Println(\u0026#34;Error deleting user:\u0026#34;, err) } } Best Practices Use Named Parameters: Named parameters make your queries more readable and maintainable. Error Handling: Always check for errors after executing queries to handle any issues gracefully. Connection Pooling: sqlx uses the database/sql package under the hood, which supports connection pooling. Make sure to configure the pool size according to your application\u0026rsquo;s needs. Migrations: Use a migration tool like golang-migrate to manage your database schema changes. Environment Variables: Store sensitive information like database credentials in environment variables or a configuration file, not hard-coded in your source code. Close the Database Connection Gracefully: Ensure you close the database connection when your application exits to avoid resource leaks. Conclusion sqlx is a powerful tool for interacting with PostgreSQL in Go. It keeps your code clean while avoiding the overhead of a full ORM. You’ve now seen how to connect to PostgreSQL, run basic CRUD operations, and structure your DB code using sqlx.\nIn the next article, we’ll go further by integrating this into a REST API and later explore GORM for higher-level abstraction.\nHappy coding!\n","href":"/2025/05/connecting-postgresql-in-go-using-sqlx.html","title":"Connecting to PostgreSQL in Go using sqlx"},{"content":"When you start building larger applications in Go, having a clean and maintainable project structure is essential. Unlike some other languages or frameworks that enforce certain patterns, Go gives you a lot of freedom in how you organize your code. While this is powerful, it can also lead to messy projects if not handled carefully.\nIn this guide, we\u0026rsquo;ll explore how to structure Go projects following clean architecture principles and best practices that many professional Go developers use.\nWhy Project Structure Matters in Go A good project structure will help you:\nMake your code easier to read and navigate. Make testing and maintenance easier. Separate concerns cleanly (API, service, data access, domain logic). Prepare your code for scaling and collaboration. Go doesn\u0026rsquo;t have a strict convention, but the community has adopted patterns that work well, especially for building web APIs, microservices, or CLI tools.\nBasic Go Project Structure Let\u0026rsquo;s start with a simple example of a Go project structure:\nmy-go-project/ ├── cmd/ │ └── myapp/ │ └── main.go ├── internal/ │ ├── ... ├── pkg/ │ ├── ... ├── go.mod ├── go.sum └── README.md Directory Breakdown cmd/\nThis directory contains the entry points for your application. Each subdirectory under cmd/ represents a different executable. For example, myapp/ could be the main application, while myapp-cli/ could be a command-line interface for the same application.\ninternal/\nThis directory contains application code that is not meant to be used by external applications. It can include business logic, data access, and other components that are specific to your application.\npkg/\nThis directory contains code that can be used by other applications. It can include libraries, utilities, and shared components that are reusable across different projects.\ngo.mod\nThis file defines the module and its dependencies. It is created when you run go mod init.\ngo.sum\nThis file contains the checksums of the dependencies listed in go.mod. It ensures that the same versions of dependencies are used across different environments.\nREADME.md\nThis file provides documentation for your project, including how to install, run, and use it.\nClean Architecture Approach (Recommended for Medium/Large Apps) For larger applications, it\u0026rsquo;s beneficial to adopt a clean architecture approach. This means organizing your code into layers that separate concerns and make it easier to test and maintain.\nSuggested structure:\nmy-go-project/ ├── cmd/ │ └── myapp/ │ └── main.go ├── internal/ │ ├── app/ │ │ ├── service/ │ │ ├── handler/ │ │ └── repository/ │ ├── domain/ │ │ ├── model/ │ │ └── service/ │ └── infrastructure/ │ ├── db/ │ ├── api/ │ └── config/ ├── pkg/ │ ├── utils/ │ └── middleware/ ├── go.mod ├── go.sum └── README.md Directory Breakdown app/\nContains the application logic, including services, handlers, and repositories. This is where the core of your application lives.\nservice/ Contains business logic and service implementations.\nhandler/ Contains HTTP handlers or gRPC handlers that interact with the outside world.\nrepository/ Contains data access code, such as database queries or API calls.\ndomain/ Contains domain models and services. This is where you define your core business entities and their behaviors.\nmodel/ Contains the domain models, which represent the core entities of your application.\nservice/ Contains domain services that encapsulate business logic related to the domain models.\ninfrastructure/\nContains code related to external systems, such as databases, APIs, and configuration.\ndb/ Contains database-related code, such as migrations and connection management.\napi/ Contains code related to external APIs, such as clients or adapters.\nconfig/ Contains configuration files and code for loading configurations.\npkg/\nContains reusable code that can be shared across different projects. This can include utility functions, middleware, and other shared components.\nutils/\nContains utility functions and helpers that can be used throughout the project.\nmiddleware/\nContains middleware functions for HTTP servers, such as logging, authentication, and error handling.\ngo.mod\nDefines the module and its dependencies.\ngo.sum\nContains the checksums of the dependencies listed in go.mod.\nREADME.md\nProvides documentation for your project.\nThis approach makes it easier to swap your database, refactor your API layer, or even reuse your business logic in different contexts.\nConclusion Structuring your Go projects effectively is crucial for maintainability and scalability. By following clean architecture principles and best practices, you can create a project structure that is easy to navigate, test, and extend.\nThis guide provides a solid foundation for structuring your Go projects, whether you\u0026rsquo;re building a simple CLI tool or a complex web application. Remember that the best structure is one that fits your specific needs and team preferences, so feel free to adapt these suggestions as necessary.\nBy following these guidelines, you\u0026rsquo;ll be well on your way to creating clean, maintainable, and scalable Go projects that are easy to work with and understand.\nHappy coding!\n","href":"/2025/05/structuring-go-projects-clean-architecture.html","title":"Structuring Go Projects: Clean Project Structure and Best Practices"},{"content":"Building a REST API in Go is one of the most practical ways to learn how Go handles HTTP servers, JSON , and struct-based logic. In this tutorial, you’ll learn how to create a simple RESTful API using the standard net/http package—without using any third-party frameworks. This is a great starting point before moving to more complex architectures.\nIn this guide, we’ll create a simple API for managing books. Each book will have an ID, title, and author.\nWhat You’ll Learn How to create HTTP server routes in Go How to handle GET, POST, PUT, and DELETE requests How to encode and decode JSON data How to organize handlers and write clean code Step 1: Define a Book Struct package main type Book struct { ID string `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } We’ll use this struct to store data in memory.\nStep 2: Step 2: Create a Global Book Slice var books = []Book{ {ID: \u0026#34;1\u0026#34;, Title: \u0026#34;Go Basics\u0026#34;, Author: \u0026#34;John Doe\u0026#34;}, {ID: \u0026#34;2\u0026#34;, Title: \u0026#34;Mastering Go\u0026#34;, Author: \u0026#34;Jane Smith\u0026#34;}, } Step 3: Create Handlers Get All Books func getBooks(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(books) } Get a Single Book func getBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for _, book := range books { if book.ID == id { json.NewEncoder(w).Encode(book) return } } http.NotFound(w, r) } Create a New Book func createBook(w http.ResponseWriter, r *http.Request) { var book Book json.NewDecoder(r.Body).Decode(\u0026amp;book) books = append(books, book) w.WriteHeader(http.StatusCreated) json.NewEncoder(w).Encode(book) } Update a Book func updateBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for i, book := range books { if book.ID == id { json.NewDecoder(r.Body).Decode(\u0026amp;books[i]) w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(books[i]) return } } http.NotFound(w, r) } Delete a Book func deleteBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for i, book := range books { if book.ID == id { books = append(books[:i], books[i+1:]...) w.WriteHeader(http.StatusNoContent) return } } http.NotFound(w, r) } Step 4: Set Up Routes func main() { http.HandleFunc(\u0026#34;/books\u0026#34;, getBooks) http.HandleFunc(\u0026#34;/books/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { switch r.Method { case http.MethodGet: getBook(w, r) case http.MethodPost: createBook(w, r) case http.MethodPut: updateBook(w, r) case http.MethodDelete: deleteBook(w, r) default: http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) } }) fmt.Println(\u0026#34;Server running on http://localhost:8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Step 5: Run the Server To run the server, save your code in a file named main.go and execute the following command in your terminal:\ngo run main.go You should see the message Server running on http://localhost:8080. You can now test your API using tools like Postman or curl.\nStep 6: Test the API Get All Books curl -X GET http://localhost:8080/books Get a Single Book curl -X GET http://localhost:8080/books/1 Create a New Book curl -X POST http://localhost:8080/books \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Learning Go\u0026#34;, \u0026#34;author\u0026#34;:\u0026#34;Alice Johnson\u0026#34;}\u0026#39; Update a Book curl -X PUT http://localhost:8080/books/1 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Go Basics Updated\u0026#34;, \u0026#34;author\u0026#34;:\u0026#34;John Doe\u0026#34;}\u0026#39; Delete a Book curl -X DELETE http://localhost:8080/books/1 Conclusion Congratulations! You’ve built a simple REST API in Go using the net/http package. This is just the beginning; you can extend this API by adding features like authentication, database integration, and more. Feel free to explore the Go documentation and other resources to deepen your understanding of Go and RESTful APIs.\nIf you have any questions or need further assistance, don’t hesitate to ask. Happy coding!\nAdditional Resources Go Documentation Go by Example Building Web Applications in Go Repository ","href":"/2025/05/how-to-build-rest-api-in-go-using-net-http.html","title":"How to Build a REST API in Go using net/http"},{"content":"JSON (JavaScript Object Notation) is a widely used data format in APIs and web applications. Go provides strong support for JSON through the standard encoding/json package. In this article, you’ll learn how to parse JSON into structs, generate JSON from Go data, use struct tags, and work with nested or dynamic structures.\nIn this article, you’ll learn:\nHow to encode Go structs to JSON How to decode JSON into Go structs Using JSON tags to customize field names Working with maps and dynamic JSON Handling nested JSON structures Best practices and error handling Encoding Structs to JSON Use json.Marshal to convert Go structs into JSON strings:\ntype User struct { Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } func main() { user := User{\u0026#34;Alice\u0026#34;, \u0026#34;alice@example.com\u0026#34;, 30} jsonData, err := json.Marshal(user) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } Decoding JSON into Structs Use json.Unmarshal to parse JSON into a struct:\nvar jsonInput = []byte(`{\u0026#34;name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;bob@example.com\u0026#34;,\u0026#34;age\u0026#34;:25}`) var user User err := json.Unmarshal(jsonInput, \u0026amp;user) if err != nil { log.Fatal(err) } fmt.Println(user.Name, user.Email, user.Age) Using Struct Tags By default, Go uses struct field names as JSON keys. Use tags to customize:\ntype Product struct { ID int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Price float64 `json:\u0026#34;price\u0026#34;` } Working with Maps and Dynamic JSON Use map[string]interface{} when the structure is not fixed:\nvar data = []byte(`{\u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34;,\u0026#34;code\u0026#34;:200}`) var result map[string]interface{} err := json.Unmarshal(data, \u0026amp;result) if err != nil { log.Fatal(err) } fmt.Println(result[\u0026#34;status\u0026#34;], result[\u0026#34;code\u0026#34;]) Nested JSON Example type Address struct { City string `json:\u0026#34;city\u0026#34;` Country string `json:\u0026#34;country\u0026#34;` } type Employee struct { Name string `json:\u0026#34;name\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` } JSON:\n{ \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Jakarta\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Indonesia\u0026#34; } } Encode JSON to File f, err := os.Create(\u0026#34;data.json\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() json.NewEncoder(f).Encode(user) Decode JSON from File f, err := os.Open(\u0026#34;data.json\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() json.NewDecoder(f).Decode(\u0026amp;user) Best Practices Always handle encoding/decoding errors Use struct tags for clean JSON output Validate incoming JSON before using Use omitempty tag to skip empty fields Conclusion Working with JSON in Go is simple, powerful, and type-safe. Whether you\u0026rsquo;re building APIs, reading config files, or exchanging data between systems, the encoding/json package gives you everything you need.\nNext, we’ll dive into building a REST API in Go using net/http.\nHappy coding!\n","href":"/2025/04/working-with-json-in-go-encode-decode.html","title":"Working with JSON in Go: Encode, Decode, and Tag Structs"},{"content":"In Go, file handling is straightforward and powerful. You can create, read, write, and manage files using standard packages like os, io, and ioutil (deprecated but still common). Understanding how to work with files is essential when building CLI tools, web servers, or any application that deals with local data.\nIn this article, you’ll learn:\nHow to create and write to a file How to read a file Appending data to files Working with directories Checking if a file exists Best practices and error handling Creating and Writing to a File To create and write content to a file:\nfunc main() { content := []byte(\u0026#34;Hello, file!\u0026#34;) err := os.WriteFile(\u0026#34;example.txt\u0026#34;, content, 0644) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;File written successfully\u0026#34;) } os.WriteFile creates the file if it doesn\u0026rsquo;t exist and replaces it if it does.\nReading a File To read the entire content of a file:\nfunc main() { data, err := os.ReadFile(\u0026#34;example.txt\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;File content:\u0026#34;, string(data)) } Appending to a File If you want to add content to an existing file without overwriting it:\nfunc main() { f, err := os.OpenFile(\u0026#34;example.txt\u0026#34;, os.O_APPEND|os.O_WRONLY, 0644) if err != nil { log.Fatal(err) } defer f.Close() if _, err := f.WriteString(\u0026#34;\\nThis is appended.\u0026#34;); err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Appended successfully\u0026#34;) } Working with Directories Create a new folder: err := os.Mkdir(\u0026#34;myfolder\u0026#34;, 0755) Create nested folders: err := os.MkdirAll(\u0026#34;path/to/folder\u0026#34;, 0755) List files in a folder: files, err := os.ReadDir(\u0026#34;.\u0026#34;) for _, file := range files { fmt.Println(file.Name()) } Check if a File Exists func fileExists(filename string) bool { _, err := os.Stat(filename) return !os.IsNotExist(err) } Deleting a File or Folder err := os.Remove(\u0026#34;example.txt\u0026#34;) // delete file err := os.RemoveAll(\u0026#34;path/to/folder\u0026#34;) // delete folder and contents Best Practices Always handle file errors (file not found, permissions) Use defer f.Close() after opening files Use os.ReadFile and os.WriteFile for simple tasks Use buffered I/O (like bufio) for large files Conclusion File handling in Go is clean and efficient. Whether you\u0026rsquo;re reading logs, saving data, or managing folders, the standard library provides everything you need. Understanding how to work with files opens the door to building robust and real-world applications in Go.\nNext, we’ll look into working with JSON in Go — another essential skill for building APIs and storing structured data.\nHappy coding!\n","href":"/2025/04/file-handling-in-go-read-write-and.html","title":"File Handling in Go: Read, Write, and Manage Files"},{"content":"When you write concurrent programs in Go, multiple goroutines may try to access and modify the same data at the same time. Without proper synchronization, this leads to race conditions, bugs, or crashes. Go provides tools like sync.Mutex, sync.RWMutex, and sync.Once to safely share data across goroutines.\nIn this article, you’ll learn:\nWhat race conditions are and how to avoid them How to use sync.Mutex to protect data Using sync.RWMutex for read-write access How sync.Once ensures code runs only once Real-world examples and best practices What Is a Race Condition? A race condition happens when two or more goroutines access the same variable at the same time, and at least one of them is modifying it. This can cause unexpected behavior or corrupted data.\nYou can detect race conditions using:\ngo run -race main.go Using sync.Mutex sync.Mutex is a mutual exclusion lock. Only one goroutine can hold the lock at a time. Use Lock() before accessing shared data, and Unlock() after.\ntype Counter struct { mu sync.Mutex value int } func (c *Counter) Increment() { c.mu.Lock() defer c.mu.Unlock() c.value++ } func (c *Counter) Value() int { c.mu.Lock() defer c.mu.Unlock() return c.value } Using sync.RWMutex sync.RWMutex allows multiple readers or one writer. It\u0026rsquo;s useful when reads are frequent but writes are rare.\ntype SafeMap struct { mu sync.RWMutex m map[string]string } func (s *SafeMap) Get(key string) string { s.mu.RLock() defer s.mu.RUnlock() return s.m[key] } func (s *SafeMap) Set(key, value string) { s.mu.Lock() defer s.mu.Unlock() s.m[key] = value } Using sync.Once sync.Once guarantees that a piece of code is only executed once, even if called from multiple goroutines. This is commonly used to initialize shared resources.\nvar once sync.Once func initialize() { fmt.Println(\u0026#34;Initialization done\u0026#34;) } func main() { for i := 0; i \u0026lt; 5; i++ { go func() { once.Do(initialize) }() } time.Sleep(time.Second) } Real-World Example: Safe Counter type SafeCounter struct { mu sync.Mutex val int } func (sc *SafeCounter) Add() { sc.mu.Lock() sc.val++ sc.mu.Unlock() } func main() { var sc SafeCounter var wg sync.WaitGroup for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) go func() { sc.Add() wg.Done() }() } wg.Wait() fmt.Println(\u0026#34;Final count:\u0026#34;, sc.val) } Best Practices Always use defer Unlock() right after Lock() Keep the locked section as short as possible Use RWMutex when many goroutines only need to read Use sync.Once to initialize global/shared data Test with go run -race to catch race conditions Conclusion Synchronization is key to building correct concurrent programs. By using sync.Mutex, sync.RWMutex, and sync.Once, you can ensure that your goroutines work together safely without corrupting shared data.\nHappy coding!\n","href":"/2025/04/synchronizing-goroutines-in-go-using.html","title":"Synchronizing Goroutines in Go: Using sync.Mutex and sync.Once"},{"content":"As your Go applications become more concurrent and complex, you\u0026rsquo;ll need a way to manage the lifecycle of your goroutines—especially when you want to cancel them, set timeouts, or propagate deadlines. This is where the context package comes in. It\u0026rsquo;s the idiomatic way in Go to control concurrent processes gracefully and reliably.\nIn this article, you’ll learn:\nWhat context is and why it’s important Using context.Background() and context.TODO() How to cancel a goroutine with context.WithCancel() How to set a timeout or deadline How to check if a context is done Real-world examples and best practices What Is Context? The context package provides a way to carry deadlines, cancellation signals, and other request-scoped values across function boundaries and between goroutines.\nIt helps you:\nCancel long-running tasks Set deadlines or timeouts Propagate cancellation across multiple goroutines Starting Point: Background and TODO ctx := context.Background() // root context, no cancel/timeout ctx := context.TODO() // use when unsure (placeholder) Cancelling a Goroutine: WithCancel You can use context.WithCancel to manually stop a goroutine:\nfunc doWork(ctx context.Context) { for { select { case \u0026lt;-ctx .done=\u0026#34;\u0026#34; :=\u0026#34;context.WithCancel(context.Background())\u0026#34; cancel=\u0026#34;\u0026#34; canceled=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; context=\u0026#34;\u0026#34; ctx=\u0026#34;\u0026#34; default:=\u0026#34;\u0026#34; dowork=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; main=\u0026#34;\u0026#34; orking...=\u0026#34;\u0026#34; oroutine=\u0026#34;\u0026#34; return=\u0026#34;\u0026#34; the=\u0026#34;\u0026#34; time.millisecond=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34;\u0026gt; When cancel() is called, the goroutine receives a signal via ctx.Done().\nSetting a Timeout: WithTimeout ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second) defer cancel() select { case \u0026lt;-time .after=\u0026#34;\u0026#34; case=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; completed=\u0026#34;\u0026#34; ctx.done=\u0026#34;\u0026#34; ctx.err=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; ontext=\u0026#34;\u0026#34; peration=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; timeout:=\u0026#34;\u0026#34;\u0026gt; WithDeadline works the same way, but with a fixed time:\ndeadline := time.Now().Add(2 * time.Second) ctx, cancel := context.WithDeadline(context.Background(), deadline) How to Use ctx.Done() The ctx.Done() channel is closed when the context is canceled or times out. Use it in select blocks to exit early.\nReal-World Example: HTTP Request Timeout func fetch(ctx context.Context, url string) error { req, err := http.NewRequestWithContext(ctx, \u0026#34;GET\u0026#34;, url, nil) if err != nil { return err } client := http.Client{} resp, err := client.Do(req) if err != nil { return err } defer resp.Body.Close() fmt.Println(\u0026#34;Status:\u0026#34;, resp.Status) return nil } func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() err := fetch(ctx, \u0026#34;https://httpbin.org/delay/2\u0026#34;) if err != nil { fmt.Println(\u0026#34;Request failed:\u0026#34;, err) } } Best Practices Always call cancel() to release resources Pass context.Context as the first argument in your functions Use context.WithTimeout for operations with time limits Use context.WithCancel for manual control Common Mistakes Not deferring cancel() → memory leak Ignoring ctx.Err() → silent failure Passing nil context or using context.TODO() in production Conclusion Understanding context is essential for writing responsive, well-behaved concurrent programs in Go. Whether you\u0026rsquo;re managing goroutines, dealing with timeouts, or handling request chains in a web server, context gives you the tools to do it cleanly and safely.\nNext, we\u0026rsquo;ll cover sync.Mutex and other tools for synchronizing data between goroutines.\nHappy coding!\n","href":"/2025/04/using-context-in-go-cancellation.html","title":"Using Context in Go: Cancellation, Timeout, and Deadlines Explained"},{"content":"One of the most powerful features of Go is its built-in support for concurrency. Go makes it easy to write programs that perform multiple tasks at the same time, thanks to goroutines and channels. Unlike traditional multithreading, Go provides a lightweight and clean way to build concurrent systems with minimal overhead and boilerplate.\nIn this article, you’ll learn:\nThe difference between concurrency and parallelism What goroutines are and how to use them How channels allow communication between goroutines Buffered vs unbuffered channels The select statement Common concurrency problems and how to avoid them Real-world examples and best practices Concurrency vs Parallelism Concurrency means doing multiple things at once (interleaved), while parallelism means running them simultaneously on different processors. Go’s concurrency model allows you to write code that is concurrent, and Go’s runtime handles whether it is executed in parallel depending on available CPU cores.\nIntroducing Goroutines A goroutine is a function that runs concurrently with other functions. You start one by using the go keyword:\nfunc sayHello() { fmt.Println(\u0026#34;Hello from goroutine!\u0026#34;) } func main() { go sayHello() fmt.Println(\u0026#34;Main function\u0026#34;) } Goroutines are lightweight and managed by the Go runtime, not the OS. You can spawn thousands of them without major performance issues.\nWhy You Need to Wait The above example might not print the goroutine output if main() exits first. You can fix this using time.Sleep or better, sync.WaitGroup:\nvar wg sync.WaitGroup func sayHi() { defer wg.Done() fmt.Println(\u0026#34;Hi!\u0026#34;) } func main() { wg.Add(1) go sayHi() wg.Wait() } Using Channels Channels are used to send and receive values between goroutines. They are typed and provide safe communication.\nfunc main() { ch := make(chan string) go func() { ch \u0026lt;- :=\u0026#34;\u0026lt;-ch\u0026#34; code=\u0026#34;\u0026#34; essage=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; from=\u0026#34;\u0026#34; goroutine=\u0026#34;\u0026#34; msg=\u0026#34;\u0026#34;\u0026gt; Buffered Channels A buffered channel allows sending without blocking, up to its capacity:\nch := make(chan int, 2) ch \u0026lt;- 1=\u0026#34;\u0026#34; 2=\u0026#34;\u0026#34; 3=\u0026#34;\u0026#34; block=\u0026#34;\u0026#34; buffer=\u0026#34;\u0026#34; ch=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; full=\u0026#34;\u0026#34; if=\u0026#34;\u0026#34; is=\u0026#34;\u0026#34; this=\u0026#34;\u0026#34; will=\u0026#34;\u0026#34;\u0026gt; Select Statement select lets you wait on multiple channel operations:\nfunc main() { ch1 := make(chan string) ch2 := make(chan string) go func() { time.Sleep(1 * time.Second) ch1 \u0026lt;- :=\u0026#34;\u0026lt;-ch2:\u0026#34; case=\u0026#34;\u0026#34; ch1=\u0026#34;\u0026#34; ch2=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; from=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; msg1=\u0026#34;\u0026#34; msg2=\u0026#34;\u0026#34; select=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34;\u0026gt; Common Problems Deadlocks: when goroutines wait forever Race conditions: two goroutines access the same variable concurrently Use go run -race to detect race conditions.\nReal-World Example: Worker Pool func worker(id int, jobs \u0026lt;-chan 2=\u0026#34;\u0026#34; 3=\u0026#34;\u0026#34; 5=\u0026#34;\u0026#34; :=\u0026#34;1;\u0026#34; chan=\u0026#34;\u0026#34; close=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; d=\u0026#34;\u0026#34; finished=\u0026#34;\u0026#34; fmt.printf=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; for=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; id=\u0026#34;\u0026#34; int=\u0026#34;\u0026#34; j=\u0026#34;\u0026#34; job=\u0026#34;\u0026#34; jobs=\u0026#34;\u0026#34; main=\u0026#34;\u0026#34; n=\u0026#34;\u0026#34; orker=\u0026#34;\u0026#34; r=\u0026#34;\u0026#34; results=\u0026#34;\u0026#34; started=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34; w=\u0026#34;\u0026#34; worker=\u0026#34;\u0026#34;\u0026gt; Best Practices Close channels only when you’re done sending Use sync.WaitGroup to wait for goroutines Don’t create unbounded goroutines — may cause memory leaks Use buffered channels to avoid blocking when needed Conclusion Goroutines and channels are the foundation of concurrency in Go. With them, you can build scalable and efficient programs without the complexity of traditional multithreading. Start small, experiment with simple patterns, and scale your knowledge step by step.\nNext, we\u0026rsquo;ll explore advanced concurrency control using sync.Mutex, sync.Once, and context for cancellation and timeouts.\nHappy coding!\n","href":"/2025/04/concurrency-in-go-goroutines-and.html","title":"Concurrency in Go: Goroutines and Channels Explained"},{"content":"Generics were introduced in Go 1.18, marking a significant evolution of the language. They allow you to write flexible, reusable code without sacrificing type safety. With generics, you can define functions, types, and data structures that work with different types, all while maintaining strong compile-time checks.\nIn this article, you’ll learn:\nWhat generics are and why they matter How to define generic functions and types Type parameters and constraints Real-world examples of generics Best practices when using generics in Go What Are Generics? Generics let you write code that works with different data types while keeping the benefits of static typing. Before generics, developers often used interface{} and type assertions to achieve flexibility, but that meant losing compile-time type safety.\nDefining a Generic Function A generic function introduces a type parameter list using square brackets [] before the function parameters.\nfunc Print[T any](value T) { fmt.Println(value) } Here, T is a type parameter, and any is a constraint (alias for interface{}). This function works with any type, like:\nPrint(10) Print(\u0026#34;Hello\u0026#34;) Print(true) Using Type Constraints You can limit what types can be passed by using constraints:\ntype Number interface { ~int | ~float64 } func Sum[T Number](a, b T) T { return a + b } Now Sum can only be called with numeric types.\nGeneric Types You can also define structs or custom types with generics:\ntype Pair[T any] struct { First T Second T } func main() { p := Pair[string]{\u0026#34;Go\u0026#34;, \u0026#34;Lang\u0026#34;} fmt.Println(p.First, p.Second) } Multiple Type Parameters You can define more than one type parameter:\ntype Map[K comparable, V any] struct { data map[K]V } The comparable constraint is required for keys in a map (they must support ==).\nReal-World Example: Generic Filter Function func Filter[T any](items []T, predicate func(T) bool) []T { var result []T for _, item := range items { if predicate(item) { result = append(result, item) } } return result } Usage:\nevens := Filter([]int{1, 2, 3, 4}, func(n int) bool { return n%2 == 0 }) Generics vs Interface Before generics, we often used interface{} and did type assertion:\nfunc PrintAny(val interface{}) { fmt.Println(val) } This works, but doesn’t give compile-time safety or clarity. With generics, you avoid runtime type errors.\nBest Practices Use generics when you write reusable logic (e.g. map, reduce, filter) Don’t overuse – avoid generics when concrete types are simpler Name type parameters clearly (T, K, V, etc.) Use type constraints to enforce correctness Conclusion Generics are a powerful addition to Go that let you write cleaner, more reusable code without giving up type safety. Whether you\u0026rsquo;re building data structures, utility functions, or abstractions, generics help reduce duplication and improve flexibility.\nNow that you understand generics, you\u0026rsquo;re ready to explore Go\u0026rsquo;s concurrency model and build high-performance programs using goroutines and channels.\nHappy coding!\n","href":"/2025/04/generics-in-go-writing-reusable-and-type-safe-code.html","title":"Generics in Go: Writing Reusable and Type-Safe Code"},{"content":"Benchmarking is the process of measuring the performance of code. In Go, benchmarking is built into the standard testing package, making it easy to test how fast your functions run. Whether you\u0026rsquo;re comparing two algorithms, optimizing critical sections of code, or experimenting with concurrency, benchmarking helps you make informed decisions.\nThis article will walk you through:\nWhat is benchmarking and why it matters How to write benchmark functions in Go Interpreting benchmark results Using b.ResetTimer(), b.StopTimer(), and b.StartTimer() Common use cases for benchmarking Best practices for writing meaningful benchmarks Why Benchmarking is Important Benchmarking allows you to evaluate performance based on data, not assumptions. You can compare the execution time of different code versions, measure improvements, and catch performance regressions early. This is crucial for optimizing critical parts of applications such as sorting, searching, or processing large datasets.\nWriting Your First Benchmark Just like test functions in Go, benchmark functions are placed in a file ending with _test.go. Benchmark functions must start with Benchmark and have this signature:\nfunc BenchmarkXxx(b *testing.B) Example:\nfunc BenchmarkAdd(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = 1 + 2 } } Go runs this loop repeatedly to get a stable measurement. The b.N is automatically adjusted to get an accurate average runtime.\nRunning Benchmarks To run all benchmarks in a package, use:\ngo test -bench=. To run a specific benchmark:\ngo test -bench=BenchmarkAdd You’ll see output like this:\nBenchmarkAdd-8 1000000000 0.25 ns/op -8 means 8 logical CPUs used 1000000000 is how many times it ran 0.25 ns/op is time per operation Controlling Timers You can use b.StopTimer() and b.StartTimer() to exclude setup code:\nfunc BenchmarkWithSetup(b *testing.B) { data := make([]int, 1000) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { _ = process(data) } } Comparing Implementations Let’s say you want to compare two ways to concatenate strings:\nfunc BenchmarkConcatPlus(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = \u0026#34;hello\u0026#34; + \u0026#34; \u0026#34; + \u0026#34;world\u0026#34; } } func BenchmarkConcatSprintf(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = fmt.Sprintf(\u0026#34;%s %s\u0026#34;, \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) } } This helps you choose the faster approach in performance-critical sections.\nBest Practices Keep benchmarks small and focused on a single operation Avoid external dependencies (e.g., file I/O, network) Isolate logic you\u0026rsquo;re testing to avoid side effects Use go test -bench with -count for averaging over multiple runs Conclusion Benchmarking in Go is simple but powerful. It helps you write better-performing programs by providing real measurements instead of guesses. Combined with testing, it becomes a critical part of writing production-ready software.\nHappy benchmarking!\n","href":"/2025/04/benchmarking-in-go-measuring.html","title":"Benchmarking in Go: Measuring Performance with testing.B"},{"content":"Testing is one of the most important parts of software development, yet often overlooked. In Go, testing is not an afterthought — it\u0026rsquo;s built into the language itself through the powerful and easy-to-use testing package. Whether you\u0026rsquo;re building a web app, API, or CLI tool, writing tests will help you catch bugs early, document your code, and refactor safely.\nThis article will help you understand:\nWhy testing matters in software development The basics of writing tests in Go Using t.Error, t.Fail, and t.Fatal Table-driven tests Running and understanding test results Measuring code coverage Best practices for writing useful tests Why Testing is Important Testing helps you ensure that your code works as expected — not just today, but as it evolves. Without tests, it\u0026rsquo;s risky to make changes because you can\u0026rsquo;t be confident you haven\u0026rsquo;t broken something.\nBenefits of testing include:\nPreventing bugs before reaching production Providing documentation for your code\u0026rsquo;s behavior Making code easier to refactor Enabling safe collaboration within teams Getting Started: Writing Your First Test In Go, a test file must end with _test.go and be in the same package as the code you want to test.\nLet’s say you have a simple math function:\npackage calculator func Add(a, b int) int { return a + b } Your test file could look like this:\npackage calculator import \u0026#34;testing\u0026#34; func TestAdd(t *testing.T) { result := Add(2, 3) expected := 5 if result != expected { t.Errorf(\u0026#34;Add(2, 3) = %d; want %d\u0026#34;, result, expected) } } Understanding t.Error, t.Fail, and t.Fatal t.Error: reports an error but continues running the test t.Fatal: reports an error and immediately stops the test t.Fail: marks the test as failed but doesn’t log a message Table-Driven Tests This is a common Go pattern for testing multiple cases in a clean way:\nfunc TestAddMultipleCases(t *testing.T) { tests := []struct { a, b int expected int }{ {1, 2, 3}, {0, 0, 0}, {-1, -1, -2}, } for _, tt := range tests { result := Add(tt.a, tt.b) if result != tt.expected { t.Errorf(\u0026#34;Add(%d, %d) = %d; want %d\u0026#34;, tt.a, tt.b, result, tt.expected) } } } Running Tests To run all tests in a package, use:\ngo test To see detailed output:\ngo test -v Code Coverage Want to know how much of your code is tested?\ngo test -cover You can even generate an HTML report:\ngo test -coverprofile=coverage.out go tool cover -html=coverage.out Where to Put Tests It’s a good practice to place tests right next to the code they are testing. This makes them easy to find and maintain. Use the same package name unless you’re doing black-box testing.\nBest Practices Write tests as you write code, not after Use table-driven tests to cover edge cases Make your test failures readable (clear messages) Group related logic into subtests using t.Run Keep test functions short and focused Conclusion Testing is not just a formality — it’s a mindset. Go makes it easy to write fast, reliable tests without third-party tools. By integrating testing into your daily development flow, you’ll gain confidence, spot bugs earlier, and create better software.\nIn the next topic, we\u0026rsquo;ll explore how to benchmark Go code and write performance tests.\nKeep testing and happy coding!\n","href":"/2025/04/testing-in-go-writing-unit-tests-with.html","title":"Testing in Go: Writing Unit Tests with the Testing Package"},{"content":"Error handling is a core part of Go programming. Unlike many languages that use exceptions, Go takes a more straightforward and explicit approach. In Go, functions often return an error as the last return value, and it\u0026rsquo;s the developer’s job to check and handle it. This method may seem verbose at first, but it leads to more robust and predictable code.\nIn this article, you\u0026rsquo;ll learn:\nWhat an error is in Go How to handle errors using if err != nil Creating custom errors Error wrapping with Go 1.13+ Custom error types Using panic and recover (when and why) Best practices for error handling What is an Error in Go? In Go, the error type is a built-in interface:\ntype error interface { Error() string } Any type that implements the Error() method satisfies the error interface. Most standard functions return an error as a way to indicate that something went wrong.\nBasic Error Handling The standard way to handle errors in Go is with if err != nil blocks:\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) func divide(a, b int) (int, error) { if b == 0 { return 0, errors.New(\u0026#34;cannot divide by zero\u0026#34;) } return a / b, nil } func main() { result, err := divide(10, 0) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } fmt.Println(\u0026#34;Result:\u0026#34;, result) } Creating Custom Errors You can create custom errors using the errors.New or fmt.Errorf functions:\nerr := errors.New(\u0026#34;something went wrong\u0026#34;) err := fmt.Errorf(\u0026#34;error occurred: %v\u0026#34;, err) Error Wrapping (Go 1.13+) Go 1.13 introduced error wrapping, which lets you keep the original error while adding context:\noriginal := errors.New(\u0026#34;file not found\u0026#34;) wrapped := fmt.Errorf(\u0026#34;cannot load config: %w\u0026#34;, original) You can later use errors.Is and errors.As to inspect wrapped errors:\nif errors.Is(wrapped, original) { fmt.Println(\u0026#34;Original error matched\u0026#34;) } Custom Error Types To add more detail or behavior, you can define your own error types:\ntype MyError struct { Code int Msg string } func (e MyError) Error() string { return fmt.Sprintf(\u0026#34;Code %d: %s\u0026#34;, e.Code, e.Msg) } Now you can return MyError from functions and check its fields with type assertions.\nPanic and Recover panic is used when your program cannot continue. It\u0026rsquo;s similar to throwing an exception but should be avoided for expected errors.\nfunc risky() { panic(\u0026#34;something went really wrong\u0026#34;) } To handle panic safely, use recover inside a deferred function:\nfunc safe() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;Recovered from panic:\u0026#34;, r) } }() risky() } Best Practices Always check and handle errors returned from functions Wrap errors with context using fmt.Errorf and %w Use custom error types for more control Avoid panic unless absolutely necessary (e.g., for programming errors) Log errors with enough context to debug later Conclusion Go’s error handling may be explicit and repetitive, but it leads to clear and predictable code. By following best practices and understanding how to create, return, and wrap errors, you’ll build programs that are easier to maintain and debug.\nIn the next topic, we\u0026rsquo;ll explore how to write tests in Go to verify the correctness of your code using go test and the testing package.\nHappy coding!\n","href":"/2025/04/error-handling-in-go-managing-errors.html","title":"Error Handling in Go: Managing Errors the Right Way"},{"content":"Interfaces are one of the most important features in Go. They allow you to write flexible, reusable, and loosely coupled code. In Go, an interface defines a set of method signatures, and any type that implements those methods satisfies the interface — without needing to explicitly declare that it does so. This is a powerful concept that supports polymorphism and clean architecture in Go applications.\nIn this article, you\u0026rsquo;ll learn:\nWhat an interface is in Go How to define and implement interfaces Implicit interface implementation Using interface as function parameters The empty interface and type assertions Real-world examples of interfaces Best practices when working with interfaces What is an Interface? An interface is a type that defines a set of method signatures. Any type that provides implementations for those methods is said to satisfy the interface.\ntype Speaker interface { Speak() string } This interface requires a method Speak that returns a string.\nImplementing an Interface Unlike other languages, Go uses implicit implementation. You don’t need to explicitly say “this struct implements an interface.” You just define the required methods.\ntype Dog struct {} func (d Dog) Speak() string { return \u0026#34;Woof!\u0026#34; } type Cat struct {} func (c Cat) Speak() string { return \u0026#34;Meow!\u0026#34; } Both Dog and Cat now satisfy the Speaker interface because they implement the Speak method.\nUsing Interface as Function Parameter Interfaces allow you to write functions that work with any type that satisfies the interface.\nfunc makeItSpeak(s Speaker) { fmt.Println(s.Speak()) } func main() { makeItSpeak(Dog{}) makeItSpeak(Cat{}) } This is very powerful for building reusable code, such as in logging, HTTP handling, and I/O.\nInterface with Multiple Methods type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type ReadWriter interface { Reader Writer } Interfaces can be composed from other interfaces, helping you build powerful abstractions.\nThe Empty Interface The empty interface interface{} can represent any type. It is often used in situations where you don’t know the exact type at compile time (e.g., in JSON decoding, generic containers).\nfunc describe(i interface{}) { fmt.Printf(\u0026#34;Value: %v, Type: %T \u0026#34;, i, i) } Type Assertion You can convert an empty interface back to a concrete type using type assertion.\nvar i interface{} = \u0026#34;hello\u0026#34; s := i.(string) fmt.Println(s) Or safely:\nif s, ok := i.(string); ok { fmt.Println(\u0026#34;String value:\u0026#34;, s) } else { fmt.Println(\u0026#34;Not a string\u0026#34;) } Type Switch Type switches are like regular switches, but for handling multiple possible types.\nfunc printType(i interface{}) { switch v := i.(type) { case string: fmt.Println(\u0026#34;It\u0026#39;s a string:\u0026#34;, v) case int: fmt.Println(\u0026#34;It\u0026#39;s an int:\u0026#34;, v) default: fmt.Println(\u0026#34;Unknown type\u0026#34;) } } Real-World Example: Logger Interface Let’s create a logger interface and different implementations:\ntype Logger interface { Log(message string) } type ConsoleLogger struct {} func (c ConsoleLogger) Log(message string) { fmt.Println(\u0026#34;[Console]\u0026#34;, message) } type FileLogger struct { File *os.File } func (f FileLogger) Log(message string) { fmt.Fprintln(f.File, \u0026#34;[File]\u0026#34;, message) } This allows you to use either logger with the same code:\nfunc logMessage(logger Logger, message string) { logger.Log(message) } Best Practices Name interfaces based on behavior (e.g., Reader, Formatter) Prefer small interfaces with one or two methods Use interface embedding for composition Only expose interfaces when they are needed (don’t over-abstract) Conclusion Interfaces are a core feature in Go that allow you to write flexible, reusable, and testable code. They help you define behavior and decouple implementation from abstraction. By understanding how to define and work with interfaces, you\u0026rsquo;ll be ready to create clean and modular Go programs.\nTry writing your own interfaces, build functions that accept them, and explore the built-in interfaces in Go’s standard library.\nHappy coding!\n","href":"/2025/04/interfaces-in-go-building-flexible-and.html","title":"Interfaces in Go: Building Flexible and Reusable Code"},{"content":"In Go, understanding pointers is essential if you want to work effectively with functions, methods, and memory-efficient code. Unlike some other languages, Go’s approach to pointers is clean and straightforward—there’s no pointer arithmetic, and most things can be done without overly complex syntax.\nThis article will help you understand:\nWhat pointers are in Go and how they work Using pointers in functions Method receivers: value vs pointer Choosing between value or pointer receiver Common mistakes with pointers Best practices for using pointers effectively What is a Pointer? A pointer is a variable that stores the memory address of another variable. You use the \u0026amp; operator to get the address and * to access the value at that address.\nfunc main() { x := 10 p := \u0026amp;x fmt.Println(*p) // 10 } Here, p is a pointer to x. *p accesses the value stored at the address.\nPointers and Functions When passing variables to functions, Go uses value semantics—meaning it passes a copy. If you want the function to modify the original variable, pass a pointer.\nfunc update(val *int) { *val = 100 } func main() { x := 10 update(\u0026amp;x) fmt.Println(x) // 100 } This is useful when working with large structs or when you need to update the caller\u0026rsquo;s data.\nPointer Receivers in Methods In Go, methods can be defined with either value receivers or pointer receivers. Pointer receivers allow methods to modify the actual object.\ntype Person struct { Name string Age int } func (p *Person) GrowUp() { p.Age++ } func main() { person := Person{\u0026#34;Alice\u0026#34;, 20} person.GrowUp() fmt.Println(person.Age) // 21 } If GrowUp() used a value receiver (i.e., func (p Person)), the change would not persist outside the method.\nValue vs Pointer Receiver Go allows both styles, but here\u0026rsquo;s when to choose each:\nValue receiver: small structs, method does not modify data Pointer receiver: large structs, method needs to modify state func (p Person) ValueGreet() { fmt.Println(\u0026#34;Hello,\u0026#34;, p.Name) } func (p *Person) PointerUpdate(name string) { p.Name = name } Go is Smart: Automatic Conversion Go is smart enough to let you call pointer receiver methods on value types and vice versa—it will automatically add or remove the \u0026amp; for you:\nperson := Person{\u0026#34;Bob\u0026#34;, 30} person.GrowUp() // Works even though GrowUp has a pointer receiver Common Mistakes Forgetting to pass \u0026amp;x when a function expects *int Trying to use *x when x is not a pointer Not understanding that value receiver methods work on copies Best Practices Use pointer receivers when your method modifies the struct or for performance Keep your struct small when using value receivers Avoid unnecessary pointer complexity—Go is designed to make things simple Conclusion Pointers in Go are powerful, but not difficult. They let you control memory usage, update values across scopes, and create efficient, flexible methods. Understanding pointers will make you a better Go developer—especially when working with structs, interfaces, and large systems.\nNow that you understand pointers, you\u0026rsquo;re ready to dive deeper into Go\u0026rsquo;s concurrency model and start using goroutines and channels. But don’t forget — great power comes with great responsibility, even in Go!\nHappy coding!\n","href":"/2025/04/understanding-pointers-in-go-reference.html","title":"Understanding Pointers in Go: Reference Types and Receivers Explained"},{"content":"In Go, a struct is a powerful way to group related data together. It allows you to define your own custom types by combining variables (also called fields). Structs are often used to model real-world entities like users, products, or messages. When combined with methods, structs become the foundation for writing clean and reusable code in Go.\nIn this article, you\u0026rsquo;ll learn:\nHow to define and use structs in Go How to attach methods to a struct The difference between value and pointer receivers Best practices for using structs and methods effectively Defining a Struct To define a struct, you use the type keyword followed by the name of the struct and the struct keyword:\ntype User struct { Name string Email string Age int } This defines a struct called User with three fields. To create a value of that struct, you can do the following:\nfunc main() { user := User{ Name: \u0026#34;Alice\u0026#34;, Email: \u0026#34;alice@example.com\u0026#34;, Age: 30, } fmt.Println(user) } You can also declare an empty struct and assign fields later:\nvar u User u.Name = \u0026#34;Bob\u0026#34; u.Email = \u0026#34;bob@example.com\u0026#34; u.Age = 25 Accessing and Updating Struct Fields To access a field, use the dot . operator:\nfmt.Println(user.Name) To update a field:\nuser.Age = 31 Structs with Functions You can write a function that accepts a struct as an argument:\nfunc printUser(u User) { fmt.Println(\u0026#34;Name:\u0026#34;, u.Name) fmt.Println(\u0026#34;Email:\u0026#34;, u.Email) fmt.Println(\u0026#34;Age:\u0026#34;, u.Age) } Methods in Go In Go, you can define a function that is associated with a struct. This is called a method.\nfunc (u User) Greet() { fmt.Println(\u0026#34;Hi, my name is\u0026#34;, u.Name) } Here, (u User) means this function is a method that can be called on a User value.\nPointer Receivers vs Value Receivers You can define methods using either a value receiver or a pointer receiver:\n// Value receiver func (u User) Info() { fmt.Println(\u0026#34;User info:\u0026#34;, u.Name, u.Email) } // Pointer receiver func (u *User) UpdateEmail(newEmail string) { u.Email = newEmail } Use a pointer receiver if the method needs to modify the original struct or if copying the struct would be expensive.\nEmbedding Structs Go allows embedding one struct into another. This can be used to extend functionality:\ntype Address struct { City string State string } type Employee struct { User Address Position string } You can now access fields from both User and Address in an Employee instance directly.\nAnonymous Structs Go also supports defining structs without giving them a name. These are used for quick data grouping:\nperson := struct { Name string Age int }{ Name: \u0026#34;Charlie\u0026#34;, Age: 22, } Best Practices Group related data using structs for better organization Use methods to define behavior related to a struct Use pointer receivers when modifying struct data Use struct embedding to promote code reuse Conclusion Structs and methods are a core part of writing structured and maintainable code in Go. By learning how to define and work with them, you\u0026rsquo;ll be better equipped to build complex systems that are easy to manage. Practice creating your own structs and adding behavior with methods to solidify your understanding.\nHappy coding!\n","href":"/2025/04/structs-and-methods-in-go-defining-and.html","title":"Structs and Methods in Go: Defining and Using Custom Types"},{"content":"Functions are an essential part of programming in any language, and Go is no exception. A function lets you organize code into reusable blocks, which helps reduce duplication and improve readability. In this article, you’ll learn how functions work in Go, how to define them, use them, and apply best practices.\nThis guide covers:\nHow to define and call a function in Go Function parameters and return values Multiple return values Named return values Variadic functions Functions as values and arguments Best practices for clean function design Defining and Calling a Function To define a function in Go, use the func keyword, followed by the function name, parameters, and return type (if any). Here\u0026rsquo;s a simple example:\npackage main import \u0026#34;fmt\u0026#34; func greet(name string) { fmt.Println(\u0026#34;Hello,\u0026#34;, name) } func main() { greet(\u0026#34;Alice\u0026#34;) } This function takes a string parameter and prints a greeting message. It is called from the main function.\nFunction Parameters and Return Values Functions can accept multiple parameters and return values. You need to specify the type for each parameter.\nfunc add(a int, b int) int { return a + b } func main() { result := add(3, 5) fmt.Println(\u0026#34;Sum:\u0026#34;, result) } Go also allows you to declare multiple parameters of the same type together, like this:\nfunc multiply(a, b int) int { return a * b } Multiple Return Values One of Go’s unique features is that a function can return more than one value.\nfunc divide(a, b int) (int, int) { quotient := a / b remainder := a % b return quotient, remainder } func main() { q, r := divide(10, 3) fmt.Println(\u0026#34;Quotient:\u0026#34;, q, \u0026#34;Remainder:\u0026#34;, r) } This is commonly used in Go for returning both result and error values.\nNamed Return Values You can also name return values in the function signature. This makes your code more readable and enables implicit return.\nfunc compute(a, b int) (sum int, product int) { sum = a + b product = a * b return } This is useful when the function logic is a bit more complex and you want to keep track of return values easily.\nVariadic Functions Sometimes, you may want to pass an arbitrary number of arguments to a function. Go supports this with variadic functions.\nfunc total(numbers ...int) int { sum := 0 for _, number := range numbers { sum += number } return sum } func main() { fmt.Println(total(1, 2, 3, 4, 5)) } The ...int means the function accepts any number of int values. Inside the function, numbers behaves like a slice.\nFunctions as Values and Arguments In Go, functions are first-class citizens. You can assign them to variables, pass them as arguments, and return them from other functions.\nfunc square(x int) int { return x * x } func apply(op func(int) int, value int) int { return op(value) } func main() { result := apply(square, 4) fmt.Println(result) } This opens up many possibilities such as writing flexible and composable code, especially when used with closures or higher-order functions.\nBest Practices Here are some general tips when writing functions in Go:\nKeep your functions short and focused on one task Use descriptive names for function and parameter names Avoid too many parameters (consider grouping them in structs) Document the purpose and behavior of your functions Conclusion Functions are a fundamental concept in Go programming. They allow you to organize your logic, make your code reusable, and improve structure. Go’s support for multiple return values, variadic functions, and treating functions as first-class values gives you powerful tools to build real-world applications.\nPractice writing your own functions, try combining features like variadic parameters with multiple returns, and use functions to structure your Go projects cleanly.\nHappy coding!\n","href":"/2025/04/understanding-functions-in-go-beginners.html","title":"Understanding Functions in Go: A Beginner's Guide"},{"content":"When building applications in Go, it\u0026rsquo;s common to work with groups of data. For example, you might want to store a list of user names, or map names to scores. In Go, you can use collections like arrays, slices, and maps to do that.\nIn this article, we’ll explore:\nWhat arrays are and how they work How slices offer more flexibility What maps are and how to use them Common operations with collections Practical examples to understand the difference between them Let’s dive in and learn how Go helps us manage grouped data efficiently.\nArrays in Go An array is a fixed-size collection of elements of the same type. Once an array is created, its size cannot change.\npackage main import \u0026#34;fmt\u0026#34; func main() { var numbers [3]int numbers[0] = 10 numbers[1] = 20 numbers[2] = 30 fmt.Println(numbers) } You can also initialize an array directly:\nnames := [3]string{\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;} Arrays have a fixed size. All elements must be of the same type, and you can access items using their index (starting from 0).\nArrays are not commonly used in large Go applications, but understanding them is key to learning slices.\nSlices in Go Slices are more flexible than arrays. They are built on top of arrays but allow dynamic resizing.\nnumbers := []int{10, 20, 30} fmt.Println(numbers) Adding elements to a slice:\nnumbers = append(numbers, 40) fmt.Println(numbers) Creating slices from existing arrays:\narr := [5]int{1, 2, 3, 4, 5} slice := arr[1:4] // includes index 1 to 3 fmt.Println(slice) Useful slice operations include append, len (length), and cap (capacity). Slices are widely used in Go because they are flexible and efficient.\nAnother great thing about slices is that they can share the same underlying array. This allows for memory-efficient manipulation of data. However, you should be cautious when modifying shared slices as changes might affect other parts of your code.\nMaps in Go Maps are key-value pairs. You can use them to store and retrieve data by key.\nscores := map[string]int{ \u0026#34;Alice\u0026#34;: 90, \u0026#34;Bob\u0026#34;: 85, } fmt.Println(scores[\u0026#34;Alice\u0026#34;]) Adding and updating values:\nscores[\u0026#34;Charlie\u0026#34;] = 88 scores[\u0026#34;Bob\u0026#34;] = 95 Deleting a value:\ndelete(scores, \u0026#34;Alice\u0026#34;) Looping through a map:\nfor name, score := range scores { fmt.Println(name, \u0026#34;has score\u0026#34;, score) } Checking if a key exists:\nvalue, exists := scores[\u0026#34;David\u0026#34;] if exists { fmt.Println(\u0026#34;Score:\u0026#34;, value) } else { fmt.Println(\u0026#34;David not found\u0026#34;) } Maps are extremely useful when you need fast lookups or need to associate labels with values. For example, they’re great for storing configuration options, lookup tables, or grouped statistics.\nChoosing Between Arrays, Slices, and Maps Use arrays when the size is known and fixed. Use slices when you need a dynamic list. Use maps when you need to associate keys to values (like name to score).\nEach data structure has its own strengths. As a Go developer, you’ll likely use slices and maps much more often than arrays, especially when working with APIs, databases, or handling JSON.\nPractical Example: Student Grades grades := map[string][]int{ \u0026#34;Alice\u0026#34;: {90, 85, 88}, \u0026#34;Bob\u0026#34;: {78, 82, 80}, } for name, gradeList := range grades { total := 0 for _, grade := range gradeList { total += grade } average := total / len(gradeList) fmt.Println(name, \u0026#34;average grade:\u0026#34;, average) } This example combines maps and slices to store multiple grades for each student and calculates the average.\nSummary Collections in Go help you group and organize data. Arrays are useful but limited by their fixed size. Slices are flexible and the most commonly used collection in Go. Maps let you link one value to another using keys.\nBy understanding and practicing with these three types of collections, you’ll be ready to write real-world programs that work with lists of data, settings, or records.\nAs you continue learning Go, try building small programs that use slices and maps. Practice manipulating data, looping through collections, and performing operations like sorting or searching. These are real-world tasks you\u0026rsquo;ll encounter as a developer.\nKeep exploring and happy coding!\n","href":"/2025/04/working-with-collections-in-go-arrays.html","title":"Working with Collections in Go: Arrays, Slices, and Maps Explained"},{"content":"Loops are a key part of programming. They let us run the same piece of code multiple times without repeating ourselves. In Go, loops are simple but powerful — and they\u0026rsquo;re built using just one keyword: for.\nIn this article, we’ll explore:\nThe basic for loop in Go Using for as a while loop Looping with range Breaking or skipping parts of loops with break and continue Real-world examples to help you understand how loops work What is a Loop? A loop is a way to repeat a block of code as long as a condition remains true. Instead of writing similar code many times, we can put it in a loop and let the program handle the repetition. This makes our code shorter, cleaner, and easier to manage. Go uses the keyword for for all loop types, which makes it both simple and flexible.\nThe Basic for Loop The most common way to write a loop in Go is with the standard for loop structure. It includes three parts: an initializer, a condition, and a post statement.\npackage main import \u0026#34;fmt\u0026#34; func main() { for i := 0; i \u0026lt; 5; i++ { fmt.Println(\u0026#34;Count:\u0026#34;, i) } } This loop will print numbers from 0 to 4. First, it starts with i = 0. Then it checks the condition i \u0026lt; 5. If true, it runs the code inside the loop. After each loop, i is increased by 1. When the condition is false, the loop stops.\nUsing for as a while Loop Go doesn’t have a while keyword. But you can use for in the same way by just writing the condition.\nfunc main() { i := 0 for i \u0026lt; 3 { fmt.Println(\u0026#34;i is:\u0026#34;, i) i++ } } This loop works exactly like a while loop. It continues running as long as the condition i \u0026lt; 3 is true. This format is useful when you don’t need a counter setup like in the basic for loop.\nInfinite Loops Sometimes you want a loop to run forever, such as when building servers or listening to user input. You can do this by writing for without a condition.\nfunc main() { for { fmt.Println(\u0026#34;This runs forever until we break it.\u0026#34;) break } } This is an infinite loop, and you control when to stop it using a break statement inside the loop.\nLooping with range Go provides a very handy way to loop over arrays, slices, strings, and maps using range. It simplifies working with collections.\nExample with a slice: func main() { fruits := []string{\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} for index, fruit := range fruits { fmt.Println(index, fruit) } } Here, range gives both the index and the value of each item. If you don’t need the index, you can ignore it using an underscore:\nfor _, fruit := range fruits { fmt.Println(fruit) } Looping through a map: You can use range to loop through key-value pairs in a map:\nfunc main() { scores := map[string]int{\u0026#34;Alice\u0026#34;: 90, \u0026#34;Bob\u0026#34;: 85} for name, score := range scores { fmt.Println(name, \u0026#34;scored\u0026#34;, score) } } Looping over a string: Strings in Go are UTF-8 encoded. Using range lets you loop through each character:\nfunc main() { word := \u0026#34;go\u0026#34; for _, char := range word { fmt.Println(char) } } Note: This prints the Unicode code points (runes) for each character. If you want the actual character, you can use fmt.Printf(\u0026quot;%c\u0026quot;, char).\nUsing break and continue To control your loop more precisely, you can use break to stop the loop early, or continue to skip the current iteration and move to the next one.\nExample with break: func main() { for i := 0; i \u0026lt; 10; i++ { if i == 5 { break } fmt.Println(i) } } Example with continue: func main() { for i := 0; i \u0026lt; 5; i++ { if i == 2 { continue } fmt.Println(i) } } In this example, when i equals 2, the loop skips that iteration and continues with the next one.\nWhy Loops Matter Loops allow you to handle tasks like processing data, creating repeated outputs, checking conditions, or iterating through user input efficiently. Whether you’re building a calculator, a file reader, or a game, you’ll probably use loops often.\nSummary Loops in Go are powerful but simple. You can use for in different styles: the traditional counter-based loop, while-like loops, infinite loops, and range-based loops for collections. You can even control the flow inside the loop with break and continue.\nWith just one keyword, Go gives you all the looping tools you need. Try writing your own loops, experiment with slices and maps, and see how you can apply them in your real projects.\nKeep learning and happy coding!\n","href":"/2025/04/understanding-loops-in-go-for-range.html","title":"Understanding Loops in Go: for, range, break, and continue Explained"},{"content":"Conditional statements are one of the essential building blocks in any programming language, including Go. They allow us to make decisions in our code — telling the program to do something only if a certain condition is true.\nIn this article, we will explore:\nThe if, else, and else if statements The switch statement Best practices for using conditionals in Go Real examples to help you practice What is a Conditional Statement? A conditional statement evaluates whether a condition is true or false. Based on that, your Go program can choose which block of code to execute.\nLet’s say you want your app to greet users differently depending on the time of day. That’s where conditional logic comes in!\nif, else if, and else The most common conditional structure is if.\nBasic if syntax: package main import \u0026#34;fmt\u0026#34; func main() { age := 20 if age \u0026gt;= 18 { fmt.Println(\u0026#34;You are an adult.\u0026#34;) } } With else: func main() { age := 15 if age \u0026gt;= 18 { fmt.Println(\u0026#34;You are an adult.\u0026#34;) } else { fmt.Println(\u0026#34;You are underage.\u0026#34;) } } With else if: func main() { hour := 14 if hour \u0026lt; 12 { fmt.Println(\u0026#34;Good morning!\u0026#34;) } else if hour \u0026lt; 18 { fmt.Println(\u0026#34;Good afternoon!\u0026#34;) } else { fmt.Println(\u0026#34;Good evening!\u0026#34;) } } You can use multiple else if statements to check different conditions.\nShort if Statement Go supports a shorter form to declare variables inside the if block:\nfunc main() { if num := 10; num%2 == 0 { fmt.Println(\u0026#34;Even number\u0026#34;) } } This is useful if you only need the variable inside the if scope.\nswitch Statement The switch statement lets you compare a value against multiple conditions. It\u0026rsquo;s a cleaner alternative to many else if blocks.\nExample: func main() { day := \u0026#34;Friday\u0026#34; switch day { case \u0026#34;Monday\u0026#34;: fmt.Println(\u0026#34;Start of the week!\u0026#34;) case \u0026#34;Friday\u0026#34;: fmt.Println(\u0026#34;Almost weekend!\u0026#34;) case \u0026#34;Saturday\u0026#34;, \u0026#34;Sunday\u0026#34;: fmt.Println(\u0026#34;Weekend time!\u0026#34;) default: fmt.Println(\u0026#34;Another day!\u0026#34;) } } You can also group cases like Saturday and Sunday above.\nBest Practices for Beginners Keep your condition logic simple. Prefer switch when comparing one variable to multiple values. Don\u0026rsquo;t forget the default case in switch. Avoid deep nesting (e.g. if-inside-if-inside-if). More Practice Examples 1. Check if a number is positive, negative, or zero: func main() { num := 0 if num \u0026gt; 0 { fmt.Println(\u0026#34;Positive\u0026#34;) } else if num \u0026lt; 0 { fmt.Println(\u0026#34;Negative\u0026#34;) } else { fmt.Println(\u0026#34;Zero\u0026#34;) } } 2. Simple login simulation: func main() { username := \u0026#34;admin\u0026#34; password := \u0026#34;1234\u0026#34; if username == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; password == \u0026#34;1234\u0026#34; { fmt.Println(\u0026#34;Login successful\u0026#34;) } else { fmt.Println(\u0026#34;Invalid credentials\u0026#34;) } } Conclusion Understanding how conditionals work in Go helps you control the flow of your programs. Start with if and else, and move on to switch when you need to compare multiple options. Use these tools to build dynamic and interactive applications.\nNext Step: Learn about loops in Go — another powerful way to control program flow!\nHappy coding!\n","href":"/2025/04/understanding-conditional-statements-in.html","title":"Understanding Conditional Statements in Go (if, switch, etc.)"},{"content":"In our series on understanding data types in the Go programming language, after discussing numeric and boolean types, we will now explore strings. Strings are one of the most frequently used data types in programming due to their ubiquitous use in handling text. In Go, strings have several unique characteristics that we will explore in this article.\nIntroduction to Strings In Go, a string is a sequence of immutable bytes. This means that once a string value is set, it cannot be changed without creating a new string.\npackage main import \u0026#34;fmt\u0026#34; func main() { s := \u0026#34;hello world\u0026#34; // s[0] = \u0026#39;H\u0026#39; // this will result in an error because strings are immutable s = \u0026#34;Hello World\u0026#34; // this is valid, creates a new string fmt.Println(s) } Output\nHello World Basic Operations Basic operations on strings include concatenation and substring extraction. Concatenation can be done using the + operator, and substrings can be obtained by slicing.\npackage main func main() { firstName := \u0026#34;John\u0026#34; lastName := \u0026#34;Doe\u0026#34; fullName := firstName + \u0026#34; \u0026#34; + lastName // String concatenation println(fullName) hello := \u0026#34;Hello, world!\u0026#34; sub := hello[7:] // Extracting a substring println(sub) } Output\nJohn Doe world! String Manipulation The strings package in Go provides many functions for string manipulation. Here are a few examples:\npackage main import \u0026#34;fmt\u0026#34; import \u0026#34;strings\u0026#34; func main() { var str = \u0026#34;Hello, World\u0026#34; fmt.Println(strings.ToLower(str)) // convert all letters to lowercase fmt.Println(strings.ToUpper(str)) // convert all letters to uppercase fmt.Println(strings.TrimSpace(\u0026#34; space remover \u0026#34;)) // trim spaces from both ends } Output\nhello, world HELLO, WORLD space remover Iteration and Transformation We can iterate over strings with a for loop, and convert strings to byte slices or rune arrays.\npackage main import \u0026#34;fmt\u0026#34; func main() { str := \u0026#34;Hello, 世界\u0026#34; for i, runeValue := range str { fmt.Printf(\u0026#34;%#U starts at byte position %d\\n\u0026#34;, runeValue, i) } // Convert string to byte slice byteSlice := []byte(str) fmt.Println(byteSlice) // Convert string to rune slice runeSlice := []rune(str) fmt.Println(runeSlice) } Output\nU+0048 \u0026#39;H\u0026#39; starts at byte position 0 U+0065 \u0026#39;e\u0026#39; starts at byte position 1 U+006C \u0026#39;l\u0026#39; starts at byte position 2 U+006C \u0026#39;l\u0026#39; starts at byte position 3 U+006F \u0026#39;o\u0026#39; starts at byte position 4 U+002C \u0026#39;,\u0026#39; starts at byte position 5 U+0020 \u0026#39; \u0026#39; starts at byte position 6 U+4E16 \u0026#39;世\u0026#39; starts at byte position 7 U+754C \u0026#39;界\u0026#39; starts at byte position 10 [72 101 108 108 111 44 32 228 184 150 231 149 140] [72 101 108 108 111 44 32 19990 30028]` Strings and Unicode Go supports Unicode characters, which means that strings can contain characters from any language. This is because Go uses UTF-8 encoding for strings, which can represent all Unicode characters.\npackage main import \u0026#34;fmt\u0026#34; func main() { const nihongo = \u0026#34;日本語\u0026#34; for index, runeValue := range nihongo { fmt.Printf(\u0026#34;%#U starts at byte position %d\\n\u0026#34;, runeValue, index) } } Output\nU+65E5 \u0026#39;日\u0026#39; starts at byte position 0 U+672C \u0026#39;本\u0026#39; starts at byte position 3 U+8A9E \u0026#39;語\u0026#39; starts at byte position 6 Conclusion Strings are a fundamental data type in Go, and understanding how to work with them is essential for any Go programmer. In this article, we explored the basics of strings in Go, including their immutability, basic operations, manipulation, iteration, and Unicode support. Armed with this knowledge, you should be well-equipped to handle strings in your Go programs.\nFor more information on strings and other data types in Go, check out the official strings package documentation.\nHappy coding!\n","href":"/2024/07/understanding-string-data-type-in-go.html","title":"Understanding String Data Type in Go: Basics and Practical Examples"},{"content":"Go, also known as Golang, is a statically typed language developed by Google. It\u0026rsquo;s known for its simplicity and efficiency, especially when it comes to systems and concurrent programming. In this article, we\u0026rsquo;ll explore the numeric types in Go and provide practical examples to illustrate their usage.\nBasic Numeric Types Go offers several basic numeric types categorized into integers, floating point numbers, and complex numbers. Here’s a quick overview:\nInteger Integer types are divided into two categories, signed and unsigned. The signed integers int8, int16, int32, int64 can hold both negative and positive values, whereas unsigned integers int8, int16, int32, int64 can only hold positive values and zero.\nHere’s an example of how you can declare and initialize an integer variable in Go:\n`package main import \u0026#34;fmt\u0026#34; func main() { var a int8 = 127 // a := int8(127) var b uint8 = 255 // b := uint8(255) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, b, b) }` Output\nType: int8 Value: 127 Type: uint8 Value: 255 Floating Point go has two floating point types: float32 and float64. The numbers represent single and double precision floating point numbers respectively.\nHere’s an example of how you can declare and initialize a floating point variable in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { var pi float64 = 3.14159 fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, pi, pi) } Output\nType: float64 Value: 3.14159` Complex Numbers Go has two complex number types: complex64 and complex128. The numbers represent complex numbers with float32 and float64 real and imaginary parts respectively.\nHere’s an example of how you can declare and initialize a complex number variable in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { c := complex(3, 4) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, c, c) } Output\nType: complex128 Value: (3+4i) Numeric Literals Go supports several numeric literals, including decimal, binary, octal, and hexadecimal. Here’s an example of how you can declare and initialize numeric literals in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { a := 42 b := 0b101010 // binary literal c := 0o52 // octal literal d := 0x2a // hexadecimal literal fmt.Println(a, b, c, d) } Output\n42 42 42 42 Numeric Operations Go supports several arithmetic operations on numeric types, including addition, subtraction, multiplication, division, and modulus. Here’s an example of how you can perform arithmetic operations in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { a := 10 b := 20 sum := a + b diff := a - b product := a * b quotient := a / b remainder := a % b fmt.Println(sum, diff, product, quotient, remainder) } Output\n30 -10 200 0 10 Conclusion Go provides a rich set of numeric types and operations that make it easy to work with numbers in your programs. By understanding the different numeric types and their usage, you can write efficient and reliable code that performs well in a variety of scenarios.\nFor more information on Go’s numeric types, you can refer to the official Go documentation .\nHappy coding!\n","href":"/2024/07/understanding-numeric-data-type-in-go.html","title":"Understanding Numeric Data Type In Go : Basics and Practical Examples"},{"content":"In the Go programming language, as in many other programming languages, the boolean data type is fundamental. It represents truth values, either true or false. Booleans are crucial in software development for decision-making, allowing developers to control the flow of execution through conditional statements like if, else, and looping constructs such as for.\nDeclaration and Initialization\nTo declare a boolean in Go, you use the keyword bool. Here\u0026rsquo;s how you can declare and initialize a boolean variable:\nvar myBool bool = true This code snippet shows how to initialize a boolean variable named myBool with the value true.\nIn this line, isOnline is a boolean variable that is initialized to true . Alternatively, Go supports type inference where the compiler automatically detects the type based on the initial value:\nisOnline := true This shorthand method is preferred in Go for its simplicity and readability.\nBoolean in conditional statement Booleans are extensively used in conditional statements. Here\u0026rsquo;s an example of how to use a boolean in an if and else statement:\npackage main import \u0026#34;fmt\u0026#34; func main() { isOnline := true if isOnline { fmt.Println(\u0026#34;User is online\u0026#34;) } else { fmt.Println(\u0026#34;User is offline\u0026#34;) } } Output\nUser is online Practical example: User Authentication Let\u0026rsquo;s create a practical example where booleans are used to check whether a user\u0026rsquo;s username and password match the expected values:\npackage main import \u0026#34;fmt\u0026#34; func main() { username := \u0026#34;admin\u0026#34; password := \u0026#34;password\u0026#34; inputUsername := \u0026#34;admin\u0026#34; inputPassword := \u0026#34;password\u0026#34; if username == inputUsername \u0026amp;\u0026amp; password == inputPassword { fmt.Println(\u0026#34;User authenticated\u0026#34;) } else { fmt.Println(\u0026#34;Invalid credentials\u0026#34;) } } Output\nUser authenticated in this example, isAuthenticated is a boolean that becomes true if both the username and password match the expected values. This boolean is then used to determine the message to display to the user.\nUsing Booleans with Loops Booleans are also useful in loops to determine when the loop should end. Here\u0026rsquo;s a simple for loop that uses a boolean condition:\npackage main import \u0026#34;fmt\u0026#34; func main() { isRunning := true count := 0 for isRunning { fmt.Println(\u0026#34;Count:\u0026#34;, count) count++ if count == 5 { isRunning = false } } } Output\nCount: 0 Count: 1 Count: 2 Count: 3 Count: 4 In this loop, the boolean expression count \u0026lt; 5 determines whether the loop should continue running.\nConclusion Booleans in Go provide a simple yet powerful way to handle decision-making in your programs. They are essential for executing different code paths under different conditions, handling user authentication, controlling loops, and more.\nAs you continue to develop in Go, you\u0026rsquo;ll find that booleans ar an indispensable part of many common programming task.\nNow that you have a good understanding of booleans in Go, you can start using them in your programs to make them more dynamic and responsive to different conditions.\nFor more information on booleans and other data types in Go, check out the official builtin package documentation.\nHappy coding!\n","href":"/2024/07/understanding-booleans-in-go-basics.html","title":"Understanding Booleans Data Type in Go: Basics and Practical Examples"},{"content":"If you\u0026rsquo;re just getting started with Laravel or even if you\u0026rsquo;ve been working with it for a while, using the right tools can make a big difference. Visual Studio Code (VS Code) is one of the most popular code editors among web developers, and thankfully, it has a great ecosystem of extensions that can help boost your productivity when working with Laravel.\nIn this article, we\u0026rsquo;ll go through five essential VS Code extensions that you should install if you\u0026rsquo;re working with Laravel. These tools will help you write code faster, reduce bugs, and improve your workflow overall.\n1. Laravel Blade Snippets This extension provides syntax highlighting and snippets for Laravel Blade. It makes writing Blade templates much easier by auto-completing common directives like @if, @foreach, @csrf, and more.\nWhy it\u0026rsquo;s helpful:\nSpeeds up writing Blade views Reduces typos in directives Supports auto-complete and syntax colors Install: You can find it on the VS Code marketplace by searching Laravel Blade Snippets by Winnie Lin.\n2. Laravel Artisan The Laravel Artisan extension allows you to run Artisan commands directly from VS Code without having to switch to the terminal. You can quickly create controllers, models, migrations, and more with just a few clicks.\nWhy it\u0026rsquo;s helpful:\nAccess Artisan commands via command palette Fast scaffolding for common tasks Works well in any Laravel version Install: Look for Artisan by Ryan Naddy in the VS Code marketplace.\n3. Laravel Extra Intellisense This extension adds improved IntelliSense support for Laravel projects, giving you better autocompletion for facades, routes, models, and other Laravel features.\nWhy it\u0026rsquo;s helpful:\nBetter code suggestions and navigation Works seamlessly with Laravel\u0026rsquo;s facades Saves time looking up class names Install: Search Laravel Extra Intellisense by amiralizadeh9480.\n4. PHP Intelephense While not Laravel-specific, this extension is a must-have for PHP developers. It provides advanced PHP IntelliSense, diagnostics, and more. Combined with Laravel Extra Intellisense, it gives a robust development experience.\nWhy it\u0026rsquo;s helpful:\nFaster autocompletion Real-time error checking Supports namespaces, classes, and functions Install: Search for PHP Intelephense by Ben Mewburn.\n5. Laravel goto Controller This extension allows you to quickly navigate from a route or Blade file to the corresponding controller method. It\u0026rsquo;s great when you\u0026rsquo;re working on medium to large Laravel projects and want to jump between files quickly.\nWhy it\u0026rsquo;s helpful:\nQuickly locate controller methods Jump between route, view, and controller Increases navigation speed Install: Look for Laravel goto Controller by codingyu.\nFinal Thoughts Using the right extensions can make your Laravel development process much smoother and more enjoyable. These five extensions cover the essentials: writing Blade templates, navigating controllers, running Artisan commands, and getting smarter IntelliSense.\nIf you\u0026rsquo;re learning Laravel, these tools can help you focus on writing code instead of memorizing every command or directive. And if you\u0026rsquo;re working on a big project, they\u0026rsquo;ll save you time and energy.\nGive them a try and see how much better your coding experience becomes. Happy coding!\n","href":"/2024/04/5-laravel-extensions-that-you-must-install-on-your-visual-studio-code.html","title":"5 Laravel extensions that you must install on your Visual Studio Code"},{"content":"Ketika kita pertama kali melangkah ke dalam dunia pengembangan web, rasanya seperti memasuki sebuah labirin yang penuh dengan kode dan logika yang rumit. Namun, ada sesuatu yang menarik tentang proses belajar bagaimana segala sesuatu terhubung dan bekerja bersama untuk membentuk sebuah aplikasi web.\nApakah Anda sedang mencari hobi baru atau ingin mengejar karier sebagai pengembang web, membangun aplikasi pertama Anda adalah pengalaman yang sangat berharga. Dengan memahami dasar-dasar pengembangan web, Anda akan memiliki dasar yang kuat untuk mempelajari teknologi-teknologi baru dan membangun aplikasi yang lebih kompleks di masa depan.\nDalam blog kali ini, saya akan membawa Anda melalui proses pembuatan aplikasi web pertama Anda dengan Laravel, sebuah framework PHP yang akan memudahkan kita mengatur dan menulis kode. Dengan Laravel, tugas-tugas yang dulu tampak rumit sekarang bisa kita lakukan dengan lebih terorganisir dan efisien.\nSaya akan menunjukkan kepada Anda bahwa siapa pun bisa mulai membuat aplikasi, dan dengan sedikit kesabaran serta ketekunan, Anda akan bisa membuat sesuatu yang bisa Anda banggakan. Jadi, mari kita mulai petualangan ini bersama-sama dan lihat apa yang bisa kita ciptakan!\nLangkah 1: Persiapan dan Instalasi Sebelum kita mulai, ada beberapa alat yang perlu Anda siapkan dan install di komputer Anda:\nPHP: Versi 7.3 atau lebih tinggi diperlukan. Unduh dari situs resmi PHP . Composer: Manajemen dependensi untuk PHP. Unduh dari situs resmi Composer . Server Web: Gunakan XAMPP atau MAMP untuk pengembangan lokal. Text Editor: Visual Studio Code atau Sublime Text disarankan. Terminal atau Command Prompt: Untuk menjalankan perintah Laravel. Node.js (Opsional): Untuk menjalankan npm atau development mode. Langkah 2: Instalasi Laravel Buka terminal atau command prompt dan jalankan perintah berikut:\ncomposer create-project laravel/laravel example-app **namaAplikasi** Sesuaikan namaAplikasi dengan nama yang Anda inginkan. Proses ini akan mengunduh dan menginstal Laravel serta dependensinya.\nLangkah 3: Menjelajahi Struktur Laravel Setelah instalasi, Anda akan memiliki struktur folder yang dapat dijelajahi sebagai berikut:\napp/: Berisi kode inti aplikasi Anda seperti controllers dan models. bootstrap/: Mengandung file app.php yang melakukan bootstrap framework dan konfigurasi autoloading. config/: Berisi semua file konfigurasi aplikasi Anda. database/: Tempat untuk migrasi database, seeders, dan factories. public/: Root publik aplikasi Anda dengan index.php yang mengarahkan semua permintaan. resources/: Berisi file view Blade, file sumber (LESS, SASS, JS), dan file bahasa. routes/: Berisi semua file rute untuk aplikasi Anda termasuk web, api, console, dan channels. storage/: Direktori untuk menyimpan file yang diunggah, cache, view dikompilasi, dan logs. tests/: Berisi tes otomatis Anda termasuk PHPUnit tests. vendor/: Berisi pustaka Composer dependensi aplikasi Anda. .env: File konfigurasi lingkungan untuk aplikasi Anda. .env.example: Template file .env. .gitignore: Menentukan file apa yang tidak akan ditrack oleh Git. artisan: Command-line interface untuk Laravel. composer.json: File konfigurasi untuk Composer. composer.lock: File kunci untuk dependensi yang diinstal oleh Composer. package.json: Menentukan dependensi Node.js. phpunit.xml: File konfigurasi untuk PHPUnit. README.md: File markdown yang berisi informasi tentang aplikasi. vite.config.js: File konfigurasi untuk Vite yang digunakan dalam pengembangan front-end. Langkah 4: Menjalankan Web Pertama Anda Jalankan perintah berikut di terminal vscode ataupun terminal kesayangan anda:\nphp artisan serve Perintah ini akan menjalankan server pengembangan lokal dan memberikan Anda URL untuk mengakses aplikasi web Anda, seperti link dibawah ini.\nhttp://127.0.0.1:8000 http://localhost:8000 Secara default Laravel akan berjalan di port 8000, jika port tersebut sudah digunakan, maka Laravel akan berjalan di port 8001, 8002, dan seterusnya, namun port tersebut bisa diubah sesuai dengan keinginan anda dengan cara seperti di bawah ini:\nphp artisan serve --port=8080 Buka browser dan kunjungi URL yang diberikan. Anda akan melihat halaman selamat datang Laravel.\n","href":"/2024/04/belajar-membuat-aplikasi-pertama-anda-dengan-laravel.html","title":"Belajar Membuat Aplikasi Pertama Anda dengan Laravel"},{"content":"Learning Golang recently opened up new perspectives for me in software development. One of the best ways to solidify your understanding is by teaching others. That’s why in this article, I’m sharing my experience installing Go on Linux—using both Snap and manual source installation.\nWriting this guide not only helps others get started, but also helps reinforce the steps in my own memory.\nInstalling Golang Using Snap Snap is a universal package manager developed by Canonical (Ubuntu’s creator). It simplifies app installation by bundling dependencies, ensuring compatibility across most Linux distributions.\nEnsure Snap is Installed\nOn many modern Linux distros, Snap is pre-installed. If not, you can install it via terminal:\nsudo apt update sudo apt install snapd Install Go via Snap\nsudo snap install go --classic Verify the Installation\ngo version That’s it! You’ve successfully installed Go using Snap.\n🛠️ Installing Golang from Official Source If you want more control over your Go installation or prefer not to use Snap, manual installation is the way to go.\nDownload the Official Go Tarball\nVisit the official Go downloads page and download the latest version. Example:\nwget https://go.dev/dl/go1.16.3.linux-amd64.tar.gz Extract the Archive to /usr/local\nsudo tar -C /usr/local -xzf go1.16.3.linux-amd64.tar.gz Update Your PATH\nAdd Go’s binary path to your environment variable:\nexport PATH=$PATH:/usr/local/go/bin Add that line to ~/.bashrc or ~/.zshrc, then apply:\nsource ~/.bashrc Verify the Installation\ngo version Snap vs Manual Installation – Which One is Better? Method Pros Cons Snap Quick, easy, auto-updates Slightly slower start-up time Source Full control, latest versions Manual setup \u0026amp; maintenance Conclusion Whether you choose Snap or manual installation, both methods are solid and effective. Snap is faster for beginners, while manual installation is great for advanced users or multi-version management.\nNow that Go is installed, you\u0026rsquo;re ready to build high-performance APIs, CLI tools, or even web servers. Happy coding with Golang!\n","href":"/2024/04/easiest-way-install-golang-on-linux.html","title":"Easiest Way to Install Golang on Linux: Snap or Manual Source?"},{"content":"Linux is a robust operating system, but occasionally you might encounter a \u0026lsquo;broken update error\u0026rsquo; when trying to update your system through the terminal. This issue can halt your system updates and potentially affect system stability. Here’s a comprehensive guide on how to resolve this error, ensuring your Linux system remains up-to-date and secure.\nUnderstanding the Error\nA broken update error in Linux typically occurs when package dependencies are unsatisfied, when there are conflicts between packages, or when the package repositories are not correctly configured. This can lead to a partial or failed update, rendering your system\u0026rsquo;s package manager unable to proceed with updates.\nStep 1: Check Internet Connection\nBefore proceeding, ensure your internet connection is stable. An interrupted or weak connection can cause update processes to fail. Use ping command to check your connectivity, for example:\nping google.com Step 2: Update Repository Lists\nStart by refreshing your repository lists. This ensures that your package manager has the latest information about available packages and their dependencies:\nsudo apt-get update For non-Debian based distributions, replace apt-get with the package manager relevant to your distribution (like yum for Fedora or pacman for Arch Linux).\nStep 3: Upgrade Packages\nAttempt to upgrade all your system packages with:\nsudo apt-get upgrade This might resolve dependency issues that were causing the update process to break.\nStep 4: Fix Broken Packages\nIf the upgrade doesn’t resolve the issue, you can specifically target and fix broken packages:\nsudo apt-get install -f The -f flag stands for “fix broken”. It repairs broken dependencies, helping the package manager to recover.\nStep 5: Clean Up\nClear out the local repository of retrieved package files. It\u0026rsquo;s a good practice to clean up the cache to free space and remove potentially corrupted files:\nsudo apt-get clean Step 6: Remove Unnecessary Packages\nRemove packages that were automatically installed to satisfy dependencies for other packages and are now no longer needed:\nsudo apt-get autoremove Step 7: Configure Package Manager\nIf the error persists, reconfigure the package manager. This can help resolve any corrupt configurations:\nsudo dpkg --configure -a Step 8: Manually Resolve Dependencies\nSometimes, you may need to manually fix dependencies. Look at the error messages carefully. They often indicate which package is causing the problem. You can then either remove, reinstall, or update that specific package.\nStep 9: Check for Repository Issues\nEnsure that your system’s repositories are correctly set up. Incorrect or outdated sources can cause update errors. The repository configuration files are typically located in /etc/apt/sources.list and /etc/apt/sources.list.d/. Make sure they contain the correct URLs and distribution names.\nStep 10: Seek Community Support\nIf you’ve tried all the above and still face issues, seek support from the Linux community. Linux has a vibrant community on forums like Ask Ubuntu, Linux Mint forums, or Fedora forums, depending on your distribution.\nIf the method above has not made any changes and is still experiencing errors, try the method below:\nStep 1: Identify and Stop the Conflicting Process\nYou can find out what process is holding the lock by using the process ID (PID) given in the error message. In your case, the PID is 1582.\nRun\nps -f -p 1582 ```in the terminal to see details about the process. If it\u0026#39;s a process that can be safely stopped, use sudo kill -9 1582\n**Step 2: Remove the Lock Files** If you are certain no other apt processes are running, you can manually remove the lock files. Use ```bash sudo rm /var/lib/apt/lists/lock Additionally, you might need to remove the lock file in the cache directory:\nsudo rm /var/cache/apt/archives/lock And the lock file in the dpkg directory:\nsudo rm /var/lib/dpkg/lock Note: This is generally not recommended unless you\u0026rsquo;re sure that no apt processes are running, as it can potentially corrupt your package database.\nStep 4 : Restart your computer\nConclusion\nResolving broken update errors in Linux involves a systematic approach to identify and fix package dependencies, configuration issues, and repository errors. By following these steps, most update issues can be resolved directly from the terminal, restoring the smooth functioning of your Linux system. Remember, regular updates are crucial for security and stability, so resolving these errors promptly is important.\n","href":"/2023/11/how-to-fix-broken-update-error-in-linux.html","title":"How to fix broken update error in linux (Terminal)"},{"content":"In the realm of modern web development, providing a seamless user experience and enhancing the overall performance of your web applications is paramount. One essential aspect that plays a pivotal role in achieving these goals is efficient data presentation and manipulation. This is where Yajra DataTables comes into the picture.\nYajra DataTables is a powerful and versatile jQuery-based plugin for Laravel, designed to simplify the process of displaying data in tabular form with advanced features such as filtering, sorting, pagination, and more. It empowers developers to create interactive and dynamic data tables effortlessly, significantly improving how data is showcased to end users.\nThis article will delve into the step-by-step process of installing and configuring Yajra DataTables in Laravel. Whether you are a seasoned Laravel developer or just starting with the framework, this guide will walk you through the necessary setup, providing you with the knowledge to harness the full potential of Yajra DataTables in your Laravel projects.\nSo, if you\u0026rsquo;re ready to elevate your data presentation game and unlock a world of possibilities in your Laravel applications, let\u0026rsquo;s dive in and get started with Yajra DataTables!\nSo let\u0026rsquo;s get started on how to install and configure Yajra Datatable in Laravel.\nThe first step you must be to visit the official website of Yajra Datatable , if you want to follow my way please follow the guide below.\n`composer require yajra/laravel-datatables-oracle:\u0026#34;^10.3.1\u0026#34;` If you want to change the version of Yajra Datatable you must change the value \u0026ldquo;^10.3.1\u0026rdquo; to an old version or if you want to get the new version you can use the script below.\ncomposer require yajra/laravel-datatables-oracle By default, you will download the latest version from Yajra Datatable.\nSo, in the next step, we will configure the provider in Laravel so that you go to the file in the path folder, Config/app.php, and then add the script below to your code.\nproviders\u0026#39; \\=\u0026gt; \\[ // ... Yajra\\\\DataTables\\\\DataTablesServiceProvider::class, \\], If you have put your code into the file app.php, now you can follow this step to publish assets and vendors from Yajra Datatable so that you can use Yajra Datatable on your project.\nphp artisan vendor:publish --tag=datatables Now you can use Datatable on your projects yeah, now if you want to call the Datatable in your blade or view you must add style and script from Datatable because Datatable is a package from jquery.\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.datatables.net/1.13.4/css/dataTables.jqueryui.min.css\u0026#34; /\u0026gt; If you have been adding the following script on the top now you add the script below to call the data table from javascript below. ```js @push(\u0026#39;after-script\u0026#39;) \u0026lt;script\u0026gt; $(\u0026#39;#tb_user\u0026#39;).DataTable({ processing: true, serverSide: true, ajax: { url: \u0026#34;{!! url()-\u0026gt;current() !!}\u0026#34;, }, columns: [ { data: \u0026#39;DT_RowIndex\u0026#39;, name: \u0026#39;id\u0026#39; }, { data: \u0026#39;photo\u0026#39;, name: \u0026#39;photo\u0026#39; }, { data: \u0026#39;email\u0026#39;, name: \u0026#39;email\u0026#39; }, { data: \u0026#39;username\u0026#39;, name: \u0026#39;username\u0026#39; }, { data: \u0026#39;action\u0026#39;, name: \u0026#39;action\u0026#39;, orderable: false, searchable: false }, ], }); \u0026lt;/script\u0026gt; @endpush And then, you must be sent data from the controller to view with script below.\nif (request()-\u0026gt;ajax()) { $query = Layanan::where(\u0026#39;users\\_id\u0026#39;, Auth::user()-\u0026gt;id)-\u0026gt;get(); return datatables()-\u0026gt;of($query) -\u0026gt;addIndexColumn() -\u0026gt;editColumn(\u0026#39;photo\u0026#39;, function ($item) { return $item-\u0026gt;photo ? \u0026#39;\u0026lt;img src=\u0026#34;\u0026#39; . url(\u0026#39;storage/\u0026#39; . $item-\u0026gt;photo) . \u0026#39;\u0026#34; style=\u0026#34;max-height: 50px;\u0026#34; /\u0026gt;\u0026#39; : \u0026#39;-\u0026#39;; }) -\u0026gt;editColumn(\u0026#39;action\u0026#39;, function ($item) { return \u0026#39; \u0026lt;a href=\u0026#34;\u0026#39; . route(\u0026#39;user.edit\u0026#39;, $item-\u0026gt;id) . \u0026#39;\u0026#34; class=\u0026#34;btn btn-sm btn-primary\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-pencil-alt\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;form action=\u0026#34;\u0026#39; . route(\u0026#39;user.destroy\u0026#39;, $item-\u0026gt;id) . \u0026#39;\u0026#34; method=\u0026#34;POST\u0026#34; style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; \u0026#39; . method\\_field(\u0026#39;delete\u0026#39;) . csrf\\_field() . \u0026#39; \u0026lt;button type=\u0026#34;submit\u0026#34; class=\u0026#34;btn btn-sm btn-danger\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-trash\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026#39;; }) -\u0026gt;rawColumns(\\[\u0026#39;photo\u0026#39;, \u0026#39;action\u0026#39;\\]) -\u0026gt;make(true); } return view(\u0026#39;user.index\u0026#39;); Okay, the data table installation and configuration are complete, now you can use and display data using the data table on Laravel, if you have any stuck or questions, you can contact me or add your comment below, Thank you.\n","href":"/2023/08/how-to-install-and-configure-yajra.html","title":"how to install and configure yajra datatable in Laravel "},{"content":"Sebelum kita melakukan cloning project Laravel dari GitHub, pastikan kamu telah menginstal tools berikut agar proses berjalan lancar.\nTools di bawah ini sangat penting. Tanpa keduanya, kamu tidak akan bisa menjalankan project Laravel dengan benar.\nGit Composer Untuk mendapatkan project Laravel dari GitHub, ada dua cara:\nMenggunakan Git Mengunduh via file ZIP Tidak ada perbedaan signifikan, hanya beda cara ambilnya. Kita bahas dua-duanya.\n💻 Cara Clone Menggunakan Git Salin URL repository dari GitHub (HTTPS atau SSH).\nBuka terminal dan jalankan:\ngit clone \u0026lt;url-repository\u0026gt; Kalau ingin beri nama folder project-nya:\ngit clone \u0026lt;url-repository\u0026gt; nama-folder Tunggu proses cloning selesai.\nMasuk ke folder project:\ncd nama-folder Install dependensi:\ncomposer install Salin file .env:\ncp .env.example .env Generate key:\nphp artisan key:generate Bersihkan konfigurasi cache (opsional):\nphp artisan config:clear Jalankan Laravel:\nphp artisan serve Lalu buka http://127.0.0.1:8000 di browser favorit kamu.\n📦 Cara Download Menggunakan ZIP Klik tombol Code di GitHub, lalu pilih Download ZIP. Ekstrak filenya. Buka terminal, masuk ke folder hasil ekstrak. Lanjutkan langkah instalasi seperti pada metode Git di atas (composer install, dll). Sebagian besar developer lebih suka menggunakan Git, tapi metode ZIP juga tetap valid. Silakan pilih yang paling nyaman buat kamu.\nSemoga bermanfaat!\n","href":"/2023/04/cara-menjalankan-project-laravel-clone.html","title":"Cara Menjalankan Project Laravel Clone dari GitHub"},{"content":"Hi, I\u0026rsquo;m Wiku Karno! 👋 Welcome to BuanaCoding – where I share my journey as a software developer and help others build better applications through practical tutorials and real-world insights.\nWhat I Do I\u0026rsquo;m a passionate software developer who loves diving deep into modern programming languages and frameworks. My expertise spans across several key areas:\nGo Programming Go is my primary language of choice. I\u0026rsquo;ve written extensively about Go fundamentals, advanced concepts like goroutines and channels, building REST APIs, working with databases, and following Go best practices. Whether you\u0026rsquo;re just starting with Go or looking to level up your skills, you\u0026rsquo;ll find comprehensive guides here.\nWeb Development I specialize in full-stack web development using:\nLaravel/PHP for robust web applications Python/FastAPI for high-performance APIs Modern development practices and clean architecture DevOps \u0026amp; Linux System administration and deployment are crucial skills for any developer. I share tutorials on:\nLinux server management and troubleshooting Docker containerization Nginx configuration and SSL setup Application deployment strategies Security \u0026amp; Best Practices Security isn\u0026rsquo;t an afterthought – it\u0026rsquo;s built into everything I do. I cover topics like:\nWeb application security Password management and authentication Modern authentication methods (Passkeys, WebAuthn) Protecting against common security threats Developer Productivity I\u0026rsquo;m always exploring tools and techniques that make developers more productive:\nCode editors and essential extensions Development environment setup Automation and workflow optimization My Mission BuanaCoding exists to bridge the gap between complex technical concepts and practical, actionable knowledge. I believe that:\nLearning should be accessible – I write for developers at all levels, from beginners to experienced professionals Real-world examples matter – Every tutorial includes practical examples you can actually use in your projects Quality over quantity – I focus on creating comprehensive, well-researched content rather than quick tips Community drives growth – The best learning happens when we share knowledge and learn from each other Why Trust My Content? I don\u0026rsquo;t just write about technologies – I use them in real projects. Every tutorial and guide is based on hands-on experience, tested solutions, and lessons learned from actual development work.\nMy content has helped thousands of developers:\nLearn Go programming from scratch to advanced concepts Build secure web applications with Laravel and FastAPI Deploy applications to production servers Implement modern security practices Optimize their development workflows Let\u0026rsquo;s Connect Whether you\u0026rsquo;re just starting your programming journey or you\u0026rsquo;re an experienced developer looking to expand your skills, I\u0026rsquo;m here to help. Feel free to reach out through the comments on any article – I read and respond to every message.\nHappy coding, and welcome to the BuanaCoding community! 🚀\nP.S. All tutorials and code examples on this site are thoroughly tested and regularly updated to reflect the latest best practices and framework versions.\n","href":"/about/","title":"About"},{"content":"If you would like to get in touch or collaborate with me — including freelance work — feel free to reach out via the contact information below.\nEmail: buanacoding@gmail.com ","href":"/contact/","title":"Contact"},{"content":"If you require any more information or have any questions about our site\u0026rsquo;s disclaimer, please feel free to contact us by email at buanacoding@gmail.com All the information on this website - https://www.buanacoding.com - is published in good faith and for general information purpose only. buanacoding does not make any warranties about the completeness, reliability and accuracy of this information. Any action you take upon the information you find on this website (buanacoding), is strictly at your own risk. buanacoding will not be liable for any losses and/or damages in connection with the use of our website. Our disclaimer was generated with the help of the Disclaimer Generator.\nFrom our website, you can visit other websites by following hyperlinks to such external sites. While we strive to provide only quality links to useful and ethical websites, we have no control over the content and nature of these sites. These links to other websites do not imply a recommendation for all the content found on these sites. Site owners and content may change without notice and may occur before we have the opportunity to remove a link which may have gone \u0026lsquo;bad\u0026rsquo;.\nPlease be also aware that when you leave our website, other sites may have different privacy policies and terms which are beyond our control. Please be sure to check the Privacy Policies of these sites as well as their \u0026ldquo;Terms of Service\u0026rdquo; before engaging in any business or uploading any information.\nConsent By using our website, you hereby consent to our disclaimer and agree to its terms.\nUpdates Should we update, amend or make any changes to this document, those changes will be prominently posted here.\n","href":"/disclaimer/","title":"Disclaimer"},{"content":"At BuanaCoding, accessible from https://www.buanacoding.com , one of our main priorities is the privacy of our visitors. This Privacy Policy document contains the types of information that are collected and recorded by BuanaCoding and how we use it.\nIf you have additional questions or require more information about our Privacy Policy, do not hesitate to contact us.\nLog Files BuanaCoding follows a standard procedure of using log files. These files log visitors when they visit websites. All hosting companies do this as part of hosting services\u0026rsquo; analytics. The information collected by log files includes Internet Protocol (IP) addresses, browser type, Internet Service Provider (ISP), date and time stamps, referring/exit pages, and possibly the number of clicks. These are not linked to any information that is personally identifiable. The purpose of the information is to analyze trends, administer the site, track users’ movement around the website, and gather demographic information.\nOur Privacy Policy was created with the help of the Privacy Policy Generator .\nGoogle DoubleClick DART Cookie Google is one of the third-party vendors on our site. It also uses cookies, known as DART cookies, to serve ads to our site visitors based on their visit to www.website.com and other sites on the internet. However, visitors may choose to decline the use of DART cookies by visiting the Google Ad and Content Network Privacy Policy .\nPrivacy Policies of Advertising Partners You may refer to this section to find the Privacy Policy for each of the advertising partners of BuanaCoding.\nThird-party ad servers or ad networks use technologies like cookies, JavaScript, or Web Beacons in their respective advertisements and links that appear on BuanaCoding, which are sent directly to users’ browsers. They automatically receive your IP address when this occurs. These technologies are used to measure the effectiveness of their advertising campaigns and/or to personalize the advertising content that you see on websites you visit.\nPlease note that BuanaCoding has no access to or control over these cookies that are used by third-party advertisers.\nThird-Party Privacy Policies BuanaCoding’s Privacy Policy does not apply to other advertisers or websites. Thus, we advise you to consult the respective Privacy Policies of these third-party ad servers for more detailed information. This may include their practices and instructions about how to opt out of certain options.\nYou can choose to disable cookies through your individual browser options. More detailed information about cookie management with specific web browsers can be found at the respective websites of those browsers.\nChildren’s Information Another part of our priority is adding protection for children while using the internet. We encourage parents and guardians to observe, participate in, and/or guide and advise their children’s online activity.\nBuanaCoding does not knowingly collect any personally identifiable information from children under the age of 13. If you think your child has provided such information on our website, we strongly encourage you to contact us immediately and we will make our best efforts to promptly remove such information from our records.\nOnline Privacy Policy Only This Privacy Policy applies only to our online activities and is valid for visitors to our website with regard to the information that they shared and/or collect in BuanaCoding. This policy does not apply to any information collected offline or via channels other than this website.\n","href":"/privacy-policy/","title":"Privacy Policy"}]