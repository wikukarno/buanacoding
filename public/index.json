[{"content":"I still remember the first time I built an API for a client\u0026rsquo;s e-commerce platform. I chose FastAPI because everyone on Twitter was hyping it up\u0026ndash;\u0026ldquo;blazing fast,\u0026rdquo; \u0026ldquo;modern Python,\u0026rdquo; \u0026ldquo;async everything.\u0026rdquo; The initial setup was smooth, and I built endpoints quickly.\nThen reality hit.\nThe client needed a complex admin panel to manage products, orders, and users. They wanted role-based permissions with granular controls. They needed integration with their existing Django authentication system. And oh, they wanted detailed audit logs for every database change.\nI spent three weeks building features that would have been three clicks in Django admin. I wrote hundreds of lines of permission logic that Django has built-in. I implemented audit trails manually while Django has well-tested libraries for this.\nThat\u0026rsquo;s when I learned a crucial lesson: framework hype doesn\u0026rsquo;t pay your bills. Shipping features does.\nDon\u0026rsquo;t get me wrong\u0026ndash;FastAPI is excellent for certain use cases (I still use it for microservices). But Django REST Framework has won hundreds of my projects because it gets complex applications to production faster with less code, fewer bugs, and better maintainability.\nIf you\u0026rsquo;re building a REST API that needs authentication, permissions, admin panels, complex database relationships, and third-party integrations\u0026ndash;Django REST Framework will save you weeks of work.\nThis guide will show you how to build production-ready REST APIs with Django REST Framework from scratch. No toy examples. No \u0026ldquo;hello world\u0026rdquo; nonsense. Just real code that handles authentication, permissions, pagination, filtering, testing, and deployment.\nBy the end, you\u0026rsquo;ll understand why companies like Instagram, Mozilla, Red Hat, and Eventbrite chose Django for their APIs.\nLet\u0026rsquo;s build something real.\nDjango REST Framework vs FastAPI: Honest Comparison Before we start, let\u0026rsquo;s address the elephant in the room. Everyone asks: \u0026ldquo;Should I use Django REST Framework or FastAPI?\u0026rdquo;\nHere\u0026rsquo;s my honest take after building 50+ production APIs with both:\nWhen to Choose Django REST Framework Complex business applications:\nE-commerce platforms with products, orders, inventory, shipping CMS with content management, versioning, workflows SaaS applications with multi-tenancy, billing, subscriptions Enterprise apps with complex permissions and user hierarchies Need Django\u0026rsquo;s ecosystem:\nDjango Admin panel (saves weeks of development) Django Auth (users, groups, permissions out of the box) Django ORM (best Python ORM, period) Thousands of Django packages (payments, notifications, storage, etc.) Database-heavy operations:\nComplex queries with joins, aggregations, subqueries Transaction management Multiple database connections Advanced ORM features (select_related, prefetch_related) Built-in batteries:\nAuthentication (session, token, JWT) Permissions system (object-level, model-level) Throttling and rate limiting Pagination (cursor, page number, limit-offset) Filtering and search Browsable API interface (test APIs in browser) Content negotiation (JSON, XML, YAML) Team considerations:\nTeam already knows Django Need strict patterns and conventions Long-term maintenance (Django has 15+ years of stability) When to Choose FastAPI Performance-critical applications:\nMicroservices handling 10,000+ req/sec Real-time applications (WebSockets, SSE) ML model serving with low latency requirements Async-first architecture:\nLots of I/O-bound operations (external APIs, file uploads) Need async database drivers (asyncpg, motor) WebSocket-heavy applications Modern Python features:\nType hints everywhere (automatic validation) Async/await native support Python 3.10+ features Lightweight APIs:\nSimple CRUD without complex business logic Stateless microservices API gateway/proxy Don\u0026rsquo;t need admin panel Greenfield projects:\nNo existing Django code Small team comfortable with minimal structure Can handle building authentication/permissions from scratch Performance Reality Check Yes, FastAPI is faster\u0026ndash;about 2-3x on raw benchmarks. But here\u0026rsquo;s the truth: for 95% of applications, this doesn\u0026rsquo;t matter.\nIf your API spends 50ms querying the database and 5ms in framework overhead, making the framework 3x faster saves you 3ms. Your users won\u0026rsquo;t notice 47ms vs 50ms response time.\nOptimize database queries, add caching, and use CDN before worrying about framework speed.\nMy Recommendation Choose Django REST Framework if:\nBuilding a monolithic application or traditional web app Need admin panel, complex auth, or Django ecosystem Database is primary data source Team knows Django or wants opinionated framework Building MVP and want to ship fast Choose FastAPI if:\nBuilding microservices or serverless functions Need extreme performance (10k+ req/sec per instance) Heavy async/await usage Small, focused APIs without complex business logic Want full control over architecture For this tutorial, we\u0026rsquo;re using Django REST Framework because it teaches you how to build complete, production-ready APIs with all the bells and whistles\u0026ndash;authentication, permissions, admin, testing, deployment.\nIf you want to collect data from external sources to populate your API, check out my web scraping guide to learn how to gather data automatically.\nNow let\u0026rsquo;s build.\nInstalling Django and Django REST Framework Let\u0026rsquo;s set up your development environment.\nPrerequisites:\nPython 3.8 or newer Basic understanding of Python, HTTP, and REST APIs Basic Django knowledge (models, migrations, URLs) helps but not required Create project directory:\nmkdir bookstore-api cd bookstore-api # Create virtual environment python -m venv venv # Activate virtual environment # On Linux/Mac: source venv/bin/activate # On Windows: # venv\\Scripts\\activate # Upgrade pip pip install --upgrade pip Install Django and DRF:\npip install django djangorestframework # Optional but recommended packages pip install django-filter # Filtering pip install djangorestframework-simplejwt # JWT authentication pip install drf-spectacular # OpenAPI/Swagger documentation pip install django-cors-headers # CORS support pip install psycopg2-binary # PostgreSQL (we\u0026#39;ll use SQLite for dev) pip install python-decouple # Environment variables pip install gunicorn # Production WSGI server # Save dependencies pip freeze \u0026gt; requirements.txt Create Django project:\n# Create project django-admin startproject bookstore . # Create API app python manage.py startapp books # Create another app for authentication python manage.py startapp accounts Your project structure:\nbookstore-api/ ├── venv/ ├── bookstore/ # Project settings │ ├── __init__.py │ ├── settings.py │ ├── urls.py │ ├── asgi.py │ └── wsgi.py ├── books/ # Books app │ ├── migrations/ │ ├── __init__.py │ ├── admin.py │ ├── apps.py │ ├── models.py │ ├── serializers.py # We\u0026#39;ll create this │ ├── views.py │ └── urls.py # We\u0026#39;ll create this ├── accounts/ # Custom auth │ └── ... ├── manage.py └── requirements.txt Configure settings.py:\n# bookstore/settings.py INSTALLED_APPS = [ \u0026#39;django.contrib.admin\u0026#39;, \u0026#39;django.contrib.auth\u0026#39;, \u0026#39;django.contrib.contenttypes\u0026#39;, \u0026#39;django.contrib.sessions\u0026#39;, \u0026#39;django.contrib.messages\u0026#39;, \u0026#39;django.contrib.staticfiles\u0026#39;, # Third party \u0026#39;rest_framework\u0026#39;, \u0026#39;rest_framework_simplejwt\u0026#39;, \u0026#39;django_filters\u0026#39;, \u0026#39;drf_spectacular\u0026#39;, \u0026#39;corsheaders\u0026#39;, # Local apps \u0026#39;books\u0026#39;, \u0026#39;accounts\u0026#39;, ] MIDDLEWARE = [ \u0026#39;django.middleware.security.SecurityMiddleware\u0026#39;, \u0026#39;corsheaders.middleware.CorsMiddleware\u0026#39;, # CORS \u0026#39;django.contrib.sessions.middleware.SessionMiddleware\u0026#39;, \u0026#39;django.middleware.common.CommonMiddleware\u0026#39;, \u0026#39;django.middleware.csrf.CsrfViewMiddleware\u0026#39;, \u0026#39;django.contrib.auth.middleware.AuthenticationMiddleware\u0026#39;, \u0026#39;django.contrib.messages.middleware.MessageMiddleware\u0026#39;, \u0026#39;django.middleware.clickjacking.XFrameOptionsMiddleware\u0026#39;, ] # Rest Framework settings REST_FRAMEWORK = { \u0026#39;DEFAULT_AUTHENTICATION_CLASSES\u0026#39;: ( \u0026#39;rest_framework_simplejwt.authentication.JWTAuthentication\u0026#39;, \u0026#39;rest_framework.authentication.SessionAuthentication\u0026#39;, ), \u0026#39;DEFAULT_PERMISSION_CLASSES\u0026#39;: [ \u0026#39;rest_framework.permissions.IsAuthenticatedOrReadOnly\u0026#39;, ], \u0026#39;DEFAULT_PAGINATION_CLASS\u0026#39;: \u0026#39;rest_framework.pagination.PageNumberPagination\u0026#39;, \u0026#39;PAGE_SIZE\u0026#39;: 10, \u0026#39;DEFAULT_FILTER_BACKENDS\u0026#39;: [ \u0026#39;django_filters.rest_framework.DjangoFilterBackend\u0026#39;, \u0026#39;rest_framework.filters.SearchFilter\u0026#39;, \u0026#39;rest_framework.filters.OrderingFilter\u0026#39;, ], \u0026#39;DEFAULT_SCHEMA_CLASS\u0026#39;: \u0026#39;drf_spectacular.openapi.AutoSchema\u0026#39;, } # CORS settings (for frontend) CORS_ALLOWED_ORIGINS = [ \u0026#34;http://localhost:3000\u0026#34;, # React \u0026#34;http://localhost:5173\u0026#34;, # Vite ] # Spectacular settings (API documentation) SPECTACULAR_SETTINGS = { \u0026#39;TITLE\u0026#39;: \u0026#39;Bookstore API\u0026#39;, \u0026#39;DESCRIPTION\u0026#39;: \u0026#39;REST API for online bookstore\u0026#39;, \u0026#39;VERSION\u0026#39;: \u0026#39;1.0.0\u0026#39;, \u0026#39;SERVE_INCLUDE_SCHEMA\u0026#39;: False, } # JWT settings from datetime import timedelta SIMPLE_JWT = { \u0026#39;ACCESS_TOKEN_LIFETIME\u0026#39;: timedelta(minutes=60), \u0026#39;REFRESH_TOKEN_LIFETIME\u0026#39;: timedelta(days=7), \u0026#39;ROTATE_REFRESH_TOKENS\u0026#39;: True, \u0026#39;BLACKLIST_AFTER_ROTATION\u0026#39;: True, } Run initial migrations:\npython manage.py migrate Create superuser:\npython manage.py createsuperuser # Enter username, email, password Test server:\npython manage.py runserver Visit http://localhost:8000/admin/ and login with your superuser credentials. If you see Django admin, you\u0026rsquo;re good to go!\nCreating Models Let\u0026rsquo;s build a bookstore API with books, authors, categories, and reviews.\nCreate models (books/models.py):\nfrom django.db import models from django.contrib.auth.models import User from django.core.validators import MinValueValidator, MaxValueValidator class Author(models.Model): name = models.CharField(max_length=200) bio = models.TextField(blank=True) birth_date = models.DateField(null=True, blank=True) website = models.URLField(blank=True) created_at = models.DateTimeField(auto_now_add=True) updated_at = models.DateTimeField(auto_now=True) class Meta: ordering = [\u0026#39;name\u0026#39;] def __str__(self): return self.name class Category(models.Model): name = models.CharField(max_length=100, unique=True) slug = models.SlugField(unique=True) description = models.TextField(blank=True) class Meta: ordering = [\u0026#39;name\u0026#39;] verbose_name_plural = \u0026#39;Categories\u0026#39; def __str__(self): return self.name class Book(models.Model): title = models.CharField(max_length=300) slug = models.SlugField(unique=True) isbn = models.CharField(max_length=13, unique=True) description = models.TextField() authors = models.ManyToManyField(Author, related_name=\u0026#39;books\u0026#39;) categories = models.ManyToManyField(Category, related_name=\u0026#39;books\u0026#39;) publisher = models.CharField(max_length=200) publication_date = models.DateField() pages = models.PositiveIntegerField() price = models.DecimalField(max_digits=10, decimal_places=2) stock = models.PositiveIntegerField(default=0) cover_image = models.URLField(blank=True) created_at = models.DateTimeField(auto_now_add=True) updated_at = models.DateTimeField(auto_now=True) class Meta: ordering = [\u0026#39;-created_at\u0026#39;] def __str__(self): return self.title @property def average_rating(self): reviews = self.reviews.all() if reviews: return sum([review.rating for review in reviews]) / len(reviews) return 0 @property def is_available(self): return self.stock \u0026gt; 0 class Review(models.Model): book = models.ForeignKey(Book, on_delete=models.CASCADE, related_name=\u0026#39;reviews\u0026#39;) user = models.ForeignKey(User, on_delete=models.CASCADE, related_name=\u0026#39;reviews\u0026#39;) rating = models.PositiveSmallIntegerField( validators=[MinValueValidator(1), MaxValueValidator(5)] ) title = models.CharField(max_length=200) comment = models.TextField() created_at = models.DateTimeField(auto_now_add=True) updated_at = models.DateTimeField(auto_now=True) class Meta: ordering = [\u0026#39;-created_at\u0026#39;] unique_together = [\u0026#39;book\u0026#39;, \u0026#39;user\u0026#39;] # One review per user per book def __str__(self): return f\u0026#34;{self.user.username} - {self.book.title} ({self.rating}/5)\u0026#34; Create and run migrations:\npython manage.py makemigrations python manage.py migrate Register models in admin (books/admin.py):\nfrom django.contrib import admin from .models import Author, Category, Book, Review @admin.register(Author) class AuthorAdmin(admin.ModelAdmin): list_display = [\u0026#39;name\u0026#39;, \u0026#39;birth_date\u0026#39;, \u0026#39;created_at\u0026#39;] search_fields = [\u0026#39;name\u0026#39;, \u0026#39;bio\u0026#39;] prepopulated_fields = {\u0026#39;slug\u0026#39;: (\u0026#39;name\u0026#39;,)} # If you add slug field @admin.register(Category) class CategoryAdmin(admin.ModelAdmin): list_display = [\u0026#39;name\u0026#39;, \u0026#39;slug\u0026#39;] search_fields = [\u0026#39;name\u0026#39;] prepopulated_fields = {\u0026#39;slug\u0026#39;: (\u0026#39;name\u0026#39;,)} @admin.register(Book) class BookAdmin(admin.ModelAdmin): list_display = [\u0026#39;title\u0026#39;, \u0026#39;isbn\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;stock\u0026#39;, \u0026#39;publication_date\u0026#39;, \u0026#39;is_available\u0026#39;] list_filter = [\u0026#39;publication_date\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;authors\u0026#39;] search_fields = [\u0026#39;title\u0026#39;, \u0026#39;isbn\u0026#39;, \u0026#39;description\u0026#39;] filter_horizontal = [\u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;] prepopulated_fields = {\u0026#39;slug\u0026#39;: (\u0026#39;title\u0026#39;,)} @admin.register(Review) class ReviewAdmin(admin.ModelAdmin): list_display = [\u0026#39;book\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;rating\u0026#39;, \u0026#39;created_at\u0026#39;] list_filter = [\u0026#39;rating\u0026#39;, \u0026#39;created_at\u0026#39;] search_fields = [\u0026#39;book__title\u0026#39;, \u0026#39;user__username\u0026#39;, \u0026#39;comment\u0026#39;] Now you can add books via Django admin at http://localhost:8000/admin/. Add a few books to test with.\nCreating Serializers Serializers convert Django models to JSON (and vice versa). Think of them as translators between Python and JSON.\nCreate serializers (books/serializers.py):\nfrom rest_framework import serializers from django.contrib.auth.models import User from .models import Author, Category, Book, Review class AuthorSerializer(serializers.ModelSerializer): books_count = serializers.SerializerMethodField() class Meta: model = Author fields = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;bio\u0026#39;, \u0026#39;birth_date\u0026#39;, \u0026#39;website\u0026#39;, \u0026#39;books_count\u0026#39;, \u0026#39;created_at\u0026#39;] def get_books_count(self, obj): return obj.books.count() class CategorySerializer(serializers.ModelSerializer): books_count = serializers.SerializerMethodField() class Meta: model = Category fields = [\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;slug\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;books_count\u0026#39;] def get_books_count(self, obj): return obj.books.count() class ReviewSerializer(serializers.ModelSerializer): user = serializers.StringRelatedField(read_only=True) user_id = serializers.IntegerField(read_only=True) class Meta: model = Review fields = [\u0026#39;id\u0026#39;, \u0026#39;book\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;user_id\u0026#39;, \u0026#39;rating\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;comment\u0026#39;, \u0026#39;created_at\u0026#39;, \u0026#39;updated_at\u0026#39;] read_only_fields = [\u0026#39;user\u0026#39;] def validate_rating(self, value): if value \u0026lt; 1 or value \u0026gt; 5: raise serializers.ValidationError(\u0026#34;Rating must be between 1 and 5\u0026#34;) return value def create(self, validated_data): # Automatically set user from request validated_data[\u0026#39;user\u0026#39;] = self.context[\u0026#39;request\u0026#39;].user return super().create(validated_data) class BookListSerializer(serializers.ModelSerializer): \u0026#34;\u0026#34;\u0026#34;Lightweight serializer for list view\u0026#34;\u0026#34;\u0026#34; authors = serializers.StringRelatedField(many=True) categories = serializers.StringRelatedField(many=True) average_rating = serializers.ReadOnlyField() is_available = serializers.ReadOnlyField() class Meta: model = Book fields = [ \u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;slug\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;stock\u0026#39;, \u0026#39;average_rating\u0026#39;, \u0026#39;is_available\u0026#39;, \u0026#39;cover_image\u0026#39;, \u0026#39;publication_date\u0026#39; ] class BookDetailSerializer(serializers.ModelSerializer): \u0026#34;\u0026#34;\u0026#34;Detailed serializer for single book view\u0026#34;\u0026#34;\u0026#34; authors = AuthorSerializer(many=True, read_only=True) categories = CategorySerializer(many=True, read_only=True) reviews = ReviewSerializer(many=True, read_only=True) average_rating = serializers.ReadOnlyField() is_available = serializers.ReadOnlyField() reviews_count = serializers.SerializerMethodField() # Write fields (accept IDs) author_ids = serializers.PrimaryKeyRelatedField( queryset=Author.objects.all(), many=True, write_only=True, source=\u0026#39;authors\u0026#39; ) category_ids = serializers.PrimaryKeyRelatedField( queryset=Category.objects.all(), many=True, write_only=True, source=\u0026#39;categories\u0026#39; ) class Meta: model = Book fields = [ \u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;slug\u0026#39;, \u0026#39;isbn\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;author_ids\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;category_ids\u0026#39;, \u0026#39;publisher\u0026#39;, \u0026#39;publication_date\u0026#39;, \u0026#39;pages\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;stock\u0026#39;, \u0026#39;cover_image\u0026#39;, \u0026#39;average_rating\u0026#39;, \u0026#39;is_available\u0026#39;, \u0026#39;reviews\u0026#39;, \u0026#39;reviews_count\u0026#39;, \u0026#39;created_at\u0026#39;, \u0026#39;updated_at\u0026#39; ] read_only_fields = [\u0026#39;slug\u0026#39;] def get_reviews_count(self, obj): return obj.reviews.count() def validate_isbn(self, value): # ISBN-13 validation if len(value) != 13 or not value.isdigit(): raise serializers.ValidationError(\u0026#34;ISBN must be 13 digits\u0026#34;) return value def validate_price(self, value): if value \u0026lt;= 0: raise serializers.ValidationError(\u0026#34;Price must be positive\u0026#34;) return value class BookCreateUpdateSerializer(serializers.ModelSerializer): \u0026#34;\u0026#34;\u0026#34;Serializer for creating/updating books\u0026#34;\u0026#34;\u0026#34; class Meta: model = Book fields = [ \u0026#39;title\u0026#39;, \u0026#39;slug\u0026#39;, \u0026#39;isbn\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;publisher\u0026#39;, \u0026#39;publication_date\u0026#39;, \u0026#39;pages\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;stock\u0026#39;, \u0026#39;cover_image\u0026#39; ] def validate(self, data): # Custom validation if data.get(\u0026#39;pages\u0026#39;, 0) \u0026lt;= 0: raise serializers.ValidationError({\u0026#34;pages\u0026#34;: \u0026#34;Pages must be positive\u0026#34;}) if data.get(\u0026#39;stock\u0026#39;, 0) \u0026lt; 0: raise serializers.ValidationError({\u0026#34;stock\u0026#34;: \u0026#34;Stock cannot be negative\u0026#34;}) return data Key serializer concepts:\nModelSerializer - automatically creates fields from model read_only=True - field appears in response but can\u0026rsquo;t be set write_only=True - field accepted in request but not shown in response SerializerMethodField - custom calculated field (method: get_\u0026lt;field_name\u0026gt;) StringRelatedField - shows __str__() representation PrimaryKeyRelatedField - shows/accepts IDs validate_\u0026lt;field\u0026gt; - field-level validation validate() - object-level validation Creating Views with ViewSets ViewSets combine multiple views (list, create, retrieve, update, delete) into one class.\nCreate views (books/views.py):\nfrom rest_framework import viewsets, filters, status from rest_framework.decorators import action from rest_framework.response import Response from rest_framework.permissions import IsAuthenticated, IsAuthenticatedOrReadOnly, AllowAny from django_filters.rest_framework import DjangoFilterBackend from django.db.models import Q from .models import Author, Category, Book, Review from .serializers import ( AuthorSerializer, CategorySerializer, BookListSerializer, BookDetailSerializer, BookCreateUpdateSerializer, ReviewSerializer ) from .permissions import IsOwnerOrReadOnly class AuthorViewSet(viewsets.ModelViewSet): \u0026#34;\u0026#34;\u0026#34; API endpoint for authors. list: Get all authors create: Create new author (admin only) retrieve: Get single author update: Update author (admin only) destroy: Delete author (admin only) \u0026#34;\u0026#34;\u0026#34; queryset = Author.objects.all() serializer_class = AuthorSerializer permission_classes = [IsAuthenticatedOrReadOnly] filter_backends = [filters.SearchFilter, filters.OrderingFilter] search_fields = [\u0026#39;name\u0026#39;, \u0026#39;bio\u0026#39;] ordering_fields = [\u0026#39;name\u0026#39;, \u0026#39;created_at\u0026#39;] ordering = [\u0026#39;name\u0026#39;] @action(detail=True, methods=[\u0026#39;get\u0026#39;]) def books(self, request, pk=None): \u0026#34;\u0026#34;\u0026#34;Get all books by this author\u0026#34;\u0026#34;\u0026#34; author = self.get_object() books = author.books.all() serializer = BookListSerializer(books, many=True) return Response(serializer.data) class CategoryViewSet(viewsets.ModelViewSet): \u0026#34;\u0026#34;\u0026#34;API endpoint for categories\u0026#34;\u0026#34;\u0026#34; queryset = Category.objects.all() serializer_class = CategorySerializer permission_classes = [IsAuthenticatedOrReadOnly] lookup_field = \u0026#39;slug\u0026#39; # Use slug instead of ID in URL filter_backends = [filters.SearchFilter] search_fields = [\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;] @action(detail=True, methods=[\u0026#39;get\u0026#39;]) def books(self, request, slug=None): \u0026#34;\u0026#34;\u0026#34;Get all books in this category\u0026#34;\u0026#34;\u0026#34; category = self.get_object() books = category.books.all() serializer = BookListSerializer(books, many=True) return Response(serializer.data) class BookViewSet(viewsets.ModelViewSet): \u0026#34;\u0026#34;\u0026#34;API endpoint for books\u0026#34;\u0026#34;\u0026#34; queryset = Book.objects.prefetch_related(\u0026#39;authors\u0026#39;, \u0026#39;categories\u0026#39;, \u0026#39;reviews\u0026#39;) permission_classes = [IsAuthenticatedOrReadOnly] filter_backends = [DjangoFilterBackend, filters.SearchFilter, filters.OrderingFilter] filterset_fields = [\u0026#39;categories__slug\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;publication_date\u0026#39;] search_fields = [\u0026#39;title\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;isbn\u0026#39;, \u0026#39;authors__name\u0026#39;] ordering_fields = [\u0026#39;title\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;publication_date\u0026#39;, \u0026#39;created_at\u0026#39;] ordering = [\u0026#39;-created_at\u0026#39;] lookup_field = \u0026#39;slug\u0026#39; def get_serializer_class(self): \u0026#34;\u0026#34;\u0026#34;Use different serializers for different actions\u0026#34;\u0026#34;\u0026#34; if self.action == \u0026#39;list\u0026#39;: return BookListSerializer elif self.action in [\u0026#39;create\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;partial_update\u0026#39;]: return BookCreateUpdateSerializer return BookDetailSerializer def get_queryset(self): \u0026#34;\u0026#34;\u0026#34;Custom filtering\u0026#34;\u0026#34;\u0026#34; queryset = super().get_queryset() # Filter by price range min_price = self.request.query_params.get(\u0026#39;min_price\u0026#39;) max_price = self.request.query_params.get(\u0026#39;max_price\u0026#39;) if min_price: queryset = queryset.filter(price__gte=min_price) if max_price: queryset = queryset.filter(price__lte=max_price) # Filter by availability available = self.request.query_params.get(\u0026#39;available\u0026#39;) if available == \u0026#39;true\u0026#39;: queryset = queryset.filter(stock__gt=0) elif available == \u0026#39;false\u0026#39;: queryset = queryset.filter(stock=0) return queryset @action(detail=True, methods=[\u0026#39;get\u0026#39;]) def reviews(self, request, slug=None): \u0026#34;\u0026#34;\u0026#34;Get all reviews for this book\u0026#34;\u0026#34;\u0026#34; book = self.get_object() reviews = book.reviews.all() serializer = ReviewSerializer(reviews, many=True) return Response(serializer.data) @action(detail=False, methods=[\u0026#39;get\u0026#39;]) def featured(self, request): \u0026#34;\u0026#34;\u0026#34;Get featured books (high rated, in stock)\u0026#34;\u0026#34;\u0026#34; books = self.get_queryset().filter(stock__gt=0)[:10] serializer = BookListSerializer(books, many=True) return Response(serializer.data) @action(detail=False, methods=[\u0026#39;get\u0026#39;]) def search_advanced(self, request): \u0026#34;\u0026#34;\u0026#34;Advanced search with multiple criteria\u0026#34;\u0026#34;\u0026#34; query = request.query_params.get(\u0026#39;q\u0026#39;, \u0026#39;\u0026#39;) books = self.get_queryset().filter( Q(title__icontains=query) | Q(description__icontains=query) | Q(authors__name__icontains=query) | Q(categories__name__icontains=query) ).distinct() serializer = BookListSerializer(books, many=True) return Response(serializer.data) class ReviewViewSet(viewsets.ModelViewSet): \u0026#34;\u0026#34;\u0026#34;API endpoint for reviews\u0026#34;\u0026#34;\u0026#34; queryset = Review.objects.select_related(\u0026#39;book\u0026#39;, \u0026#39;user\u0026#39;) serializer_class = ReviewSerializer permission_classes = [IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly] filter_backends = [DjangoFilterBackend, filters.OrderingFilter] filterset_fields = [\u0026#39;book\u0026#39;, \u0026#39;rating\u0026#39;, \u0026#39;user\u0026#39;] ordering_fields = [\u0026#39;rating\u0026#39;, \u0026#39;created_at\u0026#39;] ordering = [\u0026#39;-created_at\u0026#39;] def perform_create(self, serializer): \u0026#34;\u0026#34;\u0026#34;Set user automatically when creating review\u0026#34;\u0026#34;\u0026#34; serializer.save(user=self.request.user) def get_queryset(self): \u0026#34;\u0026#34;\u0026#34;Users can only update/delete their own reviews\u0026#34;\u0026#34;\u0026#34; queryset = super().get_queryset() if self.action in [\u0026#39;update\u0026#39;, \u0026#39;partial_update\u0026#39;, \u0026#39;destroy\u0026#39;]: return queryset.filter(user=self.request.user) return queryset Create custom permission (books/permissions.py):\nfrom rest_framework import permissions class IsOwnerOrReadOnly(permissions.BasePermission): \u0026#34;\u0026#34;\u0026#34; Custom permission: object owner can edit, others can only read. \u0026#34;\u0026#34;\u0026#34; def has_object_permission(self, request, view, obj): # Read permissions allowed for any request (GET, HEAD, OPTIONS) if request.method in permissions.SAFE_METHODS: return True # Write permissions only for object owner return obj.user == request.user URL Routing with Routers DRF Routers automatically generate URLs for ViewSets.\nCreate URLs (books/urls.py):\nfrom django.urls import path, include from rest_framework.routers import DefaultRouter from .views import AuthorViewSet, CategoryViewSet, BookViewSet, ReviewViewSet router = DefaultRouter() router.register(r\u0026#39;authors\u0026#39;, AuthorViewSet, basename=\u0026#39;author\u0026#39;) router.register(r\u0026#39;categories\u0026#39;, CategoryViewSet, basename=\u0026#39;category\u0026#39;) router.register(r\u0026#39;books\u0026#39;, BookViewSet, basename=\u0026#39;book\u0026#39;) router.register(r\u0026#39;reviews\u0026#39;, ReviewViewSet, basename=\u0026#39;review\u0026#39;) urlpatterns = [ path(\u0026#39;\u0026#39;, include(router.urls)), ] Update project URLs (bookstore/urls.py):\nfrom django.contrib import admin from django.urls import path, include from rest_framework_simplejwt.views import TokenObtainPairView, TokenRefreshView from drf_spectacular.views import SpectacularAPIView, SpectacularSwaggerView urlpatterns = [ path(\u0026#39;admin/\u0026#39;, admin.site.urls), # API endpoints path(\u0026#39;api/\u0026#39;, include(\u0026#39;books.urls\u0026#39;)), # Authentication path(\u0026#39;api/token/\u0026#39;, TokenObtainPairView.as_view(), name=\u0026#39;token_obtain_pair\u0026#39;), path(\u0026#39;api/token/refresh/\u0026#39;, TokenRefreshView.as_view(), name=\u0026#39;token_refresh\u0026#39;), # API documentation path(\u0026#39;api/schema/\u0026#39;, SpectacularAPIView.as_view(), name=\u0026#39;schema\u0026#39;), path(\u0026#39;api/docs/\u0026#39;, SpectacularSwaggerView.as_view(url_name=\u0026#39;schema\u0026#39;), name=\u0026#39;swagger-ui\u0026#39;), # Browsable API auth path(\u0026#39;api-auth/\u0026#39;, include(\u0026#39;rest_framework.urls\u0026#39;)), ] Generated URLs:\nGET /api/books/ - List all books POST /api/books/ - Create book GET /api/books/{slug}/ - Get single book PUT /api/books/{slug}/ - Update book (full) PATCH /api/books/{slug}/ - Update book (partial) DELETE /api/books/{slug}/ - Delete book GET /api/books/{slug}/reviews/ - Custom action GET /api/books/featured/ - Custom list action GET /api/authors/ - List authors POST /api/authors/ - Create author GET /api/authors/{id}/ - Get author GET /api/authors/{id}/books/ - Author\u0026#39;s books GET /api/categories/ - List categories GET /api/categories/{slug}/ - Get category GET /api/categories/{slug}/books/ - Category\u0026#39;s books GET /api/reviews/ - List reviews POST /api/reviews/ - Create review GET /api/reviews/{id}/ - Get review PATCH /api/reviews/{id}/ - Update review (owner only) DELETE /api/reviews/{id}/ - Delete review (owner only) Start server and test:\npython manage.py runserver Visit:\nhttp://localhost:8000/api/ - Browsable API http://localhost:8000/api/books/ - Books list http://localhost:8000/api/docs/ - Swagger UI documentation Authentication with JWT Let\u0026rsquo;s implement JWT (JSON Web Token) authentication.\nJWT is already configured in settings.py. Now let\u0026rsquo;s test it.\nGet JWT token:\ncurl -X POST http://localhost:8000/api/token/ \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;username\u0026#34;: \u0026#34;your_username\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;your_password\u0026#34;}\u0026#39; Response:\n{ \u0026#34;access\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGc...\u0026#34;, \u0026#34;refresh\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGc...\u0026#34; } Use access token:\ncurl http://localhost:8000/api/books/ \\ -H \u0026#34;Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGc...\u0026#34; Refresh token when expired:\ncurl -X POST http://localhost:8000/api/token/refresh/ \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;refresh\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGc...\u0026#34;}\u0026#39; Create user registration endpoint (accounts/serializers.py):\nfrom rest_framework import serializers from django.contrib.auth.models import User from django.contrib.auth.password_validation import validate_password class RegisterSerializer(serializers.ModelSerializer): password = serializers.CharField(write_only=True, required=True, validators=[validate_password]) password2 = serializers.CharField(write_only=True, required=True) class Meta: model = User fields = [\u0026#39;username\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;password\u0026#39;, \u0026#39;password2\u0026#39;, \u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;] def validate(self, attrs): if attrs[\u0026#39;password\u0026#39;] != attrs[\u0026#39;password2\u0026#39;]: raise serializers.ValidationError({\u0026#34;password\u0026#34;: \u0026#34;Password fields didn\u0026#39;t match.\u0026#34;}) return attrs def create(self, validated_data): validated_data.pop(\u0026#39;password2\u0026#39;) user = User.objects.create_user(**validated_data) return user class UserSerializer(serializers.ModelSerializer): class Meta: model = User fields = [\u0026#39;id\u0026#39;, \u0026#39;username\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;first_name\u0026#39;, \u0026#39;last_name\u0026#39;, \u0026#39;date_joined\u0026#39;] Create registration view (accounts/views.py):\nfrom rest_framework import generics, permissions from rest_framework.response import Response from django.contrib.auth.models import User from .serializers import RegisterSerializer, UserSerializer class RegisterView(generics.CreateAPIView): queryset = User.objects.all() permission_classes = [permissions.AllowAny] serializer_class = RegisterSerializer class UserProfileView(generics.RetrieveUpdateAPIView): queryset = User.objects.all() serializer_class = UserSerializer permission_classes = [permissions.IsAuthenticated] def get_object(self): return self.request.user Add URLs (accounts/urls.py):\nfrom django.urls import path from .views import RegisterView, UserProfileView urlpatterns = [ path(\u0026#39;register/\u0026#39;, RegisterView.as_view(), name=\u0026#39;register\u0026#39;), path(\u0026#39;profile/\u0026#39;, UserProfileView.as_view(), name=\u0026#39;profile\u0026#39;), ] Update project URLs:\n# bookstore/urls.py urlpatterns = [ # ... existing patterns path(\u0026#39;api/accounts/\u0026#39;, include(\u0026#39;accounts.urls\u0026#39;)), ] Test registration:\ncurl -X POST http://localhost:8000/api/accounts/register/ \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;newuser\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;SecurePass123!\u0026#34;, \u0026#34;password2\u0026#34;: \u0026#34;SecurePass123!\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Doe\u0026#34; }\u0026#39; Now you have complete auth: registration, login (JWT), and profile management.\nPermissions and Throttling Control who can access what and prevent API abuse.\nBuilt-in permissions:\nAllowAny - anyone (default) IsAuthenticated - logged in users only IsAdminUser - admin users only IsAuthenticatedOrReadOnly - read for all, write for authenticated Apply permissions per view:\nfrom rest_framework.permissions import IsAdminUser class BookViewSet(viewsets.ModelViewSet): permission_classes = [IsAuthenticatedOrReadOnly] def get_permissions(self): \u0026#34;\u0026#34;\u0026#34;Different permissions for different actions\u0026#34;\u0026#34;\u0026#34; if self.action in [\u0026#39;create\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;partial_update\u0026#39;, \u0026#39;destroy\u0026#39;]: return [IsAdminUser()] return [IsAuthenticatedOrReadOnly()] Custom object-level permissions:\n# books/permissions.py class IsAdminOrReadOnly(permissions.BasePermission): \u0026#34;\u0026#34;\u0026#34;Admin can edit, others read only\u0026#34;\u0026#34;\u0026#34; def has_permission(self, request, view): if request.method in permissions.SAFE_METHODS: return True return request.user and request.user.is_staff class IsOwnerOrAdmin(permissions.BasePermission): \u0026#34;\u0026#34;\u0026#34;Owner or admin can edit\u0026#34;\u0026#34;\u0026#34; def has_object_permission(self, request, view, obj): if request.method in permissions.SAFE_METHODS: return True if request.user.is_staff: return True return obj.user == request.user Throttling (rate limiting):\n# settings.py REST_FRAMEWORK = { # ... existing config \u0026#39;DEFAULT_THROTTLE_CLASSES\u0026#39;: [ \u0026#39;rest_framework.throttling.AnonRateThrottle\u0026#39;, \u0026#39;rest_framework.throttling.UserRateThrottle\u0026#39; ], \u0026#39;DEFAULT_THROTTLE_RATES\u0026#39;: { \u0026#39;anon\u0026#39;: \u0026#39;100/day\u0026#39;, # Anonymous users: 100 requests per day \u0026#39;user\u0026#39;: \u0026#39;1000/day\u0026#39;, # Authenticated users: 1000 per day } } Custom throttling per view:\nfrom rest_framework.throttling import UserRateThrottle class BurstRateThrottle(UserRateThrottle): rate = \u0026#39;60/min\u0026#39; # 60 requests per minute class BookViewSet(viewsets.ModelViewSet): throttle_classes = [BurstRateThrottle] Filtering, Pagination, and Search Make your API flexible and performant. Once your API is collecting data, you can analyze usage patterns, user behavior, and performance metrics with Pandas .\nFiltering Already configured with django-filter. Use query parameters:\n# Filter by category slug GET /api/books/?categories__slug=fiction # Filter by author ID GET /api/books/?authors=1 # Filter by publication date GET /api/books/?publication_date=2024-01-01 # Multiple filters GET /api/books/?categories__slug=fiction\u0026amp;min_price=10\u0026amp;max_price=50 # Filter by availability GET /api/books/?available=true Add custom filter backend:\n# books/filters.py import django_filters from .models import Book class BookFilter(django_filters.FilterSet): min_price = django_filters.NumberFilter(field_name=\u0026#39;price\u0026#39;, lookup_expr=\u0026#39;gte\u0026#39;) max_price = django_filters.NumberFilter(field_name=\u0026#39;price\u0026#39;, lookup_expr=\u0026#39;lte\u0026#39;) title = django_filters.CharFilter(lookup_expr=\u0026#39;icontains\u0026#39;) author_name = django_filters.CharFilter(field_name=\u0026#39;authors__name\u0026#39;, lookup_expr=\u0026#39;icontains\u0026#39;) class Meta: model = Book fields = [\u0026#39;categories\u0026#39;, \u0026#39;authors\u0026#39;, \u0026#39;publication_date\u0026#39;] # In views.py from .filters import BookFilter class BookViewSet(viewsets.ModelViewSet): filterset_class = BookFilter Search # Search in title, description, ISBN, author name GET /api/books/?search=python # Search authors GET /api/authors/?search=rowling Ordering # Order by price (ascending) GET /api/books/?ordering=price # Order by price (descending) GET /api/books/?ordering=-price # Multiple ordering GET /api/books/?ordering=-publication_date,title Pagination Page number pagination (default):\nGET /api/books/ # Page 1 (default) GET /api/books/?page=2 # Page 2 GET /api/books/?page_size=20 # Custom page size Response:\n{ \u0026#34;count\u0026#34;: 150, \u0026#34;next\u0026#34;: \u0026#34;http://localhost:8000/api/books/?page=3\u0026#34;, \u0026#34;previous\u0026#34;: \u0026#34;http://localhost:8000/api/books/?page=1\u0026#34;, \u0026#34;results\u0026#34;: [...] } Limit-offset pagination:\n# settings.py REST_FRAMEWORK = { \u0026#39;DEFAULT_PAGINATION_CLASS\u0026#39;: \u0026#39;rest_framework.pagination.LimitOffsetPagination\u0026#39;, \u0026#39;PAGE_SIZE\u0026#39;: 10 } GET /api/books/?limit=20\u0026amp;offset=40 # Items 41-60 Cursor pagination (best for large datasets):\n# settings.py REST_FRAMEWORK = { \u0026#39;DEFAULT_PAGINATION_CLASS\u0026#39;: \u0026#39;rest_framework.pagination.CursorPagination\u0026#39;, \u0026#39;PAGE_SIZE\u0026#39;: 10 } More efficient for large tables, prevents offset issues.\nCustom pagination:\n# books/pagination.py from rest_framework.pagination import PageNumberPagination class StandardResultsSetPagination(PageNumberPagination): page_size = 10 page_size_query_param = \u0026#39;page_size\u0026#39; max_page_size = 100 class LargeResultsSetPagination(PageNumberPagination): page_size = 100 page_size_query_param = \u0026#39;page_size\u0026#39; max_page_size = 1000 # In views.py class BookViewSet(viewsets.ModelViewSet): pagination_class = StandardResultsSetPagination Testing Your API Writing tests ensures your API works correctly and catches regressions.\nCreate tests (books/tests.py):\nfrom django.test import TestCase from django.contrib.auth.models import User from rest_framework.test import APITestCase, APIClient from rest_framework import status from django.urls import reverse from .models import Author, Category, Book, Review class BookAPITestCase(APITestCase): def setUp(self): \u0026#34;\u0026#34;\u0026#34;Set up test data\u0026#34;\u0026#34;\u0026#34; # Create users self.admin = User.objects.create_superuser(\u0026#39;admin\u0026#39;, \u0026#39;admin@test.com\u0026#39;, \u0026#39;admin123\u0026#39;) self.user = User.objects.create_user(\u0026#39;user\u0026#39;, \u0026#39;user@test.com\u0026#39;, \u0026#39;user123\u0026#39;) # Create test data self.author = Author.objects.create(name=\u0026#39;Test Author\u0026#39;) self.category = Category.objects.create(name=\u0026#39;Fiction\u0026#39;, slug=\u0026#39;fiction\u0026#39;) self.book = Book.objects.create( title=\u0026#39;Test Book\u0026#39;, slug=\u0026#39;test-book\u0026#39;, isbn=\u0026#39;1234567890123\u0026#39;, description=\u0026#39;Test description\u0026#39;, publisher=\u0026#39;Test Publisher\u0026#39;, publication_date=\u0026#39;2024-01-01\u0026#39;, pages=300, price=29.99, stock=10 ) self.book.authors.add(self.author) self.book.categories.add(self.category) self.client = APIClient() def test_list_books(self): \u0026#34;\u0026#34;\u0026#34;Test retrieving book list\u0026#34;\u0026#34;\u0026#34; response = self.client.get(\u0026#39;/api/books/\u0026#39;) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(len(response.data[\u0026#39;results\u0026#39;]), 1) def test_retrieve_book(self): \u0026#34;\u0026#34;\u0026#34;Test retrieving single book\u0026#34;\u0026#34;\u0026#34; response = self.client.get(f\u0026#39;/api/books/{self.book.slug}/\u0026#39;) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(response.data[\u0026#39;title\u0026#39;], \u0026#39;Test Book\u0026#39;) def test_create_book_unauthenticated(self): \u0026#34;\u0026#34;\u0026#34;Test creating book without authentication fails\u0026#34;\u0026#34;\u0026#34; data = { \u0026#39;title\u0026#39;: \u0026#39;New Book\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;new-book\u0026#39;, \u0026#39;isbn\u0026#39;: \u0026#39;9876543210987\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;New book description\u0026#39;, \u0026#39;publisher\u0026#39;: \u0026#39;New Publisher\u0026#39;, \u0026#39;publication_date\u0026#39;: \u0026#39;2024-02-01\u0026#39;, \u0026#39;pages\u0026#39;: 250, \u0026#39;price\u0026#39;: 19.99, \u0026#39;stock\u0026#39;: 5, \u0026#39;authors\u0026#39;: [self.author.id], \u0026#39;categories\u0026#39;: [self.category.id] } response = self.client.post(\u0026#39;/api/books/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_401_UNAUTHORIZED) def test_create_book_authenticated(self): \u0026#34;\u0026#34;\u0026#34;Test creating book as authenticated admin\u0026#34;\u0026#34;\u0026#34; self.client.force_authenticate(user=self.admin) data = { \u0026#39;title\u0026#39;: \u0026#39;New Book\u0026#39;, \u0026#39;slug\u0026#39;: \u0026#39;new-book\u0026#39;, \u0026#39;isbn\u0026#39;: \u0026#39;9876543210987\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;New book description\u0026#39;, \u0026#39;publisher\u0026#39;: \u0026#39;New Publisher\u0026#39;, \u0026#39;publication_date\u0026#39;: \u0026#39;2024-02-01\u0026#39;, \u0026#39;pages\u0026#39;: 250, \u0026#39;price\u0026#39;: 19.99, \u0026#39;stock\u0026#39;: 5, \u0026#39;authors\u0026#39;: [self.author.id], \u0026#39;categories\u0026#39;: [self.category.id] } response = self.client.post(\u0026#39;/api/books/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_201_CREATED) self.assertEqual(Book.objects.count(), 2) def test_update_book(self): \u0026#34;\u0026#34;\u0026#34;Test updating book\u0026#34;\u0026#34;\u0026#34; self.client.force_authenticate(user=self.admin) data = {\u0026#39;price\u0026#39;: 39.99} response = self.client.patch(f\u0026#39;/api/books/{self.book.slug}/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_200_OK) self.book.refresh_from_db() self.assertEqual(float(self.book.price), 39.99) def test_delete_book(self): \u0026#34;\u0026#34;\u0026#34;Test deleting book\u0026#34;\u0026#34;\u0026#34; self.client.force_authenticate(user=self.admin) response = self.client.delete(f\u0026#39;/api/books/{self.book.slug}/\u0026#39;) self.assertEqual(response.status_code, status.HTTP_204_NO_CONTENT) self.assertEqual(Book.objects.count(), 0) def test_filter_books_by_category(self): \u0026#34;\u0026#34;\u0026#34;Test filtering books by category\u0026#34;\u0026#34;\u0026#34; response = self.client.get(\u0026#39;/api/books/?categories__slug=fiction\u0026#39;) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(len(response.data[\u0026#39;results\u0026#39;]), 1) def test_search_books(self): \u0026#34;\u0026#34;\u0026#34;Test searching books\u0026#34;\u0026#34;\u0026#34; response = self.client.get(\u0026#39;/api/books/?search=Test\u0026#39;) self.assertEqual(response.status_code, status.HTTP_200_OK) self.assertEqual(len(response.data[\u0026#39;results\u0026#39;]), 1) def test_create_review(self): \u0026#34;\u0026#34;\u0026#34;Test creating review\u0026#34;\u0026#34;\u0026#34; self.client.force_authenticate(user=self.user) data = { \u0026#39;book\u0026#39;: self.book.id, \u0026#39;rating\u0026#39;: 5, \u0026#39;title\u0026#39;: \u0026#39;Great book!\u0026#39;, \u0026#39;comment\u0026#39;: \u0026#39;Really enjoyed this book.\u0026#39; } response = self.client.post(\u0026#39;/api/reviews/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_201_CREATED) self.assertEqual(Review.objects.count(), 1) def test_update_own_review(self): \u0026#34;\u0026#34;\u0026#34;Test user can update their own review\u0026#34;\u0026#34;\u0026#34; self.client.force_authenticate(user=self.user) review = Review.objects.create( book=self.book, user=self.user, rating=4, title=\u0026#39;Good\u0026#39;, comment=\u0026#39;Nice book\u0026#39; ) data = {\u0026#39;rating\u0026#39;: 5, \u0026#39;title\u0026#39;: \u0026#39;Excellent!\u0026#39;} response = self.client.patch(f\u0026#39;/api/reviews/{review.id}/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_200_OK) review.refresh_from_db() self.assertEqual(review.rating, 5) def test_cannot_update_others_review(self): \u0026#34;\u0026#34;\u0026#34;Test user cannot update another user\u0026#39;s review\u0026#34;\u0026#34;\u0026#34; other_user = User.objects.create_user(\u0026#39;other\u0026#39;, \u0026#39;other@test.com\u0026#39;, \u0026#39;pass123\u0026#39;) review = Review.objects.create( book=self.book, user=other_user, rating=4, title=\u0026#39;Good\u0026#39;, comment=\u0026#39;Nice book\u0026#39; ) self.client.force_authenticate(user=self.user) data = {\u0026#39;rating\u0026#39;: 1} response = self.client.patch(f\u0026#39;/api/reviews/{review.id}/\u0026#39;, data) self.assertEqual(response.status_code, status.HTTP_404_NOT_FOUND) Run tests:\n# Run all tests python manage.py test # Run specific app tests python manage.py test books # Run specific test case python manage.py test books.tests.BookAPITestCase # Run with verbose output python manage.py test --verbosity=2 # Keep test database for inspection python manage.py test --keepdb Test coverage:\npip install coverage # Run tests with coverage coverage run --source=\u0026#39;.\u0026#39; manage.py test # View coverage report coverage report # Generate HTML report coverage html # Open htmlcov/index.html in browser Want to automate your testing workflow? Check out my Python automation guide to learn how to automatically run tests, generate reports, and send notifications.\nDeploying to Production Let\u0026rsquo;s deploy your API to a VPS (DigitalOcean, AWS EC2, etc.).\nProduction checklist:\nEnvironment variables # settings.py from decouple import config SECRET_KEY = config(\u0026#39;SECRET_KEY\u0026#39;) DEBUG = config(\u0026#39;DEBUG\u0026#39;, default=False, cast=bool) ALLOWED_HOSTS = config(\u0026#39;ALLOWED_HOSTS\u0026#39;, cast=lambda v: [s.strip() for s in v.split(\u0026#39;,\u0026#39;)]) DATABASES = { \u0026#39;default\u0026#39;: { \u0026#39;ENGINE\u0026#39;: \u0026#39;django.db.backends.postgresql\u0026#39;, \u0026#39;NAME\u0026#39;: config(\u0026#39;DB_NAME\u0026#39;), \u0026#39;USER\u0026#39;: config(\u0026#39;DB_USER\u0026#39;), \u0026#39;PASSWORD\u0026#39;: config(\u0026#39;DB_PASSWORD\u0026#39;), \u0026#39;HOST\u0026#39;: config(\u0026#39;DB_HOST\u0026#39;, default=\u0026#39;localhost\u0026#39;), \u0026#39;PORT\u0026#39;: config(\u0026#39;DB_PORT\u0026#39;, default=\u0026#39;5432\u0026#39;), } } Create .env file:\nSECRET_KEY=your-secret-key-here DEBUG=False ALLOWED_HOSTS=yourdomain.com,www.yourdomain.com DB_NAME=bookstore_db DB_USER=bookstore_user DB_PASSWORD=secure-password DB_HOST=localhost DB_PORT=5432 Install production dependencies: pip install gunicorn psycopg2-binary whitenoise Static files: # settings.py STATIC_URL = \u0026#39;/static/\u0026#39; STATIC_ROOT = BASE_DIR / \u0026#39;staticfiles\u0026#39; MIDDLEWARE = [ \u0026#39;django.middleware.security.SecurityMiddleware\u0026#39;, \u0026#39;whitenoise.middleware.WhiteNoiseMiddleware\u0026#39;, # Add this # ... other middleware ] STATICFILES_STORAGE = \u0026#39;whitenoise.storage.CompressedManifestStaticFilesStorage\u0026#39; Security settings: # settings.py (production) DEBUG = False ALLOWED_HOSTS = [\u0026#39;yourdomain.com\u0026#39;, \u0026#39;www.yourdomain.com\u0026#39;] SECURE_SSL_REDIRECT = True SESSION_COOKIE_SECURE = True CSRF_COOKIE_SECURE = True SECURE_BROWSER_XSS_FILTER = True SECURE_CONTENT_TYPE_NOSNIFF = True X_FRAME_OPTIONS = \u0026#39;DENY\u0026#39; # CORS (adjust for your frontend) CORS_ALLOWED_ORIGINS = [ \u0026#34;https://yourdomain.com\u0026#34;, \u0026#34;https://www.yourdomain.com\u0026#34;, ] Gunicorn configuration: # gunicorn_config.py bind = \u0026#34;0.0.0.0:8000\u0026#34; workers = 3 worker_class = \u0026#34;sync\u0026#34; worker_connections = 1000 timeout = 30 keepalive = 2 Nginx configuration: # /etc/nginx/sites-available/bookstore server { listen 80; server_name yourdomain.com www.yourdomain.com; location = /favicon.ico { access_log off; log_not_found off; } location /static/ { alias /home/user/bookstore-api/staticfiles/; } location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Systemd service: # /etc/systemd/system/bookstore.service [Unit] Description=Bookstore API After=network.target [Service] User=user Group=www-data WorkingDirectory=/home/user/bookstore-api Environment=\u0026#34;PATH=/home/user/bookstore-api/venv/bin\u0026#34; ExecStart=/home/user/bookstore-api/venv/bin/gunicorn \\ --config gunicorn_config.py \\ bookstore.wsgi:application [Install] WantedBy=multi-user.target Deploy commands: # On server git clone your-repo cd bookstore-api python3 -m venv venv source venv/bin/activate pip install -r requirements.txt # Set up PostgreSQL sudo -u postgres psql CREATE DATABASE bookstore_db; CREATE USER bookstore_user WITH PASSWORD \u0026#39;password\u0026#39;; GRANT ALL PRIVILEGES ON DATABASE bookstore_db TO bookstore_user; \\q # Run migrations python manage.py migrate # Collect static files python manage.py collectstatic --noinput # Create superuser python manage.py createsuperuser # Start service sudo systemctl start bookstore sudo systemctl enable bookstore # Restart Nginx sudo systemctl restart nginx # Check status sudo systemctl status bookstore SSL with Let\u0026rsquo;s Encrypt: sudo apt install certbot python3-certbot-nginx sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com For detailed SSL setup instructions, see my Nginx with Certbot guide .\nYour API is now live at https://yourdomain.com/api/!\nNext Steps You now know how to build production-ready REST APIs with Django REST Framework. Here\u0026rsquo;s what to explore next:\nAdvanced DRF topics:\nNested serializers and writable nested relationships Custom authentication backends WebSocket support with Django Channels Caching strategies (Redis, Memcached) File uploads and cloud storage (S3, Cloudinary) API versioning Rate limiting strategies Background tasks with Celery GraphQL with Graphene-Django Related skills:\nFrontend integration: Connect React/Vue to your API Deployment automation: CI/CD with GitHub Actions, GitLab CI Monitoring: Sentry for errors, Prometheus for metrics Load balancing: Multiple Gunicorn instances behind Nginx Containerization: Docker and Kubernetes Database optimization: Query optimization, indexing, connection pooling For faster APIs, check out my FastAPI tutorial to compare Django REST Framework with FastAPI\u0026rsquo;s async approach.\nFor automation, my Python automation guide shows how to automate API testing, deployment, and monitoring tasks.\nFor data analysis on API logs, see Pandas data analysis tutorial to analyze request patterns, user behavior, and performance metrics.\nThe best way to master DRF is to build real projects. Start with a simple API (todo list, blog, inventory), add authentication, deploy it, then add complexity (payments, notifications, real-time features).\nDjango REST Framework powers some of the world\u0026rsquo;s largest APIs. You\u0026rsquo;re now equipped with the same tools.\nGo build something real.\nIf you enjoyed this tutorial, you might also like learning how to build web scrapers with Python to collect data for your APIs, or check out my guide on Python automation scripts to streamline your development workflow. For analyzing API usage data, see my Pandas data analysis tutorial .\nQuestions or feedback? Drop a comment below!\n","href":"/2025/10/how-to-build-rest-api-django-rest-framework-production-ready.html","title":"How to Build a REST API with Django REST Framework (Production Ready)"},{"content":"Last month, I spent four hours copying data from PDFs into Excel spreadsheets for a client report. Four hours of my life, gone, doing something a computer could do in four seconds. I kept thinking\u0026ndash;there has to be a better way.\nThat weekend, I wrote a simple Python script. Ten lines of code. Now that same task runs automatically every Monday morning while I drink coffee. The script has saved me over 30 hours since I wrote it, and honestly, that is not even my best automation.\nIf you have ever found yourself doing the same boring computer task over and over, this tutorial is for you. We are going to build practical Python automation scripts that actually save time. No theory, no fluff\u0026ndash;just real scripts you can use tomorrow to get hours of your life back.\nAfter you master these automation basics, you can build web scrapers to collect data automatically or schedule your scripts to run on autopilot with cron jobs .\nWhy Every Developer Needs Automation Skills Look, I get it\u0026ndash;writing a script to automate something takes time upfront. Sometimes it feels faster to just do the task manually. But here is the math that changed my mind: if a task takes 5 minutes and you do it daily, that is 30 hours per year. Spend one hour writing automation, save 29 hours annually. And that is just one task.\nThe real magic happens when you start seeing automation opportunities everywhere. That spreadsheet you update every Monday? Automate it. Those files you rename and organize? Automate it. The report you email to your boss? Definitely automate it.\nPython makes automation accessible. You do not need to be a programming genius. If you can write a loop and call a function, you can automate 90% of repetitive computer work. The standard library has most of what you need, and the remaining tools are a pip install away.\nPlus, automation compounds. You write a script once, use it forever. Share it with teammates, multiply the benefit. Chain scripts together, build workflows. Before you know it, you have automated away hours of weekly drudgery and can focus on work that actually matters.\nSetting Up Your Automation Environment Let me get your environment ready for automation. I always create a dedicated automation project to keep scripts organized.\nCreate your automation workspace:\nmkdir python-automation cd python-automation python3 -m venv venv source venv/bin/activate # On Windows: venv\\Scripts\\activate Install essential automation libraries:\n# File and data manipulation pip install pandas openpyxl xlsxwriter # PDF handling pip install PyPDF2 reportlab # Image processing pip install Pillow # Web and email pip install requests beautifulsoup4 # Scheduling pip install schedule # Utilities pip install python-dotenv # For managing credentials safely Create a basic project structure:\nautomation/ ├── scripts/ # Your automation scripts ├── data/ # Input files ├── output/ # Generated files ├── logs/ # Script logs └── config/ # Configuration files This structure keeps everything organized. Scripts read from data/, write to output/, and log to logs/. Clean separation makes debugging easier.\nVerify your setup:\nimport pandas as pd import PyPDF2 from PIL import Image import requests import schedule print(\u0026#34;All libraries imported successfully!\u0026#34;) print(\u0026#34;Ready to automate!\u0026#34;) If that runs without errors, you are ready to start automating.\nFile Organization: Never Manually Sort Files Again Let me show you my most-used automation: organizing messy folders automatically. This script sorts files by type, renames them with dates, and cleans up duplicates.\nSmart file organizer:\nimport os import shutil from pathlib import Path from datetime import datetime import hashlib def organize_files(source_dir, dest_dir): \u0026#34;\u0026#34;\u0026#34; Organize files by type into categorized folders. Handles duplicates and renames files with timestamps. \u0026#34;\u0026#34;\u0026#34; # Define file categories categories = { \u0026#39;Images\u0026#39;: [\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.gif\u0026#39;, \u0026#39;.bmp\u0026#39;, \u0026#39;.svg\u0026#39;, \u0026#39;.webp\u0026#39;], \u0026#39;Documents\u0026#39;: [\u0026#39;.pdf\u0026#39;, \u0026#39;.doc\u0026#39;, \u0026#39;.docx\u0026#39;, \u0026#39;.txt\u0026#39;, \u0026#39;.odt\u0026#39;, \u0026#39;.rtf\u0026#39;], \u0026#39;Spreadsheets\u0026#39;: [\u0026#39;.xlsx\u0026#39;, \u0026#39;.xls\u0026#39;, \u0026#39;.csv\u0026#39;, \u0026#39;.ods\u0026#39;], \u0026#39;Archives\u0026#39;: [\u0026#39;.zip\u0026#39;, \u0026#39;.rar\u0026#39;, \u0026#39;.7z\u0026#39;, \u0026#39;.tar\u0026#39;, \u0026#39;.gz\u0026#39;], \u0026#39;Videos\u0026#39;: [\u0026#39;.mp4\u0026#39;, \u0026#39;.avi\u0026#39;, \u0026#39;.mkv\u0026#39;, \u0026#39;.mov\u0026#39;, \u0026#39;.wmv\u0026#39;, \u0026#39;.flv\u0026#39;], \u0026#39;Audio\u0026#39;: [\u0026#39;.mp3\u0026#39;, \u0026#39;.wav\u0026#39;, \u0026#39;.flac\u0026#39;, \u0026#39;.aac\u0026#39;, \u0026#39;.ogg\u0026#39;, \u0026#39;.m4a\u0026#39;], \u0026#39;Code\u0026#39;: [\u0026#39;.py\u0026#39;, \u0026#39;.js\u0026#39;, \u0026#39;.html\u0026#39;, \u0026#39;.css\u0026#39;, \u0026#39;.java\u0026#39;, \u0026#39;.cpp\u0026#39;, \u0026#39;.go\u0026#39;], } # Create destination directories for category in categories.keys(): Path(dest_dir, category).mkdir(parents=True, exist_ok=True) # Track moved files moved_count = 0 skipped_count = 0 # Process each file in source directory for filename in os.listdir(source_dir): file_path = Path(source_dir, filename) # Skip directories if file_path.is_dir(): continue # Get file extension ext = file_path.suffix.lower() # Find matching category destination_category = None for category, extensions in categories.items(): if ext in extensions: destination_category = category break # Default to \u0026#39;Others\u0026#39; if no category matches if not destination_category: destination_category = \u0026#39;Others\u0026#39; Path(dest_dir, \u0026#39;Others\u0026#39;).mkdir(exist_ok=True) # Create new filename with date prefix date_prefix = datetime.now().strftime(\u0026#39;%Y%m%d\u0026#39;) new_filename = f\u0026#34;{date_prefix}_{filename}\u0026#34; dest_path = Path(dest_dir, destination_category, new_filename) # Handle duplicate filenames counter = 1 while dest_path.exists(): name_part = file_path.stem new_filename = f\u0026#34;{date_prefix}_{name_part}_{counter}{ext}\u0026#34; dest_path = Path(dest_dir, destination_category, new_filename) counter += 1 # Move file try: shutil.move(str(file_path), str(dest_path)) moved_count += 1 print(f\u0026#34;Moved: {filename} -\u0026gt; {destination_category}/{new_filename}\u0026#34;) except Exception as e: print(f\u0026#34;Error moving {filename}: {e}\u0026#34;) skipped_count += 1 print(f\u0026#34;\\nOrganization complete!\u0026#34;) print(f\u0026#34;Files moved: {moved_count}\u0026#34;) print(f\u0026#34;Files skipped: {skipped_count}\u0026#34;) # Usage organize_files(\u0026#39;/path/to/messy/downloads\u0026#39;, \u0026#39;/path/to/organized/files\u0026#39;) This script saved me countless hours. I run it on my Downloads folder weekly, and chaos becomes order instantly.\nEnhanced version with duplicate detection:\ndef get_file_hash(filepath): \u0026#34;\u0026#34;\u0026#34;Calculate MD5 hash of file content to detect duplicates.\u0026#34;\u0026#34;\u0026#34; hash_md5 = hashlib.md5() with open(filepath, \u0026#34;rb\u0026#34;) as f: for chunk in iter(lambda: f.read(4096), b\u0026#34;\u0026#34;): hash_md5.update(chunk) return hash_md5.hexdigest() def remove_duplicate_files(directory): \u0026#34;\u0026#34;\u0026#34;Find and remove duplicate files based on content hash.\u0026#34;\u0026#34;\u0026#34; hashes = {} duplicates = [] for filename in os.listdir(directory): filepath = Path(directory, filename) if filepath.is_file(): file_hash = get_file_hash(filepath) if file_hash in hashes: duplicates.append(filepath) print(f\u0026#34;Duplicate found: {filename}\u0026#34;) else: hashes[file_hash] = filepath # Remove duplicates for dup in duplicates: dup.unlink() # Delete file print(f\u0026#34;Deleted duplicate: {dup.name}\u0026#34;) print(f\u0026#34;\\nRemoved {len(duplicates)} duplicate files\u0026#34;) # Usage remove_duplicate_files(\u0026#39;/path/to/folder\u0026#39;) Run this before organizing to clean up duplicate downloads and screenshots that pile up.\nExcel Automation: Stop Manual Spreadsheet Work Excel automation is where Python really shines. Here are scripts I use constantly for data work.\nMerge multiple Excel files:\nimport pandas as pd from pathlib import Path def merge_excel_files(folder_path, output_file): \u0026#34;\u0026#34;\u0026#34; Merge all Excel files in a folder into one consolidated file. Assumes all files have the same structure. \u0026#34;\u0026#34;\u0026#34; all_data = [] # Read all Excel files for excel_file in Path(folder_path).glob(\u0026#39;*.xlsx\u0026#39;): df = pd.read_excel(excel_file) # Add source filename column df[\u0026#39;Source_File\u0026#39;] = excel_file.name all_data.append(df) print(f\u0026#34;Read {len(df)} rows from {excel_file.name}\u0026#34;) # Combine all dataframes merged_df = pd.concat(all_data, ignore_index=True) # Write to output file merged_df.to_excel(output_file, index=False) print(f\u0026#34;\\nMerged {len(all_data)} files into {output_file}\u0026#34;) print(f\u0026#34;Total rows: {len(merged_df)}\u0026#34;) return merged_df # Usage merge_excel_files(\u0026#39;monthly_reports/\u0026#39;, \u0026#39;annual_report.xlsx\u0026#39;) I use this to combine monthly sales reports into yearly summaries. Saves hours compared to copy-pasting manually.\nClean and format Excel data:\ndef clean_excel_data(input_file, output_file): \u0026#34;\u0026#34;\u0026#34; Clean messy Excel data: remove duplicates, fill nulls, format dates. \u0026#34;\u0026#34;\u0026#34; # Read Excel file df = pd.read_excel(input_file) initial_rows = len(df) # Remove duplicate rows df = df.drop_duplicates() print(f\u0026#34;Removed {initial_rows - len(df)} duplicate rows\u0026#34;) # Remove rows where all values are null df = df.dropna(how=\u0026#39;all\u0026#39;) # Fill null values in specific columns if \u0026#39;Status\u0026#39; in df.columns: df[\u0026#39;Status\u0026#39;].fillna(\u0026#39;Pending\u0026#39;, inplace=True) # Clean column names (remove spaces, lowercase) df.columns = df.columns.str.strip().str.lower().str.replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;) # Format date columns date_columns = [col for col in df.columns if \u0026#39;date\u0026#39; in col.lower()] for col in date_columns: df[col] = pd.to_datetime(df[col], errors=\u0026#39;coerce\u0026#39;) # Sort by first column df = df.sort_values(by=df.columns[0]) # Write clean data df.to_excel(output_file, index=False) print(f\u0026#34;Cleaned data saved to {output_file}\u0026#34;) print(f\u0026#34;Final row count: {len(df)}\u0026#34;) return df # Usage clean_excel_data(\u0026#39;messy_data.xlsx\u0026#39;, \u0026#39;clean_data.xlsx\u0026#39;) Generate formatted reports with openpyxl:\nfrom openpyxl import Workbook from openpyxl.styles import Font, PatternFill, Alignment from openpyxl.utils import get_column_letter def create_formatted_report(data, output_file): \u0026#34;\u0026#34;\u0026#34; Create a professionally formatted Excel report. \u0026#34;\u0026#34;\u0026#34; wb = Workbook() ws = wb.active ws.title = \u0026#34;Report\u0026#34; # Add title ws[\u0026#39;A1\u0026#39;] = \u0026#34;Monthly Sales Report\u0026#34; ws[\u0026#39;A1\u0026#39;].font = Font(size=16, bold=True) ws[\u0026#39;A1\u0026#39;].alignment = Alignment(horizontal=\u0026#39;center\u0026#39;) ws.merge_cells(\u0026#39;A1:D1\u0026#39;) # Add headers headers = [\u0026#39;Product\u0026#39;, \u0026#39;Units Sold\u0026#39;, \u0026#39;Revenue\u0026#39;, \u0026#39;Profit\u0026#39;] for col, header in enumerate(headers, 1): cell = ws.cell(row=3, column=col) cell.value = header cell.font = Font(bold=True, color=\u0026#34;FFFFFF\u0026#34;) cell.fill = PatternFill(start_color=\u0026#34;366092\u0026#34;, end_color=\u0026#34;366092\u0026#34;, fill_type=\u0026#34;solid\u0026#34;) cell.alignment = Alignment(horizontal=\u0026#39;center\u0026#39;) # Add data rows for row_idx, row_data in enumerate(data, 4): for col_idx, value in enumerate(row_data, 1): ws.cell(row=row_idx, column=col_idx, value=value) # Auto-adjust column widths for col in range(1, len(headers) + 1): column_letter = get_column_letter(col) ws.column_dimensions[column_letter].width = 15 # Add totals row last_row = len(data) + 4 ws.cell(row=last_row, column=1, value=\u0026#34;TOTAL\u0026#34;) ws.cell(row=last_row, column=1).font = Font(bold=True) # Add SUM formulas for col in range(2, len(headers) + 1): col_letter = get_column_letter(col) ws.cell(row=last_row, column=col).value = f\u0026#34;=SUM({col_letter}4:{col_letter}{last_row-1})\u0026#34; ws.cell(row=last_row, column=col).font = Font(bold=True) wb.save(output_file) print(f\u0026#34;Formatted report saved to {output_file}\u0026#34;) # Usage sales_data = [ [\u0026#39;Product A\u0026#39;, 150, 15000, 5000], [\u0026#39;Product B\u0026#39;, 200, 30000, 12000], [\u0026#39;Product C\u0026#39;, 100, 8000, 2500], ] create_formatted_report(sales_data, \u0026#39;sales_report.xlsx\u0026#39;) This creates professional reports with formatting, formulas, and styling. Way better than manually formatting in Excel.\nPDF Automation: Merge, Split, and Extract PDF manipulation is tedious manually but trivial with Python. Here are my go-to PDF scripts.\nMerge multiple PDFs:\nimport PyPDF2 from pathlib import Path def merge_pdfs(pdf_files, output_file): \u0026#34;\u0026#34;\u0026#34; Merge multiple PDF files into one. \u0026#34;\u0026#34;\u0026#34; pdf_merger = PyPDF2.PdfMerger() for pdf_file in pdf_files: print(f\u0026#34;Adding {pdf_file}\u0026#34;) pdf_merger.append(pdf_file) pdf_merger.write(output_file) pdf_merger.close() print(f\u0026#34;\\nMerged {len(pdf_files)} PDFs into {output_file}\u0026#34;) # Usage pdf_files = [\u0026#39;chapter1.pdf\u0026#39;, \u0026#39;chapter2.pdf\u0026#39;, \u0026#39;chapter3.pdf\u0026#39;] merge_pdfs(pdf_files, \u0026#39;complete_book.pdf\u0026#39;) Split PDF into individual pages:\ndef split_pdf(input_pdf, output_folder): \u0026#34;\u0026#34;\u0026#34; Split PDF into separate files, one per page. \u0026#34;\u0026#34;\u0026#34; Path(output_folder).mkdir(exist_ok=True) with open(input_pdf, \u0026#39;rb\u0026#39;) as file: pdf_reader = PyPDF2.PdfReader(file) num_pages = len(pdf_reader.pages) for page_num in range(num_pages): pdf_writer = PyPDF2.PdfWriter() pdf_writer.add_page(pdf_reader.pages[page_num]) output_filename = Path(output_folder, f\u0026#39;page_{page_num + 1}.pdf\u0026#39;) with open(output_filename, \u0026#39;wb\u0026#39;) as output_file: pdf_writer.write(output_file) print(f\u0026#34;Created {output_filename}\u0026#34;) print(f\u0026#34;\\nSplit {num_pages} pages from {input_pdf}\u0026#34;) # Usage split_pdf(\u0026#39;document.pdf\u0026#39;, \u0026#39;pages/\u0026#39;) Extract text from PDF:\ndef extract_text_from_pdf(pdf_file, output_txt): \u0026#34;\u0026#34;\u0026#34; Extract all text from PDF and save to text file. \u0026#34;\u0026#34;\u0026#34; with open(pdf_file, \u0026#39;rb\u0026#39;) as file: pdf_reader = PyPDF2.PdfReader(file) text_content = [] for page_num, page in enumerate(pdf_reader.pages, 1): text = page.extract_text() text_content.append(f\u0026#34;--- Page {page_num} ---\\n{text}\\n\u0026#34;) full_text = \u0026#39;\\n\u0026#39;.join(text_content) with open(output_txt, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as txt_file: txt_file.write(full_text) print(f\u0026#34;Extracted text from {len(pdf_reader.pages)} pages\u0026#34;) print(f\u0026#34;Saved to {output_txt}\u0026#34;) # Usage extract_text_from_pdf(\u0026#39;document.pdf\u0026#39;, \u0026#39;extracted_text.txt\u0026#39;) These three scripts handle 90% of my PDF automation needs.\nEmail Automation: Send Reports Automatically Automating emails saves tons of time for reports, notifications, and updates.\nSend simple text emails:\nimport smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart import os def send_email(to_email, subject, body): \u0026#34;\u0026#34;\u0026#34; Send a simple text email. \u0026#34;\u0026#34;\u0026#34; # Email configuration (use environment variables for security) from_email = os.getenv(\u0026#39;EMAIL_USER\u0026#39;) password = os.getenv(\u0026#39;EMAIL_PASSWORD\u0026#39;) smtp_server = \u0026#39;smtp.gmail.com\u0026#39; smtp_port = 587 # Create message msg = MIMEMultipart() msg[\u0026#39;From\u0026#39;] = from_email msg[\u0026#39;To\u0026#39;] = to_email msg[\u0026#39;Subject\u0026#39;] = subject msg.attach(MIMEText(body, \u0026#39;plain\u0026#39;)) # Send email try: server = smtplib.SMTP(smtp_server, smtp_port) server.starttls() server.login(from_email, password) server.send_message(msg) server.quit() print(f\u0026#34;Email sent successfully to {to_email}\u0026#34;) except Exception as e: print(f\u0026#34;Failed to send email: {e}\u0026#34;) # Usage (set environment variables first) send_email(\u0026#39;recipient@example.com\u0026#39;, \u0026#39;Weekly Report\u0026#39;, \u0026#39;Here is this week\u0026#39;s summary...\u0026#39;) Send emails with attachments:\nfrom email.mime.base import MIMEBase from email import encoders def send_email_with_attachment(to_email, subject, body, attachment_path): \u0026#34;\u0026#34;\u0026#34; Send email with file attachment. \u0026#34;\u0026#34;\u0026#34; from_email = os.getenv(\u0026#39;EMAIL_USER\u0026#39;) password = os.getenv(\u0026#39;EMAIL_PASSWORD\u0026#39;) msg = MIMEMultipart() msg[\u0026#39;From\u0026#39;] = from_email msg[\u0026#39;To\u0026#39;] = to_email msg[\u0026#39;Subject\u0026#39;] = subject msg.attach(MIMEText(body, \u0026#39;plain\u0026#39;)) # Attach file with open(attachment_path, \u0026#39;rb\u0026#39;) as attachment: part = MIMEBase(\u0026#39;application\u0026#39;, \u0026#39;octet-stream\u0026#39;) part.set_payload(attachment.read()) encoders.encode_base64(part) part.add_header( \u0026#39;Content-Disposition\u0026#39;, f\u0026#39;attachment; filename= {Path(attachment_path).name}\u0026#39;, ) msg.attach(part) # Send try: server = smtplib.SMTP(\u0026#39;smtp.gmail.com\u0026#39;, 587) server.starttls() server.login(from_email, password) server.send_message(msg) server.quit() print(f\u0026#34;Email with attachment sent to {to_email}\u0026#34;) except Exception as e: print(f\u0026#34;Failed to send email: {e}\u0026#34;) # Usage send_email_with_attachment(\u0026#39;boss@company.com\u0026#39;, \u0026#39;Weekly Report\u0026#39;, \u0026#39;Please find attached this week\u0026#39;s report.\u0026#39;, \u0026#39;report.xlsx\u0026#39;) Send formatted HTML emails:\nfrom email.mime.text import MIMEText def send_html_email(to_email, subject, html_content): \u0026#34;\u0026#34;\u0026#34; Send formatted HTML email. \u0026#34;\u0026#34;\u0026#34; from_email = os.getenv(\u0026#39;EMAIL_USER\u0026#39;) password = os.getenv(\u0026#39;EMAIL_PASSWORD\u0026#39;) msg = MIMEMultipart(\u0026#39;alternative\u0026#39;) msg[\u0026#39;From\u0026#39;] = from_email msg[\u0026#39;To\u0026#39;] = to_email msg[\u0026#39;Subject\u0026#39;] = subject # Attach HTML content html_part = MIMEText(html_content, \u0026#39;html\u0026#39;) msg.attach(html_part) # Send try: server = smtplib.SMTP(\u0026#39;smtp.gmail.com\u0026#39;, 587) server.starttls() server.login(from_email, password) server.send_message(msg) server.quit() print(f\u0026#34;HTML email sent to {to_email}\u0026#34;) except Exception as e: print(f\u0026#34;Failed to send email: {e}\u0026#34;) # Usage with formatted content html = \u0026#34;\u0026#34;\u0026#34; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Weekly Sales Report\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Here are this week\u0026#39;s highlights:\u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;Total Sales: $50,000\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;New Customers: 25\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Conversion Rate: 3.5%\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;p\u0026gt;Great work team!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; send_html_email(\u0026#39;team@company.com\u0026#39;, \u0026#39;Weekly Sales Report\u0026#39;, html) I use email automation for daily reports, error notifications, and backup confirmations.\nImage Processing: Batch Resize, Watermark, Convert Image manipulation is tedious when done manually but instant with Python.\nBatch resize images:\nfrom PIL import Image from pathlib import Path def batch_resize_images(input_folder, output_folder, width=800): \u0026#34;\u0026#34;\u0026#34; Resize all images in folder while maintaining aspect ratio. \u0026#34;\u0026#34;\u0026#34; Path(output_folder).mkdir(exist_ok=True) image_extensions = [\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.bmp\u0026#39;, \u0026#39;.gif\u0026#39;] processed = 0 for image_file in Path(input_folder).iterdir(): if image_file.suffix.lower() in image_extensions: try: img = Image.open(image_file) # Calculate new height maintaining aspect ratio aspect_ratio = img.height / img.width new_height = int(width * aspect_ratio) # Resize resized_img = img.resize((width, new_height), Image.LANCZOS) # Save output_path = Path(output_folder, image_file.name) resized_img.save(output_path) print(f\u0026#34;Resized: {image_file.name}\u0026#34;) processed += 1 except Exception as e: print(f\u0026#34;Error processing {image_file.name}: {e}\u0026#34;) print(f\u0026#34;\\nResized {processed} images\u0026#34;) # Usage batch_resize_images(\u0026#39;original_photos/\u0026#39;, \u0026#39;resized_photos/\u0026#39;, width=1200) Add watermark to images:\nfrom PIL import Image, ImageDraw, ImageFont def add_watermark(image_path, watermark_text, output_path): \u0026#34;\u0026#34;\u0026#34; Add text watermark to image. \u0026#34;\u0026#34;\u0026#34; img = Image.open(image_path) draw = ImageDraw.Draw(img) # Calculate watermark position (bottom right) font = ImageFont.load_default() text_bbox = draw.textbbox((0, 0), watermark_text, font=font) text_width = text_bbox[2] - text_bbox[0] text_height = text_bbox[3] - text_bbox[1] margin = 10 x = img.width - text_width - margin y = img.height - text_height - margin # Add semi-transparent watermark draw.text((x, y), watermark_text, fill=(255, 255, 255, 128), font=font) img.save(output_path) print(f\u0026#34;Added watermark to {output_path}\u0026#34;) # Usage add_watermark(\u0026#39;photo.jpg\u0026#39;, \u0026#39;© 2025 Your Name\u0026#39;, \u0026#39;photo_watermarked.jpg\u0026#39;) Convert image formats in batch:\ndef batch_convert_images(input_folder, output_folder, output_format=\u0026#39;PNG\u0026#39;): \u0026#34;\u0026#34;\u0026#34; Convert all images to specified format. \u0026#34;\u0026#34;\u0026#34; Path(output_folder).mkdir(exist_ok=True) for image_file in Path(input_folder).glob(\u0026#39;*\u0026#39;): if image_file.suffix.lower() in [\u0026#39;.jpg\u0026#39;, \u0026#39;.jpeg\u0026#39;, \u0026#39;.png\u0026#39;, \u0026#39;.bmp\u0026#39;]: try: img = Image.open(image_file) # Convert RGBA to RGB if saving as JPEG if output_format.upper() == \u0026#39;JPEG\u0026#39; and img.mode == \u0026#39;RGBA\u0026#39;: img = img.convert(\u0026#39;RGB\u0026#39;) output_filename = image_file.stem + f\u0026#39;.{output_format.lower()}\u0026#39; output_path = Path(output_folder, output_filename) img.save(output_path, output_format) print(f\u0026#34;Converted: {image_file.name} -\u0026gt; {output_filename}\u0026#34;) except Exception as e: print(f\u0026#34;Error converting {image_file.name}: {e}\u0026#34;) # Usage batch_convert_images(\u0026#39;jpg_images/\u0026#39;, \u0026#39;png_images/\u0026#39;, \u0026#39;PNG\u0026#39;) These scripts handle image preparation for websites, social media, and documentation.\nScheduling: Run Scripts Automatically The schedule library makes it easy to run scripts periodically without cron or Task Scheduler.\nBasic scheduling:\nimport schedule import time def job(): print(f\u0026#34;Running scheduled task at {datetime.now()}\u0026#34;) # Your automation code here # Schedule examples schedule.every(10).minutes.do(job) schedule.every().hour.do(job) schedule.every().day.at(\u0026#34;10:30\u0026#34;).do(job) schedule.every().monday.do(job) schedule.every().wednesday.at(\u0026#34;13:15\u0026#34;).do(job) # Keep script running while True: schedule.run_pending() time.sleep(1) Real-world scheduled automation:\nimport schedule import time from datetime import datetime def backup_files(): \u0026#34;\u0026#34;\u0026#34;Daily backup at 2 AM.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;[{datetime.now()}] Starting backup...\u0026#34;) # Your backup logic here print(\u0026#34;Backup complete!\u0026#34;) def send_daily_report(): \u0026#34;\u0026#34;\u0026#34;Send email report at 9 AM.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;[{datetime.now()}] Sending daily report...\u0026#34;) # Your report generation and email logic print(\u0026#34;Report sent!\u0026#34;) def clean_temp_files(): \u0026#34;\u0026#34;\u0026#34;Clean temp files every hour.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;[{datetime.now()}] Cleaning temp files...\u0026#34;) # Your cleanup logic print(\u0026#34;Cleanup complete!\u0026#34;) # Schedule tasks schedule.every().day.at(\u0026#34;02:00\u0026#34;).do(backup_files) schedule.every().day.at(\u0026#34;09:00\u0026#34;).do(send_daily_report) schedule.every().hour.do(clean_temp_files) print(\u0026#34;Scheduler started. Press Ctrl+C to exit.\u0026#34;) # Run scheduler while True: schedule.run_pending() time.sleep(60) # Check every minute For production, deploy this to a server or use system schedulers like cron for reliability. Check our guide on automating tasks with cron jobs and shell scripts for production scheduling.\nWeb Scraping for Data Collection Web scraping automates data collection from websites. Combined with scheduling, you can build automated data pipelines.\nBasic web scraper:\nimport requests from bs4 import BeautifulSoup import csv from datetime import datetime def scrape_product_prices(url): \u0026#34;\u0026#34;\u0026#34; Scrape product prices from e-commerce site. \u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#39; } response = requests.get(url, headers=headers) soup = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) products = [] # Adjust selectors for your target site for item in soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;product-card\u0026#39;): name = item.find(\u0026#39;h3\u0026#39;, class_=\u0026#39;product-name\u0026#39;).text.strip() price = item.find(\u0026#39;span\u0026#39;, class_=\u0026#39;price\u0026#39;).text.strip() products.append({ \u0026#39;name\u0026#39;: name, \u0026#39;price\u0026#39;: price, \u0026#39;scraped_at\u0026#39;: datetime.now().isoformat() }) return products def save_to_csv(data, filename): \u0026#34;\u0026#34;\u0026#34;Save scraped data to CSV.\u0026#34;\u0026#34;\u0026#34; with open(filename, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: writer = csv.DictWriter(f, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) # Usage products = scrape_product_prices(\u0026#39;https://example.com/products\u0026#39;) save_to_csv(products, f\u0026#39;prices_{datetime.now().strftime(\u0026#34;%Y%m%d\u0026#34;)}.csv\u0026#39;) For more web scraping techniques, check the full guide on building web scrapers with BeautifulSoup and Requests .\nPutting It All Together: A Complete Automation Workflow Let me show you how to combine these scripts into a complete automation workflow. This example generates and emails a daily report automatically.\nComplete automated reporting system:\nimport pandas as pd from pathlib import Path from datetime import datetime import schedule import time def generate_daily_report(): \u0026#34;\u0026#34;\u0026#34; Complete workflow: collect data, process, format, and email. \u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;[{datetime.now()}] Starting daily report generation...\u0026#34;) # Step 1: Collect data (could be from database, API, or files) data = { \u0026#39;Product\u0026#39;: [\u0026#39;Product A\u0026#39;, \u0026#39;Product B\u0026#39;, \u0026#39;Product C\u0026#39;], \u0026#39;Sales\u0026#39;: [150, 200, 180], \u0026#39;Revenue\u0026#39;: [15000, 30000, 25000] } df = pd.DataFrame(data) # Step 2: Process and analyze df[\u0026#39;Profit\u0026#39;] = df[\u0026#39;Revenue\u0026#39;] * 0.3 total_revenue = df[\u0026#39;Revenue\u0026#39;].sum() total_profit = df[\u0026#39;Profit\u0026#39;].sum() # Step 3: Generate Excel report output_file = f\u0026#39;reports/daily_report_{datetime.now().strftime(\u0026#34;%Y%m%d\u0026#34;)}.xlsx\u0026#39; Path(\u0026#39;reports\u0026#39;).mkdir(exist_ok=True) with pd.ExcelWriter(output_file, engine=\u0026#39;openpyxl\u0026#39;) as writer: df.to_excel(writer, sheet_name=\u0026#39;Sales Data\u0026#39;, index=False) # Add summary sheet summary = pd.DataFrame({ \u0026#39;Metric\u0026#39;: [\u0026#39;Total Revenue\u0026#39;, \u0026#39;Total Profit\u0026#39;, \u0026#39;Number of Products\u0026#39;], \u0026#39;Value\u0026#39;: [total_revenue, total_profit, len(df)] }) summary.to_excel(writer, sheet_name=\u0026#39;Summary\u0026#39;, index=False) print(f\u0026#34;Report generated: {output_file}\u0026#34;) # Step 4: Send email with attachment send_report_email(output_file, total_revenue, total_profit) print(f\u0026#34;[{datetime.now()}] Daily report complete!\u0026#34;) def send_report_email(report_file, revenue, profit): \u0026#34;\u0026#34;\u0026#34;Send report via email.\u0026#34;\u0026#34;\u0026#34; subject = f\u0026#34;Daily Sales Report - {datetime.now().strftime(\u0026#39;%Y-%m-%d\u0026#39;)}\u0026#34; body = f\u0026#34;\u0026#34;\u0026#34; Daily Sales Report Summary: - Total Revenue: ${revenue:,.2f} - Total Profit: ${profit:,.2f} Detailed report attached. Automated report generated by Python \u0026#34;\u0026#34;\u0026#34; # Use email function from earlier send_email_with_attachment( to_email=\u0026#39;manager@company.com\u0026#39;, subject=subject, body=body, attachment_path=report_file ) # Schedule to run daily at 8 AM schedule.every().day.at(\u0026#34;08:00\u0026#34;).do(generate_daily_report) print(\u0026#34;Daily report automation started\u0026#34;) print(\u0026#34;Will run every day at 8:00 AM\u0026#34;) while True: schedule.run_pending() time.sleep(60) This complete workflow collects data, processes it, generates a formatted Excel report, and emails it automatically. Run it on a server, and you never have to manually create reports again.\nBest Practices for Production Automation Making automation reliable for production requires attention to detail. Here is what I have learned from maintaining automated systems.\n1. Error handling and logging:\nimport logging from datetime import datetime # Configure logging logging.basicConfig( level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;, handlers=[ logging.FileHandler(f\u0026#39;logs/automation_{datetime.now().strftime(\u0026#34;%Y%m%d\u0026#34;)}.log\u0026#39;), logging.StreamHandler() ] ) def safe_automation_task(): \u0026#34;\u0026#34;\u0026#34;Template for reliable automation with proper error handling.\u0026#34;\u0026#34;\u0026#34; logging.info(\u0026#34;Starting automation task\u0026#34;) try: # Your automation code here result = perform_task() logging.info(f\u0026#34;Task completed successfully: {result}\u0026#34;) return result except FileNotFoundError as e: logging.error(f\u0026#34;File not found: {e}\u0026#34;) send_error_notification(\u0026#34;File missing\u0026#34;, str(e)) except Exception as e: logging.error(f\u0026#34;Unexpected error: {e}\u0026#34;, exc_info=True) send_error_notification(\u0026#34;Automation failed\u0026#34;, str(e)) finally: logging.info(\u0026#34;Automation task finished\u0026#34;) def send_error_notification(title, message): \u0026#34;\u0026#34;\u0026#34;Send email alert when automation fails.\u0026#34;\u0026#34;\u0026#34; send_email( to_email=os.getenv(\u0026#39;ADMIN_EMAIL\u0026#39;), subject=f\u0026#34;Automation Error: {title}\u0026#34;, body=f\u0026#34;An error occurred in automation:\\n\\n{message}\u0026#34; ) 2. Configuration management:\nimport json from pathlib import Path def load_config(config_file=\u0026#39;config.json\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Load configuration from external file.\u0026#34;\u0026#34;\u0026#34; with open(config_file, \u0026#39;r\u0026#39;) as f: config = json.load(f) return config # config.json example: { \u0026#34;input_folder\u0026#34;: \u0026#34;/path/to/data\u0026#34;, \u0026#34;output_folder\u0026#34;: \u0026#34;/path/to/output\u0026#34;, \u0026#34;email_recipients\u0026#34;: [\u0026#34;user1@example.com\u0026#34;, \u0026#34;user2@example.com\u0026#34;], \u0026#34;schedule_time\u0026#34;: \u0026#34;08:00\u0026#34;, \u0026#34;enable_notifications\u0026#34;: true } 3. Retry logic for reliability:\nimport time def retry_on_failure(func, max_attempts=3, delay=5): \u0026#34;\u0026#34;\u0026#34; Retry function on failure with exponential backoff. \u0026#34;\u0026#34;\u0026#34; for attempt in range(1, max_attempts + 1): try: return func() except Exception as e: if attempt == max_attempts: logging.error(f\u0026#34;Failed after {max_attempts} attempts: {e}\u0026#34;) raise wait_time = delay * (2 ** (attempt - 1)) # Exponential backoff logging.warning(f\u0026#34;Attempt {attempt} failed, retrying in {wait_time}s: {e}\u0026#34;) time.sleep(wait_time) # Usage result = retry_on_failure(lambda: risky_operation()) These patterns make automation robust enough for production use.\nCommon Pitfalls and How to Avoid Them I have made plenty of mistakes with automation. Here are the biggest ones and how to avoid them.\nPath issues: Always use absolute paths or Path objects. Relative paths break when scripts run from different directories or via schedulers.\n# Bad with open(\u0026#39;data.csv\u0026#39;, \u0026#39;r\u0026#39;) as f: # This breaks when run from cron # Good from pathlib import Path script_dir = Path(__file__).parent data_file = script_dir / \u0026#39;data\u0026#39; / \u0026#39;data.csv\u0026#39; with open(data_file, \u0026#39;r\u0026#39;) as f: # Works everywhere Hardcoded credentials: Never put passwords or API keys in code. Use environment variables or config files with restricted permissions.\n# Bad password = \u0026#39;mypassword123\u0026#39; # Good import os password = os.getenv(\u0026#39;EMAIL_PASSWORD\u0026#39;) if not password: raise ValueError(\u0026#34;EMAIL_PASSWORD environment variable not set\u0026#34;) No error notifications: Automations fail silently and you discover it weeks later. Always add email or Slack alerts for failures.\nOvercomplicating: Start simple. A 20-line script that works beats a complex framework you never finish.\nNot testing error cases: Test what happens when files are missing, networks fail, or data is malformed. Handle these gracefully.\nWhat to Automate Next You have learned the fundamentals. Here is where to go next based on your needs.\nFor data analysts: Automate report generation end-to-end. Pull data from databases or APIs, process with pandas, generate visualizations, create formatted Excel or PDF reports, and email stakeholders automatically.\nFor developers: Automate your development workflow. Scripts for environment setup, dependency updates, running test suites, deploying to servers, and generating documentation save hours weekly.\nFor content creators: Automate image processing pipelines. Batch resize, watermark, convert formats, and upload to cloud storage. Add web scraping to collect content ideas.\nFor system administrators: Automate server monitoring, log analysis, backup verification, and disk cleanup. Combine with cron jobs for scheduled maintenance.\nFor e-commerce: Automate inventory updates, price monitoring, order processing, and customer communication. Web scraping competitors combined with email automation creates powerful workflows.\nThe best automation solves your actual problems. Look at your daily routine and ask: what am I doing repeatedly that a computer could handle? Start there.\nWrapping Up Automation changed how I work. I used to spend hours on tedious tasks. Now those tasks run themselves while I focus on work that actually matters. The scripts in this guide save me 10-15 hours weekly, and I keep finding new things to automate.\nStart small. Pick one annoying task from your daily routine and automate it. Get that working, then move to the next. Build a collection of scripts that compound into serious time savings. Share them with teammates and multiply the benefit.\nRemember: the goal is not perfect code, it is saving time. A crude script that works is infinitely better than a beautiful one you never finish. Write something quick and dirty, use it for a while, then refactor if needed.\nPython makes automation accessible. You do not need to be an expert. Basic Python plus the scripts in this guide handle most automation needs. The hard part is not the code, it is identifying what to automate and actually doing it.\nSo what are you going to automate first? Pick something that annoys you daily and write a script this weekend. Future you will thank you.\nNow go reclaim some time.\n","href":"/2025/10/python-automation-scripts-every-developer-should-know.html","title":"Python Automation Scripts Every Developer Should Know (Save Hours Weekly)"},{"content":"I still remember my first encounter with a messy dataset\u0026ndash;15,000 rows of sales data in Excel, cells with typos, missing values everywhere, and my boss wanting insights \u0026ldquo;by tomorrow morning.\u0026rdquo; I spent 8 hours manually cleaning data, copy-pasting formulas, and creating pivot tables. My eyes hurt, my brain hurt, and I barely finished on time.\nThen I discovered Python Pandas.\nThe same analysis that took me 8 hours? I automated it in 30 minutes. Data cleaning that required hundreds of manual clicks? Five lines of code. Complex calculations across thousands of rows? Instant.\nThat\u0026rsquo;s when I realized data analysis isn\u0026rsquo;t about clicking faster\u0026ndash;it\u0026rsquo;s about working smarter.\nIf you\u0026rsquo;ve ever felt frustrated wrestling with spreadsheets, copying formulas until your fingers hurt, or manually fixing data errors one cell at a time, this guide is for you. I\u0026rsquo;ll show you how to go from zero Pandas knowledge to confidently analyzing real datasets and extracting meaningful insights.\nNo theory-heavy lectures. No \u0026ldquo;hello world\u0026rdquo; toy examples. Just practical, production-ready techniques you can use immediately at work or for personal projects.\nBy the end of this guide, you\u0026rsquo;ll be able to load messy data, clean it automatically, perform complex analyses, create visualizations, and generate reports\u0026ndash;all with Python Pandas.\nLet\u0026rsquo;s turn you into a data analysis machine.\nWhy Pandas is a Game-Changer for Data Analysis Before we dive into code, let me show you why Pandas matters.\nWhat makes Pandas special:\nSpeed: Pandas processes millions of rows in seconds. Tasks that take hours in Excel run instantly.\nAutomation: Write once, run forever. Your analysis becomes a script you can reuse with any dataset.\nPower: Pandas handles complex operations Excel can\u0026rsquo;t do\u0026ndash;advanced filtering, multi-level grouping, time series analysis, merging datasets from multiple sources.\nIntegration: Works with databases (SQL), APIs, web scraping, machine learning libraries, and cloud services.\nReproducibility: Your analysis is code, not manual clicks. Anyone can verify your work, catch errors, and build on your insights.\nReal-world impact:\nI\u0026rsquo;ve used Pandas to analyze customer behavior for marketing campaigns (boosted conversion 35%), clean financial data for quarterly reports (reduced errors from 12% to 0.3%), and process sensor data from IoT devices (handled 5 million records in under 2 minutes).\nData analysts, business intelligence professionals, researchers, marketers, financial analysts\u0026ndash;they all use Pandas because it\u0026rsquo;s the fastest path from raw data to actionable insights.\nNow let\u0026rsquo;s get your hands dirty with actual code.\nInstalling Pandas and Setting Up Your Environment First, you need Python installed. I recommend Python 3.8 or newer.\nInstall Pandas:\npip install pandas numpy matplotlib seaborn openpyxl This installs:\npandas - core library numpy - numerical operations (Pandas dependency) matplotlib - basic plotting seaborn - beautiful statistical visualizations openpyxl - Excel file support Verify installation:\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns print(f\u0026#34;Pandas version: {pd.__version__}\u0026#34;) print(f\u0026#34;NumPy version: {np.__version__}\u0026#34;) If this runs without errors, you\u0026rsquo;re ready.\nPro tip: Use Jupyter Notebook or VS Code for data analysis. Jupyter lets you run code in cells and see outputs immediately, perfect for exploring data interactively.\nInstall Jupyter:\npip install jupyter jupyter notebook Now let\u0026rsquo;s start analyzing data.\nPandas Basics: DataFrames and Series Pandas has two main data structures:\nSeries: A single column of data (1-dimensional) DataFrame: A table with rows and columns (2-dimensional)\nYou\u0026rsquo;ll work with DataFrames 90% of the time, but let\u0026rsquo;s understand both.\nCreating a Series import pandas as pd # Create Series from list prices = pd.Series([29.99, 49.99, 19.99, 39.99]) print(prices) Output:\n0 29.99 1 49.99 2 19.99 3 39.99 dtype: float64 The left column is the index (automatic row numbers), right column is the data.\nSeries with custom index:\nprices = pd.Series( [29.99, 49.99, 19.99, 39.99], index=[\u0026#39;product_a\u0026#39;, \u0026#39;product_b\u0026#39;, \u0026#39;product_c\u0026#39;, \u0026#39;product_d\u0026#39;] ) print(prices) print(f\u0026#34;\\nProduct B price: ${prices[\u0026#39;product_b\u0026#39;]}\u0026#34;) Output:\nproduct_a 29.99 product_b 49.99 product_c 19.99 product_d 39.99 dtype: float64 Product B price: $49.99 Creating a DataFrame DataFrames are where the magic happens. Think of them as programmable spreadsheets.\nFrom dictionary:\ndata = { \u0026#39;product\u0026#39;: [\u0026#39;Laptop\u0026#39;, \u0026#39;Mouse\u0026#39;, \u0026#39;Keyboard\u0026#39;, \u0026#39;Monitor\u0026#39;], \u0026#39;price\u0026#39;: [899.99, 29.99, 79.99, 299.99], \u0026#39;stock\u0026#39;: [15, 150, 45, 30], \u0026#39;category\u0026#39;: [\u0026#39;Electronics\u0026#39;, \u0026#39;Accessories\u0026#39;, \u0026#39;Accessories\u0026#39;, \u0026#39;Electronics\u0026#39;] } df = pd.DataFrame(data) print(df) Output:\nproduct price stock category 0 Laptop 899.99 15 Electronics 1 Mouse 29.99 150 Accessories 2 Keyboard 79.99 45 Accessories 3 Monitor 299.99 30 Electronics From list of lists:\ndata = [ [\u0026#39;Laptop\u0026#39;, 899.99, 15, \u0026#39;Electronics\u0026#39;], [\u0026#39;Mouse\u0026#39;, 29.99, 150, \u0026#39;Accessories\u0026#39;], [\u0026#39;Keyboard\u0026#39;, 79.99, 45, \u0026#39;Accessories\u0026#39;], [\u0026#39;Monitor\u0026#39;, 299.99, 30, \u0026#39;Electronics\u0026#39;] ] df = pd.DataFrame(data, columns=[\u0026#39;product\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;stock\u0026#39;, \u0026#39;category\u0026#39;]) print(df) Same output as before.\nQuick data exploration:\n# First 5 rows print(df.head()) # Last 3 rows print(df.tail(3)) # DataFrame info (data types, memory usage) print(df.info()) # Statistical summary print(df.describe()) # Column names print(df.columns) # Shape (rows, columns) print(f\u0026#34;Shape: {df.shape}\u0026#34;) # Output: Shape: (4, 4) These functions are your first step every time you load a dataset\u0026ndash;they help you understand what you\u0026rsquo;re working with.\nReading Data from Files Real data analysis starts with loading data from external sources.\nReading CSV Files CSV (Comma-Separated Values) is the most common data format.\nBasic CSV reading:\n# Read CSV file df = pd.read_csv(\u0026#39;sales_data.csv\u0026#39;) # Show first rows print(df.head()) CSV with custom options:\ndf = pd.read_csv( \u0026#39;sales_data.csv\u0026#39;, sep=\u0026#39;;\u0026#39;, # Custom separator (default is comma) encoding=\u0026#39;utf-8\u0026#39;, # Handle special characters thousands=\u0026#39;,\u0026#39;, # Parse \u0026#34;1,000\u0026#34; as 1000 decimal=\u0026#39;.\u0026#39;, # Decimal separator parse_dates=[\u0026#39;order_date\u0026#39;], # Convert string to datetime na_values=[\u0026#39;NA\u0026#39;, \u0026#39;N/A\u0026#39;, \u0026#39;-\u0026#39;, \u0026#39;\u0026#39;], # Treat these as missing values skiprows=2, # Skip first 2 rows nrows=1000 # Read only first 1000 rows (good for testing) ) Reading from URL:\nurl = \u0026#39;https://example.com/data.csv\u0026#39; df = pd.read_csv(url) Reading Excel Files # Read first sheet df = pd.read_excel(\u0026#39;sales_report.xlsx\u0026#39;) # Read specific sheet df = pd.read_excel(\u0026#39;sales_report.xlsx\u0026#39;, sheet_name=\u0026#39;Q1 Sales\u0026#39;) # Read multiple sheets sheets = pd.read_excel(\u0026#39;sales_report.xlsx\u0026#39;, sheet_name=None) # Returns dict df_q1 = sheets[\u0026#39;Q1 Sales\u0026#39;] df_q2 = sheets[\u0026#39;Q2 Sales\u0026#39;] # Skip rows and specify columns df = pd.read_excel( \u0026#39;sales_report.xlsx\u0026#39;, sheet_name=\u0026#39;Sales\u0026#39;, skiprows=3, # Skip header rows usecols=\u0026#39;A:E\u0026#39;, # Read only columns A to E nrows=500 # Read first 500 rows ) Reading JSON Files # Read JSON file df = pd.read_json(\u0026#39;data.json\u0026#39;) # Read JSON with specific orientation df = pd.read_json(\u0026#39;data.json\u0026#39;, orient=\u0026#39;records\u0026#39;) # Read nested JSON df = pd.json_normalize(data, record_path=\u0026#39;items\u0026#39;) Reading from SQL Database import sqlite3 # Connect to database conn = sqlite3.connect(\u0026#39;database.db\u0026#39;) # Read query results into DataFrame df = pd.read_sql(\u0026#39;SELECT * FROM sales WHERE amount \u0026gt; 100\u0026#39;, conn) # Or read entire table df = pd.read_sql_table(\u0026#39;sales\u0026#39;, conn) conn.close() For this guide, let\u0026rsquo;s create sample sales data to practice with:\nimport pandas as pd import numpy as np # Create realistic sales dataset np.random.seed(42) dates = pd.date_range(\u0026#39;2024-01-01\u0026#39;, periods=500, freq=\u0026#39;D\u0026#39;) sales_data = { \u0026#39;order_id\u0026#39;: range(1, 501), \u0026#39;order_date\u0026#39;: np.random.choice(dates, 500), \u0026#39;customer_id\u0026#39;: np.random.randint(1000, 1100, 500), \u0026#39;product\u0026#39;: np.random.choice([\u0026#39;Laptop\u0026#39;, \u0026#39;Mouse\u0026#39;, \u0026#39;Keyboard\u0026#39;, \u0026#39;Monitor\u0026#39;, \u0026#39;Headphones\u0026#39;, \u0026#39;Webcam\u0026#39;], 500), \u0026#39;category\u0026#39;: np.random.choice([\u0026#39;Electronics\u0026#39;, \u0026#39;Accessories\u0026#39;], 500), \u0026#39;quantity\u0026#39;: np.random.randint(1, 10, 500), \u0026#39;unit_price\u0026#39;: np.random.uniform(20, 1000, 500).round(2), \u0026#39;discount\u0026#39;: np.random.choice([0, 5, 10, 15, 20], 500), \u0026#39;region\u0026#39;: np.random.choice([\u0026#39;North\u0026#39;, \u0026#39;South\u0026#39;, \u0026#39;East\u0026#39;, \u0026#39;West\u0026#39;], 500), \u0026#39;payment_method\u0026#39;: np.random.choice([\u0026#39;Credit Card\u0026#39;, \u0026#39;PayPal\u0026#39;, \u0026#39;Bank Transfer\u0026#39;], 500) } df = pd.DataFrame(sales_data) # Add some missing values (realistic scenario) df.loc[np.random.choice(df.index, 20), \u0026#39;discount\u0026#39;] = np.nan df.loc[np.random.choice(df.index, 15), \u0026#39;region\u0026#39;] = np.nan # Calculate total amount df[\u0026#39;total_amount\u0026#39;] = (df[\u0026#39;unit_price\u0026#39;] * df[\u0026#39;quantity\u0026#39;] * (1 - df[\u0026#39;discount\u0026#39;].fillna(0)/100)).round(2) # Save to CSV for practice df.to_csv(\u0026#39;sales_data.csv\u0026#39;, index=False) print(df.head(10)) print(f\u0026#34;\\nDataset shape: {df.shape}\u0026#34;) print(f\u0026#34;\\nColumn types:\\n{df.dtypes}\u0026#34;) Now we have a realistic dataset to practice with. Save this script and run it to create sales_data.csv.\nData Exploration: Understanding Your Dataset Before analyzing, you need to understand what you\u0026rsquo;re working with.\n# Load our sales data df = pd.read_csv(\u0026#39;sales_data.csv\u0026#39;, parse_dates=[\u0026#39;order_date\u0026#39;]) # Basic info print(\u0026#34;Dataset shape:\u0026#34;, df.shape) # (500, 11) - 500 rows, 11 columns print(\u0026#34;\\nColumn names:\u0026#34;, df.columns.tolist()) print(\u0026#34;\\nData types:\\n\u0026#34;, df.dtypes) # First and last rows print(\u0026#34;\\nFirst 5 rows:\u0026#34;) print(df.head()) print(\u0026#34;\\nLast 5 rows:\u0026#34;) print(df.tail()) # Statistical summary print(\u0026#34;\\nNumerical summary:\u0026#34;) print(df.describe()) # Missing values count print(\u0026#34;\\nMissing values:\u0026#34;) print(df.isna().sum()) # Unique values per column print(\u0026#34;\\nUnique values:\u0026#34;) for col in df.columns: print(f\u0026#34;{col}: {df[col].nunique()} unique values\u0026#34;) # Memory usage print(\u0026#34;\\nMemory usage:\u0026#34;) print(df.memory_usage(deep=True)) This gives you a complete picture: data types, missing values, statistical distribution, and memory footprint.\nCheck for duplicates:\n# Check duplicate rows print(f\u0026#34;Duplicate rows: {df.duplicated().sum()}\u0026#34;) # Check duplicates based on specific columns print(f\u0026#34;Duplicate order IDs: {df.duplicated(subset=[\u0026#39;order_id\u0026#39;]).sum()}\u0026#34;) # View duplicate rows duplicates = df[df.duplicated(subset=[\u0026#39;order_id\u0026#39;], keep=False)] print(duplicates) Selecting and Filtering Data This is where Pandas shows its power. You can slice, filter, and extract data with surgical precision.\nSelecting Columns # Select single column (returns Series) products = df[\u0026#39;product\u0026#39;] print(type(products)) # \u0026lt;class \u0026#39;pandas.core.series.Series\u0026#39;\u0026gt; # Select single column (returns DataFrame) products_df = df[[\u0026#39;product\u0026#39;]] print(type(products_df)) # \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; # Select multiple columns subset = df[[\u0026#39;order_id\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;total_amount\u0026#39;]] print(subset.head()) # Select columns by position first_three_cols = df.iloc[:, :3] # First 3 columns print(first_three_cols.head()) Selecting Rows # First 10 rows first_ten = df.head(10) # Rows 10 to 20 rows_10_to_20 = df.iloc[10:20] # Specific rows by index specific_rows = df.iloc[[0, 5, 10, 15]] # Every 10th row every_tenth = df.iloc[::10] Filtering with Conditions This is the bread and butter of data analysis.\nSimple filters:\n# Products with quantity \u0026gt; 5 high_quantity = df[df[\u0026#39;quantity\u0026#39;] \u0026gt; 5] print(f\u0026#34;High quantity orders: {len(high_quantity)}\u0026#34;) # Laptop sales only laptops = df[df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;] print(f\u0026#34;Laptop orders: {len(laptops)}\u0026#34;) # Sales with discount discounted = df[df[\u0026#39;discount\u0026#39;] \u0026gt; 0] print(f\u0026#34;Discounted orders: {len(discounted)}\u0026#34;) # High value orders (total_amount \u0026gt; 500) high_value = df[df[\u0026#39;total_amount\u0026#39;] \u0026gt; 500] print(high_value[[\u0026#39;order_id\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;total_amount\u0026#39;]].head()) Multiple conditions:\n# AND condition: Laptops with quantity \u0026gt; 3 laptops_bulk = df[(df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;) \u0026amp; (df[\u0026#39;quantity\u0026#39;] \u0026gt; 3)] # OR condition: Laptop or Monitor premium_products = df[(df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;) | (df[\u0026#39;product\u0026#39;] == \u0026#39;Monitor\u0026#39;)] # Complex condition: High value orders from North or South region high_value_regions = df[ (df[\u0026#39;total_amount\u0026#39;] \u0026gt; 500) \u0026amp; ((df[\u0026#39;region\u0026#39;] == \u0026#39;North\u0026#39;) | (df[\u0026#39;region\u0026#39;] == \u0026#39;South\u0026#39;)) ] print(f\u0026#34;Found {len(high_value_regions)} high-value orders from North/South\u0026#34;) Important: Use \u0026amp; for AND, | for OR, and ~ for NOT. Always use parentheses around each condition.\nString filtering:\n# Products containing \u0026#34;top\u0026#34; (case-insensitive) contains_top = df[df[\u0026#39;product\u0026#39;].str.contains(\u0026#39;top\u0026#39;, case=False, na=False)] # Products starting with \u0026#34;Key\u0026#34; starts_with_key = df[df[\u0026#39;product\u0026#39;].str.startswith(\u0026#39;Key\u0026#39;, na=False)] # Products ending with \u0026#34;s\u0026#34; ends_with_s = df[df[\u0026#39;product\u0026#39;].str.endswith(\u0026#39;s\u0026#39;, na=False)] # Region is not null has_region = df[df[\u0026#39;region\u0026#39;].notna()] Filter by date range:\n# Orders in January 2024 jan_orders = df[(df[\u0026#39;order_date\u0026#39;] \u0026gt;= \u0026#39;2024-01-01\u0026#39;) \u0026amp; (df[\u0026#39;order_date\u0026#39;] \u0026lt; \u0026#39;2024-02-01\u0026#39;)] # Recent orders (last 30 days from latest date) latest_date = df[\u0026#39;order_date\u0026#39;].max() cutoff_date = latest_date - pd.Timedelta(days=30) recent_orders = df[df[\u0026#39;order_date\u0026#39;] \u0026gt;= cutoff_date] print(f\u0026#34;January orders: {len(jan_orders)}\u0026#34;) print(f\u0026#34;Last 30 days orders: {len(recent_orders)}\u0026#34;) Using .loc and .iloc:\n# .loc: label-based selection # Select rows 0-5, columns \u0026#39;product\u0026#39; and \u0026#39;total_amount\u0026#39; subset = df.loc[0:5, [\u0026#39;product\u0026#39;, \u0026#39;total_amount\u0026#39;]] # .iloc: position-based selection # Select first 5 rows, columns 3-5 subset = df.iloc[0:5, 3:6] # .loc with condition # High value Laptop orders, show specific columns result = df.loc[ (df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;) \u0026amp; (df[\u0026#39;total_amount\u0026#39;] \u0026gt; 500), [\u0026#39;order_id\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;quantity\u0026#39;, \u0026#39;total_amount\u0026#39;] ] print(result.head()) .query() method (cleaner syntax):\n# Instead of this: result = df[(df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;) \u0026amp; (df[\u0026#39;total_amount\u0026#39;] \u0026gt; 500)] # You can write this: result = df.query(\u0026#34;product == \u0026#39;Laptop\u0026#39; and total_amount \u0026gt; 500\u0026#34;) # Complex query result = df.query(\u0026#34;product in [\u0026#39;Laptop\u0026#39;, \u0026#39;Monitor\u0026#39;] and quantity \u0026gt; 2 and region == \u0026#39;North\u0026#39;\u0026#34;) # Using variables in query min_amount = 500 result = df.query(\u0026#34;total_amount \u0026gt; @min_amount\u0026#34;) The .query() method is more readable for complex filters.\nData Cleaning: Handling Missing Values and Duplicates Real-world data is messy. Let\u0026rsquo;s clean it.\nIdentifying Missing Data # Count missing values per column print(df.isna().sum()) # Percentage of missing values print(df.isna().sum() / len(df) * 100) # Rows with any missing value rows_with_na = df[df.isna().any(axis=1)] print(f\u0026#34;Rows with missing data: {len(rows_with_na)}\u0026#34;) # Visualize missing data import matplotlib.pyplot as plt missing_data = df.isna().sum() missing_data = missing_data[missing_data \u0026gt; 0] if len(missing_data) \u0026gt; 0: plt.figure(figsize=(10, 5)) missing_data.plot(kind=\u0026#39;bar\u0026#39;) plt.title(\u0026#39;Missing Values by Column\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.xticks(rotation=45) plt.tight_layout() plt.show() Handling Missing Values Option 1: Drop missing values\n# Drop rows with any missing value df_clean = df.dropna() # Drop rows where specific column is missing df_clean = df.dropna(subset=[\u0026#39;region\u0026#39;]) # Drop columns with missing values df_clean = df.dropna(axis=1) # Drop rows with missing values in multiple columns df_clean = df.dropna(subset=[\u0026#39;discount\u0026#39;, \u0026#39;region\u0026#39;]) # Keep rows with at least N non-null values df_clean = df.dropna(thresh=10) # Keep rows with at least 10 non-null values Option 2: Fill missing values\n# Fill with specific value df[\u0026#39;discount\u0026#39;] = df[\u0026#39;discount\u0026#39;].fillna(0) df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].fillna(\u0026#39;Unknown\u0026#39;) # Fill with mean, median, or mode df[\u0026#39;discount\u0026#39;] = df[\u0026#39;discount\u0026#39;].fillna(df[\u0026#39;discount\u0026#39;].mean()) df[\u0026#39;unit_price\u0026#39;] = df[\u0026#39;unit_price\u0026#39;].fillna(df[\u0026#39;unit_price\u0026#39;].median()) df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].fillna(df[\u0026#39;region\u0026#39;].mode()[0]) # Most common value # Forward fill (use previous value) df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].fillna(method=\u0026#39;ffill\u0026#39;) # Backward fill (use next value) df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].fillna(method=\u0026#39;bfill\u0026#39;) # Fill different columns with different values df = df.fillna({ \u0026#39;discount\u0026#39;: 0, \u0026#39;region\u0026#39;: \u0026#39;Unknown\u0026#39;, \u0026#39;payment_method\u0026#39;: \u0026#39;Credit Card\u0026#39; }) Option 3: Interpolate (estimate from surrounding values)\n# Linear interpolation (good for time series) df[\u0026#39;unit_price\u0026#39;] = df[\u0026#39;unit_price\u0026#39;].interpolate(method=\u0026#39;linear\u0026#39;) # Polynomial interpolation df[\u0026#39;unit_price\u0026#39;] = df[\u0026#39;unit_price\u0026#39;].interpolate(method=\u0026#39;polynomial\u0026#39;, order=2) # Time-based interpolation df = df.sort_values(\u0026#39;order_date\u0026#39;) df[\u0026#39;total_amount\u0026#39;] = df[\u0026#39;total_amount\u0026#39;].interpolate(method=\u0026#39;time\u0026#39;) Practical approach for our sales data:\n# Discount: missing means no discount df[\u0026#39;discount\u0026#39;] = df[\u0026#39;discount\u0026#39;].fillna(0) # Region: if missing, fill with most common region for that product for product in df[\u0026#39;product\u0026#39;].unique(): most_common_region = df[df[\u0026#39;product\u0026#39;] == product][\u0026#39;region\u0026#39;].mode() if len(most_common_region) \u0026gt; 0: df.loc[(df[\u0026#39;product\u0026#39;] == product) \u0026amp; (df[\u0026#39;region\u0026#39;].isna()), \u0026#39;region\u0026#39;] = most_common_region[0] # Any remaining missing regions: fill with \u0026#39;Unknown\u0026#39; df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].fillna(\u0026#39;Unknown\u0026#39;) # Verify no missing values print(\u0026#34;\\nMissing values after cleaning:\u0026#34;) print(df.isna().sum()) Removing Duplicates # Remove duplicate rows (keep first occurrence) df_clean = df.drop_duplicates() # Remove duplicates based on specific columns df_clean = df.drop_duplicates(subset=[\u0026#39;order_id\u0026#39;]) # Keep last occurrence instead of first df_clean = df.drop_duplicates(subset=[\u0026#39;order_id\u0026#39;], keep=\u0026#39;last\u0026#39;) # Mark all duplicates (including first occurrence) as True all_duplicates = df.duplicated(subset=[\u0026#39;order_id\u0026#39;], keep=False) print(df[all_duplicates]) Handling Invalid Data # Remove negative quantities (invalid) df = df[df[\u0026#39;quantity\u0026#39;] \u0026gt; 0] # Remove zero or negative prices df = df[df[\u0026#39;unit_price\u0026#39;] \u0026gt; 0] # Cap outliers (values beyond reasonable range) # Example: cap prices at 99th percentile upper_limit = df[\u0026#39;unit_price\u0026#39;].quantile(0.99) df.loc[df[\u0026#39;unit_price\u0026#39;] \u0026gt; upper_limit, \u0026#39;unit_price\u0026#39;] = upper_limit # Replace invalid strings df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].replace({\u0026#39;N/A\u0026#39;: \u0026#39;Unknown\u0026#39;, \u0026#39;null\u0026#39;: \u0026#39;Unknown\u0026#39;, \u0026#39;\u0026#39;: \u0026#39;Unknown\u0026#39;}) # Strip whitespace from strings df[\u0026#39;product\u0026#39;] = df[\u0026#39;product\u0026#39;].str.strip() df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].str.strip() # Convert to title case (consistent formatting) df[\u0026#39;product\u0026#39;] = df[\u0026#39;product\u0026#39;].str.title() df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].str.title() After cleaning, your data is ready for serious analysis.\nData Transformation and Manipulation Now let\u0026rsquo;s reshape and transform data to extract insights.\nAdding and Modifying Columns # Add new column (calculated) df[\u0026#39;revenue\u0026#39;] = df[\u0026#39;quantity\u0026#39;] * df[\u0026#39;unit_price\u0026#39;] # Add column based on condition df[\u0026#39;order_size\u0026#39;] = df[\u0026#39;quantity\u0026#39;].apply(lambda x: \u0026#39;Bulk\u0026#39; if x \u0026gt;= 5 else \u0026#39;Regular\u0026#39;) # Or using np.where (faster for large datasets) df[\u0026#39;order_size\u0026#39;] = np.where(df[\u0026#39;quantity\u0026#39;] \u0026gt;= 5, \u0026#39;Bulk\u0026#39;, \u0026#39;Regular\u0026#39;) # Multiple conditions with np.select conditions = [ df[\u0026#39;total_amount\u0026#39;] \u0026lt; 100, (df[\u0026#39;total_amount\u0026#39;] \u0026gt;= 100) \u0026amp; (df[\u0026#39;total_amount\u0026#39;] \u0026lt; 500), df[\u0026#39;total_amount\u0026#39;] \u0026gt;= 500 ] choices = [\u0026#39;Low\u0026#39;, \u0026#39;Medium\u0026#39;, \u0026#39;High\u0026#39;] df[\u0026#39;value_segment\u0026#39;] = np.select(conditions, choices, default=\u0026#39;Unknown\u0026#39;) # Modify existing column df[\u0026#39;discount\u0026#39;] = df[\u0026#39;discount\u0026#39;].round(0) # Round to integer df[\u0026#39;product\u0026#39;] = df[\u0026#39;product\u0026#39;].str.upper() # Convert to uppercase # Create date-based columns df[\u0026#39;order_year\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.year df[\u0026#39;order_month\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.month df[\u0026#39;order_day\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.day df[\u0026#39;order_weekday\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.day_name() df[\u0026#39;order_quarter\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.quarter # Extract hour from datetime (if you had timestamps) # df[\u0026#39;order_hour\u0026#39;] = df[\u0026#39;order_datetime\u0026#39;].dt.hour Renaming Columns # Rename specific columns df = df.rename(columns={ \u0026#39;unit_price\u0026#39;: \u0026#39;price_per_unit\u0026#39;, \u0026#39;total_amount\u0026#39;: \u0026#39;order_total\u0026#39; }) # Rename all columns (lowercase, replace spaces with underscores) df.columns = df.columns.str.lower().str.replace(\u0026#39; \u0026#39;, \u0026#39;_\u0026#39;) # Rename using function df = df.rename(columns=lambda x: x.replace(\u0026#39;_\u0026#39;, \u0026#39; \u0026#39;).title()) Sorting Data # Sort by single column df_sorted = df.sort_values(\u0026#39;total_amount\u0026#39;, ascending=False) # Sort by multiple columns df_sorted = df.sort_values([\u0026#39;region\u0026#39;, \u0026#39;total_amount\u0026#39;], ascending=[True, False]) # Sort by index df_sorted = df.sort_index() # Sort in place (modify original DataFrame) df.sort_values(\u0026#39;order_date\u0026#39;, inplace=True) Binning and Discretization Converting continuous values to categories.\n# Create age bins for numerical data # Example: categorize order amounts bins = [0, 100, 500, 1000, float(\u0026#39;inf\u0026#39;)] labels = [\u0026#39;Low\u0026#39;, \u0026#39;Medium\u0026#39;, \u0026#39;High\u0026#39;, \u0026#39;Premium\u0026#39;] df[\u0026#39;amount_category\u0026#39;] = pd.cut(df[\u0026#39;total_amount\u0026#39;], bins=bins, labels=labels) # Equal-width bins (automatic) df[\u0026#39;price_bin\u0026#39;] = pd.cut(df[\u0026#39;unit_price\u0026#39;], bins=5) # 5 equal bins # Equal-frequency bins (quantiles) df[\u0026#39;price_quartile\u0026#39;] = pd.qcut(df[\u0026#39;unit_price\u0026#39;], q=4, labels=[\u0026#39;Q1\u0026#39;, \u0026#39;Q2\u0026#39;, \u0026#39;Q3\u0026#39;, \u0026#39;Q4\u0026#39;]) print(df[[\u0026#39;unit_price\u0026#39;, \u0026#39;price_quartile\u0026#39;]].head(10)) Applying Functions # Apply function to column def categorize_discount(discount): if discount == 0: return \u0026#39;No Discount\u0026#39; elif discount \u0026lt;= 10: return \u0026#39;Small Discount\u0026#39; else: return \u0026#39;Large Discount\u0026#39; df[\u0026#39;discount_category\u0026#39;] = df[\u0026#39;discount\u0026#39;].apply(categorize_discount) # Apply lambda function df[\u0026#39;price_rounded\u0026#39;] = df[\u0026#39;unit_price\u0026#39;].apply(lambda x: round(x, 0)) # Apply to multiple columns df[\u0026#39;total_items\u0026#39;] = df.apply(lambda row: row[\u0026#39;quantity\u0026#39;] * 1, axis=1) # Apply to entire DataFrame df_normalized = df[[\u0026#39;quantity\u0026#39;, \u0026#39;unit_price\u0026#39;, \u0026#39;total_amount\u0026#39;]].apply(lambda x: (x - x.min()) / (x.max() - x.min())) Replacing Values # Replace specific value df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].replace(\u0026#39;Unknown\u0026#39;, \u0026#39;Not Specified\u0026#39;) # Replace multiple values df[\u0026#39;payment_method\u0026#39;] = df[\u0026#39;payment_method\u0026#39;].replace({ \u0026#39;Credit Card\u0026#39;: \u0026#39;CC\u0026#39;, \u0026#39;Bank Transfer\u0026#39;: \u0026#39;BT\u0026#39;, \u0026#39;PayPal\u0026#39;: \u0026#39;PP\u0026#39; }) # Replace using regex df[\u0026#39;product\u0026#39;] = df[\u0026#39;product\u0026#39;].str.replace(r\u0026#39;\\s+\u0026#39;, \u0026#39;_\u0026#39;, regex=True) # Replace spaces with underscores Grouping and Aggregating Data This is where you extract real insights. Think of this as \u0026ldquo;pivot tables on steroids.\u0026rdquo;\nBasic Grouping # Group by single column and calculate mean avg_by_product = df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;total_amount\u0026#39;].mean() print(avg_by_product) # Group and apply multiple aggregations product_stats = df.groupby(\u0026#39;product\u0026#39;).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;], \u0026#39;quantity\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;order_id\u0026#39;: \u0026#39;count\u0026#39; }) print(product_stats) # Reset index to make groups into columns product_stats = product_stats.reset_index() print(product_stats) Multiple Group By # Group by multiple columns region_product_sales = df.groupby([\u0026#39;region\u0026#39;, \u0026#39;product\u0026#39;])[\u0026#39;total_amount\u0026#39;].sum() print(region_product_sales) # Unstack to create pivot-like table region_product_pivot = region_product_sales.unstack(fill_value=0) print(region_product_pivot) # Group by multiple columns with multiple aggregations summary = df.groupby([\u0026#39;region\u0026#39;, \u0026#39;product\u0026#39;]).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;], \u0026#39;quantity\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;order_id\u0026#39;: \u0026#39;count\u0026#39; }).round(2) # Flatten multi-level columns summary.columns = [\u0026#39;_\u0026#39;.join(col).strip() for col in summary.columns.values] summary = summary.reset_index() print(summary) Advanced Aggregations # Custom aggregation function def revenue_range(x): return x.max() - x.min() product_metrics = df.groupby(\u0026#39;product\u0026#39;).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;min\u0026#39;, \u0026#39;max\u0026#39;, revenue_range], \u0026#39;quantity\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;], \u0026#39;discount\u0026#39;: \u0026#39;mean\u0026#39; }) # Named aggregations (cleaner syntax) product_summary = df.groupby(\u0026#39;product\u0026#39;).agg( total_revenue=(\u0026#39;total_amount\u0026#39;, \u0026#39;sum\u0026#39;), avg_order_value=(\u0026#39;total_amount\u0026#39;, \u0026#39;mean\u0026#39;), total_quantity=(\u0026#39;quantity\u0026#39;, \u0026#39;sum\u0026#39;), num_orders=(\u0026#39;order_id\u0026#39;, \u0026#39;count\u0026#39;), avg_discount=(\u0026#39;discount\u0026#39;, \u0026#39;mean\u0026#39;) ).round(2) print(product_summary.sort_values(\u0026#39;total_revenue\u0026#39;, ascending=False)) Time-Based Grouping # Set date as index df_time = df.set_index(\u0026#39;order_date\u0026#39;).sort_index() # Resample by day, week, month daily_sales = df_time[\u0026#39;total_amount\u0026#39;].resample(\u0026#39;D\u0026#39;).sum() weekly_sales = df_time[\u0026#39;total_amount\u0026#39;].resample(\u0026#39;W\u0026#39;).sum() monthly_sales = df_time[\u0026#39;total_amount\u0026#39;].resample(\u0026#39;M\u0026#39;).sum() print(\u0026#34;Monthly sales:\u0026#34;) print(monthly_sales) # Multiple aggregations with resampling monthly_summary = df_time.resample(\u0026#39;M\u0026#39;).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;], \u0026#39;quantity\u0026#39;: \u0026#39;sum\u0026#39; }) print(monthly_summary) # Group by month and year separately df[\u0026#39;year_month\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.to_period(\u0026#39;M\u0026#39;) monthly_by_product = df.groupby([\u0026#39;year_month\u0026#39;, \u0026#39;product\u0026#39;])[\u0026#39;total_amount\u0026#39;].sum().unstack() print(monthly_by_product) Pivot Tables Pandas pivot tables work like Excel pivot tables but with more features.\n# Basic pivot table pivot = df.pivot_table( values=\u0026#39;total_amount\u0026#39;, index=\u0026#39;product\u0026#39;, columns=\u0026#39;region\u0026#39;, aggfunc=\u0026#39;sum\u0026#39;, fill_value=0 ) print(pivot) # Multiple aggregations pivot_multi = df.pivot_table( values=\u0026#39;total_amount\u0026#39;, index=\u0026#39;product\u0026#39;, columns=\u0026#39;region\u0026#39;, aggfunc=[\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;], fill_value=0 ) print(pivot_multi) # Multiple values pivot_advanced = df.pivot_table( values=[\u0026#39;total_amount\u0026#39;, \u0026#39;quantity\u0026#39;], index=\u0026#39;product\u0026#39;, columns=\u0026#39;region\u0026#39;, aggfunc=\u0026#39;sum\u0026#39;, fill_value=0, margins=True, # Add totals margins_name=\u0026#39;Total\u0026#39; ) print(pivot_advanced) Filtering After Grouping # Products with total revenue \u0026gt; $10,000 high_revenue_products = df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum() high_revenue_products = high_revenue_products[high_revenue_products \u0026gt; 10000] print(\u0026#34;High revenue products:\u0026#34;) print(high_revenue_products) # Regions with average order value \u0026gt; $200 high_value_regions = df.groupby(\u0026#39;region\u0026#39;)[\u0026#39;total_amount\u0026#39;].mean() high_value_regions = high_value_regions[high_value_regions \u0026gt; 200] print(\u0026#34;\\nHigh value regions:\u0026#34;) print(high_value_regions) # Using filter method # Keep only products with more than 50 orders popular_products = df.groupby(\u0026#39;product\u0026#39;).filter(lambda x: len(x) \u0026gt; 50) print(f\u0026#34;\\nPopular products (\u0026gt;50 orders): {popular_products[\u0026#39;product\u0026#39;].nunique()}\u0026#34;) Merging and Combining DataFrames Real-world analysis often requires combining data from multiple sources.\nCreating Sample DataFrames # Customer data customers = pd.DataFrame({ \u0026#39;customer_id\u0026#39;: [1001, 1002, 1003, 1004, 1005], \u0026#39;customer_name\u0026#39;: [\u0026#39;Alice Johnson\u0026#39;, \u0026#39;Bob Smith\u0026#39;, \u0026#39;Carol White\u0026#39;, \u0026#39;David Brown\u0026#39;, \u0026#39;Eve Davis\u0026#39;], \u0026#39;customer_segment\u0026#39;: [\u0026#39;Premium\u0026#39;, \u0026#39;Regular\u0026#39;, \u0026#39;Premium\u0026#39;, \u0026#39;Regular\u0026#39;, \u0026#39;VIP\u0026#39;], \u0026#39;signup_date\u0026#39;: pd.to_datetime([\u0026#39;2023-01-15\u0026#39;, \u0026#39;2023-03-20\u0026#39;, \u0026#39;2023-02-10\u0026#39;, \u0026#39;2023-04-05\u0026#39;, \u0026#39;2023-01-30\u0026#39;]) }) # Product catalog products = pd.DataFrame({ \u0026#39;product\u0026#39;: [\u0026#39;Laptop\u0026#39;, \u0026#39;Mouse\u0026#39;, \u0026#39;Keyboard\u0026#39;, \u0026#39;Monitor\u0026#39;, \u0026#39;Headphones\u0026#39;], \u0026#39;cost_price\u0026#39;: [650.00, 15.00, 45.00, 180.00, 50.00], \u0026#39;supplier\u0026#39;: [\u0026#39;Tech Corp\u0026#39;, \u0026#39;Accessories Inc\u0026#39;, \u0026#39;Accessories Inc\u0026#39;, \u0026#39;Tech Corp\u0026#39;, \u0026#39;Audio Ltd\u0026#39;] }) Merge (SQL-style Joins) # Inner join (only matching rows) df_with_customers = df.merge(customers, on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;inner\u0026#39;) # Left join (keep all from left DataFrame) df_with_customers = df.merge(customers, on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;left\u0026#39;) # Right join (keep all from right DataFrame) df_with_customers = df.merge(customers, on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;right\u0026#39;) # Outer join (keep all rows from both) df_with_customers = df.merge(customers, on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;outer\u0026#39;) # Merge with different column names # If left has \u0026#39;cust_id\u0026#39; and right has \u0026#39;customer_id\u0026#39;: # df.merge(customers, left_on=\u0026#39;cust_id\u0026#39;, right_on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;left\u0026#39;) # Merge on multiple columns # df.merge(other_df, on=[\u0026#39;customer_id\u0026#39;, \u0026#39;product\u0026#39;], how=\u0026#39;inner\u0026#39;) # Example: Add customer names to sales data df_enriched = df.merge(customers[[\u0026#39;customer_id\u0026#39;, \u0026#39;customer_name\u0026#39;, \u0026#39;customer_segment\u0026#39;]], on=\u0026#39;customer_id\u0026#39;, how=\u0026#39;left\u0026#39;) # Add product cost prices df_enriched = df_enriched.merge(products[[\u0026#39;product\u0026#39;, \u0026#39;cost_price\u0026#39;]], on=\u0026#39;product\u0026#39;, how=\u0026#39;left\u0026#39;) # Calculate profit margin df_enriched[\u0026#39;profit\u0026#39;] = df_enriched[\u0026#39;total_amount\u0026#39;] - (df_enriched[\u0026#39;cost_price\u0026#39;] * df_enriched[\u0026#39;quantity\u0026#39;]) df_enriched[\u0026#39;profit_margin\u0026#39;] = (df_enriched[\u0026#39;profit\u0026#39;] / df_enriched[\u0026#39;total_amount\u0026#39;] * 100).round(2) print(df_enriched[[\u0026#39;order_id\u0026#39;, \u0026#39;customer_name\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;total_amount\u0026#39;, \u0026#39;profit\u0026#39;, \u0026#39;profit_margin\u0026#39;]].head(10)) Concatenating DataFrames # Vertically stack DataFrames (add rows) df_jan = df[df[\u0026#39;order_date\u0026#39;].dt.month == 1] df_feb = df[df[\u0026#39;order_date\u0026#39;].dt.month == 2] df_combined = pd.concat([df_jan, df_feb], ignore_index=True) # Horizontally stack DataFrames (add columns) df_part1 = df[[\u0026#39;order_id\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;quantity\u0026#39;]] df_part2 = df[[\u0026#39;unit_price\u0026#39;, \u0026#39;total_amount\u0026#39;]] df_combined = pd.concat([df_part1, df_part2], axis=1) # Concat multiple DataFrames dfs = [] for month in range(1, 13): df_month = df[df[\u0026#39;order_date\u0026#39;].dt.month == month] dfs.append(df_month) df_full_year = pd.concat(dfs, ignore_index=True) Data Visualization with Pandas Pandas has built-in plotting that uses Matplotlib under the hood.\nimport matplotlib.pyplot as plt # Set style for better-looking plots plt.style.use(\u0026#39;ggplot\u0026#39;) # Basic line plot df.groupby(\u0026#39;order_date\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().plot(figsize=(12, 5)) plt.title(\u0026#39;Daily Sales Revenue\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Revenue ($)\u0026#39;) plt.tight_layout() plt.show() # Bar plot - Revenue by product product_revenue = df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().sort_values(ascending=False) product_revenue.plot(kind=\u0026#39;bar\u0026#39;, figsize=(10, 5), color=\u0026#39;steelblue\u0026#39;) plt.title(\u0026#39;Total Revenue by Product\u0026#39;) plt.xlabel(\u0026#39;Product\u0026#39;) plt.ylabel(\u0026#39;Revenue ($)\u0026#39;) plt.xticks(rotation=45) plt.tight_layout() plt.show() # Horizontal bar plot product_revenue.plot(kind=\u0026#39;barh\u0026#39;, figsize=(10, 6), color=\u0026#39;coral\u0026#39;) plt.title(\u0026#39;Total Revenue by Product\u0026#39;) plt.xlabel(\u0026#39;Revenue ($)\u0026#39;) plt.tight_layout() plt.show() # Histogram - Distribution of order amounts df[\u0026#39;total_amount\u0026#39;].plot(kind=\u0026#39;hist\u0026#39;, bins=30, figsize=(10, 5), edgecolor=\u0026#39;black\u0026#39;) plt.title(\u0026#39;Distribution of Order Amounts\u0026#39;) plt.xlabel(\u0026#39;Order Amount ($)\u0026#39;) plt.ylabel(\u0026#39;Frequency\u0026#39;) plt.tight_layout() plt.show() # Box plot - Detect outliers df.boxplot(column=\u0026#39;total_amount\u0026#39;, by=\u0026#39;region\u0026#39;, figsize=(10, 6)) plt.title(\u0026#39;Order Amount Distribution by Region\u0026#39;) plt.suptitle(\u0026#39;\u0026#39;) # Remove default title plt.xlabel(\u0026#39;Region\u0026#39;) plt.ylabel(\u0026#39;Order Amount ($)\u0026#39;) plt.tight_layout() plt.show() # Scatter plot - Quantity vs Total Amount df.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;quantity\u0026#39;, y=\u0026#39;total_amount\u0026#39;, figsize=(10, 6), alpha=0.5) plt.title(\u0026#39;Quantity vs Total Amount\u0026#39;) plt.xlabel(\u0026#39;Quantity\u0026#39;) plt.ylabel(\u0026#39;Total Amount ($)\u0026#39;) plt.tight_layout() plt.show() # Multiple plots in subplots fig, axes = plt.subplots(2, 2, figsize=(14, 10)) # Plot 1: Daily sales df.groupby(\u0026#39;order_date\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().plot(ax=axes[0, 0], color=\u0026#39;green\u0026#39;) axes[0, 0].set_title(\u0026#39;Daily Sales\u0026#39;) axes[0, 0].set_xlabel(\u0026#39;Date\u0026#39;) axes[0, 0].set_ylabel(\u0026#39;Revenue ($)\u0026#39;) # Plot 2: Sales by region df.groupby(\u0026#39;region\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().plot(kind=\u0026#39;bar\u0026#39;, ax=axes[0, 1], color=\u0026#39;orange\u0026#39;) axes[0, 1].set_title(\u0026#39;Sales by Region\u0026#39;) axes[0, 1].set_xlabel(\u0026#39;Region\u0026#39;) axes[0, 1].set_ylabel(\u0026#39;Revenue ($)\u0026#39;) # Plot 3: Distribution of quantities df[\u0026#39;quantity\u0026#39;].plot(kind=\u0026#39;hist\u0026#39;, bins=20, ax=axes[1, 0], edgecolor=\u0026#39;black\u0026#39;, color=\u0026#39;purple\u0026#39;) axes[1, 0].set_title(\u0026#39;Quantity Distribution\u0026#39;) axes[1, 0].set_xlabel(\u0026#39;Quantity\u0026#39;) axes[1, 0].set_ylabel(\u0026#39;Frequency\u0026#39;) # Plot 4: Payment method distribution df[\u0026#39;payment_method\u0026#39;].value_counts().plot(kind=\u0026#39;pie\u0026#39;, ax=axes[1, 1], autopct=\u0026#39;%1.1f%%\u0026#39;) axes[1, 1].set_title(\u0026#39;Payment Method Distribution\u0026#39;) axes[1, 1].set_ylabel(\u0026#39;\u0026#39;) plt.tight_layout() plt.show() Using Seaborn for Advanced Visualizations import seaborn as sns # Set Seaborn style sns.set_style(\u0026#39;whitegrid\u0026#39;) # Correlation heatmap numeric_cols = df.select_dtypes(include=[np.number]).columns corr_matrix = df[numeric_cols].corr() plt.figure(figsize=(10, 8)) sns.heatmap(corr_matrix, annot=True, cmap=\u0026#39;coolwarm\u0026#39;, center=0, fmt=\u0026#39;.2f\u0026#39;) plt.title(\u0026#39;Correlation Matrix\u0026#39;) plt.tight_layout() plt.show() # Count plot - Orders by region plt.figure(figsize=(10, 5)) sns.countplot(data=df, x=\u0026#39;region\u0026#39;, palette=\u0026#39;Set2\u0026#39;) plt.title(\u0026#39;Number of Orders by Region\u0026#39;) plt.xlabel(\u0026#39;Region\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.tight_layout() plt.show() # Box plot - Amount by payment method plt.figure(figsize=(12, 6)) sns.boxplot(data=df, x=\u0026#39;payment_method\u0026#39;, y=\u0026#39;total_amount\u0026#39;, palette=\u0026#39;Set3\u0026#39;) plt.title(\u0026#39;Order Amount by Payment Method\u0026#39;) plt.xlabel(\u0026#39;Payment Method\u0026#39;) plt.ylabel(\u0026#39;Order Amount ($)\u0026#39;) plt.tight_layout() plt.show() # Violin plot (combines box plot and distribution) plt.figure(figsize=(12, 6)) sns.violinplot(data=df, x=\u0026#39;region\u0026#39;, y=\u0026#39;total_amount\u0026#39;, palette=\u0026#39;muted\u0026#39;) plt.title(\u0026#39;Order Amount Distribution by Region\u0026#39;) plt.xlabel(\u0026#39;Region\u0026#39;) plt.ylabel(\u0026#39;Order Amount ($)\u0026#39;) plt.tight_layout() plt.show() # Pair plot (scatterplot matrix) sample_df = df[[\u0026#39;quantity\u0026#39;, \u0026#39;unit_price\u0026#39;, \u0026#39;discount\u0026#39;, \u0026#39;total_amount\u0026#39;]].sample(200) sns.pairplot(sample_df, diag_kind=\u0026#39;kde\u0026#39;) plt.tight_layout() plt.show() # Line plot with confidence interval daily_sales = df.groupby(\u0026#39;order_date\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().reset_index() plt.figure(figsize=(14, 6)) sns.lineplot(data=daily_sales, x=\u0026#39;order_date\u0026#39;, y=\u0026#39;total_amount\u0026#39;, color=\u0026#39;darkblue\u0026#39;) plt.title(\u0026#39;Daily Sales Trend\u0026#39;) plt.xlabel(\u0026#39;Date\u0026#39;) plt.ylabel(\u0026#39;Revenue ($)\u0026#39;) plt.xticks(rotation=45) plt.tight_layout() plt.show() Real-World Project: Sales Data Analysis Let\u0026rsquo;s put everything together with a complete analysis workflow.\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns # Load data df = pd.read_csv(\u0026#39;sales_data.csv\u0026#39;, parse_dates=[\u0026#39;order_date\u0026#39;]) print(\u0026#34;=\u0026#34;*50) print(\u0026#34;SALES DATA ANALYSIS REPORT\u0026#34;) print(\u0026#34;=\u0026#34;*50) # 1. DATA OVERVIEW print(\u0026#34;\\n1. DATA OVERVIEW\u0026#34;) print(f\u0026#34;Total orders: {len(df):,}\u0026#34;) print(f\u0026#34;Date range: {df[\u0026#39;order_date\u0026#39;].min().date()} to {df[\u0026#39;order_date\u0026#39;].max().date()}\u0026#34;) print(f\u0026#34;Total revenue: ${df[\u0026#39;total_amount\u0026#39;].sum():,.2f}\u0026#34;) print(f\u0026#34;Average order value: ${df[\u0026#39;total_amount\u0026#39;].mean():.2f}\u0026#34;) print(f\u0026#34;Unique customers: {df[\u0026#39;customer_id\u0026#39;].nunique()}\u0026#34;) print(f\u0026#34;Unique products: {df[\u0026#39;product\u0026#39;].nunique()}\u0026#34;) # 2. MISSING DATA CHECK print(\u0026#34;\\n2. DATA QUALITY\u0026#34;) missing_data = df.isna().sum() if missing_data.sum() == 0: print(\u0026#34;No missing values detected.\u0026#34;) else: print(\u0026#34;Missing values:\u0026#34;) print(missing_data[missing_data \u0026gt; 0]) # 3. REVENUE ANALYSIS print(\u0026#34;\\n3. REVENUE ANALYSIS\u0026#34;) # Total revenue by product product_revenue = df.groupby(\u0026#39;product\u0026#39;).agg({ \u0026#39;total_amount\u0026#39;: \u0026#39;sum\u0026#39;, \u0026#39;order_id\u0026#39;: \u0026#39;count\u0026#39; }).round(2) product_revenue.columns = [\u0026#39;Total Revenue\u0026#39;, \u0026#39;Number of Orders\u0026#39;] product_revenue[\u0026#39;Avg Order Value\u0026#39;] = (product_revenue[\u0026#39;Total Revenue\u0026#39;] / product_revenue[\u0026#39;Number of Orders\u0026#39;]).round(2) product_revenue = product_revenue.sort_values(\u0026#39;Total Revenue\u0026#39;, ascending=False) print(\u0026#34;\\nRevenue by Product:\u0026#34;) print(product_revenue) # Top selling products by quantity top_products = df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;quantity\u0026#39;].sum().sort_values(ascending=False) print(\u0026#34;\\nTop Selling Products (by quantity):\u0026#34;) print(top_products) # Revenue by region region_revenue = df.groupby(\u0026#39;region\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().sort_values(ascending=False) print(\u0026#34;\\nRevenue by Region:\u0026#34;) print(region_revenue) # 4. TIME-BASED ANALYSIS print(\u0026#34;\\n4. TIME-BASED TRENDS\u0026#34;) # Monthly revenue trend df[\u0026#39;year_month\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.to_period(\u0026#39;M\u0026#39;) monthly_revenue = df.groupby(\u0026#39;year_month\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum() print(\u0026#34;\\nMonthly Revenue:\u0026#34;) print(monthly_revenue) # Best performing month best_month = monthly_revenue.idxmax() print(f\u0026#34;\\nBest performing month: {best_month} (${monthly_revenue.max():,.2f})\u0026#34;) # Day of week analysis df[\u0026#39;weekday\u0026#39;] = df[\u0026#39;order_date\u0026#39;].dt.day_name() weekday_sales = df.groupby(\u0026#39;weekday\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum().reindex([ \u0026#39;Monday\u0026#39;, \u0026#39;Tuesday\u0026#39;, \u0026#39;Wednesday\u0026#39;, \u0026#39;Thursday\u0026#39;, \u0026#39;Friday\u0026#39;, \u0026#39;Saturday\u0026#39;, \u0026#39;Sunday\u0026#39; ]) print(\u0026#34;\\nRevenue by Day of Week:\u0026#34;) print(weekday_sales) # 5. CUSTOMER SEGMENTATION print(\u0026#34;\\n5. CUSTOMER INSIGHTS\u0026#34;) # Customer purchase frequency customer_orders = df.groupby(\u0026#39;customer_id\u0026#39;).agg({ \u0026#39;order_id\u0026#39;: \u0026#39;count\u0026#39;, \u0026#39;total_amount\u0026#39;: \u0026#39;sum\u0026#39; }).round(2) customer_orders.columns = [\u0026#39;Number of Orders\u0026#39;, \u0026#39;Total Spent\u0026#39;] customer_orders[\u0026#39;Avg Order Value\u0026#39;] = (customer_orders[\u0026#39;Total Spent\u0026#39;] / customer_orders[\u0026#39;Number of Orders\u0026#39;]).round(2) print(f\u0026#34;\\nCustomer Statistics:\u0026#34;) print(f\u0026#34;Average orders per customer: {customer_orders[\u0026#39;Number of Orders\u0026#39;].mean():.2f}\u0026#34;) print(f\u0026#34;Average customer lifetime value: ${customer_orders[\u0026#39;Total Spent\u0026#39;].mean():.2f}\u0026#34;) # Top 10 customers top_customers = customer_orders.sort_values(\u0026#39;Total Spent\u0026#39;, ascending=False).head(10) print(\u0026#34;\\nTop 10 Customers by Revenue:\u0026#34;) print(top_customers) # 6. DISCOUNT ANALYSIS print(\u0026#34;\\n6. DISCOUNT IMPACT\u0026#34;) # Revenue with vs without discount discount_comparison = df.groupby(df[\u0026#39;discount\u0026#39;] \u0026gt; 0).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;] }).round(2) discount_comparison.index = [\u0026#39;No Discount\u0026#39;, \u0026#39;With Discount\u0026#39;] print(\u0026#34;\\nDiscount Impact:\u0026#34;) print(discount_comparison) # Average discount by product avg_discount = df[df[\u0026#39;discount\u0026#39;] \u0026gt; 0].groupby(\u0026#39;product\u0026#39;)[\u0026#39;discount\u0026#39;].mean().sort_values(ascending=False) print(\u0026#34;\\nAverage Discount by Product:\u0026#34;) print(avg_discount) # 7. PAYMENT METHOD ANALYSIS print(\u0026#34;\\n7. PAYMENT METHOD PREFERENCES\u0026#34;) payment_stats = df.groupby(\u0026#39;payment_method\u0026#39;).agg({ \u0026#39;order_id\u0026#39;: \u0026#39;count\u0026#39;, \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;] }).round(2) payment_stats.columns = [\u0026#39;Order Count\u0026#39;, \u0026#39;Total Revenue\u0026#39;, \u0026#39;Avg Order Value\u0026#39;] print(payment_stats) # 8. KEY INSIGHTS SUMMARY print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50) print(\u0026#34;KEY INSIGHTS\u0026#34;) print(\u0026#34;=\u0026#34;*50) insights = [] # Insight 1: Best product best_product = product_revenue.index[0] best_product_revenue = product_revenue.iloc[0][\u0026#39;Total Revenue\u0026#39;] insights.append(f\u0026#34;1. Top product: {best_product} generated ${best_product_revenue:,.2f}\u0026#34;) # Insight 2: Best region best_region = region_revenue.index[0] best_region_revenue = region_revenue.iloc[0] insights.append(f\u0026#34;2. Top region: {best_region} with ${best_region_revenue:,.2f} in sales\u0026#34;) # Insight 3: Discount impact avg_discounted = df[df[\u0026#39;discount\u0026#39;] \u0026gt; 0][\u0026#39;total_amount\u0026#39;].mean() avg_regular = df[df[\u0026#39;discount\u0026#39;] == 0][\u0026#39;total_amount\u0026#39;].mean() discount_impact = ((avg_discounted - avg_regular) / avg_regular * 100) insights.append(f\u0026#34;3. Discounted orders are {abs(discount_impact):.1f}% {\u0026#39;higher\u0026#39; if discount_impact \u0026gt; 0 else \u0026#39;lower\u0026#39;} than regular orders\u0026#34;) # Insight 4: Best day best_day = weekday_sales.idxmax() insights.append(f\u0026#34;4. Best sales day: {best_day}\u0026#34;) # Insight 5: Payment preference preferred_payment = payment_stats[\u0026#39;Order Count\u0026#39;].idxmax() payment_pct = (payment_stats.loc[preferred_payment, \u0026#39;Order Count\u0026#39;] / len(df) * 100) insights.append(f\u0026#34;5. Preferred payment: {preferred_payment} ({payment_pct:.1f}% of orders)\u0026#34;) for insight in insights: print(insight) # 9. VISUALIZATIONS print(\u0026#34;\\n9. Generating visualizations...\u0026#34;) fig = plt.figure(figsize=(16, 12)) # Plot 1: Revenue by product ax1 = plt.subplot(3, 3, 1) product_revenue[\u0026#39;Total Revenue\u0026#39;].plot(kind=\u0026#39;bar\u0026#39;, color=\u0026#39;steelblue\u0026#39;, ax=ax1) ax1.set_title(\u0026#39;Revenue by Product\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax1.set_ylabel(\u0026#39;Revenue ($)\u0026#39;) ax1.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) # Plot 2: Orders by region ax2 = plt.subplot(3, 3, 2) region_revenue.plot(kind=\u0026#39;bar\u0026#39;, color=\u0026#39;coral\u0026#39;, ax=ax2) ax2.set_title(\u0026#39;Revenue by Region\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax2.set_ylabel(\u0026#39;Revenue ($)\u0026#39;) ax2.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) # Plot 3: Daily sales trend ax3 = plt.subplot(3, 3, 3) daily_sales = df.groupby(\u0026#39;order_date\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum() daily_sales.plot(ax=ax3, color=\u0026#39;green\u0026#39;, linewidth=2) ax3.set_title(\u0026#39;Daily Sales Trend\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax3.set_ylabel(\u0026#39;Revenue ($)\u0026#39;) ax3.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) # Plot 4: Order amount distribution ax4 = plt.subplot(3, 3, 4) df[\u0026#39;total_amount\u0026#39;].hist(bins=30, ax=ax4, edgecolor=\u0026#39;black\u0026#39;, color=\u0026#39;purple\u0026#39;) ax4.set_title(\u0026#39;Order Amount Distribution\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax4.set_xlabel(\u0026#39;Order Amount ($)\u0026#39;) ax4.set_ylabel(\u0026#39;Frequency\u0026#39;) # Plot 5: Quantity distribution ax5 = plt.subplot(3, 3, 5) df[\u0026#39;quantity\u0026#39;].value_counts().sort_index().plot(kind=\u0026#39;bar\u0026#39;, ax=ax5, color=\u0026#39;orange\u0026#39;) ax5.set_title(\u0026#39;Quantity Distribution\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax5.set_xlabel(\u0026#39;Quantity\u0026#39;) ax5.set_ylabel(\u0026#39;Count\u0026#39;) # Plot 6: Payment method distribution ax6 = plt.subplot(3, 3, 6) payment_counts = df[\u0026#39;payment_method\u0026#39;].value_counts() ax6.pie(payment_counts.values, labels=payment_counts.index, autopct=\u0026#39;%1.1f%%\u0026#39;, startangle=90) ax6.set_title(\u0026#39;Payment Method Distribution\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) # Plot 7: Sales by weekday ax7 = plt.subplot(3, 3, 7) weekday_sales.plot(kind=\u0026#39;bar\u0026#39;, ax=ax7, color=\u0026#39;teal\u0026#39;) ax7.set_title(\u0026#39;Sales by Day of Week\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax7.set_ylabel(\u0026#39;Revenue ($)\u0026#39;) ax7.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) # Plot 8: Monthly revenue trend ax8 = plt.subplot(3, 3, 8) monthly_revenue_values = monthly_revenue.values months = [str(m) for m in monthly_revenue.index] ax8.plot(months, monthly_revenue_values, marker=\u0026#39;o\u0026#39;, linewidth=2, markersize=8, color=\u0026#39;darkblue\u0026#39;) ax8.set_title(\u0026#39;Monthly Revenue Trend\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax8.set_ylabel(\u0026#39;Revenue ($)\u0026#39;) ax8.tick_params(axis=\u0026#39;x\u0026#39;, rotation=45) ax8.grid(True, alpha=0.3) # Plot 9: Top 10 customers ax9 = plt.subplot(3, 3, 9) top_10_customers = customer_orders.sort_values(\u0026#39;Total Spent\u0026#39;, ascending=False).head(10) top_10_customers[\u0026#39;Total Spent\u0026#39;].plot(kind=\u0026#39;barh\u0026#39;, ax=ax9, color=\u0026#39;darkgreen\u0026#39;) ax9.set_title(\u0026#39;Top 10 Customers\u0026#39;, fontweight=\u0026#39;bold\u0026#39;) ax9.set_xlabel(\u0026#39;Total Spent ($)\u0026#39;) plt.tight_layout() plt.savefig(\u0026#39;sales_analysis_report.png\u0026#39;, dpi=300, bbox_inches=\u0026#39;tight\u0026#39;) print(\u0026#34;Visualizations saved to \u0026#39;sales_analysis_report.png\u0026#39;\u0026#34;) plt.show() # 10. EXPORT RESULTS print(\u0026#34;\\n10. Exporting results...\u0026#34;) # Create Excel report with multiple sheets with pd.ExcelWriter(\u0026#39;sales_analysis_results.xlsx\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) as writer: # Summary sheet summary_data = { \u0026#39;Metric\u0026#39;: [ \u0026#39;Total Orders\u0026#39;, \u0026#39;Total Revenue\u0026#39;, \u0026#39;Average Order Value\u0026#39;, \u0026#39;Unique Customers\u0026#39;, \u0026#39;Unique Products\u0026#39;, \u0026#39;Date Range\u0026#39; ], \u0026#39;Value\u0026#39;: [ len(df), f\u0026#34;${df[\u0026#39;total_amount\u0026#39;].sum():,.2f}\u0026#34;, f\u0026#34;${df[\u0026#39;total_amount\u0026#39;].mean():.2f}\u0026#34;, df[\u0026#39;customer_id\u0026#39;].nunique(), df[\u0026#39;product\u0026#39;].nunique(), f\u0026#34;{df[\u0026#39;order_date\u0026#39;].min().date()} to {df[\u0026#39;order_date\u0026#39;].max().date()}\u0026#34; ] } pd.DataFrame(summary_data).to_excel(writer, sheet_name=\u0026#39;Summary\u0026#39;, index=False) # Product performance product_revenue.to_excel(writer, sheet_name=\u0026#39;Product Performance\u0026#39;) # Regional performance region_revenue.to_frame().to_excel(writer, sheet_name=\u0026#39;Regional Performance\u0026#39;) # Monthly trends monthly_revenue.to_frame().to_excel(writer, sheet_name=\u0026#39;Monthly Trends\u0026#39;) # Customer analysis customer_orders.sort_values(\u0026#39;Total Spent\u0026#39;, ascending=False).to_excel(writer, sheet_name=\u0026#39;Customer Analysis\u0026#39;) # Payment methods payment_stats.to_excel(writer, sheet_name=\u0026#39;Payment Methods\u0026#39;) print(\u0026#34;Results exported to \u0026#39;sales_analysis_results.xlsx\u0026#39;\u0026#34;) print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50) print(\u0026#34;ANALYSIS COMPLETE\u0026#34;) print(\u0026#34;=\u0026#34;*50) This complete analysis script:\nLoads and validates data Performs revenue analysis by product, region, and time Analyzes customer behavior Evaluates discount impact Studies payment preferences Generates key insights Creates visualizations Exports results to Excel Run this script on any sales dataset and get instant, professional insights.\nExporting Your Results After analysis, export results for reports or presentations.\nExport to CSV # Export full DataFrame df.to_csv(\u0026#39;cleaned_sales_data.csv\u0026#39;, index=False) # Export specific columns df[[\u0026#39;order_id\u0026#39;, \u0026#39;product\u0026#39;, \u0026#39;total_amount\u0026#39;]].to_csv(\u0026#39;sales_summary.csv\u0026#39;, index=False) # Export aggregated results product_summary = df.groupby(\u0026#39;product\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum() product_summary.to_csv(\u0026#39;product_revenue.csv\u0026#39;, header=[\u0026#39;Total Revenue\u0026#39;]) # Custom separator (semicolon for European Excel) df.to_csv(\u0026#39;sales_data_eu.csv\u0026#39;, sep=\u0026#39;;\u0026#39;, index=False, decimal=\u0026#39;,\u0026#39;) Export to Excel # Single sheet df.to_excel(\u0026#39;sales_report.xlsx\u0026#39;, sheet_name=\u0026#39;Sales Data\u0026#39;, index=False) # Multiple sheets with pd.ExcelWriter(\u0026#39;complete_report.xlsx\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) as writer: df.to_excel(writer, sheet_name=\u0026#39;Raw Data\u0026#39;, index=False) product_summary = df.groupby(\u0026#39;product\u0026#39;).agg({ \u0026#39;total_amount\u0026#39;: [\u0026#39;sum\u0026#39;, \u0026#39;mean\u0026#39;, \u0026#39;count\u0026#39;] }).round(2) product_summary.to_excel(writer, sheet_name=\u0026#39;Product Summary\u0026#39;) monthly_summary = df.groupby(df[\u0026#39;order_date\u0026#39;].dt.to_period(\u0026#39;M\u0026#39;))[\u0026#39;total_amount\u0026#39;].sum() monthly_summary.to_excel(writer, sheet_name=\u0026#39;Monthly Trends\u0026#39;) # Format Excel output with pd.ExcelWriter(\u0026#39;formatted_report.xlsx\u0026#39;, engine=\u0026#39;openpyxl\u0026#39;) as writer: df.to_excel(writer, sheet_name=\u0026#39;Sales\u0026#39;, index=False) # Access workbook and worksheet for formatting workbook = writer.book worksheet = writer.sheets[\u0026#39;Sales\u0026#39;] # Auto-adjust column widths for column in worksheet.columns: max_length = 0 column_letter = column[0].column_letter for cell in column: try: if len(str(cell.value)) \u0026gt; max_length: max_length = len(str(cell.value)) except: pass adjusted_width = (max_length + 2) worksheet.column_dimensions[column_letter].width = adjusted_width Export to JSON # Standard JSON df.to_json(\u0026#39;sales_data.json\u0026#39;, orient=\u0026#39;records\u0026#39;, indent=2) # Different orientations df.to_json(\u0026#39;sales_split.json\u0026#39;, orient=\u0026#39;split\u0026#39;) # {index, columns, data} df.to_json(\u0026#39;sales_index.json\u0026#39;, orient=\u0026#39;index\u0026#39;) # {index: {column: value}} df.to_json(\u0026#39;sales_columns.json\u0026#39;, orient=\u0026#39;columns\u0026#39;) # {column: {index: value}} df.to_json(\u0026#39;sales_values.json\u0026#39;, orient=\u0026#39;values\u0026#39;) # Just values array Export to HTML # Basic HTML table df.head(20).to_html(\u0026#39;sales_preview.html\u0026#39;, index=False) # Styled HTML html_string = \u0026#39;\u0026#39;\u0026#39; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; margin: 20px; }} h1 {{ color: #333; }} table {{ border-collapse: collapse; width: 100%; margin-top: 20px; }} th {{ background-color: #4CAF50; color: white; padding: 12px; text-align: left; }} td {{ border: 1px solid #ddd; padding: 8px; }} tr:nth-child(even) {{ background-color: #f2f2f2; }} tr:hover {{ background-color: #ddd; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Sales Analysis Report\u0026lt;/h1\u0026gt; {table} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#39;\u0026#39;\u0026#39; with open(\u0026#39;sales_report.html\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(html_string.format(table=df.head(50).to_html(index=False))) Performance Optimization Tips For large datasets, optimize Pandas operations.\nMemory Optimization # Check memory usage print(df.memory_usage(deep=True)) print(f\u0026#34;Total memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\u0026#34;) # Optimize data types # Before print(df.dtypes) # Convert to more efficient types df[\u0026#39;order_id\u0026#39;] = df[\u0026#39;order_id\u0026#39;].astype(\u0026#39;int32\u0026#39;) # Instead of int64 df[\u0026#39;quantity\u0026#39;] = df[\u0026#39;quantity\u0026#39;].astype(\u0026#39;int8\u0026#39;) # Instead of int64 df[\u0026#39;product\u0026#39;] = df[\u0026#39;product\u0026#39;].astype(\u0026#39;category\u0026#39;) # Instead of object df[\u0026#39;region\u0026#39;] = df[\u0026#39;region\u0026#39;].astype(\u0026#39;category\u0026#39;) df[\u0026#39;payment_method\u0026#39;] = df[\u0026#39;payment_method\u0026#39;].astype(\u0026#39;category\u0026#39;) # After print(df.memory_usage(deep=True)) print(f\u0026#34;Total memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\u0026#34;) # Memory savings Reading Large Files in Chunks # Read large CSV in chunks chunk_size = 10000 chunks = [] for chunk in pd.read_csv(\u0026#39;huge_dataset.csv\u0026#39;, chunksize=chunk_size): # Process each chunk chunk_processed = chunk[chunk[\u0026#39;total_amount\u0026#39;] \u0026gt; 100] chunks.append(chunk_processed) # Combine processed chunks df_final = pd.concat(chunks, ignore_index=True) # Or process chunks without storing in memory for chunk in pd.read_csv(\u0026#39;huge_dataset.csv\u0026#39;, chunksize=chunk_size): # Process and export immediately summary = chunk.groupby(\u0026#39;product\u0026#39;)[\u0026#39;total_amount\u0026#39;].sum() summary.to_csv(\u0026#39;output.csv\u0026#39;, mode=\u0026#39;a\u0026#39;, header=False) # Append mode Vectorization (Fast Operations) # Slow: Using loops result = [] for index, row in df.iterrows(): # NEVER DO THIS! result.append(row[\u0026#39;quantity\u0026#39;] * row[\u0026#39;unit_price\u0026#39;]) df[\u0026#39;revenue\u0026#39;] = result # Fast: Vectorized operations df[\u0026#39;revenue\u0026#39;] = df[\u0026#39;quantity\u0026#39;] * df[\u0026#39;unit_price\u0026#39;] # 100x faster! # Slow: Apply with complex function df[\u0026#39;category\u0026#39;] = df[\u0026#39;total_amount\u0026#39;].apply(lambda x: \u0026#39;High\u0026#39; if x \u0026gt; 500 else \u0026#39;Low\u0026#39;) # Fast: Use np.where or np.select df[\u0026#39;category\u0026#39;] = np.where(df[\u0026#39;total_amount\u0026#39;] \u0026gt; 500, \u0026#39;High\u0026#39;, \u0026#39;Low\u0026#39;) # Multiple conditions conditions = [ df[\u0026#39;total_amount\u0026#39;] \u0026lt; 100, (df[\u0026#39;total_amount\u0026#39;] \u0026gt;= 100) \u0026amp; (df[\u0026#39;total_amount\u0026#39;] \u0026lt; 500), df[\u0026#39;total_amount\u0026#39;] \u0026gt;= 500 ] choices = [\u0026#39;Low\u0026#39;, \u0026#39;Medium\u0026#39;, \u0026#39;High\u0026#39;] df[\u0026#39;category\u0026#39;] = np.select(conditions, choices) Use .loc and .iloc Instead of Chained Indexing # Bad (chained indexing - can cause warnings) df[df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;][\u0026#39;total_amount\u0026#39;] = df[df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;][\u0026#39;total_amount\u0026#39;] * 1.1 # Good (using .loc) df.loc[df[\u0026#39;product\u0026#39;] == \u0026#39;Laptop\u0026#39;, \u0026#39;total_amount\u0026#39;] *= 1.1 Next Steps: Continuing Your Pandas Journey You now have the core skills to analyze real-world datasets with Pandas. Here\u0026rsquo;s how to level up:\nPractice with real datasets:\nKaggle Datasets - thousands of free datasets Google Dataset Search - find datasets from anywhere UCI Machine Learning Repository - classic datasets Data.gov - US government open data Advanced Pandas topics:\nMulti-index DataFrames (hierarchical indexing) Time series analysis with advanced resampling Window functions (rolling, expanding, ewm) Custom aggregations and transformations Performance profiling and optimization Integration with SQL databases (SQLAlchemy) Related tools to learn:\nNumPy - numerical computing (Pandas is built on NumPy) Matplotlib/Seaborn - advanced data visualization Plotly - interactive visualizations Scikit-learn - machine learning (uses Pandas DataFrames) Statsmodels - statistical modeling Dask - parallel computing for datasets larger than RAM Automate your workflows: Combine Pandas with automation tools to build end-to-end data pipelines. Check out my guide on /2025/10/python-automation-scripts-every-developer-should-know.html to learn how to schedule Pandas scripts that run automatically, send email reports, and integrate with APIs.\nWeb scraping + data analysis: Collect your own datasets from websites and analyze them with Pandas. My tutorial /2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html shows you how to scrape data and export it to CSV for Pandas analysis.\nThe best way to master Pandas is to work on real projects. Find a dataset that interests you\u0026ndash;sales data from your company, personal finance records, public health statistics, sports data, anything\u0026ndash;and start asking questions. What patterns exist? What insights can you extract? What story does the data tell?\nEvery professional data analyst, data scientist, and business intelligence expert uses Pandas daily. You\u0026rsquo;re now equipped with the same tools.\nGo build something with data.\nFound this guide helpful? Check out my other Python tutorials:\n/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html /2025/10/python-automation-scripts-every-developer-should-know.html Questions or feedback? Drop a comment below, and I\u0026rsquo;ll help you out.\n","href":"/2025/10/how-to-analyze-data-python-pandas-from-zero-to-insights.html","title":"How to Analyze Data with Python Pandas: From Zero to Data Insights"},{"content":"I spent three weeks last month manually copying product prices from competitor websites into spreadsheets for a client\u0026rsquo;s market analysis. Three. Weeks. Every day, opening dozens of browser tabs, copying prices, checking specifications, pasting into Excel. My eyes hurt, my wrists hurt, and I kept making mistakes because humans aren\u0026rsquo;t meant to do repetitive tasks for hours.\nThen I learned web scraping. That same job that took three weeks? Now runs automatically in twenty minutes while I grab coffee. The data is cleaner, more accurate, and I can run it daily instead of monthly. Web scraping literally gave me my life back.\nIf you\u0026rsquo;ve ever found yourself manually copying data from websites, this tutorial is for you. We\u0026rsquo;re going to build real web scrapers from scratch using Python\u0026rsquo;s BeautifulSoup and Requests libraries\u0026ndash;no fluff, just practical code you can actually use. By the end, you\u0026rsquo;ll be automating data collection like a pro.\nOnce you\u0026rsquo;ve got web scraping down, you can level up by building APIs to serve your scraped data with FastAPI , or automate your scrapers to run on schedules using Linux cron jobs .\nWhy Web Scraping Is a Superpower You Need Look, I get it\u0026ndash;web scraping sounds technical and maybe a little intimidating. But here\u0026rsquo;s the thing: it\u0026rsquo;s just automating what you already do manually. When you visit a website and copy information, you\u0026rsquo;re reading HTML, finding the data you want, and recording it somewhere. Web scraping is teaching your computer to do exactly that, but thousands of times faster and without mistakes.\nThink about all the use cases: price monitoring for e-commerce, collecting job postings for market research, gathering news articles for sentiment analysis, tracking real estate listings, monitoring competitor products, aggregating reviews, building datasets for machine learning. The list goes on. Any time you need data from websites and the site doesn\u0026rsquo;t provide an API, web scraping is your answer.\nThe best part? BeautifulSoup and Requests make it genuinely easy. We\u0026rsquo;re not talking about complex browser automation or reverse-engineering APIs (though we\u0026rsquo;ll touch on that later). We\u0026rsquo;re talking about straightforward Python code that fetches HTML and extracts data. If you can write a for loop and understand basic HTML structure, you can build scrapers.\nPrerequisites and What You\u0026rsquo;ll Learn Before we dive in, here\u0026rsquo;s what you need:\nPython 3.8 or higher installed ( check our Python installation guides if you need help) Basic Python knowledge (variables, functions, lists, dictionaries) Understanding of HTML structure (tags, attributes, classes) - don\u0026rsquo;t worry, I\u0026rsquo;ll explain as we go A code editor (VS Code, PyCharm, or even a simple text editor) Internet connection for installing libraries and testing scrapers What we\u0026rsquo;ll build together:\nA simple scraper to extract article titles and links from a blog A product scraper that collects names, prices, and images from an e-commerce site A news aggregator that scrapes headlines from multiple sources Handling pagination to scrape multiple pages automatically Dealing with common anti-scraping measures (politely and ethically) Setting Up Your Scraping Environment Let\u0026rsquo;s get your environment ready. I always create a separate virtual environment for scraping projects\u0026ndash;it keeps dependencies isolated and prevents version conflicts.\nCreate a new project directory:\nmkdir web-scraping-tutorial cd web-scraping-tutorial Set up a virtual environment:\n# On macOS/Linux python3 -m venv venv source venv/bin/activate # On Windows python -m venv venv venv\\Scripts\\activate You should see (venv) at the beginning of your command prompt, indicating the virtual environment is active.\nInstall the required libraries:\npip install requests beautifulsoup4 lxml Here\u0026rsquo;s what each library does:\nrequests: Fetches web pages by sending HTTP requests (it\u0026rsquo;s like your browser, but for Python) beautifulsoup4: Parses HTML and lets you extract data using simple Python code lxml: Fast HTML parser that BeautifulSoup uses under the hood (faster than Python\u0026rsquo;s built-in parser) Optional but recommended:\npip install fake-useragent # Rotate user agents to avoid detection pip install pandas # For exporting scraped data to CSV/Excel Verify your installation:\nimport requests from bs4 import BeautifulSoup print(f\u0026#34;Requests version: {requests.__version__}\u0026#34;) print(f\u0026#34;BeautifulSoup imported successfully\u0026#34;) print(\u0026#34;Setup complete!\u0026#34;) If this runs without errors, you\u0026rsquo;re ready to start scraping.\nUnderstanding How Web Scraping Actually Works Before writing code, let\u0026rsquo;s understand the process at a high level. When you visit a website in your browser, here\u0026rsquo;s what happens:\nYour browser sends an HTTP request to the server The server responds with HTML, CSS, and JavaScript Your browser parses the HTML and renders it visually JavaScript might load additional content dynamically Web scraping replicates steps 1 and 2, but instead of rendering visually, we parse the HTML programmatically to extract data. Here\u0026rsquo;s the basic flow:\nYour Python Script ? Send HTTP Request (requests library) ? Receive HTML Response ? Parse HTML (BeautifulSoup) ? Extract Desired Data ? Store or Process Data The key insight: websites are just text files (HTML) with a structure. BeautifulSoup lets you navigate that structure like a tree of nested tags. Once you understand how to locate elements in HTML, extracting data is straightforward.\nQuick HTML refresher:\nHTML is made up of tags that nest inside each other:\n\u0026lt;div class=\u0026#34;product\u0026#34;\u0026gt; \u0026lt;h2 class=\u0026#34;product-title\u0026#34;\u0026gt;Laptop\u0026lt;/h2\u0026gt; \u0026lt;span class=\u0026#34;price\u0026#34;\u0026gt;$999\u0026lt;/span\u0026gt; \u0026lt;a href=\u0026#34;/product/123\u0026#34;\u0026gt;View Details\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; Each element has:\nTag name: div, h2, span, a Attributes: class=\u0026quot;product\u0026quot;, href=\u0026quot;/product/123\u0026quot; Text content: \u0026ldquo;Laptop\u0026rdquo;, \u0026ldquo;$999\u0026rdquo;, \u0026ldquo;View Details\u0026rdquo; BeautifulSoup lets you find elements by tag name, class, ID, or any attribute, then extract text or attributes.\nYour First Web Scraper: Fetching and Parsing HTML Let\u0026rsquo;s build the simplest possible scraper. We\u0026rsquo;ll fetch a web page and extract its title\u0026ndash;just to see the process end-to-end.\nCreate a file called first_scraper.py:\nimport requests from bs4 import BeautifulSoup def scrape_page_title(url): \u0026#34;\u0026#34;\u0026#34;Fetch a web page and extract its title.\u0026#34;\u0026#34;\u0026#34; # Send HTTP GET request response = requests.get(url) # Check if request was successful if response.status_code == 200: print(f\u0026#34;Successfully fetched {url}\u0026#34;) else: print(f\u0026#34;Failed to fetch page. Status code: {response.status_code}\u0026#34;) return None # Parse HTML content soup = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) # Extract the page title title = soup.find(\u0026#39;title\u0026#39;) if title: return title.get_text() else: return \u0026#34;No title found\u0026#34; # Test the scraper url = \u0026#34;https://www.buanacoding.com\u0026#34; page_title = scrape_page_title(url) print(f\u0026#34;Page Title: {page_title}\u0026#34;) Run this with python first_scraper.py. You should see the page title printed.\nLet\u0026rsquo;s break down what\u0026rsquo;s happening:\nrequests.get(url) sends an HTTP GET request and returns a Response object response.status_code tells us if the request succeeded (200 means success) response.content contains the raw HTML as bytes BeautifulSoup(response.content, 'lxml') parses the HTML into a navigable tree structure soup.find('title') searches for the first \u0026lt;title\u0026gt; tag title.get_text() extracts the text content inside the tag This is the foundation of every scraper: fetch HTML, parse it, extract data. Everything else is variations on this theme.\nExtracting Data: Finding Elements with BeautifulSoup BeautifulSoup provides multiple ways to find elements. Let\u0026rsquo;s explore the most useful methods:\nFind a single element:\n# Find by tag name h1 = soup.find(\u0026#39;h1\u0026#39;) # Find by class product = soup.find(\u0026#39;div\u0026#39;, class_=\u0026#39;product-card\u0026#39;) # Find by ID header = soup.find(id=\u0026#39;main-header\u0026#39;) # Find by multiple attributes link = soup.find(\u0026#39;a\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;btn\u0026#39;, \u0026#39;data-id\u0026#39;: \u0026#39;123\u0026#39;}) Find all matching elements:\n# Find all paragraphs paragraphs = soup.find_all(\u0026#39;p\u0026#39;) # Find all elements with a specific class products = soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;product\u0026#39;) # Find all links links = soup.find_all(\u0026#39;a\u0026#39;) # Limit results first_five_links = soup.find_all(\u0026#39;a\u0026#39;, limit=5) Extracting text and attributes:\nelement = soup.find(\u0026#39;a\u0026#39;, class_=\u0026#39;product-link\u0026#39;) # Get text content text = element.get_text() # Or: text = element.text # Get attribute value href = element.get(\u0026#39;href\u0026#39;) # Or: href = element[\u0026#39;href\u0026#39;] # Get all attributes as dictionary all_attrs = element.attrs Navigating the tree:\n# Get parent element parent = element.parent # Get all children children = element.children # Returns iterator child_list = list(element.children) # Get next sibling next_elem = element.next_sibling # Find within a parent (scoped search) product_div = soup.find(\u0026#39;div\u0026#39;, class_=\u0026#39;product\u0026#39;) price_within_product = product_div.find(\u0026#39;span\u0026#39;, class_=\u0026#39;price\u0026#39;) Building a Real Scraper: Blog Article Extractor Let\u0026rsquo;s build something actually useful: a scraper that extracts article titles, publication dates, and links from a blog.\nCreate blog_scraper.py:\nimport requests from bs4 import BeautifulSoup import time def scrape_blog_articles(url): \u0026#34;\u0026#34;\u0026#34;Scrape article information from a blog homepage.\u0026#34;\u0026#34;\u0026#34; # Set a custom user-agent to be polite headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#39; } try: response = requests.get(url, headers=headers, timeout=10) response.raise_for_status() # Raise exception for bad status codes except requests.exceptions.RequestException as e: print(f\u0026#34;Error fetching {url}: {e}\u0026#34;) return [] soup = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) # Find all article containers (adjust selectors for your target site) articles = soup.find_all(\u0026#39;article\u0026#39;, class_=\u0026#39;post\u0026#39;) scraped_data = [] for article in articles: # Extract title title_elem = article.find(\u0026#39;h2\u0026#39;, class_=\u0026#39;post-title\u0026#39;) title = title_elem.get_text(strip=True) if title_elem else \u0026#39;No title\u0026#39; # Extract link link_elem = article.find(\u0026#39;a\u0026#39;) link = link_elem.get(\u0026#39;href\u0026#39;) if link_elem else None # Extract date date_elem = article.find(\u0026#39;time\u0026#39;, class_=\u0026#39;post-date\u0026#39;) date = date_elem.get(\u0026#39;datetime\u0026#39;) if date_elem else \u0026#39;No date\u0026#39; # Extract excerpt excerpt_elem = article.find(\u0026#39;p\u0026#39;, class_=\u0026#39;excerpt\u0026#39;) excerpt = excerpt_elem.get_text(strip=True) if excerpt_elem else \u0026#39;\u0026#39; scraped_data.append({ \u0026#39;title\u0026#39;: title, \u0026#39;url\u0026#39;: link, \u0026#39;date\u0026#39;: date, \u0026#39;excerpt\u0026#39;: excerpt }) return scraped_data def save_to_file(data, filename=\u0026#39;articles.txt\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Save scraped articles to a text file.\u0026#34;\u0026#34;\u0026#34; with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for idx, article in enumerate(data, 1): f.write(f\u0026#34;{idx}. {article[\u0026#39;title\u0026#39;]}\\n\u0026#34;) f.write(f\u0026#34; URL: {article[\u0026#39;url\u0026#39;]}\\n\u0026#34;) f.write(f\u0026#34; Date: {article[\u0026#39;date\u0026#39;]}\\n\u0026#34;) f.write(f\u0026#34; Excerpt: {article[\u0026#39;excerpt\u0026#39;][:100]}...\\n\u0026#34;) f.write(\u0026#34;\\n\u0026#34;) print(f\u0026#34;Saved {len(data)} articles to {filename}\u0026#34;) # Test the scraper if __name__ == \u0026#34;__main__\u0026#34;: blog_url = \u0026#34;https://www.buanacoding.com/blog\u0026#34; print(f\u0026#34;Scraping {blog_url}...\u0026#34;) articles = scrape_blog_articles(blog_url) if articles: print(f\u0026#34;Found {len(articles)} articles\u0026#34;) save_to_file(articles) # Print first article as sample print(\u0026#34;\\nSample article:\u0026#34;) print(f\u0026#34;Title: {articles[0][\u0026#39;title\u0026#39;]}\u0026#34;) print(f\u0026#34;URL: {articles[0][\u0026#39;url\u0026#39;]}\u0026#34;) else: print(\u0026#34;No articles found. Check your selectors.\u0026#34;) Important notes:\nSelectors are site-specific: The classes like post-title and excerpt need to match the actual HTML structure of your target site. Inspect the page source to find the correct selectors.\nUser-Agent header: We\u0026rsquo;re identifying ourselves as a browser. The default python-requests/x.x.x user-agent gets blocked by many sites.\nError handling: Always wrap requests in try-except blocks. Networks fail, servers go down, and your scraper should handle that gracefully.\nBeing polite: We\u0026rsquo;ll add delays between requests soon\u0026ndash;don\u0026rsquo;t hammer servers.\nHandling Pagination: Scraping Multiple Pages Most blogs and e-commerce sites split content across pages. Let\u0026rsquo;s scrape all pages automatically:\ndef scrape_multiple_pages(base_url, num_pages): \u0026#34;\u0026#34;\u0026#34;Scrape articles from multiple pages.\u0026#34;\u0026#34;\u0026#34; all_articles = [] for page_num in range(1, num_pages + 1): # Construct URL for current page # Common patterns: # - https://site.com/blog/page/2 # - https://site.com/blog?page=2 # - https://site.com/blog/2 url = f\u0026#34;{base_url}/page/{page_num}\u0026#34; # Adjust pattern as needed print(f\u0026#34;Scraping page {page_num}...\u0026#34;) articles = scrape_blog_articles(url) all_articles.extend(articles) # Be polite: add delay between requests time.sleep(2) # 2 second delay # Stop if page returned no articles (we\u0026#39;ve hit the end) if not articles: print(f\u0026#34;No more articles found at page {page_num}\u0026#34;) break return all_articles # Usage all_articles = scrape_multiple_pages(\u0026#34;https://www.buanacoding.com/blog\u0026#34;, num_pages=5) print(f\u0026#34;Total articles scraped: {len(all_articles)}\u0026#34;) The key here is identifying the pagination pattern. Open your target site, click through a few pages, and watch how the URL changes. Then replicate that pattern in your code.\nExtracting Product Data: E-commerce Scraper Let\u0026rsquo;s build a more complex scraper for e-commerce data\u0026ndash;products with names, prices, ratings, and images:\nimport requests from bs4 import BeautifulSoup import csv import time def scrape_products(url): \u0026#34;\u0026#34;\u0026#34;Scrape product information from an e-commerce page.\u0026#34;\u0026#34;\u0026#34; headers = { \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#39; } try: response = requests.get(url, headers=headers, timeout=10) response.raise_for_status() except requests.exceptions.RequestException as e: print(f\u0026#34;Error: {e}\u0026#34;) return [] soup = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) # Find all product cards products = soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;product-card\u0026#39;) scraped_products = [] for product in products: # Extract product name name_elem = product.find(\u0026#39;h3\u0026#39;, class_=\u0026#39;product-name\u0026#39;) name = name_elem.get_text(strip=True) if name_elem else \u0026#39;N/A\u0026#39; # Extract price price_elem = product.find(\u0026#39;span\u0026#39;, class_=\u0026#39;price\u0026#39;) price_text = price_elem.get_text(strip=True) if price_elem else \u0026#39;N/A\u0026#39; # Clean price: remove currency symbols and convert to float price = clean_price(price_text) # Extract rating rating_elem = product.find(\u0026#39;div\u0026#39;, class_=\u0026#39;rating\u0026#39;) rating = rating_elem.get(\u0026#39;data-rating\u0026#39;) if rating_elem else \u0026#39;N/A\u0026#39; # Extract image URL img_elem = product.find(\u0026#39;img\u0026#39;) image_url = img_elem.get(\u0026#39;src\u0026#39;) if img_elem else \u0026#39;N/A\u0026#39; # Extract product URL link_elem = product.find(\u0026#39;a\u0026#39;, class_=\u0026#39;product-link\u0026#39;) product_url = link_elem.get(\u0026#39;href\u0026#39;) if link_elem else \u0026#39;N/A\u0026#39; # Make relative URLs absolute if product_url and not product_url.startswith(\u0026#39;http\u0026#39;): product_url = f\u0026#34;https://example.com{product_url}\u0026#34; scraped_products.append({ \u0026#39;name\u0026#39;: name, \u0026#39;price\u0026#39;: price, \u0026#39;rating\u0026#39;: rating, \u0026#39;image\u0026#39;: image_url, \u0026#39;url\u0026#39;: product_url }) return scraped_products def clean_price(price_text): \u0026#34;\u0026#34;\u0026#34;Convert price text to float.\u0026#34;\u0026#34;\u0026#34; import re # Remove currency symbols and commas price_clean = re.sub(r\u0026#39;[^\\d.]\u0026#39;, \u0026#39;\u0026#39;, price_text) try: return float(price_clean) except ValueError: return 0.0 def save_to_csv(products, filename=\u0026#39;products.csv\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Save products to CSV file.\u0026#34;\u0026#34;\u0026#34; if not products: print(\u0026#34;No products to save\u0026#34;) return keys = products[0].keys() with open(filename, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: writer = csv.DictWriter(f, fieldnames=keys) writer.writeheader() writer.writerows(products) print(f\u0026#34;Saved {len(products)} products to {filename}\u0026#34;) # Usage if __name__ == \u0026#34;__main__\u0026#34;: url = \u0026#34;https://example-ecommerce.com/products\u0026#34; products = scrape_products(url) if products: save_to_csv(products) print(f\u0026#34;Scraped {len(products)} products successfully\u0026#34;) else: print(\u0026#34;No products found\u0026#34;) This scraper demonstrates several important techniques:\nCleaning extracted data (removing currency symbols from prices) Converting relative URLs to absolute URLs Exporting data to CSV for easy analysis Using regex to extract numbers from messy text Being Ethical: Respecting Robots.txt and Rate Limits Here\u0026rsquo;s the thing nobody likes to talk about: scraping can be rude if done carelessly. Websites cost money to run, and aggressive scrapers can cause real harm. Let\u0026rsquo;s be responsible.\nAlways check robots.txt:\nEvery site has a robots.txt file at the root (e.g., https://example.com/robots.txt) that tells crawlers which paths are allowed:\nUser-agent: * Disallow: /admin/ Disallow: /api/ Crawl-delay: 10 This means: don\u0026rsquo;t scrape /admin/ or /api/, and wait 10 seconds between requests.\nCheck robots.txt programmatically:\nimport requests def check_robots_txt(base_url): \u0026#34;\u0026#34;\u0026#34;Fetch and display robots.txt content.\u0026#34;\u0026#34;\u0026#34; robots_url = f\u0026#34;{base_url}/robots.txt\u0026#34; try: response = requests.get(robots_url) if response.status_code == 200: print(response.text) else: print(f\u0026#34;No robots.txt found (status {response.status_code})\u0026#34;) except requests.exceptions.RequestException as e: print(f\u0026#34;Error fetching robots.txt: {e}\u0026#34;) check_robots_txt(\u0026#34;https://www.buanacoding.com\u0026#34;) Implement rate limiting:\nimport time from datetime import datetime class RateLimiter: \u0026#34;\u0026#34;\u0026#34;Simple rate limiter to control request frequency.\u0026#34;\u0026#34;\u0026#34; def __init__(self, requests_per_second=1): self.delay = 1.0 / requests_per_second self.last_request = None def wait(self): \u0026#34;\u0026#34;\u0026#34;Wait if necessary to maintain rate limit.\u0026#34;\u0026#34;\u0026#34; if self.last_request: elapsed = time.time() - self.last_request if elapsed \u0026lt; self.delay: time.sleep(self.delay - elapsed) self.last_request = time.time() # Usage rate_limiter = RateLimiter(requests_per_second=0.5) # 1 request every 2 seconds for url in urls_to_scrape: rate_limiter.wait() response = requests.get(url) # ... process response Best practices summary:\nStart conservatively: 1 request every 10-15 seconds Respect robots.txt directives Use realistic User-Agent headers Implement retries with exponential backoff Cache responses to avoid repeat requests Scrape during off-peak hours if possible Include contact info in your User-Agent so webmasters can reach you Handling Common Challenges and Anti-Scraping Measures Real-world scraping is messier than tutorials let on. Here\u0026rsquo;s how to handle common issues:\n1. Websites that block the default User-Agent:\nfrom fake_useragent import UserAgent ua = UserAgent() headers = { \u0026#39;User-Agent\u0026#39;: ua.random, # Randomize user agent \u0026#39;Accept\u0026#39;: \u0026#39;text/html,application/xhtml+xml\u0026#39;, \u0026#39;Accept-Language\u0026#39;: \u0026#39;en-US,en;q=0.9\u0026#39;, \u0026#39;Referer\u0026#39;: \u0026#39;https://google.com\u0026#39;, } response = requests.get(url, headers=headers) 2. Session cookies and persistent connections:\nsession = requests.Session() session.headers.update(headers) # Cookies persist across requests response1 = session.get(url1) response2 = session.get(url2) # Cookies from response1 are sent 3. Handling timeouts and retries:\nfrom requests.adapters import HTTPAdapter from requests.packages.urllib3.util.retry import Retry def create_session(): \u0026#34;\u0026#34;\u0026#34;Create a session with retry logic.\u0026#34;\u0026#34;\u0026#34; session = requests.Session() retry = Retry( total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504] ) adapter = HTTPAdapter(max_retries=retry) session.mount(\u0026#39;http://\u0026#39;, adapter) session.mount(\u0026#39;https://\u0026#39;, adapter) return session session = create_session() response = session.get(url, timeout=10) 4. Dealing with missing or inconsistent data:\ndef safe_extract(soup, selector, attribute=None, default=\u0026#39;N/A\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Safely extract data with fallback.\u0026#34;\u0026#34;\u0026#34; elem = soup.find(*selector) if isinstance(selector, tuple) else soup.find(selector) if elem: if attribute: return elem.get(attribute, default) return elem.get_text(strip=True) return default # Usage title = safe_extract(soup, (\u0026#39;h1\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;title\u0026#39;})) price = safe_extract(soup, (\u0026#39;span\u0026#39;, {\u0026#39;class\u0026#39;: \u0026#39;price\u0026#39;}), default=\u0026#39;0.00\u0026#39;) 5. Parsing relative URLs correctly:\nfrom urllib.parse import urljoin base_url = \u0026#34;https://example.com/products/\u0026#34; relative_url = \u0026#34;../images/product.jpg\u0026#34; absolute_url = urljoin(base_url, relative_url) # Result: https://example.com/images/product.jpg Storing Your Scraped Data Effectively You\u0026rsquo;ve scraped data\u0026ndash;now what? Let\u0026rsquo;s look at storage options:\nCSV export (best for tabular data):\nimport csv def export_to_csv(data, filename): \u0026#34;\u0026#34;\u0026#34;Export list of dictionaries to CSV.\u0026#34;\u0026#34;\u0026#34; if not data: return with open(filename, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: writer = csv.DictWriter(f, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) export_to_csv(scraped_products, \u0026#39;products.csv\u0026#39;) JSON export (preserves nested structures):\nimport json def export_to_json(data, filename): \u0026#34;\u0026#34;\u0026#34;Export data to JSON file.\u0026#34;\u0026#34;\u0026#34; with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(data, f, indent=2, ensure_ascii=False) export_to_json(scraped_products, \u0026#39;products.json\u0026#39;) SQLite database (queryable, efficient):\nimport sqlite3 def store_in_database(products): \u0026#34;\u0026#34;\u0026#34;Store products in SQLite database.\u0026#34;\u0026#34;\u0026#34; conn = sqlite3.connect(\u0026#39;products.db\u0026#39;) cursor = conn.cursor() # Create table cursor.execute(\u0026#39;\u0026#39;\u0026#39; CREATE TABLE IF NOT EXISTS products ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, price REAL, rating TEXT, url TEXT UNIQUE, scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) \u0026#39;\u0026#39;\u0026#39;) # Insert products for product in products: try: cursor.execute(\u0026#39;\u0026#39;\u0026#39; INSERT OR REPLACE INTO products (name, price, rating, url) VALUES (?, ?, ?, ?) \u0026#39;\u0026#39;\u0026#39;, (product[\u0026#39;name\u0026#39;], product[\u0026#39;price\u0026#39;], product[\u0026#39;rating\u0026#39;], product[\u0026#39;url\u0026#39;])) except sqlite3.Error as e: print(f\u0026#34;Error inserting {product[\u0026#39;name\u0026#39;]}: {e}\u0026#34;) conn.commit() conn.close() store_in_database(scraped_products) Using pandas for data manipulation:\nimport pandas as pd # Convert to DataFrame df = pd.DataFrame(scraped_products) # Clean and analyze df[\u0026#39;price\u0026#39;] = pd.to_numeric(df[\u0026#39;price\u0026#39;], errors=\u0026#39;coerce\u0026#39;) df = df.dropna(subset=[\u0026#39;price\u0026#39;]) # Remove rows with missing prices # Calculate statistics print(f\u0026#34;Average price: ${df[\u0026#39;price\u0026#39;].mean():.2f}\u0026#34;) print(f\u0026#34;Price range: ${df[\u0026#39;price\u0026#39;].min():.2f} - ${df[\u0026#39;price\u0026#39;].max():.2f}\u0026#34;) # Export df.to_csv(\u0026#39;products_cleaned.csv\u0026#39;, index=False) df.to_excel(\u0026#39;products.xlsx\u0026#39;, index=False) Building a Complete News Aggregator Let\u0026rsquo;s tie everything together with a production-ready news scraper:\nimport requests from bs4 import BeautifulSoup import csv from datetime import datetime import time import logging # Configure logging logging.basicConfig( level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;, handlers=[ logging.FileHandler(\u0026#39;scraper.log\u0026#39;), logging.StreamHandler() ] ) class NewsAggregator: \u0026#34;\u0026#34;\u0026#34;Scrape news headlines from multiple sources.\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.session = requests.Session() self.session.headers.update({ \u0026#39;User-Agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\u0026#39; }) self.articles = [] def scrape_source(self, url, selectors): \u0026#34;\u0026#34;\u0026#34;Scrape a single news source.\u0026#34;\u0026#34;\u0026#34; try: logging.info(f\u0026#34;Scraping {url}\u0026#34;) response = self.session.get(url, timeout=15) response.raise_for_status() soup = BeautifulSoup(response.content, \u0026#39;lxml\u0026#39;) articles = soup.select(selectors[\u0026#39;article_container\u0026#39;]) for article in articles: headline = article.select_one(selectors[\u0026#39;headline\u0026#39;]) link = article.select_one(selectors[\u0026#39;link\u0026#39;]) if headline and link: self.articles.append({ \u0026#39;headline\u0026#39;: headline.get_text(strip=True), \u0026#39;url\u0026#39;: link.get(\u0026#39;href\u0026#39;), \u0026#39;source\u0026#39;: url, \u0026#39;scraped_at\u0026#39;: datetime.now().isoformat() }) logging.info(f\u0026#34;Scraped {len(articles)} articles from {url}\u0026#34;) time.sleep(3) # Be polite except requests.exceptions.RequestException as e: logging.error(f\u0026#34;Error scraping {url}: {e}\u0026#34;) except Exception as e: logging.error(f\u0026#34;Unexpected error: {e}\u0026#34;) def scrape_all_sources(self, sources): \u0026#34;\u0026#34;\u0026#34;Scrape multiple news sources.\u0026#34;\u0026#34;\u0026#34; for source in sources: self.scrape_source(source[\u0026#39;url\u0026#39;], source[\u0026#39;selectors\u0026#39;]) def export_results(self, filename=\u0026#39;news.csv\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Export scraped articles to CSV.\u0026#34;\u0026#34;\u0026#34; if not self.articles: logging.warning(\u0026#34;No articles to export\u0026#34;) return with open(filename, \u0026#39;w\u0026#39;, newline=\u0026#39;\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: writer = csv.DictWriter(f, fieldnames=self.articles[0].keys()) writer.writeheader() writer.writerows(self.articles) logging.info(f\u0026#34;Exported {len(self.articles)} articles to {filename}\u0026#34;) # Configuration for different news sources news_sources = [ { \u0026#39;url\u0026#39;: \u0026#39;https://news.ycombinator.com\u0026#39;, \u0026#39;selectors\u0026#39;: { \u0026#39;article_container\u0026#39;: \u0026#39;.athing\u0026#39;, \u0026#39;headline\u0026#39;: \u0026#39;.titleline \u0026gt; a\u0026#39;, \u0026#39;link\u0026#39;: \u0026#39;.titleline \u0026gt; a\u0026#39; } }, # Add more sources here ] # Run the aggregator if __name__ == \u0026#34;__main__\u0026#34;: aggregator = NewsAggregator() aggregator.scrape_all_sources(news_sources) aggregator.export_results() print(f\u0026#34;Successfully scraped {len(aggregator.articles)} articles\u0026#34;) This scraper demonstrates production patterns:\nLogging for debugging and monitoring Session management for efficiency Graceful error handling Configurable selectors for multiple sources Structured output with timestamps When BeautifulSoup Isn\u0026rsquo;t Enough: JavaScript-Heavy Sites BeautifulSoup only sees the HTML the server sends\u0026ndash;it doesn\u0026rsquo;t execute JavaScript. If a site loads content dynamically (most modern single-page apps do), you\u0026rsquo;ll need different tools.\nCheck if you actually need JavaScript rendering:\nOpen your browser\u0026rsquo;s DevTools (F12), go to the Network tab, and look for XHR/Fetch requests. Often sites load data via JSON APIs that you can call directly\u0026ndash;this is faster and more reliable than rendering JavaScript.\nExample: Calling an API directly instead of scraping:\n# Instead of scraping the rendered HTML... response = requests.get(\u0026#39;https://site.com/products?page=1\u0026amp;limit=100\u0026#39;) data = response.json() # If the endpoint returns JSON # You get structured data directly for product in data[\u0026#39;results\u0026#39;]: print(product[\u0026#39;name\u0026#39;], product[\u0026#39;price\u0026#39;]) When you do need JavaScript rendering, use Selenium:\nfrom selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # Setup (install: pip install selenium) driver = webdriver.Chrome() # Or Firefox, Edge, etc. try: driver.get(\u0026#39;https://example.com\u0026#39;) # Wait for elements to load wait = WebDriverWait(driver, 10) products = wait.until( EC.presence_of_all_elements_located((By.CLASS_NAME, \u0026#39;product-card\u0026#39;)) ) # Extract data for product in products: name = product.find_element(By.CLASS_NAME, \u0026#39;name\u0026#39;).text price = product.find_element(By.CLASS_NAME, \u0026#39;price\u0026#39;).text print(name, price) finally: driver.quit() Selenium is powerful but much slower than Requests+BeautifulSoup. Use it sparingly, and consider deploying your scrapers with Docker if they need browser automation in production.\nDebugging Your Scrapers When They Break Scrapers break constantly\u0026ndash;websites change their HTML structure without warning. Here\u0026rsquo;s how to debug effectively:\n1. Save the HTML for offline testing:\nresponse = requests.get(url) with open(\u0026#39;debug.html\u0026#39;, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(response.text) # Then test parsing locally without hitting the website with open(\u0026#39;debug.html\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: soup = BeautifulSoup(f.read(), \u0026#39;lxml\u0026#39;) 2. Print the actual HTML you\u0026rsquo;re parsing:\nprint(soup.prettify()) # Formatted HTML print(soup.find(\u0026#39;div\u0026#39;, class_=\u0026#39;product\u0026#39;)) # Specific element 3. Check what your selectors actually find:\nproducts = soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;product\u0026#39;) print(f\u0026#34;Found {len(products)} products\u0026#34;) if not products: # Selector might be wrong print(\u0026#34;No products found. Checking alternative selectors...\u0026#34;) alt_products = soup.find_all(\u0026#39;div\u0026#39;, class_=\u0026#39;item\u0026#39;) print(f\u0026#34;Found {len(alt_products)} items with class \u0026#39;item\u0026#39;\u0026#34;) 4. Validate extracted data immediately:\ndef validate_product(product): \u0026#34;\u0026#34;\u0026#34;Check if extracted product data is valid.\u0026#34;\u0026#34;\u0026#34; issues = [] if not product.get(\u0026#39;name\u0026#39;): issues.append(\u0026#34;Missing name\u0026#34;) if not product.get(\u0026#39;price\u0026#39;) or product[\u0026#39;price\u0026#39;] \u0026lt;= 0: issues.append(\u0026#34;Invalid price\u0026#34;) if product.get(\u0026#39;url\u0026#39;) and not product[\u0026#39;url\u0026#39;].startswith(\u0026#39;http\u0026#39;): issues.append(\u0026#34;Invalid URL\u0026#34;) if issues: logging.warning(f\u0026#34;Product validation failed: {issues}\u0026#34;) return False return True # Use validation if validate_product(scraped_product): products.append(scraped_product) 5. Use logging extensively:\nimport logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) logger.debug(f\u0026#34;Fetching {url}\u0026#34;) logger.info(f\u0026#34;Found {len(products)} products\u0026#34;) logger.warning(f\u0026#34;Missing price for product: {product_name}\u0026#34;) logger.error(f\u0026#34;Failed to parse page: {e}\u0026#34;) Scheduling Your Scrapers to Run Automatically Once your scraper works, you\u0026rsquo;ll want to run it regularly without manual intervention. If you\u0026rsquo;re on Linux, cron jobs are perfect for scheduling automated tasks .\nBasic cron job (runs daily at 3 AM):\n# Open crontab editor crontab -e # Add this line (adjust paths) 0 3 * * * /usr/bin/python3 /path/to/your/scraper.py \u0026gt;\u0026gt; /path/to/scraper.log 2\u0026gt;\u0026amp;1 Or use Python\u0026rsquo;s schedule library:\nimport schedule import time def job(): \u0026#34;\u0026#34;\u0026#34;Run the scraper.\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Starting scraper at {datetime.now()}\u0026#34;) # Your scraping code here # Schedule job schedule.every().day.at(\u0026#34;03:00\u0026#34;).do(job) schedule.every().hour.do(job) # Or every hour schedule.every(30).minutes.do(job) # Or every 30 minutes # Keep script running while True: schedule.run_pending() time.sleep(60) # Check every minute For production deployments, containerize with Docker and use proper job schedulers like cron, systemd timers, or cloud services like AWS Lambda for serverless scraping.\nWhat To Do Next: Taking Your Scraping Further You\u0026rsquo;ve learned the fundamentals\u0026ndash;here\u0026rsquo;s how to level up:\n1. Build a data pipeline: Scrape data, clean it, store it in a database, and serve it via an API using FastAPI .\n2. Add monitoring and alerts: Track scraper success rates and get notified when things break. Use tools like Sentry for error tracking.\n3. Scale up with Scrapy: For large-scale scraping (thousands of pages), the Scrapy framework provides concurrency, middleware, and built-in best practices.\n4. Learn about browser automation: Selenium and Playwright let you scrape JavaScript-heavy sites, fill forms, and interact with pages.\n5. Explore data analysis: Use pandas to analyze your scraped data, create visualizations, and extract insights.\n6. Deploy to the cloud: Run your scrapers on VPS servers with proper HTTPS setup for production reliability.\n7. Stay secure: If you\u0026rsquo;re handling scraped data containing sensitive information, review security best practices to keep data safe.\nThe Bottom Line Web scraping transformed how I work, and I hope this guide does the same for you. The ability to programmatically gather data opens up countless possibilities\u0026ndash;market research, price monitoring, content aggregation, dataset building for machine learning, and so much more.\nRemember: scrape responsibly, respect website owners, and always consider whether an official API exists before scraping. When done ethically, web scraping is an incredibly powerful skill that will serve you throughout your career.\nStart with simple projects and gradually increase complexity. Scrape sites you\u0026rsquo;re genuinely interested in\u0026ndash;the best way to learn is by solving real problems. And when your scrapers break (they will), don\u0026rsquo;t get discouraged. Websites change, and adapting scrapers is part of the game.\nNow go automate some tedious data collection and reclaim your time. What\u0026rsquo;s the first thing you\u0026rsquo;re going to scrape?\n","href":"/2025/10/how-to-build-web-scraper-python-beautifulsoup-requests.html","title":"How to Build a Web Scraper in Python with BeautifulSoup and Requests"},{"content":"Automation separates beginners from experienced system administrators. Instead of manually running backups, monitoring logs, or cleaning temporary files, you write scripts once and let cron run them automatically.\nCron handles time-based scheduling. Shell scripts do the actual work. Combined, they automate everything from database backups to system monitoring, log rotation, security scans, and report generation.\nThis guide covers cron syntax, writing production-ready shell scripts, automated backups, monitoring, error handling, and notifications for reliable automation.\nWhat is cron and how it works Cron is a daemon that runs continuously on Linux systems. It checks crontab files every minute and executes scheduled commands.\nCheck if cron is running:\nsystemctl status cron If not running, start it:\nsudo systemctl enable --now cron Cron reads schedules from several locations:\nUser crontabs: Each user has a crontab file edited with crontab -e System crontab: /etc/crontab for system-wide jobs Cron directories: Scripts in /etc/cron.hourly/, /etc/cron.daily/, /etc/cron.weekly/, /etc/cron.monthly/ Cron.d: Individual job files in /etc/cron.d/ Most personal automation uses user crontabs. System maintenance uses system cron.\nCrontab syntax explained Crontab entries have 5 time fields plus the command:\n* * * * * command to execute │ │ │ │ │ │ │ │ │ └─── Day of week (0-7, both 0 and 7 are Sunday) │ │ │ └───── Month (1-12) │ │ └─────── Day of month (1-31) │ └───────── Hour (0-23) └─────────── Minute (0-59) Examples:\n# Run at 2:30 AM every day 30 2 * * * /usr/local/bin/backup.sh # Run every hour on the hour 0 * * * * /usr/local/bin/check-status.sh # Run every 15 minutes */15 * * * * /usr/local/bin/monitor.sh # Run at 5 PM Monday through Friday 0 17 * * 1-5 /usr/local/bin/workday-report.sh # Run at midnight on the 1st of every month 0 0 1 * * /usr/local/bin/monthly-cleanup.sh # Run every Sunday at 3 AM 0 3 * * 0 /usr/local/bin/weekly-backup.sh # Run every 6 hours 0 */6 * * * /usr/local/bin/periodic-check.sh # Run twice daily at 9 AM and 6 PM 0 9,18 * * * /usr/local/bin/twice-daily.sh # Run every weekday at 8:30 AM 30 8 * * 1-5 /usr/local/bin/morning-routine.sh # Run every 30 minutes between 9 AM and 5 PM */30 9-17 * * * /usr/local/bin/business-hours-check.sh Special shortcuts:\n@reboot # Run once at startup @yearly # Run once a year (0 0 1 1 *) @annually # Same as @yearly @monthly # Run once a month (0 0 1 * *) @weekly # Run once a week (0 0 * * 0) @daily # Run once a day (0 0 * * *) @midnight # Same as @daily @hourly # Run once an hour (0 * * * *) # Example: Run backup script at startup @reboot /usr/local/bin/startup-backup.sh Manage your crontab Edit your crontab:\ncrontab -e This opens your crontab in the default editor. First time might ask which editor to use (choose nano if unsure).\nView current crontab:\ncrontab -l Remove all cron jobs:\ncrontab -r Edit another user\u0026rsquo;s crontab (requires root):\nsudo crontab -u username -e Your crontab file can include environment variables:\n# Set email for notifications MAILTO=admin@example.com # Set shell SHELL=/bin/bash # Set PATH PATH=/usr/local/bin:/usr/bin:/bin # Jobs 30 2 * * * /usr/local/bin/backup.sh 0 * * * * /usr/local/bin/check-status.sh Write your first automation script Let\u0026rsquo;s create a script that backs up a directory daily.\nCreate the script:\nsudo nano /usr/local/bin/backup.sh Basic backup script:\n#!/bin/bash # Daily backup script # Backs up /var/www to /backups # Variables BACKUP_SOURCE=\u0026#34;/var/www\u0026#34; BACKUP_DEST=\u0026#34;/backups\u0026#34; DATE=$(date +%Y%m%d_%H%M%S) BACKUP_FILE=\u0026#34;backup_${DATE}.tar.gz\u0026#34; LOG_FILE=\u0026#34;/var/log/backup.log\u0026#34; # Create backup directory if it doesn\u0026#39;t exist mkdir -p \u0026#34;$BACKUP_DEST\u0026#34; # Log start echo \u0026#34;[$(date)] Starting backup...\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; # Create backup if tar -czf \u0026#34;${BACKUP_DEST}/${BACKUP_FILE}\u0026#34; \u0026#34;$BACKUP_SOURCE\u0026#34; 2\u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34;; then echo \u0026#34;[$(date)] Backup successful: ${BACKUP_FILE}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; # Delete backups older than 7 days find \u0026#34;$BACKUP_DEST\u0026#34; -name \u0026#34;backup_*.tar.gz\u0026#34; -mtime +7 -delete echo \u0026#34;[$(date)] Old backups cleaned\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; else echo \u0026#34;[$(date)] Backup FAILED!\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; exit 1 fi echo \u0026#34;[$(date)] Backup completed\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; Make it executable:\nsudo chmod +x /usr/local/bin/backup.sh Test it manually:\nsudo /usr/local/bin/backup.sh Check the log:\ncat /var/log/backup.log Add to crontab to run daily at 2 AM:\ncrontab -e Add line:\n0 2 * * * /usr/local/bin/backup.sh Database backup automation MySQL/MariaDB backup script:\n#!/bin/bash # MySQL backup script # Configuration DB_USER=\u0026#34;backup_user\u0026#34; DB_PASS=\u0026#34;secure_password\u0026#34; DB_NAME=\u0026#34;mydatabase\u0026#34; BACKUP_DIR=\u0026#34;/backups/mysql\u0026#34; DATE=$(date +%Y%m%d_%H%M%S) BACKUP_FILE=\u0026#34;${BACKUP_DIR}/mysql_${DB_NAME}_${DATE}.sql.gz\u0026#34; LOG_FILE=\u0026#34;/var/log/mysql-backup.log\u0026#34; # Create backup directory mkdir -p \u0026#34;$BACKUP_DIR\u0026#34; # Dump database echo \u0026#34;[$(date)] Starting MySQL backup...\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; if mysqldump -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \u0026#34;$DB_NAME\u0026#34; | gzip \u0026gt; \u0026#34;$BACKUP_FILE\u0026#34;; then echo \u0026#34;[$(date)] Backup successful: ${BACKUP_FILE}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; # Delete backups older than 14 days find \u0026#34;$BACKUP_DIR\u0026#34; -name \u0026#34;mysql_*.sql.gz\u0026#34; -mtime +14 -delete # Get backup size SIZE=$(du -h \u0026#34;$BACKUP_FILE\u0026#34; | cut -f1) echo \u0026#34;[$(date)] Backup size: ${SIZE}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; else echo \u0026#34;[$(date)] Backup FAILED!\u0026#34; \u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34; exit 1 fi PostgreSQL backup:\n#!/bin/bash # PostgreSQL backup script DB_NAME=\u0026#34;mydatabase\u0026#34; DB_USER=\u0026#34;postgres\u0026#34; BACKUP_DIR=\u0026#34;/backups/postgres\u0026#34; DATE=$(date +%Y%m%d_%H%M%S) BACKUP_FILE=\u0026#34;${BACKUP_DIR}/postgres_${DB_NAME}_${DATE}.sql.gz\u0026#34; mkdir -p \u0026#34;$BACKUP_DIR\u0026#34; # Backup database pg_dump -U \u0026#34;$DB_USER\u0026#34; \u0026#34;$DB_NAME\u0026#34; | gzip \u0026gt; \u0026#34;$BACKUP_FILE\u0026#34; # Clean old backups find \u0026#34;$BACKUP_DIR\u0026#34; -name \u0026#34;postgres_*.sql.gz\u0026#34; -mtime +14 -delete For PostgreSQL, create .pgpass file to avoid password prompts:\necho \u0026#34;localhost:5432:*:postgres:your_password\u0026#34; \u0026gt; ~/.pgpass chmod 600 ~/.pgpass System monitoring automation Disk space monitoring script:\n#!/bin/bash # Check disk space and alert if over threshold THRESHOLD=80 EMAIL=\u0026#34;admin@example.com\u0026#34; # Check each mounted filesystem df -H | grep -vE \u0026#39;^Filesystem|tmpfs|cdrom\u0026#39; | awk \u0026#39;{ print $5 \u0026#34; \u0026#34; $1 \u0026#34; \u0026#34; $6 }\u0026#39; | while read output; do USAGE=$(echo $output | awk \u0026#39;{ print $1}\u0026#39; | sed \u0026#39;s/%//g\u0026#39;) PARTITION=$(echo $output | awk \u0026#39;{ print $2 }\u0026#39;) MOUNT=$(echo $output | awk \u0026#39;{ print $3 }\u0026#39;) if [ $USAGE -ge $THRESHOLD ]; then echo \u0026#34;Disk space alert: ${PARTITION} mounted on ${MOUNT} is ${USAGE}% full\u0026#34; | \\ mail -s \u0026#34;Disk Space Alert on $(hostname)\u0026#34; \u0026#34;$EMAIL\u0026#34; fi done Run every hour:\n0 * * * * /usr/local/bin/check-disk.sh Memory monitoring:\n#!/bin/bash # Monitor memory usage THRESHOLD=90 EMAIL=\u0026#34;admin@example.com\u0026#34; MEMORY_USAGE=$(free | grep Mem | awk \u0026#39;{print ($3/$2) * 100.0}\u0026#39; | cut -d\u0026#39;.\u0026#39; -f1) if [ $MEMORY_USAGE -ge $THRESHOLD ]; then { echo \u0026#34;Memory usage is at ${MEMORY_USAGE}%\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;Top memory-consuming processes:\u0026#34; ps aux --sort=-%mem | head -n 10 } | mail -s \u0026#34;Memory Alert on $(hostname)\u0026#34; \u0026#34;$EMAIL\u0026#34; fi Service availability check:\n#!/bin/bash # Check if critical services are running SERVICES=(\u0026#34;nginx\u0026#34; \u0026#34;mysql\u0026#34; \u0026#34;redis\u0026#34;) EMAIL=\u0026#34;admin@example.com\u0026#34; ALERT=\u0026#34;\u0026#34; for SERVICE in \u0026#34;${SERVICES[@]}\u0026#34;; do if ! systemctl is-active --quiet \u0026#34;$SERVICE\u0026#34;; then ALERT=\u0026#34;${ALERT}Service ${SERVICE} is DOWN!\\n\u0026#34; # Try to restart systemctl start \u0026#34;$SERVICE\u0026#34; if systemctl is-active --quiet \u0026#34;$SERVICE\u0026#34;; then ALERT=\u0026#34;${ALERT}Service ${SERVICE} restarted successfully.\\n\\n\u0026#34; else ALERT=\u0026#34;${ALERT}Service ${SERVICE} FAILED to restart!\\n\\n\u0026#34; fi fi done if [ -n \u0026#34;$ALERT\u0026#34; ]; then echo -e \u0026#34;$ALERT\u0026#34; | mail -s \u0026#34;Service Alert on $(hostname)\u0026#34; \u0026#34;$EMAIL\u0026#34; fi Log rotation and cleanup Clean old logs:\n#!/bin/bash # Rotate and compress logs LOG_DIR=\u0026#34;/var/log/myapp\u0026#34; DAYS_TO_KEEP=30 # Compress logs older than 1 day find \u0026#34;$LOG_DIR\u0026#34; -name \u0026#34;*.log\u0026#34; -mtime +1 -exec gzip {} \\; # Delete compressed logs older than 30 days find \u0026#34;$LOG_DIR\u0026#34; -name \u0026#34;*.log.gz\u0026#34; -mtime +$DAYS_TO_KEEP -delete # Truncate current log if larger than 100MB for LOG in \u0026#34;$LOG_DIR\u0026#34;/*.log; do if [ -f \u0026#34;$LOG\u0026#34; ]; then SIZE=$(stat -f%z \u0026#34;$LOG\u0026#34; 2\u0026gt;/dev/null || stat -c%s \u0026#34;$LOG\u0026#34; 2\u0026gt;/dev/null) if [ $SIZE -gt 104857600 ]; then cp \u0026#34;$LOG\u0026#34; \u0026#34;${LOG}.old\u0026#34; \u0026gt; \u0026#34;$LOG\u0026#34; gzip \u0026#34;${LOG}.old\u0026#34; fi fi done Clean temporary files:\n#!/bin/bash # Clean temporary files and caches # Clean apt cache (Ubuntu/Debian) apt-get clean # Clean old kernels (keep last 2) apt-get autoremove --purge -y # Clean systemd journal (keep last 7 days) journalctl --vacuum-time=7d # Clean temp directories find /tmp -type f -atime +7 -delete find /var/tmp -type f -atime +30 -delete # Clean old user cache find /home/*/.cache -type f -atime +30 -delete 2\u0026gt;/dev/null echo \u0026#34;Cleanup completed on $(date)\u0026#34; Run weekly:\n0 3 * * 0 /usr/local/bin/cleanup.sh Error handling and notifications Script with proper error handling:\n#!/bin/bash # Backup script with error handling set -euo pipefail # Exit on error, undefined variables, pipe failures # Configuration SCRIPT_NAME=\u0026#34;backup\u0026#34; LOG_FILE=\u0026#34;/var/log/${SCRIPT_NAME}.log\u0026#34; EMAIL=\u0026#34;admin@example.com\u0026#34; BACKUP_SOURCE=\u0026#34;/var/www\u0026#34; BACKUP_DEST=\u0026#34;/backups\u0026#34; # Logging function log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } # Error handler error_exit() { log \u0026#34;ERROR: $1\u0026#34; echo \u0026#34;Backup failed on $(hostname): $1\u0026#34; | mail -s \u0026#34;Backup Error\u0026#34; \u0026#34;$EMAIL\u0026#34; exit 1 } # Cleanup function (runs on exit) cleanup() { if [ $? -ne 0 ]; then log \u0026#34;Script exited with error\u0026#34; fi } trap cleanup EXIT # Main script log \u0026#34;Starting backup\u0026#34; # Check if source exists [ -d \u0026#34;$BACKUP_SOURCE\u0026#34; ] || error_exit \u0026#34;Backup source $BACKUP_SOURCE not found\u0026#34; # Check if destination is writable [ -w \u0026#34;$BACKUP_DEST\u0026#34; ] || error_exit \u0026#34;Cannot write to $BACKUP_DEST\u0026#34; # Check available disk space (need at least 10GB) AVAILABLE=$(df \u0026#34;$BACKUP_DEST\u0026#34; | tail -1 | awk \u0026#39;{print $4}\u0026#39;) [ $AVAILABLE -gt 10485760 ] || error_exit \u0026#34;Not enough disk space\u0026#34; # Create backup DATE=$(date +%Y%m%d_%H%M%S) BACKUP_FILE=\u0026#34;${BACKUP_DEST}/backup_${DATE}.tar.gz\u0026#34; if tar -czf \u0026#34;$BACKUP_FILE\u0026#34; \u0026#34;$BACKUP_SOURCE\u0026#34; 2\u0026gt;\u0026gt; \u0026#34;$LOG_FILE\u0026#34;; then SIZE=$(du -h \u0026#34;$BACKUP_FILE\u0026#34; | cut -f1) log \u0026#34;Backup successful: ${BACKUP_FILE} (${SIZE})\u0026#34; # Verify backup integrity if tar -tzf \u0026#34;$BACKUP_FILE\u0026#34; \u0026gt; /dev/null 2\u0026gt;\u0026amp;1; then log \u0026#34;Backup integrity verified\u0026#34; else error_exit \u0026#34;Backup verification failed\u0026#34; fi # Cleanup old backups DELETED=$(find \u0026#34;$BACKUP_DEST\u0026#34; -name \u0026#34;backup_*.tar.gz\u0026#34; -mtime +7 -delete -print | wc -l) log \u0026#34;Deleted $DELETED old backups\u0026#34; # Send success notification echo \u0026#34;Backup completed successfully: ${SIZE}\u0026#34; | mail -s \u0026#34;Backup Success\u0026#34; \u0026#34;$EMAIL\u0026#34; else error_exit \u0026#34;Backup creation failed\u0026#34; fi log \u0026#34;Backup completed successfully\u0026#34; Key error handling techniques:\nSet strict mode:\nset -e # Exit on any error set -u # Exit on undefined variable set -o pipefail # Exit if any pipe command fails Check prerequisites:\n# Check if command exists command -v mysql \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 || { echo \u0026#34;mysql not found\u0026#34;; exit 1; } # Check if file exists [ -f /path/to/file ] || { echo \u0026#34;File not found\u0026#34;; exit 1; } # Check if directory is writable [ -w /path/to/dir ] || { echo \u0026#34;Directory not writable\u0026#34;; exit 1; } Capture command exit status:\nif ! mysqldump database \u0026gt; backup.sql; then echo \u0026#34;Mysqldump failed\u0026#34; exit 1 fi # Or check $? after command mysqldump database \u0026gt; backup.sql if [ $? -ne 0 ]; then echo \u0026#34;Mysqldump failed\u0026#34; exit 1 fi Use trap for cleanup:\ncleanup() { rm -f /tmp/tempfile echo \u0026#34;Cleaned up\u0026#34; } trap cleanup EXIT Send email notifications Install mail utilities:\n# Ubuntu/Debian sudo apt install mailutils # Configure mail server or use external SMTP Send simple email from script:\necho \u0026#34;This is the message body\u0026#34; | mail -s \u0026#34;Subject Line\u0026#34; user@example.com Send file as attachment:\necho \u0026#34;See attached log\u0026#34; | mail -s \u0026#34;Daily Report\u0026#34; -A /var/log/report.log user@example.com HTML email:\n{ echo \u0026#34;To: admin@example.com\u0026#34; echo \u0026#34;Subject: Server Report\u0026#34; echo \u0026#34;Content-Type: text/html\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;h1\u0026gt;Server Status Report\u0026lt;/h1\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;p\u0026gt;Disk usage: $(df -h / | tail -1 | awk \u0026#39;{print $5}\u0026#39;)\u0026lt;/p\u0026gt;\u0026#34; echo \u0026#34;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; } | sendmail -t Send to multiple recipients:\necho \u0026#34;Alert message\u0026#34; | mail -s \u0026#34;Alert\u0026#34; user1@example.com,user2@example.com Advanced cron techniques Run job every X minutes:\n# Every 5 minutes */5 * * * * /path/to/script.sh # Every 30 minutes */30 * * * * /path/to/script.sh # Every 2 hours 0 */2 * * * /path/to/script.sh Run during specific hours:\n# Every 15 minutes from 9 AM to 5 PM */15 9-17 * * * /path/to/script.sh # Every hour from 9 PM to 6 AM 0 21-23,0-6 * * * /path/to/script.sh Run on specific days:\n# Weekdays only (Monday-Friday) 0 9 * * 1-5 /path/to/script.sh # Weekends only 0 10 * * 6,7 /path/to/script.sh # First day of month 0 2 1 * * /path/to/script.sh # Last day of month (runs on 28th-31st, script checks if tomorrow is next month) 0 2 28-31 * * [ $(date -d tomorrow +\\%d) -eq 1 ] \u0026amp;\u0026amp; /path/to/script.sh Prevent overlapping jobs:\nCreate a lock file:\n#!/bin/bash LOCKFILE=/var/lock/myscript.lock # Check if already running if [ -f \u0026#34;$LOCKFILE\u0026#34; ]; then echo \u0026#34;Script already running\u0026#34; exit 1 fi # Create lock file touch \u0026#34;$LOCKFILE\u0026#34; # Remove lock on exit trap \u0026#34;rm -f $LOCKFILE\u0026#34; EXIT # Your script here sleep 60 echo \u0026#34;Job completed\u0026#34; Random delays:\nAvoid all servers hitting an API at the exact same time:\n# Sleep random time between 0-300 seconds (5 minutes) sleep $((RANDOM % 300)) # Then run the actual command Redirect output:\n# Discard all output * * * * * /path/to/script.sh \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 # Save output to log * * * * * /path/to/script.sh \u0026gt;\u0026gt; /var/log/cron.log 2\u0026gt;\u0026amp;1 # Email only errors (stdout discarded, stderr emailed) * * * * * /path/to/script.sh \u0026gt; /dev/null System cron directories Instead of crontab syntax, drop executable scripts in these directories:\n/etc/cron.hourly/ # Runs every hour /etc/cron.daily/ # Runs daily /etc/cron.weekly/ # Runs weekly /etc/cron.monthly/ # Runs monthly Example daily backup script:\nsudo nano /etc/cron.daily/backup #!/bin/bash tar -czf /backups/daily-$(date +%Y%m%d).tar.gz /var/www Make executable:\nsudo chmod +x /etc/cron.daily/backup Scripts in these directories run at times defined in /etc/crontab.\nDebugging cron jobs Check cron logs:\n# Ubuntu/Debian grep CRON /var/log/syslog # RHEL/CentOS grep CRON /var/log/cron # See only today\u0026#39;s cron activity grep CRON /var/log/syslog | grep \u0026#34;$(date \u0026#39;+%b %e\u0026#39;)\u0026#34; Check mail:\nCron emails output to the user. Check mail:\nmail Test the command:\nRun the exact command from crontab manually:\n/usr/local/bin/backup.sh Add debugging:\nAdd to script:\n#!/bin/bash set -x # Print each command before executing # Or redirect to debug log exec 2\u0026gt;/tmp/script-debug.log set -x Check environment:\nCron runs with minimal environment. Print variables:\n* * * * * env \u0026gt; /tmp/cron-env.txt Compare with your shell environment:\nenv \u0026gt; /tmp/shell-env.txt diff /tmp/cron-env.txt /tmp/shell-env.txt Common issues:\nPath problems - use absolute paths:\n# Wrong * * * * * backup.sh # Correct * * * * * /usr/local/bin/backup.sh Missing environment - set in crontab:\nPATH=/usr/local/bin:/usr/bin:/bin SHELL=/bin/bash Permissions - make script executable:\nchmod +x /path/to/script.sh Tips for production automation Always log:\nexec \u0026gt;\u0026gt; /var/log/myscript.log 2\u0026gt;\u0026amp;1 echo \u0026#34;[$(date)] Script started\u0026#34; Handle errors:\nset -euo pipefail Use absolute paths:\n/usr/bin/mysql instead of mysql /var/www/app instead of ../app Test before scheduling: Run manually multiple times before adding to cron.\nAdd monitoring: Send notifications on failure.\nPrevent overlaps: Use lock files for long-running jobs.\nClean up after yourself: Delete temporary files, close connections.\nDocument your cron jobs: Add comments explaining what each job does:\n# Database backup - runs at 2 AM daily 0 2 * * * /usr/local/bin/backup-mysql.sh # Disk space check - every hour 0 * * * * /usr/local/bin/check-disk.sh Rotate and limit logs: Don\u0026rsquo;t let log files grow forever.\nVersion control: Keep scripts in git, deploy to /usr/local/bin/.\nUse configuration files: Don\u0026rsquo;t hardcode credentials in scripts:\n# Load config source /etc/myapp/config.sh # Use variables from config mysql -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; Command reference Crontab commands:\ncrontab -e # Edit crontab crontab -l # List cron jobs crontab -r # Remove all jobs crontab -u user -e # Edit user\u0026#39;s crontab Common schedules:\n@reboot # At startup @hourly # Every hour @daily # Every day at midnight @weekly # Every Sunday at midnight @monthly # First day of month at midnight */5 * * * * # Every 5 minutes 0 * * * * # Every hour 0 0 * * * # Every day at midnight 0 2 * * * # Every day at 2 AM 0 0 * * 0 # Every Sunday at midnight 0 0 1 * * # First of every month Script template:\n#!/bin/bash set -euo pipefail SCRIPT_NAME=$(basename \u0026#34;$0\u0026#34;) LOG_FILE=\u0026#34;/var/log/${SCRIPT_NAME}.log\u0026#34; log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } log \u0026#34;Script started\u0026#34; # Your code here log \u0026#34;Script completed\u0026#34; Wrapping up Automation saves time and reduces errors. Cron schedules tasks, shell scripts do the work. Write scripts that handle errors, log activity, and send notifications.\nStart with simple tasks like backups and monitoring. Test thoroughly before adding to cron. Use absolute paths, check exit codes, and always log what happens.\nFor critical automation, consider systemd timers as an alternative to cron. They offer better logging, dependency management, and recovery from missed runs.\nAutomate routine tasks and focus your time on actual problems instead of repetitive maintenance. Good automation runs quietly until something breaks, then your logs and alerts tell you what happened.\nFor more Linux automation, check out the systemd service management guide and firewall configuration article.\n","href":"/2025/10/how-to-automate-tasks-cron-jobs-shell-scripts-linux.html","title":"How to Automate Tasks with Cron Jobs and Shell Scripts on Linux"},{"content":"Managing services is fundamental to Linux server administration. Every web server, database, application, or background process runs as a service that needs to start, stop, restart, and recover from failures.\nSystemd replaced older init systems and is now the standard on virtually all major Linux distributions. If you work with Linux servers, you need to know systemd and systemctl inside out.\nThis guide covers everything from basic service management to creating custom services, using timer units instead of cron, troubleshooting failures, and analyzing logs with journalctl.\nWhat is systemd and why it matters Systemd is the init system that starts as the first process (PID 1) when Linux boots. It manages all other processes on your system.\nBefore systemd, most distributions used SysVinit, which started services sequentially using shell scripts. Systemd starts services in parallel, handling dependencies automatically. Your system boots faster and services are more reliable.\nSystemd isn\u0026rsquo;t just an init system. It includes:\nService management (systemctl) Log management (journalctl) Timer units (cron replacement) Socket activation Resource limiting (cgroups) Network management (systemd-networkd) Login management (systemd-logind) Check your systemd version:\nsystemd --version You\u0026rsquo;ll see something like systemd 249 (249.11-ubuntu3). Different versions support different features, but core functionality is consistent.\nBasic systemctl commands Systemctl is your main tool for managing services. Here are the essential commands you\u0026rsquo;ll use constantly:\n# Start a service (runs immediately, won\u0026#39;t persist after reboot) sudo systemctl start nginx # Stop a service sudo systemctl stop nginx # Restart a service (stop then start) sudo systemctl restart nginx # Reload service configuration without stopping sudo systemctl reload nginx # Enable service to start at boot sudo systemctl enable nginx # Disable service from starting at boot sudo systemctl disable nginx # Enable and start in one command sudo systemctl enable --now nginx # Disable and stop in one command sudo systemctl disable --now nginx # Check if service is running systemctl status nginx # Check if service is enabled at boot systemctl is-enabled nginx # Check if service is currently active systemctl is-active nginx The --now flag is useful because you often want to both enable at boot AND start immediately.\nUnderstanding service status The status command shows detailed service information:\nsystemctl status nginx Output looks like:\n● nginx.service - A high performance web server and a reverse proxy server Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2025-10-23 09:15:32 UTC; 2 days ago Docs: man:nginx(8) Main PID: 1234 (nginx) Tasks: 5 (limit: 4915) Memory: 12.3M CPU: 1.234s CGroup: /system.slice/nginx.service ├─1234 nginx: master process /usr/sbin/nginx -g daemon off; └─1235 nginx: worker process Oct 23 09:15:32 server systemd[1]: Starting nginx.service... Oct 23 09:15:32 server systemd[1]: Started nginx.service. Key information:\nLoaded: Where unit file is located and if it\u0026rsquo;s enabled Active: Current state - active (running), inactive (dead), or failed Main PID: Process ID of the main service process Tasks: Number of processes/threads Memory/CPU: Resource usage CGroup: Process hierarchy Recent logs: Last few log entries Active states you\u0026rsquo;ll see:\nactive (running): Service is running active (exited): Service completed successfully (one-shot services) active (waiting): Service is waiting for an event inactive (dead): Service is stopped failed: Service crashed or failed to start activating: Service is starting up deactivating: Service is shutting down List all services See all services on your system:\n# List all loaded services systemctl list-units --type=service # List all services including inactive ones systemctl list-units --type=service --all # Show only running services systemctl list-units --type=service --state=running # Show failed services systemctl list-units --type=service --state=failed # List enabled services systemctl list-unit-files --type=service --state=enabled # List disabled services systemctl list-unit-files --type=service --state=disabled Filter output with grep:\n# Find all running web servers systemctl list-units --type=service --state=running | grep -E \u0026#39;nginx|apache|httpd\u0026#39; # Find database services systemctl list-units --type=service | grep -E \u0026#39;mysql|postgres|mongo|redis\u0026#39; View and analyze service logs Systemd includes journald for centralized logging. Use journalctl to view logs:\n# View logs for a specific service journalctl -u nginx # View only today\u0026#39;s logs journalctl -u nginx --since today # Last 50 lines journalctl -u nginx -n 50 # Follow logs in real-time (like tail -f) journalctl -u nginx -f # Logs from the last hour journalctl -u nginx --since \u0026#34;1 hour ago\u0026#34; # Logs between specific times journalctl -u nginx --since \u0026#34;2025-10-26 09:00:00\u0026#34; --until \u0026#34;2025-10-26 10:00:00\u0026#34; # Show logs with priority level (error and above) journalctl -u nginx -p err # Show kernel messages journalctl -k # Show logs from current boot journalctl -b # Show logs from previous boot journalctl -b -1 # Show logs in reverse (newest first) journalctl -u nginx -r # Export logs to file journalctl -u nginx \u0026gt; nginx-logs.txt Priority levels from highest to lowest:\n0: emerg (system unusable) 1: alert (action must be taken) 2: crit (critical conditions) 3: err (error conditions) 4: warning 5: notice 6: info 7: debug Check journal disk usage:\njournalctl --disk-usage Clean old logs:\n# Keep only last 2 days sudo journalctl --vacuum-time=2d # Keep only 500MB sudo journalctl --vacuum-size=500M # Keep only last 1000 entries per journal sudo journalctl --vacuum-files=10 Create a custom systemd service Let\u0026rsquo;s create a service for a Node.js application. This same pattern works for Python, Go, or any application.\nFirst, create your app. Example Node.js server:\nsudo mkdir -p /var/www/myapp sudo nano /var/www/myapp/server.js Simple server:\nconst http = require(\u0026#39;http\u0026#39;); const server = http.createServer((req, res) =\u0026gt; { res.writeHead(200, {\u0026#39;Content-Type\u0026#39;: \u0026#39;text/plain\u0026#39;}); res.end(\u0026#39;Hello from systemd service!\\n\u0026#39;); }); server.listen(3000, () =\u0026gt; { console.log(\u0026#39;Server running on port 3000\u0026#39;); }); Create a systemd service file:\nsudo nano /etc/systemd/system/myapp.service Basic service file:\n[Unit] Description=My Node.js Application After=network.target [Service] Type=simple User=www-data WorkingDirectory=/var/www/myapp ExecStart=/usr/bin/node /var/www/myapp/server.js Restart=always RestartSec=10 StandardOutput=journal StandardError=journal SyslogIdentifier=myapp [Install] WantedBy=multi-user.target Explanation:\n[Unit] section:\nDescription: Human-readable description After: Start after network is available Requires: Hard dependency (service fails if dependency fails) Wants: Soft dependency (service starts even if dependency fails) [Service] section:\nType=simple: Process doesn\u0026rsquo;t fork (default) Type=forking: Process forks to background (old daemons) Type=oneshot: Process exits after completion User: Run as this user (not root for security) WorkingDirectory: Current directory for the process ExecStart: Command to start service ExecStop: Command to stop (optional, systemd sends SIGTERM by default) Restart=always: Restart if crashes Restart=on-failure: Restart only on failure RestartSec: Wait before restarting StandardOutput=journal: Send stdout to journal StandardError=journal: Send stderr to journal [Install] section:\nWantedBy=multi-user.target: Enable at normal multi-user boot Set permissions:\nsudo chown -R www-data:www-data /var/www/myapp Load and start the service:\n# Reload systemd to recognize new service sudo systemctl daemon-reload # Start service sudo systemctl start myapp # Check status sudo systemctl status myapp # Enable at boot sudo systemctl enable myapp # View logs journalctl -u myapp -f Test the service:\ncurl http://localhost:3000 Should return \u0026ldquo;Hello from systemd service!\u0026rdquo;.\nTest automatic restart by killing the process:\n# Find PID systemctl status myapp # Kill it sudo kill -9 [PID] # Check status - should restart automatically systemctl status myapp Advanced service file options Add resource limits and security:\n[Unit] Description=My Node.js Application After=network.target Wants=redis.service After=redis.service [Service] Type=simple User=www-data Group=www-data WorkingDirectory=/var/www/myapp Environment=\u0026#34;NODE_ENV=production\u0026#34; Environment=\u0026#34;PORT=3000\u0026#34; EnvironmentFile=/etc/myapp/config ExecStart=/usr/bin/node /var/www/myapp/server.js ExecReload=/bin/kill -HUP $MAINPID ExecStop=/bin/kill -TERM $MAINPID Restart=always RestartSec=10 StartLimitInterval=100 StartLimitBurst=5 # Resource limits CPUQuota=50% MemoryLimit=1G MemoryHigh=800M TasksMax=100 # Security PrivateTmp=true NoNewPrivileges=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/var/www/myapp/uploads # Logging StandardOutput=journal StandardError=journal SyslogIdentifier=myapp [Install] WantedBy=multi-user.target New options explained:\nDependencies:\nWants=redis.service: Start after Redis if available Multiple After= lines define order Environment:\nEnvironment: Set environment variables inline EnvironmentFile: Load variables from file Lifecycle:\nExecReload: Command for reload action ExecStop: Custom stop command $MAINPID: Variable containing main process PID Restart limits:\nStartLimitInterval: Time window for restart limit StartLimitBurst: Max restarts in that window Resource controls:\nCPUQuota=50%: Limit to 50% of one CPU core MemoryLimit=1G: Hard limit (kills process if exceeded) MemoryHigh=800M: Soft limit (triggers swapping) TasksMax=100: Limit processes/threads Security:\nPrivateTmp=true: Service gets private /tmp NoNewPrivileges=true: Can\u0026rsquo;t gain new privileges ProtectSystem=strict: Makes most of filesystem read-only ProtectHome=true: Makes /home inaccessible ReadWritePaths: Exceptions to read-only filesystem Create environment file:\nsudo mkdir /etc/myapp sudo nano /etc/myapp/config Add variables:\nNODE_ENV=production PORT=3000 DATABASE_URL=postgresql://user:pass@localhost/mydb REDIS_URL=redis://localhost:6379 After editing service file:\nsudo systemctl daemon-reload sudo systemctl restart myapp Create systemd timer units Timer units replace cron jobs. They\u0026rsquo;re more reliable, easier to debug, and integrate better with systemd.\nExample: Backup script that runs daily at 2 AM.\nCreate the service that does the work:\nsudo nano /etc/systemd/system/backup.service [Unit] Description=Daily backup job Wants=network-online.target After=network-online.target [Service] Type=oneshot User=backup ExecStart=/usr/local/bin/backup.sh StandardOutput=journal StandardError=journal Create the timer that triggers it:\nsudo nano /etc/systemd/system/backup.timer [Unit] Description=Run backup daily at 2 AM Requires=backup.service [Timer] OnCalendar=daily OnCalendar=*-*-* 02:00:00 Persistent=true Unit=backup.service [Install] WantedBy=timers.target Timer options:\nOnCalendar examples:\n# Every minute OnCalendar=*:*:00 # Every 5 minutes OnCalendar=*:0/5 # Every hour OnCalendar=hourly OnCalendar=*:00:00 # Every day at 3:30 AM OnCalendar=daily OnCalendar=*-*-* 03:30:00 # Every Monday at 9 AM OnCalendar=Mon *-*-* 09:00:00 # First day of month at midnight OnCalendar=*-*-01 00:00:00 # Weekdays at 6 PM OnCalendar=Mon..Fri *-*-* 18:00:00 Other timer options:\nOnBootSec=15min: Run 15 minutes after boot OnUnitActiveSec=1h: Run 1 hour after service last ran OnUnitInactiveSec=30min: Run 30 min after service became inactive Persistent=true: Run missed jobs after system was off AccuracySec=1min: Allow up to 1 minute timing variation (saves power) Enable and start timer:\nsudo systemctl daemon-reload sudo systemctl enable backup.timer sudo systemctl start backup.timer Check timer status:\n# See timer status systemctl status backup.timer # List all active timers systemctl list-timers # List all timers including inactive systemctl list-timers --all # Show detailed timer info systemctl show backup.timer View when timer last ran and next run:\nsystemctl list-timers backup.timer Output shows:\nNEXT LEFT LAST PASSED UNIT ACTIVATES Thu 2025-10-26 02:00:00 UTC 4h 23min Wed 2025-10-25 02:00:00 UTC 19h ago backup.timer backup.service Test timer manually:\n# Trigger service immediately sudo systemctl start backup.service # Check logs journalctl -u backup.service -n 20 Service dependencies and ordering Control how services start relative to each other.\nExample: Web app requires database and Redis:\n[Unit] Description=Web Application After=network.target postgresql.service redis.service Requires=postgresql.service Wants=redis.service [Service] Type=simple ExecStart=/usr/bin/myapp [Install] WantedBy=multi-user.target Dependency types:\nRequires:\nHard dependency If postgresql.service fails, webapp fails too If you stop postgresql, webapp stops Wants:\nSoft dependency Webapp starts even if redis fails Stopping redis doesn\u0026rsquo;t stop webapp Use this for optional dependencies Requisite:\nLike Requires but checks immediately Fails instantly if dependency isn\u0026rsquo;t already running BindsTo:\nStronger than Requires Service stops when dependency stops PartOf:\nStopping/restarting dependency affects this service Used for multi-part services Ordering (After/Before):\nAfter=redis.service: Start after Redis Before=nginx.service: Start before nginx Doesn\u0026rsquo;t create dependency, just ordering Example multi-tier application:\n# Database (starts first) [Unit] Description=PostgreSQL Database Before=webapp.service # Redis cache (starts before webapp) [Unit] Description=Redis Cache Before=webapp.service # Web application (starts after database and Redis) [Unit] Description=Web Application After=postgresql.service redis.service Requires=postgresql.service Wants=redis.service Before=nginx.service # Nginx (starts last) [Unit] Description=Nginx Reverse Proxy After=webapp.service Wants=webapp.service This ensures proper startup order: PostgreSQL -\u0026gt; Redis -\u0026gt; Webapp -\u0026gt; Nginx.\nTroubleshoot failed services When a service fails, follow this debugging process:\nStep 1: Check status\nsystemctl status servicename Look for error messages in the output.\nStep 2: View recent logs\njournalctl -u servicename -n 50 Check the last 50 log entries for errors.\nStep 3: View all logs from latest attempt\njournalctl -u servicename --since \u0026#34;5 minutes ago\u0026#34; Step 4: Examine the unit file\nsystemctl cat servicename Verify:\nExecStart path is correct WorkingDirectory exists User has permissions Dependencies are correct Step 5: Test the command manually\nRun the ExecStart command as the specified user:\n# Switch to service user sudo -u www-data bash # Run the command /usr/bin/node /var/www/myapp/server.js See actual error messages.\nStep 6: Check file permissions\nls -la /var/www/myapp Service user needs read/execute on files and directories.\nStep 7: Verify dependencies\nsystemctl list-dependencies servicename Check if required services are running.\nCommon failure causes:\nPermission denied:\n# Fix ownership sudo chown -R www-data:www-data /var/www/myapp # Fix permissions sudo chmod 755 /var/www/myapp sudo chmod 644 /var/www/myapp/server.js Port already in use:\n# Find what\u0026#39;s using the port sudo lsof -i :3000 sudo netstat -tulpn | grep 3000 # Kill the process or change your app\u0026#39;s port Wrong file path:\n# Verify file exists ls -la /usr/bin/node which node # Update ExecStart with correct path Missing dependencies:\n# For Node.js apps cd /var/www/myapp npm install # For Python apps pip install -r requirements.txt Service starts then immediately exits:\nAdd to service file:\n[Service] RemainAfterExit=yes Or check why process exits (look at logs).\nEnable debug logging:\n[Service] Environment=\u0026#34;DEBUG=*\u0026#34; Environment=\u0026#34;LOG_LEVEL=debug\u0026#34; Analyze core dumps if service crashed:\n# Enable core dumps sudo systemctl edit servicename Add:\n[Service] LimitCORE=infinity After crash:\ncoredumpctl list coredumpctl info coredumpctl dump \u0026gt; core.dump Reload systemd and reset failed services After editing unit files:\n# Reload systemd configuration sudo systemctl daemon-reload Always run this after creating or editing .service or .timer files.\nReset failed state:\n# Reset one service sudo systemctl reset-failed servicename # Reset all failed services sudo systemctl reset-failed This clears the \u0026ldquo;failed\u0026rdquo; state so you can restart the service.\nOverride default service settings Don\u0026rsquo;t edit system unit files directly. Use overrides:\nsudo systemctl edit nginx This creates /etc/systemd/system/nginx.service.d/override.conf.\nAdd your changes:\n[Service] Restart=always RestartSec=10 CPUQuota=50% Save and exit. Changes apply automatically.\nTo edit the full unit file:\nsudo systemctl edit --full nginx This copies the unit file to /etc/systemd/system/nginx.service where you can edit it.\nView active configuration with overrides applied:\nsystemctl cat nginx Remove overrides:\nsudo systemctl revert nginx Mask and unmask services Masking prevents a service from starting:\n# Mask service (can\u0026#39;t be started manually or automatically) sudo systemctl mask servicename # Unmask to allow starting again sudo systemctl unmask servicename Masked services show as:\nLoaded: masked (Reason: Unit servicename.service is masked.) Use masking to prevent services from starting after package updates.\nSystem targets (runlevels) Targets group services that should run together. Like old runlevels.\nCommon targets:\n# List all targets systemctl list-units --type=target # Check default target systemctl get-default # Set default target sudo systemctl set-default multi-user.target Main targets:\npoweroff.target: Shutdown system rescue.target: Single user mode multi-user.target: Multi-user, no GUI graphical.target: Multi-user with GUI reboot.target: Reboot system Boot to different target:\n# Boot to rescue mode (like single user mode) sudo systemctl isolate rescue.target # Boot to multi-user (text mode) sudo systemctl isolate multi-user.target # Boot to graphical mode sudo systemctl isolate graphical.target Set default boot target:\n# Text mode by default sudo systemctl set-default multi-user.target # GUI by default sudo systemctl set-default graphical.target Monitor resource usage See current resource usage per service:\n# Top-like view of services systemd-cgtop # Resource usage for specific service systemctl status servicename systemd-cgtop shows:\nCPU usage Memory usage I/O operations Tasks (processes/threads) Press:\nc to sort by CPU m to sort by memory t to sort by tasks q to quit Set resource limits in unit files (covered earlier):\n[Service] CPUQuota=25% MemoryMax=512M TasksMax=50 IOWeight=500 Check current limits:\nsystemctl show servicename -p CPUQuota -p MemoryMax -p TasksMax Command reference # Service management systemctl start service systemctl stop service systemctl restart service systemctl reload service systemctl enable service systemctl disable service systemctl enable --now service systemctl status service systemctl is-enabled service systemctl is-active service # Logs journalctl -u service journalctl -u service -f journalctl -u service --since today journalctl -u service -n 50 journalctl -b journalctl -p err # List services systemctl list-units --type=service systemctl list-units --type=service --state=failed systemctl list-timers # Configuration systemctl daemon-reload systemctl edit service systemctl cat service systemctl show service # Troubleshooting systemctl reset-failed systemctl list-dependencies service systemd-cgtop systemctl status -l service # System control systemctl reboot systemctl poweroff systemctl suspend systemctl hibernate systemctl get-default systemctl set-default multi-user.target Wrapping up Systemd manages all services on modern Linux systems. Use systemctl to start, stop, enable, and monitor services. View logs with journalctl for troubleshooting.\nCreate custom services by writing .service files in /etc/systemd/system/. Define how to start the service, which user runs it, restart policies, and resource limits.\nReplace cron jobs with timer units for better logging and reliability. Timers trigger services on schedules.\nDebug failed services by checking status, viewing logs, examining unit files, and testing commands manually. Common issues are permissions, wrong paths, and port conflicts.\nSystemd gives you complete control over services, logging, and resource management in one consistent interface. Master these commands and you can manage any Linux server.\nFor more server management, check out the SSH security guide, firewall configuration, and cron automation articles.\n","href":"/2025/10/how-to-manage-linux-services-systemd-systemctl-guide.html","title":"How to Manage Linux Services with Systemd - Complete SystemCTL Guide"},{"content":"Firewalls control what network traffic can reach your Linux server. Without a firewall, every service you run is exposed to the internet. Attackers scan for open ports and exploit vulnerable services. A firewall blocks unwanted traffic while allowing legitimate connections.\nThis guide covers firewall management on Linux using UFW (Ubuntu/Debian) and Firewalld (RHEL/CentOS). You\u0026rsquo;ll learn how to allow and deny ports, manage application profiles, restrict access by IP address, configure zones, set up port forwarding, and troubleshoot common issues.\nWhy firewalls matter for Linux servers Every network service binds to a port. SSH on 22, HTTP on 80, MySQL on 3306. If these ports are open to the internet, attackers can access them.\nA firewall blocks ports by default and you explicitly allow only what\u0026rsquo;s needed. This reduces your attack surface dramatically.\nReal example: You install MySQL for a web app. Without a firewall, MySQL listens on 0.0.0.0:3306 (all interfaces). Anyone can try to connect. With a firewall, you allow port 3306 only from localhost or your application server IP. External access blocked.\nFirewalls also stop port scans, DDoS amplification attacks, and block known malicious IPs.\nUFW vs Firewalld: Which to use UFW is default on Ubuntu, Debian, and derivatives. Simple syntax, perfect for basic setups.\nFirewalld is default on RHEL, CentOS, Fedora, AlmaLinux, Rocky Linux. More features, zone-based, suited for enterprise.\nBoth use nftables or iptables as backend. Both work well. Use whichever comes with your distro.\nThis guide covers both. Skip to the section for your system.\nUFW Firewall (Ubuntu/Debian) Install UFW UFW comes pre-installed on Ubuntu. If missing:\nsudo apt update sudo apt install ufw -y Check status:\nsudo ufw status Should show \u0026ldquo;Status: inactive\u0026rdquo; initially.\nSet default policies Default deny incoming, allow outgoing:\nsudo ufw default deny incoming sudo ufw default allow outgoing This blocks all incoming connections unless explicitly allowed. Outgoing connections (your server connecting to update servers, APIs, etc.) are allowed.\nAllow SSH before enabling firewall Critical: Allow SSH before enabling UFW or you\u0026rsquo;ll lock yourself out of remote servers.\nsudo ufw allow ssh # Or specific port if you changed it sudo ufw allow 2222/tcp Enable UFW sudo ufw enable Confirm with \u0026ldquo;y\u0026rdquo;. UFW is now active.\nCheck status:\nsudo ufw status verbose Shows:\nStatus: active Logging: on (low) Default: deny (incoming), allow (outgoing), disabled (routed) To Action From -- ------ ---- 22/tcp ALLOW IN Anywhere Allow ports and services Allow specific ports:\n# Allow port 80 (HTTP) sudo ufw allow 80/tcp # Allow port 443 (HTTPS) sudo ufw allow 443/tcp # Allow UDP port sudo ufw allow 53/udp # Allow port range sudo ufw allow 6000:6007/tcp Allow by service name:\nsudo ufw allow http sudo ufw allow https sudo ufw allow \u0026#39;Nginx Full\u0026#39; # HTTP + HTTPS sudo ufw allow \u0026#39;OpenSSH\u0026#39; Service names come from /etc/services and application profiles in /etc/ufw/applications.d/.\nDeny specific ports sudo ufw deny 3306/tcp # Block MySQL from external access sudo ufw deny 5432/tcp # Block PostgreSQL Deny takes precedence if there\u0026rsquo;s a conflict.\nAllow from specific IP addresses Allow SSH only from your office IP:\nsudo ufw allow from 203.0.113.50 to any port 22 Allow MySQL only from application server:\nsudo ufw allow from 10.0.0.5 to any port 3306 Allow entire subnet:\nsudo ufw allow from 192.168.1.0/24 to any port 22 Deny specific IPs Block an abusive IP:\nsudo ufw deny from 198.51.100.50 Block entire subnet:\nsudo ufw deny from 198.51.100.0/24 Delete rules List rules with numbers:\nsudo ufw status numbered Output:\nTo Action From -- ------ ---- [ 1] 22/tcp ALLOW IN Anywhere [ 2] 80/tcp ALLOW IN Anywhere [ 3] 443/tcp ALLOW IN Anywhere Delete by number:\nsudo ufw delete 2 # Deletes rule 2 (port 80) Or delete by rule specification:\nsudo ufw delete allow 80/tcp Rate limiting (prevent brute-force) Limit SSH connections (max 6 attempts in 30 seconds):\nsudo ufw limit ssh This is similar to fail2ban but built into UFW. Useful for SSH protection.\nAllow specific interfaces Only allow connections on specific network interface:\nsudo ufw allow in on eth0 to any port 80 sudo ufw allow in on eth1 to any port 3306 # Database on private network UFW application profiles List available profiles:\nsudo ufw app list Common profiles:\nNginx Full (80, 443) Nginx HTTP (80) Nginx HTTPS (443) OpenSSH (22) Apache Full (80, 443) Allow profile:\nsudo ufw allow \u0026#39;Nginx Full\u0026#39; View profile details:\nsudo ufw app info \u0026#39;Nginx Full\u0026#39; Create custom profile in /etc/ufw/applications.d/myapp:\n[MyApp] title=My Application description=Custom app using port 8080 ports=8080/tcp Update app list and allow:\nsudo ufw app update MyApp sudo ufw allow MyApp UFW logging Enable logging:\nsudo ufw logging on Log levels: off, low, medium, high, full.\nsudo ufw logging medium View logs:\nsudo tail -f /var/log/ufw.log Logs show blocked connections, allowed connections, and rule matches.\nDisable and reset UFW Disable temporarily:\nsudo ufw disable Re-enable:\nsudo ufw enable Reset to factory defaults (deletes all rules):\nsudo ufw reset Firewalld (CentOS/RHEL/Fedora) Install Firewalld Firewalld comes pre-installed on RHEL/CentOS. If missing:\nsudo dnf install firewalld -y # RHEL 8+, Fedora sudo yum install firewalld -y # CentOS 7 Start and enable:\nsudo systemctl start firewalld sudo systemctl enable firewalld Check status:\nsudo firewall-cmd --state Should return \u0026ldquo;running\u0026rdquo;.\nUnderstand zones Firewalld uses zones to define trust levels. Default zone is \u0026ldquo;public\u0026rdquo;.\nList zones:\nsudo firewall-cmd --get-zones Common zones:\ndrop: Drop all incoming, no reply block: Reject all incoming with icmp-host-prohibited public: Default, low trust (for internet-facing interfaces) external: For masquerading/NAT dmz: Isolated servers work: Medium trust home: Higher trust trusted: Full access, all traffic allowed Check active zones:\nsudo firewall-cmd --get-active-zones Check default zone:\nsudo firewall-cmd --get-default-zone Set default zone:\nsudo firewall-cmd --set-default-zone=public View current rules List all rules in default zone:\nsudo firewall-cmd --list-all List specific zone:\nsudo firewall-cmd --zone=public --list-all Allow services Allow HTTP:\nsudo firewall-cmd --zone=public --add-service=http Allow HTTPS:\nsudo firewall-cmd --zone=public --add-service=https Common services: ssh, http, https, mysql, postgresql, smtp, dns, ftp.\nList available services:\nsudo firewall-cmd --get-services Make rules permanent By default, Firewalld rules are temporary (lost on restart). Add --permanent:\nsudo firewall-cmd --permanent --zone=public --add-service=http Reload to apply permanent rules:\nsudo firewall-cmd --reload Or add to runtime and save:\nsudo firewall-cmd --zone=public --add-service=http sudo firewall-cmd --runtime-to-permanent Allow ports Allow specific port:\nsudo firewall-cmd --zone=public --add-port=8080/tcp --permanent sudo firewall-cmd --reload Allow port range:\nsudo firewall-cmd --zone=public --add-port=5000-5010/tcp --permanent sudo firewall-cmd --reload Allow UDP:\nsudo firewall-cmd --zone=public --add-port=53/udp --permanent sudo firewall-cmd --reload Remove services and ports sudo firewall-cmd --zone=public --remove-service=http --permanent sudo firewall-cmd --zone=public --remove-port=8080/tcp --permanent sudo firewall-cmd --reload Rich rules (advanced filtering) Allow SSH only from specific IP:\nsudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;203.0.113.50\u0026#34; port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;22\u0026#34; accept\u0026#39; sudo firewall-cmd --reload Block specific IP:\nsudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;198.51.100.50\u0026#34; reject\u0026#39; sudo firewall-cmd --reload Allow port range from subnet:\nsudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;10.0.0.0/24\u0026#34; port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;3000-3999\u0026#34; accept\u0026#39; sudo firewall-cmd --reload Rate limit SSH (max 5 connections per minute):\nsudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule service name=\u0026#34;ssh\u0026#34; limit value=\u0026#34;5/m\u0026#34; accept\u0026#39; sudo firewall-cmd --reload List rich rules:\nsudo firewall-cmd --zone=public --list-rich-rules Remove rich rule:\nsudo firewall-cmd --permanent --zone=public --remove-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;203.0.113.50\u0026#34; port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;22\u0026#34; accept\u0026#39; sudo firewall-cmd --reload Assign interfaces to zones Check which zone an interface is in:\nsudo firewall-cmd --get-zone-of-interface=eth0 Change interface zone:\nsudo firewall-cmd --zone=trusted --change-interface=eth1 --permanent sudo firewall-cmd --reload Example: eth0 (internet) in public zone, eth1 (internal network) in trusted zone.\nPort forwarding Forward external port 80 to internal port 8080:\nsudo firewall-cmd --zone=public --add-forward-port=port=80:proto=tcp:toport=8080 --permanent sudo firewall-cmd --reload Forward to different IP:\nsudo firewall-cmd --zone=public --add-forward-port=port=80:proto=tcp:toaddr=10.0.0.10:toport=80 --permanent sudo firewall-cmd --reload Enable masquerading first for forwarding to different IP:\nsudo firewall-cmd --zone=public --add-masquerade --permanent sudo firewall-cmd --reload Custom services Create custom service definition:\nsudo firewall-cmd --permanent --new-service=myapp Configure service:\nsudo firewall-cmd --permanent --service=myapp --set-description=\u0026#34;My Custom Application\u0026#34; sudo firewall-cmd --permanent --service=myapp --set-short=\u0026#34;MyApp\u0026#34; sudo firewall-cmd --permanent --service=myapp --add-port=8080/tcp sudo firewall-cmd --permanent --service=myapp --add-port=8443/tcp sudo firewall-cmd --reload Allow custom service:\nsudo firewall-cmd --zone=public --add-service=myapp --permanent sudo firewall-cmd --reload Firewalld logging Enable logging for denied packets:\nsudo firewall-cmd --set-log-denied=all Options: all, unicast, broadcast, multicast, off.\nView logs in journalctl:\nsudo journalctl -f -u firewalld Or check kernel logs:\nsudo dmesg | grep -i REJECT Panic mode (emergency) Block all network traffic:\nsudo firewall-cmd --panic-on This drops ALL traffic. Use only in emergencies (active attack, testing).\nDisable panic mode:\nsudo firewall-cmd --panic-off Check panic status:\nsudo firewall-cmd --query-panic Common firewall scenarios Web server (HTTP/HTTPS) UFW:\nsudo ufw allow \u0026#39;Nginx Full\u0026#39; # Or sudo ufw allow 80/tcp sudo ufw allow 443/tcp Firewalld:\nsudo firewall-cmd --permanent --add-service=http sudo firewall-cmd --permanent --add-service=https sudo firewall-cmd --reload Database server (restrict to app server) UFW:\nsudo ufw allow from 10.0.0.5 to any port 3306 # MySQL sudo ufw allow from 10.0.0.5 to any port 5432 # PostgreSQL Firewalld:\nsudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;10.0.0.5\u0026#34; port protocol=\u0026#34;tcp\u0026#34; port=\u0026#34;3306\u0026#34; accept\u0026#39; sudo firewall-cmd --reload SSH from specific IP only UFW:\nsudo ufw delete allow 22/tcp # Remove general SSH rule sudo ufw allow from 203.0.113.50 to any port 22 Firewalld:\nsudo firewall-cmd --permanent --zone=public --remove-service=ssh sudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule family=\u0026#34;ipv4\u0026#34; source address=\u0026#34;203.0.113.50\u0026#34; service name=\u0026#34;ssh\u0026#34; accept\u0026#39; sudo firewall-cmd --reload Block country IP ranges Get IP ranges (example: block IP from specific country):\n# Using ipset for large lists sudo ipset create blocklist hash:net sudo ipset add blocklist 198.51.100.0/24 sudo ipset add blocklist 203.0.113.0/24 UFW doesn\u0026rsquo;t support ipset directly. Use iptables:\nsudo iptables -I INPUT -m set --match-set blocklist src -j DROP Firewalld with ipset:\nsudo firewall-cmd --permanent --new-ipset=blocklist --type=hash:net sudo firewall-cmd --permanent --ipset=blocklist --add-entry=198.51.100.0/24 sudo firewall-cmd --permanent --zone=public --add-rich-rule=\u0026#39;rule source ipset=\u0026#34;blocklist\u0026#34; drop\u0026#39; sudo firewall-cmd --reload Testing your firewall Check if ports are open from external machine:\n# From another machine nmap server-ip Or use telnet:\ntelnet server-ip 80 # Should connect if HTTP is allowed telnet server-ip 3306 # Should fail if MySQL is blocked Online tools:\nhttps://www.yougetsignal.com/tools/open-ports/ https://mxtoolbox.com/PortScan.aspx Test from server itself (won\u0026rsquo;t work, tests local not external):\n# This tests local firewall, not external access curl localhost:80 # Tests if service runs, not firewall Test specific IPs:\n# Allow from specific IP, test from that IP ssh user@server-ip # Should work from allowed IP # Try from different IP, should fail Backup and restore firewall rules UFW backup:\n# Backup rules sudo cp /etc/ufw/user.rules /backup/ufw-user.rules sudo cp /etc/ufw/user6.rules /backup/ufw-user6.rules # Restore sudo cp /backup/ufw-user.rules /etc/ufw/user.rules sudo cp /backup/ufw-user6.rules /etc/ufw/user6.rules sudo ufw reload Firewalld backup:\n# Backup entire config sudo tar -czf firewall-backup.tar.gz /etc/firewalld/ # Restore sudo tar -xzf firewall-backup.tar.gz -C / sudo firewall-cmd --reload Troubleshooting Can\u0026rsquo;t connect after enabling firewall:\nCheck you allowed the service: sudo ufw status or sudo firewall-cmd --list-all Verify service is running: sudo systemctl status nginx Check service binds to correct interface: sudo netstat -tulpn | grep :80 Rule not working:\nUFW: Rules are immediately active after adding Firewalld: Did you forget --permanent? Add rule again with --permanent and --reload Check rule order: Earlier rules take precedence Locked out of SSH:\nUse hosting provider console/VNC to access server Check firewall allowed SSH: sudo ufw allow ssh or sudo firewall-cmd --add-service=ssh If SSH port changed, allow correct port Service allowed but still can\u0026rsquo;t connect:\nCheck if service is running: sudo systemctl status servicename Verify service binds to 0.0.0.0 not 127.0.0.1: sudo netstat -tulpn Check application-level firewall settings Look for SELinux blocking (RHEL/CentOS): sudo ausearch -m avc -ts recent Security tips Use default deny policy. Only allow what\u0026rsquo;s needed.\nAllow SSH only from known IPs if possible. Public SSH gets hammered by bots.\nBlock unused ports. If you\u0026rsquo;re not running a service, block its port explicitly.\nEnable logging to monitor blocked connections. Patterns show attack attempts.\nUpdate firewall rules when deploying new services. Don\u0026rsquo;t leave ports open \u0026ldquo;temporarily\u0026rdquo;.\nTest rules in staging before production. Broken rules can cause outages.\nDocument your rules. Comment why each rule exists.\nRegular audit: Review rules quarterly. Remove unused rules.\nCombine with fail2ban for brute-force protection on allowed services.\nCommon mistakes to avoid Don\u0026rsquo;t enable firewall before allowing SSH on remote servers. You\u0026rsquo;ll lock yourself out.\nDon\u0026rsquo;t forget --permanent flag with firewalld. Temporary rules vanish on restart.\nDon\u0026rsquo;t allow services you don\u0026rsquo;t use. Default profiles might open ports you don\u0026rsquo;t need.\nDon\u0026rsquo;t rely on firewall alone. Defense in depth - use firewalls, service configuration, regular updates, monitoring.\nDon\u0026rsquo;t block ICMP entirely. Breaks path MTU discovery. Allow essential ICMP types.\nDon\u0026rsquo;t forget about IPv6. Configure rules for both IPv4 and IPv6.\nWrapping up Firewalls protect Linux servers from unwanted access. UFW works great on Ubuntu/Debian with simple allow/deny commands. Firewalld gives you more control on RHEL/CentOS with zones and rich rules.\nStart with default deny incoming, allow outgoing. Only allow required services. Restrict database ports to application server IPs. Use rate limiting for public services like SSH. Test rules before deploying to production.\nKeep rules documented and backed up. Review regularly and remove unused rules. Use firewall together with SSH hardening, fail2ban, and regular updates.\nFor more Linux security, check out the SSH hardening guide and systemd service management article.\n","href":"/2025/10/how-to-manage-firewall-linux-ufw-firewalld.html","title":"How to Set Up and Manage Firewall on Linux with UFW and Firewalld"},{"content":"SSH is the main way you access Linux servers remotely. If SSH gets compromised, attackers own your entire server. Default SSH setups are insecure - they allow password logins, permit root access, and get hammered by brute-force bots trying millions of password combinations.\nThis guide hardens your SSH server on Ubuntu. You\u0026rsquo;ll disable passwords and use SSH keys, block root login, change the default port, set up fail2ban to stop brute-force attacks, add two-factor authentication, configure firewall rules, and monitor for suspicious activity.\nWhy SSH security matters SSH runs on port 22 by default. Bots constantly scan the internet for port 22, trying username/password combinations. Check your auth logs and you\u0026rsquo;ll see thousands of failed attempts from IPs worldwide.\nOne weak password means full server access. Attackers install cryptominers, steal data, use your server for DDoS attacks, or hold it for ransom.\nSecuring SSH stops 99% of automated attacks and makes targeted attacks much harder.\nCheck your current SSH security status Before making changes, see what you\u0026rsquo;re dealing with:\n# Check failed login attempts sudo grep \u0026#34;Failed password\u0026#34; /var/log/auth.log | tail -20 # See which IPs are trying to brute-force you sudo grep \u0026#34;Failed password\u0026#34; /var/log/auth.log | awk \u0026#39;{print $(NF-3)}\u0026#39; | sort | uniq -c | sort -rn | head -10 # Check current SSH config sudo sshd -T | grep -E \u0026#34;permitrootlogin|passwordauthentication|port\u0026#34; You\u0026rsquo;ll probably see hundreds or thousands of failed attempts. That\u0026rsquo;s normal for any server on the internet.\nGenerate SSH key pair (if you don\u0026rsquo;t have one) On your local machine (not the server), generate keys:\n# Generate Ed25519 key (recommended - more secure, shorter) ssh-keygen -t ed25519 -C \u0026#34;your_email@example.com\u0026#34; # Or RSA key for compatibility with older systems ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email@example.com\u0026#34; Save to default location (~/.ssh/id_ed25519 or ~/.ssh/id_rsa). Set a strong passphrase.\nThis creates two files:\nPrivate key: id_ed25519 (keep this secret, never share) Public key: id_ed25519.pub (safe to share) Copy SSH key to server # Copy key to server (replace user and IP) ssh-copy-id user@server-ip # Or manually if ssh-copy-id isn\u0026#39;t available cat ~/.ssh/id_ed25519.pub | ssh user@server-ip \u0026#34;mkdir -p ~/.ssh \u0026amp;\u0026amp; cat \u0026gt;\u0026gt; ~/.ssh/authorized_keys \u0026amp;\u0026amp; chmod 700 ~/.ssh \u0026amp;\u0026amp; chmod 600 ~/.ssh/authorized_keys\u0026#34; Test the key works:\nssh user@server-ip You should log in without entering your server password (you\u0026rsquo;ll need your key passphrase if you set one).\nDisable password authentication Once keys work, disable passwords. Edit SSH config:\nsudo nano /etc/ssh/sshd_config Find and change these lines (uncomment if needed by removing #):\nPasswordAuthentication no PubkeyAuthentication yes ChallengeResponseAuthentication no UsePAM no Test config before restarting:\nsudo sshd -t No output means success. If there are errors, fix them before continuing.\nRestart SSH (keep current session open!):\nsudo systemctl restart sshd Open a NEW terminal and test login:\nssh user@server-ip If it works, close the new terminal. If it fails, fix it in your still-open original session.\nDisable root login Never allow direct root SSH access. Attackers always try root login first.\nIn /etc/ssh/sshd_config:\nPermitRootLogin no Instead, log in as a regular user and use sudo:\nssh user@server-ip sudo su - # Switch to root after login if needed Test and restart:\nsudo sshd -t sudo systemctl restart sshd Change SSH port (optional but recommended) Changing from port 22 reduces bot spam. Pick a port above 1024:\nPort 2222 Update firewall before restarting SSH:\n# Allow new SSH port sudo ufw allow 2222/tcp # Check it\u0026#39;s added sudo ufw status Restart SSH:\nsudo systemctl restart sshd Connect with new port:\nssh -p 2222 user@server-ip After confirming it works, remove old port 22 from firewall:\nsudo ufw delete allow 22/tcp Remember your port or you\u0026rsquo;ll lock yourself out. Add to ~/.ssh/config on your local machine:\nHost myserver HostName server-ip Port 2222 User username Now just use ssh myserver.\nInstall and configure fail2ban Fail2ban blocks IPs after repeated failed login attempts.\nInstall:\nsudo apt update sudo apt install fail2ban -y Create local config (don\u0026rsquo;t edit /etc/fail2ban/jail.conf directly):\nsudo cp /etc/fail2ban/jail.conf /etc/fail2ban/jail.local sudo nano /etc/fail2ban/jail.local Find [sshd] section and configure:\n[sshd] enabled = true port = 2222 # Change to your SSH port filter = sshd logpath = /var/log/auth.log maxretry = 3 # Ban after 3 failed attempts findtime = 600 # Within 10 minutes bantime = 3600 # Ban for 1 hour (3600 seconds) For permanent bans after repeated offenses:\n[sshd] enabled = true port = 2222 maxretry = 3 findtime = 600 bantime = 3600 # Ban forever after 3 bans within 1 week bantime.increment = true bantime.maxtime = 5w bantime.factor = 24 Start fail2ban:\nsudo systemctl enable fail2ban sudo systemctl start fail2ban Check status:\nsudo fail2ban-client status sshd View banned IPs:\nsudo fail2ban-client get sshd banned Manually ban/unban:\n# Ban an IP sudo fail2ban-client set sshd banip 1.2.3.4 # Unban an IP sudo fail2ban-client set sshd unbanip 1.2.3.4 Set up two-factor authentication Add 2FA for extra security. You\u0026rsquo;ll need your SSH key AND a code from your phone.\nInstall Google Authenticator:\nsudo apt install libpam-google-authenticator -y Run setup:\ngoogle-authenticator Answer the prompts:\nTime-based tokens? Yes Update .google_authenticator file? Yes Disallow multiple uses? Yes Increase time window? No (unless your server clock is off) Enable rate-limiting? Yes Scan the QR code with Google Authenticator app on your phone. Save the emergency scratch codes somewhere safe.\nEdit PAM config:\nsudo nano /etc/pam.d/sshd Add at the top:\nauth required pam_google_authenticator.so Comment out this line (add # at start):\n# @include common-auth Edit SSH config:\nsudo nano /etc/ssh/sshd_config Enable challenge-response:\nChallengeResponseAuthentication yes AuthenticationMethods publickey,keyboard-interactive Restart SSH:\nsudo systemctl restart sshd Test in a new terminal. You\u0026rsquo;ll need:\nYour SSH private key The 6-digit code from Google Authenticator Configure firewall rules Use UFW to limit SSH access.\nEnable UFW if not already:\nsudo ufw status If inactive:\n# Allow SSH first (your port) sudo ufw allow 2222/tcp # Enable firewall sudo ufw enable Limit SSH connections (allows max 6 connections per IP in 30 seconds):\nsudo ufw limit 2222/tcp Allow SSH only from specific IPs (if you have a static IP):\n# Delete the general rule first sudo ufw delete allow 2222/tcp # Allow only your IP sudo ufw allow from YOUR_IP_ADDRESS to any port 2222 proto tcp Check rules:\nsudo ufw status numbered Restrict SSH to specific users or groups Only allow certain users to SSH:\nsudo nano /etc/ssh/sshd_config Add:\nAllowUsers user1 user2 Or allow by group:\nAllowGroups sshusers Create the group and add users:\nsudo groupadd sshusers sudo usermod -aG sshusers username Restart SSH:\nsudo systemctl restart sshd Set SSH session timeouts Disconnect idle sessions automatically:\nIn /etc/ssh/sshd_config:\nClientAliveInterval 300 # Send keepalive every 5 minutes ClientAliveCountMax 2 # Disconnect after 2 failed keepalives (10 min total) This disconnects sessions idle for 10 minutes.\nDisable empty passwords Make sure no accounts have empty passwords:\nPermitEmptyPasswords no Check for empty passwords:\nsudo awk -F: \u0026#39;($2 == \u0026#34;\u0026#34;) {print $1}\u0026#39; /etc/shadow If any users show up, set passwords:\nsudo passwd username Monitor SSH logs Watch live authentication attempts:\nsudo tail -f /var/log/auth.log See successful logins:\nsudo grep \u0026#34;Accepted\u0026#34; /var/log/auth.log | tail -20 Failed logins:\nsudo grep \u0026#34;Failed\u0026#34; /var/log/auth.log | tail -20 See which users logged in:\nlast -20 Currently logged in users:\nw Set up email alerts for SSH logins (install mailutils first):\nsudo apt install mailutils -y Add to /etc/ssh/sshd_config:\nForceCommand echo \u0026#34;SSH Login: $(whoami) from $(echo $SSH_CLIENT | awk \u0026#39;{print $1}\u0026#39;) at $(date)\u0026#34; | mail -s \u0026#34;SSH Login Alert\u0026#34; your@email.com; bash Or use a script in /etc/profile.d/ssh-login-alert.sh:\n#!/bin/bash if [ -n \u0026#34;$SSH_CLIENT\u0026#34; ]; then IP=$(echo $SSH_CLIENT | awk \u0026#39;{print $1}\u0026#39;) echo \u0026#34;SSH login to $(hostname) as $(whoami) from $IP at $(date)\u0026#34; | mail -s \u0026#34;SSH Login: $(hostname)\u0026#34; your@email.com fi Disable unnecessary SSH features Turn off features you don\u0026rsquo;t need:\nX11Forwarding no # Disable GUI forwarding AllowTcpForwarding no # Disable port forwarding (breaks SSH tunnels) PermitTunnel no # Disable tun device forwarding Only disable AllowTcpForwarding if you\u0026rsquo;re sure you don\u0026rsquo;t need SSH tunnels.\nUse SSH key passphrases Always protect your private key with a passphrase. If someone steals your key file, the passphrase protects it.\nAdd passphrase to existing key:\nssh-keygen -p -f ~/.ssh/id_ed25519 Use ssh-agent so you only enter the passphrase once per session:\neval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 Add to ~/.bashrc or ~/.zshrc:\nif [ -z \u0026#34;$SSH_AUTH_SOCK\u0026#34; ]; then eval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 fi Test your SSH security Try logging in without your key (should fail):\nssh -o PubkeyAuthentication=no user@server-ip Try as root (should fail):\nssh root@server-ip Intentionally fail logins 3 times and check fail2ban bans your IP:\n# On server sudo fail2ban-client status sshd Scan your SSH port with nmap (from another machine):\nnmap -p 2222 server-ip Should show port open with SSH service.\nBackup and recovery Backup your SSH config:\nsudo cp /etc/ssh/sshd_config /etc/ssh/sshd_config.backup If you mess up, restore it:\nsudo cp /etc/ssh/sshd_config.backup /etc/ssh/sshd_config sudo systemctl restart sshd Keep a copy of your private key in a secure location (encrypted USB drive, password manager).\nVerify your security settings Check your SSH config to make sure everything is set correctly:\nsudo sshd -T | grep -E \u0026#34;passwordauthentication|permitrootlogin|port|pubkeyauthentication\u0026#34; You should see:\npasswordauthentication no permitrootlogin no port 2222 pubkeyauthentication yes Check fail2ban is running:\nsudo systemctl status fail2ban sudo fail2ban-client status sshd Verify firewall rules:\nsudo ufw status Your custom SSH port should be allowed, old port 22 should be removed.\nCommon mistakes to avoid Don\u0026rsquo;t disable PasswordAuthentication before testing key authentication. Test keys work first.\nDon\u0026rsquo;t close your current SSH session before testing new settings. Keep it open until you confirm login works in a second session.\nDon\u0026rsquo;t forget to update firewall rules when changing SSH port. You\u0026rsquo;ll lock yourself out.\nDon\u0026rsquo;t lose your private key or forget your passphrase. You\u0026rsquo;ll be locked out permanently.\nDon\u0026rsquo;t set fail2ban maxretry too low (like 1 or 2). You might ban yourself with typos.\nDon\u0026rsquo;t skip 2FA for root or admin users. They\u0026rsquo;re the highest value targets.\nWrapping up SSH security needs multiple layers. Disable passwords and use keys. Block root login. Add fail2ban to stop brute-force attacks. Configure firewall rules to limit access. Add 2FA for critical servers.\nMonitor logs for suspicious activity. Test changes carefully before disconnecting. Keep backups of configs and keys.\nA hardened SSH server stops automated attacks and makes targeted attacks harder. Use this together with firewall configuration and other security measures.\nFor more Linux security, check out the firewall configuration guide and systemd service management article.\n","href":"/2025/10/how-to-secure-ssh-server-ubuntu-hardening-guide.html","title":"How to Secure SSH Server on Ubuntu - Complete Hardening Guide"},{"content":"Multi-tenant applications let you serve multiple customers (tenants) with one codebase. Each tenant gets their own data, subdomain, and isolated environment, but you maintain just one application. This is how most SaaS products work - Slack, Shopify, and Basecamp all use multi-tenancy.\nThis guide shows you how to build multi-tenant Laravel apps. You\u0026rsquo;ll learn the different tenancy models, why database-per-tenant wins for security, how to set up tenant isolation with Stancl/Tenancy package, handle subdomain routing, manage tenant databases, and scale your multi-tenant SaaS.\nUnderstanding multi-tenancy models There are three ways to isolate tenant data:\nDatabase per tenant (recommended) Each tenant gets their own database. acme_db, beta_db, charlie_db, etc. Strongest isolation - a security breach in one tenant\u0026rsquo;s database doesn\u0026rsquo;t expose others. Easy backups and restores per tenant. Simple to migrate big tenants to dedicated servers.\nCons: More databases to manage. Requires connection pooling.\nSchema per tenant One database, multiple schemas. PostgreSQL supports this well. MySQL uses separate databases anyway.\nPros: Better than shared tables. Cons: Not as isolated as separate databases.\nShared database with tenant_id All tenants share the same tables. Every query needs WHERE tenant_id = ?. One missing WHERE clause leaks all tenant data.\nPros: Simple, cheap. Cons: Dangerous. Don\u0026rsquo;t use this for anything with real customer data.\nFor production SaaS, use database-per-tenant.\nInstall Stancl Tenancy package Stancl/Tenancy automates multi-tenancy in Laravel. It handles database switching, tenant detection, and migrations.\nInstall:\ncomposer require stancl/tenancy Run the installer:\nphp artisan tenancy:install This publishes config, migrations, and routes. Run migrations:\nphp artisan migrate This creates tenants and domains tables in your central database.\nConfigure tenancy Open config/tenancy.php. Key settings:\nreturn [ \u0026#39;tenant_model\u0026#39; =\u0026gt; \\App\\Models\\Tenant::class, \u0026#39;id_generator\u0026#39; =\u0026gt; Stancl\\Tenancy\\UUIDGenerator::class, \u0026#39;database\u0026#39; =\u0026gt; [ \u0026#39;prefix\u0026#39; =\u0026gt; \u0026#39;tenant\u0026#39;, \u0026#39;suffix\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;manager\u0026#39; =\u0026gt; Stancl\\Tenancy\\Database\\DatabaseManager::class, ], \u0026#39;features\u0026#39; =\u0026gt; [ Stancl\\Tenancy\\Features\\TenantConfig::class, Stancl\\Tenancy\\Features\\TenantsTable::class, Stancl\\Tenancy\\Features\\UserImpersonation::class, ], ]; By default, tenant databases are named tenant{id}. Example: tenant123, tenantacme.\nCreate tenant model The package includes a base Tenant model. Extend it:\nphp artisan make:model Tenant \u0026lt;?php namespace App\\Models; use Stancl\\Tenancy\\Database\\Models\\Tenant as BaseTenant; use Stancl\\Tenancy\\Contracts\\TenantWithDatabase; use Stancl\\Tenancy\\Database\\Concerns\\HasDatabase; use Stancl\\Tenancy\\Database\\Concerns\\HasDomains; class Tenant extends BaseTenant implements TenantWithDatabase { use HasDatabase, HasDomains; protected $fillable = [ \u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;, ]; public static function getCustomColumns(): array { return [ \u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;, ]; } } Set up subdomain routing Tenants access your app via subdomains: acme.yoursaas.com, beta.yoursaas.com.\nConfigure domains in .env:\nCENTRAL_DOMAINS=yoursaas.com,www.yoursaas.com Add tenant routes in routes/tenant.php (created by installer):\n\u0026lt;?php use Illuminate\\Support\\Facades\\Route; use Stancl\\Tenancy\\Middleware\\InitializeTenancyByDomain; use Stancl\\Tenancy\\Middleware\\PreventAccessFromCentralDomains; Route::middleware([ \u0026#39;web\u0026#39;, InitializeTenancyByDomain::class, PreventAccessFromCentralDomains::class, ])-\u0026gt;group(function () { Route::get(\u0026#39;/\u0026#39;, function () { return \u0026#39;This is your tenant area: \u0026#39; . tenant(\u0026#39;id\u0026#39;); }); Route::get(\u0026#39;/dashboard\u0026#39;, function () { return view(\u0026#39;dashboard\u0026#39;); })-\u0026gt;middleware([\u0026#39;auth\u0026#39;]); }); Central routes (login, signup, landing page) stay in routes/web.php. They work on yoursaas.com.\nTenant routes work on *.yoursaas.com subdomains.\nCreate a tenant When a customer signs up, create a tenant:\nuse App\\Models\\Tenant; $tenant = Tenant::create([ \u0026#39;id\u0026#39; =\u0026gt; \u0026#39;acme\u0026#39;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Acme Corporation\u0026#39;, \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;admin@acme.com\u0026#39;, ]); $tenant-\u0026gt;domains()-\u0026gt;create([ \u0026#39;domain\u0026#39; =\u0026gt; \u0026#39;acme.yoursaas.com\u0026#39;, ]); This creates:\nA record in the tenants table A new database named tenantacme Runs all tenant migrations on the new database Creates a domain record linking acme.yoursaas.com to this tenant Now when a user visits acme.yoursaas.com, Stancl automatically switches to the tenantacme database.\nSeparate central and tenant migrations Central migrations (users, billing, tenants table) run on your main database. Tenant migrations (posts, products, orders) run on each tenant\u0026rsquo;s database.\nMove tenant migrations to database/migrations/tenant:\nmkdir database/migrations/tenant mv database/migrations/*_create_posts_table.php database/migrations/tenant/ Migrations in database/migrations/tenant run automatically when you create a tenant or manually with:\nphp artisan tenants:migrate This runs migrations on all tenant databases.\nRoll back tenant migrations:\nphp artisan tenants:migrate:rollback Run code for all tenants Execute commands or code for every tenant:\nphp artisan tenants:run \u0026#39;cache:clear\u0026#39; Or in code:\nuse Stancl\\Tenancy\\Facades\\Tenancy; Tenancy::all()-\u0026gt;each(function ($tenant) { $tenant-\u0026gt;run(function () { // Code runs in this tenant\u0026#39;s context Post::where(\u0026#39;published\u0026#39;, false)-\u0026gt;delete(); }); }); Useful for:\nSeeding data in all tenant databases Running maintenance tasks Generating reports per tenant Cleaning up old data Handle tenant signup Create a signup flow that provisions a new tenant:\nuse App\\Models\\Tenant; use Illuminate\\Support\\Str; public function store(Request $request) { $validated = $request-\u0026gt;validate([ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;required|string\u0026#39;, \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email\u0026#39;, \u0026#39;subdomain\u0026#39; =\u0026gt; \u0026#39;required|alpha_dash|unique:domains,domain\u0026#39;, ]); $subdomain = Str::slug($validated[\u0026#39;subdomain\u0026#39;]); $tenant = Tenant::create([ \u0026#39;id\u0026#39; =\u0026gt; $subdomain, \u0026#39;name\u0026#39; =\u0026gt; $validated[\u0026#39;name\u0026#39;], \u0026#39;email\u0026#39; =\u0026gt; $validated[\u0026#39;email\u0026#39;], ]); $tenant-\u0026gt;domains()-\u0026gt;create([ \u0026#39;domain\u0026#39; =\u0026gt; $subdomain . \u0026#39;.yoursaas.com\u0026#39;, ]); // Create admin user in tenant database $tenant-\u0026gt;run(function () use ($validated) { \\App\\Models\\User::create([ \u0026#39;name\u0026#39; =\u0026gt; $validated[\u0026#39;name\u0026#39;], \u0026#39;email\u0026#39; =\u0026gt; $validated[\u0026#39;email\u0026#39;], \u0026#39;password\u0026#39; =\u0026gt; bcrypt(\u0026#39;temporary-password\u0026#39;), ]); }); return redirect(\u0026#39;https://\u0026#39; . $subdomain . \u0026#39;.yoursaas.com/login\u0026#39;); } Queue database creation for large setups:\ndispatch(new CreateTenantDatabase($tenant)); Access tenant context Get current tenant anywhere in tenant routes:\n$tenant = tenant(); // Returns current Tenant model $tenantId = tenant(\u0026#39;id\u0026#39;); $tenantName = tenant(\u0026#39;name\u0026#39;); Check if running in tenant context:\nif (tenancy()-\u0026gt;initialized) { // We\u0026#39;re in a tenant } Switch to tenant context manually:\n$tenant = Tenant::find(\u0026#39;acme\u0026#39;); $tenant-\u0026gt;run(function () { // Code here runs in acme\u0026#39;s context $posts = Post::all(); // Queries acme\u0026#39;s database }); Share data between central and tenant databases Some data lives in the central database (plans, global settings) and tenant databases (customer posts, orders).\nUse explicit connections:\n// Query central database $plans = DB::connection(\u0026#39;central\u0026#39;)-\u0026gt;table(\u0026#39;plans\u0026#39;)-\u0026gt;get(); // Query tenant database (automatic in tenant routes) $posts = Post::all(); // Or explicit tenant connection $posts = DB::connection(\u0026#39;tenant\u0026#39;)-\u0026gt;table(\u0026#39;posts\u0026#39;)-\u0026gt;get(); Store billing, subscriptions, and tenant metadata in central. Store customer data in tenant databases.\nHandle custom domains Let tenants use custom domains like app.acmecorp.com instead of acme.yoursaas.com:\n$tenant = Tenant::find(\u0026#39;acme\u0026#39;); $tenant-\u0026gt;domains()-\u0026gt;create([ \u0026#39;domain\u0026#39; =\u0026gt; \u0026#39;app.acmecorp.com\u0026#39;, ]); Now both acme.yoursaas.com and app.acmecorp.com route to the same tenant.\nFor DNS, create a CNAME record:\napp.acmecorp.com CNAME yoursaas.com Configure your web server to accept any domain. With Laravel Sail or Valet, this works automatically.\nFor production, use a wildcard SSL certificate or Let\u0026rsquo;s Encrypt to provision SSL per custom domain.\nSeed tenant databases Create seeders for tenant data:\nphp artisan make:seeder TenantDatabaseSeeder \u0026lt;?php namespace Database\\Seeders; use Illuminate\\Database\\Seeder; use App\\Models\\Post; use App\\Models\\Category; class TenantDatabaseSeeder extends Seeder { public function run() { Category::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;General\u0026#39;]); Category::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;Announcements\u0026#39;]); Post::factory()-\u0026gt;count(10)-\u0026gt;create(); } } Run for all tenants:\nphp artisan tenants:seed --class=TenantDatabaseSeeder Or for one tenant:\n$tenant = Tenant::find(\u0026#39;acme\u0026#39;); $tenant-\u0026gt;run(function () { $this-\u0026gt;call(TenantDatabaseSeeder::class); }); Test multi-tenancy Test tenant isolation:\nuse App\\Models\\Tenant; use App\\Models\\Post; it(\u0026#39;isolates data between tenants\u0026#39;, function () { $tenant1 = Tenant::create([\u0026#39;id\u0026#39; =\u0026gt; \u0026#39;tenant1\u0026#39;]); $tenant2 = Tenant::create([\u0026#39;id\u0026#39; =\u0026gt; \u0026#39;tenant2\u0026#39;]); $tenant1-\u0026gt;run(function () { Post::create([\u0026#39;title\u0026#39; =\u0026gt; \u0026#39;Tenant 1 Post\u0026#39;]); }); $tenant2-\u0026gt;run(function () { $posts = Post::all(); expect($posts)-\u0026gt;toHaveCount(0); // Tenant 2 can\u0026#39;t see Tenant 1\u0026#39;s data }); }); Test subdomain routing:\nit(\u0026#39;serves tenant routes on subdomains\u0026#39;, function () { $tenant = Tenant::create([\u0026#39;id\u0026#39; =\u0026gt; \u0026#39;acme\u0026#39;]); $tenant-\u0026gt;domains()-\u0026gt;create([\u0026#39;domain\u0026#39; =\u0026gt; \u0026#39;acme.localhost\u0026#39;]); $response = $this-\u0026gt;get(\u0026#39;http://acme.localhost/dashboard\u0026#39;); $response-\u0026gt;assertOk(); expect(tenant(\u0026#39;id\u0026#39;))-\u0026gt;toBe(\u0026#39;acme\u0026#39;); }); Implement tenant impersonation Let super admins log into any tenant for support:\nuse Stancl\\Tenancy\\Features\\UserImpersonation; // In your central admin panel $tenant = Tenant::find(\u0026#39;acme\u0026#39;); $impersonationUrl = UserImpersonation::makeUrl($tenant, 1); // 1 = user ID in tenant DB return redirect($impersonationUrl); This logs you into the tenant as that user. Add the feature to config/tenancy.php:\n\u0026#39;features\u0026#39; =\u0026gt; [ Stancl\\Tenancy\\Features\\UserImpersonation::class, ], Handle tenant deletion Delete a tenant and all their data:\n$tenant = Tenant::find(\u0026#39;acme\u0026#39;); // This deletes the tenant database and all records $tenant-\u0026gt;delete(); To soft delete (keep data but mark inactive):\n$tenant-\u0026gt;update([\u0026#39;status\u0026#39; =\u0026gt; \u0026#39;inactive\u0026#39;]); Archive data before deleting:\n// Export tenant database $tenant-\u0026gt;run(function () { $data = [ \u0026#39;users\u0026#39; =\u0026gt; User::all(), \u0026#39;posts\u0026#39; =\u0026gt; Post::all(), ]; Storage::put(\u0026#34;archives/{$tenant-\u0026gt;id}.json\u0026#34;, json_encode($data)); }); $tenant-\u0026gt;delete(); Scale multi-tenant applications Connection pooling is critical. Don\u0026rsquo;t create new database connections on every request. Use Laravel\u0026rsquo;s connection pool or PgBouncer for PostgreSQL.\nCache tenant data:\n$tenant = Cache::remember(\u0026#34;tenant:{$domain}\u0026#34;, 3600, function () use ($domain) { return Tenant::whereDomain($domain)-\u0026gt;first(); }); Move large tenants to dedicated servers:\n// In tenants table, add server column $tenant-\u0026gt;update([\u0026#39;server\u0026#39; =\u0026gt; \u0026#39;tenant-db-2.yoursaas.com\u0026#39;]); // Override database connection config([\u0026#39;database.connections.tenant.host\u0026#39; =\u0026gt; $tenant-\u0026gt;server]); Queue tenant operations:\ndispatch(new ProcessTenantReport($tenant)); Monitor per-tenant usage and throttle heavy users.\nSecurity considerations Never trust tenant IDs from user input. Always get the current tenant from the authenticated context:\n// Bad - user can send any tenant_id $posts = Post::where(\u0026#39;tenant_id\u0026#39;, $request-\u0026gt;tenant_id)-\u0026gt;get(); // Good - Stancl automatically scopes queries $posts = Post::all(); // Only returns current tenant\u0026#39;s posts Validate subdomain inputs to prevent creating tenants with reserved names:\n$reserved = [\u0026#39;www\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;api\u0026#39;, \u0026#39;mail\u0026#39;, \u0026#39;ftp\u0026#39;]; if (in_array($request-\u0026gt;subdomain, $reserved)) { return back()-\u0026gt;withErrors([\u0026#39;subdomain\u0026#39; =\u0026gt; \u0026#39;This subdomain is reserved.\u0026#39;]); } Encrypt sensitive tenant data with Laravel\u0026rsquo;s encrypted casts.\nLog tenant switches for audit trails:\nevent(new TenantSwitched($oldTenant, $newTenant)); Don\u0026rsquo;t share sessions across tenants. Each tenant should have isolated sessions.\nCompare Stancl vs Spatie multi-tenancy Stancl Tenancy:\nDatabase-per-tenant by default Automatic migrations, seeders, queues Subdomain and domain routing built-in More features, more automation Spatie Multitenancy:\nSimpler, more lightweight Database or shared DB with scopes More manual control Better for custom setups For most SaaS apps, use Stancl. For advanced custom requirements, Spatie gives more flexibility.\nSummary Multi-tenancy lets you serve multiple customers with one application. Use database-per-tenant for strong isolation, subdomain routing for easy tenant identification, and Stancl/Tenancy package to automate database creation and switching.\nSeparate central data (billing, plans) from tenant data (customer content). Create tenants on signup with automatic database provisioning. Test tenant isolation to ensure no data leaks.\nScale with connection pooling, caching, and dedicated servers for large tenants. Monitor usage and queue heavy operations.\nFor more on Laravel architecture, see Laravel Performance Optimization and Laravel Security Best Practices .\n","href":"/2025/10/how-to-build-multi-tenant-applications-laravel.html","title":"How to Build Multi-Tenant Applications in Laravel"},{"content":"Social login lets users sign in with Google, Facebook, or GitHub instead of creating another password. Laravel Socialite makes OAuth integration simple with just a few lines of code. Users click \u0026lsquo;Login with Google\u0026rsquo;, authenticate on Google\u0026rsquo;s site, and return to your app with their profile data.\nThis guide walks through setting up Socialite, configuring OAuth apps with Google, Facebook, and GitHub, handling callbacks, creating or linking user accounts, storing tokens securely, and managing edge cases like duplicate emails.\nWhy use social login Users hate creating new accounts. Social login removes friction:\nNo registration forms to fill out No passwords to remember Verified email addresses automatically Faster conversion from visitor to user Users trust Google/Facebook/GitHub security Offer social login as an option, not the only option. Some users prefer email/password. Give them both choices.\nInstall Laravel Socialite Install via Composer:\ncomposer require laravel/socialite Socialite works out of the box. No service provider registration needed in Laravel 11+.\nCreate OAuth applications Before coding, create OAuth apps with each provider to get client IDs and secrets.\nGoogle OAuth setup Go to Google Cloud Console Create a new project or select existing one Navigate to APIs \u0026amp; Services \u0026gt; Credentials Click Create Credentials \u0026gt; OAuth client ID Select Web application Add authorized redirect URI: https://yourdomain.com/login/google/callback Copy Client ID and Client secret Facebook OAuth setup Go to Facebook Developers Create an app or select existing one Add Facebook Login product Go to Settings \u0026gt; Basic Copy App ID and App Secret In Facebook Login \u0026gt; Settings, add redirect URI: https://yourdomain.com/login/facebook/callback GitHub OAuth setup Go to GitHub Developer Settings Click New OAuth App Fill in application name and homepage URL Set Authorization callback URL: https://yourdomain.com/login/github/callback Copy Client ID and generate Client Secret Configure OAuth credentials Add credentials to config/services.php:\nreturn [ // ... other services \u0026#39;google\u0026#39; =\u0026gt; [ \u0026#39;client_id\u0026#39; =\u0026gt; env(\u0026#39;GOOGLE_CLIENT_ID\u0026#39;), \u0026#39;client_secret\u0026#39; =\u0026gt; env(\u0026#39;GOOGLE_CLIENT_SECRET\u0026#39;), \u0026#39;redirect\u0026#39; =\u0026gt; env(\u0026#39;GOOGLE_REDIRECT_URI\u0026#39;), ], \u0026#39;facebook\u0026#39; =\u0026gt; [ \u0026#39;client_id\u0026#39; =\u0026gt; env(\u0026#39;FACEBOOK_CLIENT_ID\u0026#39;), \u0026#39;client_secret\u0026#39; =\u0026gt; env(\u0026#39;FACEBOOK_CLIENT_SECRET\u0026#39;), \u0026#39;redirect\u0026#39; =\u0026gt; env(\u0026#39;FACEBOOK_REDIRECT_URI\u0026#39;), ], \u0026#39;github\u0026#39; =\u0026gt; [ \u0026#39;client_id\u0026#39; =\u0026gt; env(\u0026#39;GITHUB_CLIENT_ID\u0026#39;), \u0026#39;client_secret\u0026#39; =\u0026gt; env(\u0026#39;GITHUB_CLIENT_SECRET\u0026#39;), \u0026#39;redirect\u0026#39; =\u0026gt; env(\u0026#39;GITHUB_REDIRECT_URI\u0026#39;), ], ]; Add to .env:\nGOOGLE_CLIENT_ID=your-google-client-id GOOGLE_CLIENT_SECRET=your-google-client-secret GOOGLE_REDIRECT_URI=https://yourdomain.com/login/google/callback FACEBOOK_CLIENT_ID=your-facebook-app-id FACEBOOK_CLIENT_SECRET=your-facebook-app-secret FACEBOOK_REDIRECT_URI=https://yourdomain.com/login/facebook/callback GITHUB_CLIENT_ID=your-github-client-id GITHUB_CLIENT_SECRET=your-github-client-secret GITHUB_REDIRECT_URI=https://yourdomain.com/login/github/callback Never commit these secrets to Git. Keep .env in .gitignore.\nCreate database tables Create a social_accounts table to store OAuth data:\nphp artisan make:migration create_social_accounts_table \u0026lt;?php use Illuminate\\Database\\Migrations\\Migration; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Support\\Facades\\Schema; return new class extends Migration { public function up() { Schema::create(\u0026#39;social_accounts\u0026#39;, function (Blueprint $table) { $table-\u0026gt;id(); $table-\u0026gt;foreignId(\u0026#39;user_id\u0026#39;)-\u0026gt;constrained()-\u0026gt;onDelete(\u0026#39;cascade\u0026#39;); $table-\u0026gt;string(\u0026#39;provider\u0026#39;); // google, facebook, github $table-\u0026gt;string(\u0026#39;provider_id\u0026#39;); // user ID from provider $table-\u0026gt;text(\u0026#39;access_token\u0026#39;)-\u0026gt;nullable(); $table-\u0026gt;text(\u0026#39;refresh_token\u0026#39;)-\u0026gt;nullable(); $table-\u0026gt;timestamp(\u0026#39;token_expires_at\u0026#39;)-\u0026gt;nullable(); $table-\u0026gt;timestamps(); $table-\u0026gt;unique([\u0026#39;provider\u0026#39;, \u0026#39;provider_id\u0026#39;]); }); } public function down() { Schema::dropIfExists(\u0026#39;social_accounts\u0026#39;); } }; Run migration:\nphp artisan migrate Make password nullable in users table since social users might not have passwords:\nSchema::table(\u0026#39;users\u0026#39;, function (Blueprint $table) { $table-\u0026gt;string(\u0026#39;password\u0026#39;)-\u0026gt;nullable()-\u0026gt;change(); }); Create SocialAccount model php artisan make:model SocialAccount \u0026lt;?php namespace App\\Models; use Illuminate\\Database\\Eloquent\\Model; use Illuminate\\Database\\Eloquent\\Relations\\BelongsTo; class SocialAccount extends Model { protected $fillable = [ \u0026#39;user_id\u0026#39;, \u0026#39;provider\u0026#39;, \u0026#39;provider_id\u0026#39;, \u0026#39;access_token\u0026#39;, \u0026#39;refresh_token\u0026#39;, \u0026#39;token_expires_at\u0026#39;, ]; protected $casts = [ \u0026#39;token_expires_at\u0026#39; =\u0026gt; \u0026#39;datetime\u0026#39;, ]; public function user(): BelongsTo { return $this-\u0026gt;belongsTo(User::class); } } Add relationship to User model:\nuse Illuminate\\Database\\Eloquent\\Relations\\HasMany; public function socialAccounts(): HasMany { return $this-\u0026gt;hasMany(SocialAccount::class); } Create routes Add routes in routes/web.php:\nuse App\\Http\\Controllers\\SocialAuthController; Route::prefix(\u0026#39;login\u0026#39;)-\u0026gt;group(function () { Route::get(\u0026#39;{provider}\u0026#39;, [SocialAuthController::class, \u0026#39;redirect\u0026#39;]) -\u0026gt;where(\u0026#39;provider\u0026#39;, \u0026#39;google|facebook|github\u0026#39;) -\u0026gt;name(\u0026#39;social.redirect\u0026#39;); Route::get(\u0026#39;{provider}/callback\u0026#39;, [SocialAuthController::class, \u0026#39;callback\u0026#39;]) -\u0026gt;where(\u0026#39;provider\u0026#39;, \u0026#39;google|facebook|github\u0026#39;) -\u0026gt;name(\u0026#39;social.callback\u0026#39;); }); This creates:\n/login/google - redirects to Google /login/google/callback - handles Google response Same for facebook and github Create controller php artisan make:controller SocialAuthController \u0026lt;?php namespace App\\Http\\Controllers; use App\\Models\\User; use App\\Models\\SocialAccount; use Illuminate\\Http\\Request; use Illuminate\\Support\\Str; use Laravel\\Socialite\\Facades\\Socialite; class SocialAuthController extends Controller { public function redirect(string $provider) { return Socialite::driver($provider)-\u0026gt;redirect(); } public function callback(string $provider) { try { $socialUser = Socialite::driver($provider)-\u0026gt;user(); } catch (\\Exception $e) { return redirect(\u0026#39;/login\u0026#39;)-\u0026gt;withErrors(\u0026#39;Authentication failed. Please try again.\u0026#39;); } // Find or create user $user = $this-\u0026gt;findOrCreateUser($socialUser, $provider); // Log the user in auth()-\u0026gt;login($user, true); return redirect()-\u0026gt;intended(\u0026#39;/dashboard\u0026#39;); } protected function findOrCreateUser($socialUser, $provider) { // Check if user already has this social account $socialAccount = SocialAccount::where(\u0026#39;provider\u0026#39;, $provider) -\u0026gt;where(\u0026#39;provider_id\u0026#39;, $socialUser-\u0026gt;getId()) -\u0026gt;first(); if ($socialAccount) { // Update tokens $socialAccount-\u0026gt;update([ \u0026#39;access_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;token, \u0026#39;refresh_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;refreshToken, \u0026#39;token_expires_at\u0026#39; =\u0026gt; $socialUser-\u0026gt;expiresIn ? now()-\u0026gt;addSeconds($socialUser-\u0026gt;expiresIn) : null, ]); return $socialAccount-\u0026gt;user; } // Check if user exists with this email $user = User::where(\u0026#39;email\u0026#39;, $socialUser-\u0026gt;getEmail())-\u0026gt;first(); if (!$user) { // Create new user $user = User::create([ \u0026#39;name\u0026#39; =\u0026gt; $socialUser-\u0026gt;getName(), \u0026#39;email\u0026#39; =\u0026gt; $socialUser-\u0026gt;getEmail(), \u0026#39;email_verified_at\u0026#39; =\u0026gt; now(), \u0026#39;password\u0026#39; =\u0026gt; null, ]); } // Create social account $user-\u0026gt;socialAccounts()-\u0026gt;create([ \u0026#39;provider\u0026#39; =\u0026gt; $provider, \u0026#39;provider_id\u0026#39; =\u0026gt; $socialUser-\u0026gt;getId(), \u0026#39;access_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;token, \u0026#39;refresh_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;refreshToken, \u0026#39;token_expires_at\u0026#39; =\u0026gt; $socialUser-\u0026gt;expiresIn ? now()-\u0026gt;addSeconds($socialUser-\u0026gt;expiresIn) : null, ]); return $user; } } This handles the full OAuth flow:\nUser clicks \u0026lsquo;Login with Google\u0026rsquo; redirect() sends them to Google User authenticates on Google Google redirects to /login/google/callback callback() receives user data Find or create user and social account Log user in Add social login buttons to login page \u0026lt;!-- resources/views/auth/login.blade.php --\u0026gt; \u0026lt;div class=\u0026#34;social-login\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{ route(\u0026#39;social.redirect\u0026#39;, \u0026#39;google\u0026#39;) }}\u0026#34; class=\u0026#34;btn btn-google\u0026#34;\u0026gt; Login with Google \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;{{ route(\u0026#39;social.redirect\u0026#39;, \u0026#39;facebook\u0026#39;) }}\u0026#34; class=\u0026#34;btn btn-facebook\u0026#34;\u0026gt; Login with Facebook \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;{{ route(\u0026#39;social.redirect\u0026#39;, \u0026#39;github\u0026#39;) }}\u0026#34; class=\u0026#34;btn btn-github\u0026#34;\u0026gt; Login with GitHub \u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; Style the buttons to match provider branding. Google, Facebook, and GitHub have brand guidelines for button design.\nHandle missing emails Some providers don\u0026rsquo;t always provide emails. GitHub users can hide their email. Handle this:\nprotected function findOrCreateUser($socialUser, $provider) { $email = $socialUser-\u0026gt;getEmail(); if (!$email) { // Generate a temporary email or ask user for email return redirect(\u0026#39;/register/complete\u0026#39;) -\u0026gt;with(\u0026#39;social_data\u0026#39;, [ \u0026#39;provider\u0026#39; =\u0026gt; $provider, \u0026#39;provider_id\u0026#39; =\u0026gt; $socialUser-\u0026gt;getId(), \u0026#39;name\u0026#39; =\u0026gt; $socialUser-\u0026gt;getName(), ]); } // ... rest of the logic } For GitHub, request the user:email scope:\npublic function redirect(string $provider) { $scopes = []; if ($provider === \u0026#39;github\u0026#39;) { $scopes = [\u0026#39;user:email\u0026#39;]; } return Socialite::driver($provider) -\u0026gt;scopes($scopes) -\u0026gt;redirect(); } Request additional scopes Request more permissions if you need extra data:\n// Google - request profile and email (default) Socialite::driver(\u0026#39;google\u0026#39;) -\u0026gt;scopes([\u0026#39;profile\u0026#39;, \u0026#39;email\u0026#39;]) -\u0026gt;redirect(); // Facebook - request email and public profile Socialite::driver(\u0026#39;facebook\u0026#39;) -\u0026gt;scopes([\u0026#39;email\u0026#39;, \u0026#39;public_profile\u0026#39;]) -\u0026gt;redirect(); // GitHub - request user email Socialite::driver(\u0026#39;github\u0026#39;) -\u0026gt;scopes([\u0026#39;user:email\u0026#39;]) -\u0026gt;redirect(); Only request what you need. More scopes mean more permission prompts, which reduces conversion.\nRetrieve additional user data Access extra fields from the provider:\n$socialUser = Socialite::driver($provider)-\u0026gt;user(); // Common fields (all providers) $name = $socialUser-\u0026gt;getName(); $email = $socialUser-\u0026gt;getEmail(); $avatar = $socialUser-\u0026gt;getAvatar(); $id = $socialUser-\u0026gt;getId(); $token = $socialUser-\u0026gt;token; // Provider-specific fields $raw = $socialUser-\u0026gt;getRaw(); // All data from provider // Example: Get phone from Facebook if ($provider === \u0026#39;facebook\u0026#39; \u0026amp;\u0026amp; isset($raw[\u0026#39;phone\u0026#39;])) { $phone = $raw[\u0026#39;phone\u0026#39;]; } Check provider documentation for available fields.\nEncrypt access tokens OAuth tokens are sensitive. Encrypt them before storing:\nAdd cast to SocialAccount model:\nprotected $casts = [ \u0026#39;access_token\u0026#39; =\u0026gt; \u0026#39;encrypted\u0026#39;, \u0026#39;refresh_token\u0026#39; =\u0026gt; \u0026#39;encrypted\u0026#39;, \u0026#39;token_expires_at\u0026#39; =\u0026gt; \u0026#39;datetime\u0026#39;, ]; Laravel automatically encrypts these fields when saving and decrypts when reading.\nLink social accounts to existing users Let logged-in users link social accounts:\n// Add to routes Route::middleware(\u0026#39;auth\u0026#39;)-\u0026gt;group(function () { Route::get(\u0026#39;account/link/{provider}\u0026#39;, [SocialAuthController::class, \u0026#39;linkRedirect\u0026#39;]) -\u0026gt;name(\u0026#39;social.link\u0026#39;); Route::get(\u0026#39;account/link/{provider}/callback\u0026#39;, [SocialAuthController::class, \u0026#39;linkCallback\u0026#39;]) -\u0026gt;name(\u0026#39;social.link.callback\u0026#39;); }); Controller methods:\npublic function linkRedirect(string $provider) { return Socialite::driver($provider)-\u0026gt;redirect(); } public function linkCallback(string $provider) { $socialUser = Socialite::driver($provider)-\u0026gt;user(); $user = auth()-\u0026gt;user(); // Check if this social account is already linked to another user $existing = SocialAccount::where(\u0026#39;provider\u0026#39;, $provider) -\u0026gt;where(\u0026#39;provider_id\u0026#39;, $socialUser-\u0026gt;getId()) -\u0026gt;first(); if ($existing \u0026amp;\u0026amp; $existing-\u0026gt;user_id !== $user-\u0026gt;id) { return redirect(\u0026#39;/account/settings\u0026#39;) -\u0026gt;withErrors(\u0026#39;This social account is already linked to another user.\u0026#39;); } // Link or update social account $user-\u0026gt;socialAccounts()-\u0026gt;updateOrCreate( [ \u0026#39;provider\u0026#39; =\u0026gt; $provider, \u0026#39;provider_id\u0026#39; =\u0026gt; $socialUser-\u0026gt;getId(), ], [ \u0026#39;access_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;token, \u0026#39;refresh_token\u0026#39; =\u0026gt; $socialUser-\u0026gt;refreshToken, \u0026#39;token_expires_at\u0026#39; =\u0026gt; $socialUser-\u0026gt;expiresIn ? now()-\u0026gt;addSeconds($socialUser-\u0026gt;expiresIn) : null, ] ); return redirect(\u0026#39;/account/settings\u0026#39;) -\u0026gt;with(\u0026#39;success\u0026#39;, ucfirst($provider) . \u0026#39; account linked successfully.\u0026#39;); } This lets users add Google to an existing account created with email/password.\nUnlink social accounts Let users remove linked accounts:\nRoute::delete(\u0026#39;account/unlink/{provider}\u0026#39;, function (string $provider) { $user = auth()-\u0026gt;user(); // Don\u0026#39;t unlink if it\u0026#39;s the only login method if ($user-\u0026gt;socialAccounts()-\u0026gt;count() === 1 \u0026amp;\u0026amp; !$user-\u0026gt;password) { return back()-\u0026gt;withErrors(\u0026#39;Cannot unlink your only login method. Set a password first.\u0026#39;); } $user-\u0026gt;socialAccounts()-\u0026gt;where(\u0026#39;provider\u0026#39;, $provider)-\u0026gt;delete(); return back()-\u0026gt;with(\u0026#39;success\u0026#39;, ucfirst($provider) . \u0026#39; account unlinked.\u0026#39;); })-\u0026gt;name(\u0026#39;social.unlink\u0026#39;); Always check users have another way to log in before unlinking.\nRefresh expired tokens OAuth tokens expire. Refresh them when needed:\nuse Laravel\\Socialite\\Facades\\Socialite; public function refreshToken(SocialAccount $socialAccount) { if (!$socialAccount-\u0026gt;refresh_token) { throw new \\Exception(\u0026#39;No refresh token available\u0026#39;); } try { $provider = Socialite::driver($socialAccount-\u0026gt;provider); $newToken = $provider-\u0026gt;refreshToken($socialAccount-\u0026gt;refresh_token); $socialAccount-\u0026gt;update([ \u0026#39;access_token\u0026#39; =\u0026gt; $newToken-\u0026gt;token, \u0026#39;token_expires_at\u0026#39; =\u0026gt; now()-\u0026gt;addSeconds($newToken-\u0026gt;expiresIn), ]); return $socialAccount; } catch (\\Exception $e) { // Token refresh failed - user needs to re-authenticate throw $e; } } Refresh tokens automatically before making API calls:\npublic function getAccessToken(SocialAccount $socialAccount) { // Check if token expired or expires in next 5 minutes if ($socialAccount-\u0026gt;token_expires_at \u0026amp;\u0026amp; $socialAccount-\u0026gt;token_expires_at-\u0026gt;subMinutes(5)-\u0026gt;isPast()) { $this-\u0026gt;refreshToken($socialAccount); $socialAccount-\u0026gt;refresh(); } return $socialAccount-\u0026gt;access_token; } Handle OAuth errors Catch and handle authentication failures:\npublic function callback(string $provider) { try { $socialUser = Socialite::driver($provider)-\u0026gt;user(); } catch (\\Laravel\\Socialite\\Two\\InvalidStateException $e) { // User clicked \u0026#34;back\u0026#34; or session expired return redirect(\u0026#39;/login\u0026#39;)-\u0026gt;withErrors(\u0026#39;Authentication session expired. Please try again.\u0026#39;); } catch (\\GuzzleHttp\\Exception\\ClientException $e) { // OAuth provider error return redirect(\u0026#39;/login\u0026#39;)-\u0026gt;withErrors(\u0026#39;Unable to connect to \u0026#39; . ucfirst($provider) . \u0026#39;. Please try again later.\u0026#39;); } catch (\\Exception $e) { // Generic error \\Log::error(\u0026#39;Social auth error: \u0026#39; . $e-\u0026gt;getMessage()); return redirect(\u0026#39;/login\u0026#39;)-\u0026gt;withErrors(\u0026#39;Authentication failed. Please try again.\u0026#39;); } // ... rest of callback logic } Log errors for debugging but show user-friendly messages.\nTest social login Test OAuth flows:\nuse Laravel\\Socialite\\Facades\\Socialite; use Laravel\\Socialite\\Two\\User as SocialUser; it(\u0026#39;creates user from Google OAuth\u0026#39;, function () { $socialUser = Mockery::mock(SocialUser::class); $socialUser-\u0026gt;shouldReceive(\u0026#39;getId\u0026#39;)-\u0026gt;andReturn(\u0026#39;google-id-123\u0026#39;); $socialUser-\u0026gt;shouldReceive(\u0026#39;getEmail\u0026#39;)-\u0026gt;andReturn(\u0026#39;user@example.com\u0026#39;); $socialUser-\u0026gt;shouldReceive(\u0026#39;getName\u0026#39;)-\u0026gt;andReturn(\u0026#39;John Doe\u0026#39;); $socialUser-\u0026gt;token = \u0026#39;fake-access-token\u0026#39;; $socialUser-\u0026gt;refreshToken = null; $socialUser-\u0026gt;expiresIn = 3600; Socialite::shouldReceive(\u0026#39;driver-\u0026gt;user\u0026#39;)-\u0026gt;andReturn($socialUser); $response = get(\u0026#39;/login/google/callback\u0026#39;); $response-\u0026gt;assertRedirect(\u0026#39;/dashboard\u0026#39;); assertDatabaseHas(\u0026#39;users\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;user@example.com\u0026#39;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;John Doe\u0026#39;, ]); assertDatabaseHas(\u0026#39;social_accounts\u0026#39;, [ \u0026#39;provider\u0026#39; =\u0026gt; \u0026#39;google\u0026#39;, \u0026#39;provider_id\u0026#39; =\u0026gt; \u0026#39;google-id-123\u0026#39;, ]); }); Mock Socialite to avoid hitting real OAuth APIs in tests.\nSecurity best practices Use HTTPS in production. OAuth tokens are sensitive.\nStore tokens encrypted with Laravel\u0026rsquo;s encrypted casts.\nValidate redirect URIs match exactly in OAuth app settings. Attackers can exploit misconfigurations.\nRegenerate session ID after login to prevent session fixation:\n$request-\u0026gt;session()-\u0026gt;regenerate(); Laravel does this automatically in auth()-\u0026gt;login(), but explicit is better.\nRate limit OAuth endpoints to prevent abuse:\nRoute::get(\u0026#39;login/{provider}/callback\u0026#39;, [SocialAuthController::class, \u0026#39;callback\u0026#39;]) -\u0026gt;middleware(\u0026#39;throttle:10,1\u0026#39;); Don\u0026rsquo;t trust user data from providers blindly. Validate and sanitize names and emails.\nSummary Laravel Socialite makes OAuth integration simple. Install Socialite, create OAuth apps with Google, Facebook, and GitHub, configure credentials in config/services.php, and add routes for redirecting to providers and handling callbacks.\nCreate a social_accounts table to store OAuth data linked to users. Use updateOrCreate to find existing users by email or create new ones. Handle edge cases like missing emails, duplicate accounts, and token expiration.\nEncrypt access tokens, refresh them before expiration, and test OAuth flows with mocks. Always use HTTPS and validate redirect URIs in production.\nFor more on Laravel authentication, see Laravel 2FA Implementation and Laravel API Authentication with Sanctum .\n","href":"/2025/10/how-to-implement-laravel-socialite-oauth-login-google-facebook-github.html","title":"How to Implement Laravel Socialite for OAuth Login (Google, Facebook, GitHub)"},{"content":"Automated testing catches bugs before users see them. Laravel makes testing easy with built-in support for PHPUnit and Pest. You write tests that run your code and check the results match what you expect. When you change something, run the tests to make sure nothing broke.\nThis guide shows you how to set up testing in Laravel, write feature tests for HTTP endpoints, write unit tests for business logic, test databases with factories, mock external services, test APIs, and integrate tests into your deployment pipeline.\nWhy automated testing matters Manual testing is slow and error-prone. You can\u0026rsquo;t manually test every feature after every change. Automated tests run in seconds and catch regressions.\nBenefits:\nCatch bugs before deployment Refactor with confidence - tests tell you if you broke something Document how your code should work Speed up development - tests are faster than manual clicking Enable continuous deployment - deploy automatically when tests pass Write tests for code that matters. Don\u0026rsquo;t aim for 100% coverage. Focus on business logic, critical user flows, and anything that\u0026rsquo;s broken before.\nLaravel\u0026rsquo;s test structure Laravel includes two test directories:\ntests/Feature - test complete features like HTTP requests, database operations, full flows tests/Unit - test individual methods and classes in isolation Feature tests are more valuable. They test real user scenarios. Unit tests are for complex business logic that needs isolation.\nLaravel uses PHPUnit by default. The phpunit.xml file in your project root configures how tests run.\nInstall Pest (optional but recommended) Pest offers cleaner syntax than PHPUnit. Install it:\ncomposer require pestphp/pest --dev --with-all-dependencies composer require pestphp/pest-plugin-laravel --dev php artisan pest:install This adds Pest configuration to your project. You can use Pest and PHPUnit tests in the same project.\nWrite your first feature test Generate a test:\nphp artisan make:test PostTest This creates tests/Feature/PostTest.php. With PHPUnit:\n\u0026lt;?php namespace Tests\\Feature; use Tests\\TestCase; use App\\Models\\User; use App\\Models\\Post; use Illuminate\\Foundation\\Testing\\RefreshDatabase; class PostTest extends TestCase { use RefreshDatabase; public function test_user_can_create_post() { $user = User::factory()-\u0026gt;create(); $response = $this-\u0026gt;actingAs($user)-\u0026gt;post(\u0026#39;/posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My First Post\u0026#39;, \u0026#39;body\u0026#39; =\u0026gt; \u0026#39;This is the post content.\u0026#39;, ]); $response-\u0026gt;assertStatus(201); $this-\u0026gt;assertDatabaseHas(\u0026#39;posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My First Post\u0026#39;, \u0026#39;user_id\u0026#39; =\u0026gt; $user-\u0026gt;id, ]); } } With Pest:\n\u0026lt;?php use App\\Models\\User; use App\\Models\\Post; use function Pest\\Laravel\\actingAs; use function Pest\\Laravel\\post; use function Pest\\Laravel\\assertDatabaseHas; it(\u0026#39;allows authenticated users to create posts\u0026#39;, function () { $user = User::factory()-\u0026gt;create(); $response = actingAs($user)-\u0026gt;post(\u0026#39;/posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My First Post\u0026#39;, \u0026#39;body\u0026#39; =\u0026gt; \u0026#39;This is the post content.\u0026#39;, ]); $response-\u0026gt;assertStatus(201); assertDatabaseHas(\u0026#39;posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My First Post\u0026#39;, \u0026#39;user_id\u0026#39; =\u0026gt; $user-\u0026gt;id, ]); }); Run tests:\nphp artisan test # or ./vendor/bin/pest Understanding RefreshDatabase The RefreshDatabase trait (PHPUnit) or uses(RefreshDatabase::class) (Pest) runs migrations before tests and rolls back after each test.\nThis gives you a clean database for every test. Tests don\u0026rsquo;t interfere with each other.\nFor Pest, add to tests/Pest.php:\nuses(Tests\\TestCase::class, Illuminate\\Foundation\\Testing\\RefreshDatabase::class) -\u0026gt;in(\u0026#39;Feature\u0026#39;); Now all feature tests automatically use RefreshDatabase.\nFor faster tests, use DatabaseTransactions instead. It wraps tests in transactions and rolls them back instead of migrating. The tradeoff: it doesn\u0026rsquo;t catch migration bugs.\nCreate test data with factories Don\u0026rsquo;t create test records manually. Use factories:\nphp artisan make:factory PostFactory Define the factory in database/factories/PostFactory.php:\n\u0026lt;?php namespace Database\\Factories; use Illuminate\\Database\\Eloquent\\Factories\\Factory; class PostFactory extends Factory { public function definition() { return [ \u0026#39;title\u0026#39; =\u0026gt; fake()-\u0026gt;sentence(), \u0026#39;body\u0026#39; =\u0026gt; fake()-\u0026gt;paragraphs(3, true), \u0026#39;published_at\u0026#39; =\u0026gt; fake()-\u0026gt;dateTimeBetween(\u0026#39;-1 month\u0026#39;, \u0026#39;now\u0026#39;), ]; } public function draft() { return $this-\u0026gt;state(fn (array $attributes) =\u0026gt; [ \u0026#39;published_at\u0026#39; =\u0026gt; null, ]); } } Use in tests:\n// Create one post $post = Post::factory()-\u0026gt;create(); // Create multiple posts $posts = Post::factory()-\u0026gt;count(10)-\u0026gt;create(); // Create draft post using state $draft = Post::factory()-\u0026gt;draft()-\u0026gt;create(); // Create with specific attributes $post = Post::factory()-\u0026gt;create([ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;Custom Title\u0026#39;, ]); // Create with relationships $post = Post::factory() -\u0026gt;for(User::factory()) -\u0026gt;has(Comment::factory()-\u0026gt;count(3)) -\u0026gt;create(); Factories keep tests fast and readable. Change factory definitions once and all tests update.\nWrite unit tests Unit tests check small pieces of logic in isolation.\nphp artisan make:test --unit CalculatorTest PHPUnit example:\n\u0026lt;?php namespace Tests\\Unit; use PHPUnit\\Framework\\TestCase; use App\\Services\\Calculator; class CalculatorTest extends TestCase { public function test_it_calculates_total_with_tax() { $calculator = new Calculator(); $total = $calculator-\u0026gt;calculateWithTax(100, 0.15); $this-\u0026gt;assertEquals(115, $total); } } Pest example:\n\u0026lt;?php use App\\Services\\Calculator; it(\u0026#39;calculates total with tax\u0026#39;, function () { $calculator = new Calculator(); $total = $calculator-\u0026gt;calculateWithTax(100, 0.15); expect($total)-\u0026gt;toBe(115.0); }); Unit tests use plain PHPUnit TestCase, not Laravel\u0026rsquo;s TestCase. They don\u0026rsquo;t boot Laravel or touch the database. This makes them fast.\nTest HTTP responses Laravel provides assertions for HTTP responses:\n$response = $this-\u0026gt;get(\u0026#39;/posts\u0026#39;); $response-\u0026gt;assertOk(); // 200 $response-\u0026gt;assertStatus(201); // specific status $response-\u0026gt;assertRedirect(\u0026#39;/posts/1\u0026#39;); // redirect $response-\u0026gt;assertJson([\u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My Post\u0026#39;]); // JSON response $response-\u0026gt;assertJsonStructure([\u0026#39;data\u0026#39; =\u0026gt; [\u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;]]); // JSON structure $response-\u0026gt;assertJsonCount(10, \u0026#39;data\u0026#39;); // array count $response-\u0026gt;assertSee(\u0026#39;My Post\u0026#39;); // HTML content $response-\u0026gt;assertDontSee(\u0026#39;Secret\u0026#39;); // not in response Test validation errors:\n$response = $this-\u0026gt;post(\u0026#39;/posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, // empty title \u0026#39;body\u0026#39; =\u0026gt; \u0026#39;Content\u0026#39;, ]); $response-\u0026gt;assertStatus(422); $response-\u0026gt;assertJsonValidationErrors([\u0026#39;title\u0026#39;]); Test authentication and authorization Test login:\nit(\u0026#39;allows users to log in with valid credentials\u0026#39;, function () { $user = User::factory()-\u0026gt;create([ \u0026#39;password\u0026#39; =\u0026gt; bcrypt(\u0026#39;password123\u0026#39;), ]); $response = post(\u0026#39;/login\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;password123\u0026#39;, ]); $response-\u0026gt;assertRedirect(\u0026#39;/dashboard\u0026#39;); expect(auth()-\u0026gt;check())-\u0026gt;toBeTrue(); }); Test protected routes:\nit(\u0026#39;blocks guests from accessing dashboard\u0026#39;, function () { $response = get(\u0026#39;/dashboard\u0026#39;); $response-\u0026gt;assertRedirect(\u0026#39;/login\u0026#39;); }); it(\u0026#39;allows authenticated users to access dashboard\u0026#39;, function () { $user = User::factory()-\u0026gt;create(); $response = actingAs($user)-\u0026gt;get(\u0026#39;/dashboard\u0026#39;); $response-\u0026gt;assertOk(); }); Test authorization with roles:\nit(\u0026#39;prevents non-admins from deleting posts\u0026#39;, function () { $user = User::factory()-\u0026gt;create(); $post = Post::factory()-\u0026gt;create(); $response = actingAs($user)-\u0026gt;delete(\u0026#34;/posts/{$post-\u0026gt;id}\u0026#34;); $response-\u0026gt;assertForbidden(); }); Mock external services Don\u0026rsquo;t hit real APIs in tests. Mock them:\nuse Illuminate\\Support\\Facades\\Http; it(\u0026#39;fetches weather data from API\u0026#39;, function () { Http::fake([ \u0026#39;api.weather.com/*\u0026#39; =\u0026gt; Http::response([ \u0026#39;temperature\u0026#39; =\u0026gt; 72, \u0026#39;condition\u0026#39; =\u0026gt; \u0026#39;sunny\u0026#39;, ], 200), ]); $service = new WeatherService(); $weather = $service-\u0026gt;getCurrentWeather(\u0026#39;New York\u0026#39;); expect($weather[\u0026#39;temperature\u0026#39;])-\u0026gt;toBe(72); Http::assertSent(function ($request) { return $request-\u0026gt;url() === \u0026#39;https://api.weather.com/current?city=New+York\u0026#39;; }); }); Mock your own classes with Laravel\u0026rsquo;s mock helper:\nit(\u0026#39;charges user via payment service\u0026#39;, function () { $this-\u0026gt;mock(PaymentService::class, function ($mock) { $mock-\u0026gt;shouldReceive(\u0026#39;charge\u0026#39;) -\u0026gt;once() -\u0026gt;with(100) -\u0026gt;andReturn(true); }); $service = app(OrderService::class); $result = $service-\u0026gt;createOrder([\u0026#39;total\u0026#39; =\u0026gt; 100]); expect($result)-\u0026gt;toBeTrue(); }); Only mock external dependencies you don\u0026rsquo;t control. Test your own code with real instances.\nTest databases with assertions Check records exist:\nuse function Pest\\Laravel\\assertDatabaseHas; use function Pest\\Laravel\\assertDatabaseMissing; it(\u0026#39;stores post in database\u0026#39;, function () { $user = User::factory()-\u0026gt;create(); actingAs($user)-\u0026gt;post(\u0026#39;/posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My Post\u0026#39;, \u0026#39;body\u0026#39; =\u0026gt; \u0026#39;Content\u0026#39;, ]); assertDatabaseHas(\u0026#39;posts\u0026#39;, [ \u0026#39;title\u0026#39; =\u0026gt; \u0026#39;My Post\u0026#39;, \u0026#39;user_id\u0026#39; =\u0026gt; $user-\u0026gt;id, ]); }); it(\u0026#39;deletes post from database\u0026#39;, function () { $post = Post::factory()-\u0026gt;create(); delete(\u0026#34;/posts/{$post-\u0026gt;id}\u0026#34;); assertDatabaseMissing(\u0026#39;posts\u0026#39;, [\u0026#39;id\u0026#39; =\u0026gt; $post-\u0026gt;id]); }); Count records:\nassertDatabaseCount(\u0026#39;posts\u0026#39;, 5); Test soft deletes:\nuse function Pest\\Laravel\\assertSoftDeleted; it(\u0026#39;soft deletes posts\u0026#39;, function () { $post = Post::factory()-\u0026gt;create(); delete(\u0026#34;/posts/{$post-\u0026gt;id}\u0026#34;); assertSoftDeleted(\u0026#39;posts\u0026#39;, [\u0026#39;id\u0026#39; =\u0026gt; $post-\u0026gt;id]); }); Test JSON APIs For API endpoints:\nit(\u0026#39;returns paginated posts as JSON\u0026#39;, function () { Post::factory()-\u0026gt;count(15)-\u0026gt;create(); $response = get(\u0026#39;/api/posts?page=1\u0026amp;per_page=10\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJsonStructure([ \u0026#39;data\u0026#39; =\u0026gt; [ \u0026#39;*\u0026#39; =\u0026gt; [\u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;body\u0026#39;, \u0026#39;created_at\u0026#39;] ], \u0026#39;links\u0026#39;, \u0026#39;meta\u0026#39; =\u0026gt; [\u0026#39;current_page\u0026#39;, \u0026#39;total\u0026#39;], ]) -\u0026gt;assertJsonCount(10, \u0026#39;data\u0026#39;); }); Test API authentication with Sanctum:\nit(\u0026#39;requires authentication for protected endpoints\u0026#39;, function () { $response = get(\u0026#39;/api/user/profile\u0026#39;); $response-\u0026gt;assertUnauthorized(); }); it(\u0026#39;returns user profile with valid token\u0026#39;, function () { $user = User::factory()-\u0026gt;create(); $token = $user-\u0026gt;createToken(\u0026#39;test\u0026#39;)-\u0026gt;plainTextToken; $response = withHeader(\u0026#39;Authorization\u0026#39;, \u0026#34;Bearer {$token}\u0026#34;) -\u0026gt;get(\u0026#39;/api/user/profile\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJson([\u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email]); }); See: Laravel API Pagination and Filtering for testing pagination.\nOrganize tests with datasets Pest datasets let you run the same test with different inputs:\nit(\u0026#39;validates email format\u0026#39;, function ($email, $isValid) { $response = post(\u0026#39;/register\u0026#39;, [\u0026#39;email\u0026#39; =\u0026gt; $email]); if ($isValid) { $response-\u0026gt;assertSessionHasNoErrors(\u0026#39;email\u0026#39;); } else { $response-\u0026gt;assertSessionHasErrors(\u0026#39;email\u0026#39;); } })-\u0026gt;with([ [\u0026#39;user@example.com\u0026#39;, true], [\u0026#39;invalid-email\u0026#39;, false], [\u0026#39;@example.com\u0026#39;, false], [\u0026#39;user@\u0026#39;, false], ]); This runs the test four times with different emails.\nRun tests in parallel Speed up test runs by running tests in parallel:\nphp artisan test --parallel # or ./vendor/bin/pest --parallel Install the paratest package first:\ncomposer require brianium/paratest --dev This runs tests across multiple processes. A 2-minute test suite might run in 30 seconds.\nFor Pest, parallelization works out of the box. For PHPUnit, use --parallel=4 to set process count.\nTest coverage Check which code your tests cover:\nphp artisan test --coverage This shows percentage of code covered by tests. Aim for 70-80% coverage of important code, not 100% of everything.\nAdd minimum coverage threshold to phpunit.xml:\n\u0026lt;coverage\u0026gt; \u0026lt;report\u0026gt; \u0026lt;html outputDirectory=\u0026#34;coverage\u0026#34;/\u0026gt; \u0026lt;/report\u0026gt; \u0026lt;/coverage\u0026gt; Generate an HTML coverage report:\nphp artisan test --coverage-html coverage Open coverage/index.html to see detailed line-by-line coverage.\nSet up continuous integration Run tests automatically on every commit. Example GitHub Actions workflow:\nCreate .github/workflows/tests.yml:\nname: Tests on: [push, pull_request] jobs: tests: runs-on: ubuntu-latest services: mysql: image: mysql:8.0 env: MYSQL_ROOT_PASSWORD: password MYSQL_DATABASE: test ports: - 3306:3306 options: --health-cmd=\u0026#34;mysqladmin ping\u0026#34; --health-interval=10s --health-timeout=5s --health-retries=3 steps: - uses: actions/checkout@v3 - name: Setup PHP uses: shivammathur/setup-php@v2 with: php-version: 8.2 extensions: mbstring, pdo_mysql - name: Install dependencies run: composer install --no-interaction --prefer-dist - name: Copy .env run: php -r \u0026#34;file_exists(\u0026#39;.env\u0026#39;) || copy(\u0026#39;.env.example\u0026#39;, \u0026#39;.env\u0026#39;);\u0026#34; - name: Generate key run: php artisan key:generate - name: Run migrations run: php artisan migrate --env=testing - name: Run tests run: php artisan test --parallel This runs tests on every push. If tests fail, the build fails and you know not to merge.\nCache Composer dependencies to speed up builds:\n- name: Cache Composer dependencies uses: actions/cache@v3 with: path: vendor key: composer-${{ hashFiles(\u0026#39;composer.lock\u0026#39;) }} Best practices for Laravel testing Write tests before fixing bugs. This ensures the bug stays fixed.\nKeep tests fast. Slow tests don\u0026rsquo;t get run. Use factories instead of seeders. Mock external APIs. Use RefreshDatabase only when needed.\nTest behavior, not implementation. If you refactor a method but behavior stays the same, tests should still pass.\nDon\u0026rsquo;t test framework code. Laravel\u0026rsquo;s validation, routing, and ORM are already tested. Test your business logic.\nName tests clearly. Use it('allows users to create posts') not test_create_post(). The test name should describe what happens.\nOne assertion per test when possible. This makes failures easier to debug.\nAvoid testing private methods. Test public methods that call private methods. If you need to test a private method, maybe it should be public or in its own class.\nUse database transactions for faster tests when you don\u0026rsquo;t need to test migrations:\nuse Illuminate\\Foundation\\Testing\\DatabaseTransactions; uses(DatabaseTransactions::class)-\u0026gt;in(\u0026#39;Feature\u0026#39;); Don\u0026rsquo;t share state between tests. Each test should be independent. Use factories to create data per test.\nTesting common scenarios Test pagination:\nit(\u0026#39;paginates posts\u0026#39;, function () { Post::factory()-\u0026gt;count(25)-\u0026gt;create(); $response = get(\u0026#39;/posts?page=2\u0026amp;per_page=10\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJsonCount(10, \u0026#39;data\u0026#39;) -\u0026gt;assertJsonPath(\u0026#39;meta.current_page\u0026#39;, 2); }); Test file uploads:\nuse Illuminate\\Http\\UploadedFile; use Illuminate\\Support\\Facades\\Storage; it(\u0026#39;uploads user avatar\u0026#39;, function () { Storage::fake(\u0026#39;public\u0026#39;); $file = UploadedFile::fake()-\u0026gt;image(\u0026#39;avatar.jpg\u0026#39;); $response = post(\u0026#39;/profile/avatar\u0026#39;, [ \u0026#39;avatar\u0026#39; =\u0026gt; $file, ]); $response-\u0026gt;assertOk(); Storage::disk(\u0026#39;public\u0026#39;)-\u0026gt;assertExists(\u0026#39;avatars/\u0026#39; . $file-\u0026gt;hashName()); }); Test emails:\nuse Illuminate\\Support\\Facades\\Mail; use App\\Mail\\WelcomeEmail; it(\u0026#39;sends welcome email to new users\u0026#39;, function () { Mail::fake(); post(\u0026#39;/register\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;user@example.com\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;password\u0026#39;, ]); Mail::assertSent(WelcomeEmail::class, function ($mail) { return $mail-\u0026gt;hasTo(\u0026#39;user@example.com\u0026#39;); }); }); Test queued jobs:\nuse Illuminate\\Support\\Facades\\Queue; use App\\Jobs\\ProcessOrder; it(\u0026#39;queues order processing job\u0026#39;, function () { Queue::fake(); post(\u0026#39;/orders\u0026#39;, [\u0026#39;total\u0026#39; =\u0026gt; 100]); Queue::assertPushed(ProcessOrder::class); }); Debugging failing tests When a test fails, Laravel shows the failure message and stack trace.\nAdd dump() or dd() in tests to inspect values:\nit(\u0026#39;calculates total\u0026#39;, function () { $cart = new Cart(); $total = $cart-\u0026gt;calculateTotal(); dump($total); // shows value expect($total)-\u0026gt;toBe(100); }); Use $this-\u0026gt;withoutExceptionHandling() in PHPUnit or Pest to see full error details instead of HTTP status codes:\nit(\u0026#39;creates post\u0026#39;, function () { $this-\u0026gt;withoutExceptionHandling(); $response = post(\u0026#39;/posts\u0026#39;, [\u0026#39;title\u0026#39; =\u0026gt; \u0026#39;Test\u0026#39;]); $response-\u0026gt;assertOk(); }); Check test database after failed tests:\nphp artisan tinker --env=testing Then query the database to see what was created.\nSummary Automated testing catches bugs early and lets you refactor with confidence. Use feature tests for HTTP flows and user scenarios. Use unit tests for isolated business logic.\nSet up Pest for cleaner syntax or stick with PHPUnit if you prefer. Use factories to create test data and RefreshDatabase to keep tests isolated. Mock external APIs with Http::fake() and test assertions cover databases, JSON responses, and validation.\nRun tests in parallel to keep them fast and integrate tests into CI/CD with GitHub Actions or your preferred tool. Aim for good coverage of critical code, not 100% of everything.\nFor more on Laravel quality, see Laravel Security Best Practices and Laravel Performance Optimization .\n","href":"/2025/10/how-to-set-up-automated-testing-laravel-phpunit-pest.html","title":"How to Set Up Automated Testing in Laravel with PHPUnit and Pest"},{"content":"Role-Based Access Control (RBAC) lets you manage what users can do in your application by assigning them roles and permissions. Instead of checking if a specific user can edit posts, you check if they have the editor role or the edit-posts permission. This makes access control flexible and maintainable.\nLaravel\u0026rsquo;s Spatie Permission package handles all the database tables, relationships, and helper methods you need for RBAC. This guide walks through installing Spatie Permission, creating roles and permissions, assigning them to users, protecting routes and controllers, using Blade directives, and testing everything.\nWhen to use RBAC Use RBAC when different user types need different access levels. Common scenarios:\nAdmin panels where admins manage everything but editors only manage content Multi-tenant apps where users have different permissions per organization SaaS applications with tiered feature access (free, pro, enterprise) Content management systems with writers, editors, and publishers Any app where you need an admin UI to manage who can do what Skip RBAC if your app only has two user types (admin and regular user) with simple checks. A simple is_admin boolean might be enough. Use RBAC when you need three or more roles or when non-developers need to manage permissions.\nInstall Spatie Permission package Install via Composer:\ncomposer require spatie/laravel-permission Publish the migration and config:\nphp artisan vendor:publish --provider=\u0026#34;Spatie\\Permission\\PermissionServiceProvider\u0026#34; This creates:\nMigration file for roles and permissions tables Config file at config/permission.php Run migrations:\nphp artisan migrate This adds four tables:\nroles - stores role names permissions - stores permission names model_has_roles - assigns roles to users model_has_permissions - assigns permissions directly to users role_has_permissions - assigns permissions to roles Add the HasRoles trait to User model Open app/Models/User.php and add the trait:\n\u0026lt;?php namespace App\\Models; use Illuminate\\Foundation\\Auth\\User as Authenticatable; use Spatie\\Permission\\Traits\\HasRoles; class User extends Authenticatable { use HasRoles; // ... rest of your model } This adds methods like assignRole(), hasPermissionTo(), can(), etc. to your User model.\nCreate roles and permissions Create roles and permissions in a seeder, migration, or admin panel. Here\u0026rsquo;s a seeder approach:\nphp artisan make:seeder RolePermissionSeeder Edit database/seeders/RolePermissionSeeder.php:\n\u0026lt;?php namespace Database\\Seeders; use Illuminate\\Database\\Seeder; use Spatie\\Permission\\Models\\Role; use Spatie\\Permission\\Models\\Permission; class RolePermissionSeeder extends Seeder { public function run() { // Reset cached roles and permissions app()[\\Spatie\\Permission\\PermissionRegistrar::class]-\u0026gt;forgetCachedPermissions(); // Create permissions Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;create-post\u0026#39;]); Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;edit-post\u0026#39;]); Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;delete-post\u0026#39;]); Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;publish-post\u0026#39;]); Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;manage-users\u0026#39;]); Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;manage-roles\u0026#39;]); // Create roles and assign permissions $writerRole = Role::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;writer\u0026#39;]); $writerRole-\u0026gt;givePermissionTo([\u0026#39;create-post\u0026#39;, \u0026#39;edit-post\u0026#39;]); $editorRole = Role::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;editor\u0026#39;]); $editorRole-\u0026gt;givePermissionTo([\u0026#39;create-post\u0026#39;, \u0026#39;edit-post\u0026#39;, \u0026#39;delete-post\u0026#39;, \u0026#39;publish-post\u0026#39;]); $adminRole = Role::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;admin\u0026#39;]); $adminRole-\u0026gt;givePermissionTo(Permission::all()); // Assign super admin to first user $superAdmin = \\App\\Models\\User::find(1); if ($superAdmin) { $superAdmin-\u0026gt;assignRole(\u0026#39;admin\u0026#39;); } } } Run the seeder:\nphp artisan db:seed --class=RolePermissionSeeder Assign roles to users Assign roles when creating users or through an admin interface:\nuse App\\Models\\User; $user = User::find(1); // Assign a single role $user-\u0026gt;assignRole(\u0026#39;writer\u0026#39;); // Assign multiple roles $user-\u0026gt;assignRole([\u0026#39;writer\u0026#39;, \u0026#39;editor\u0026#39;]); // Or using role model $user-\u0026gt;assignRole(Role::findByName(\u0026#39;admin\u0026#39;)); // Remove roles $user-\u0026gt;removeRole(\u0026#39;writer\u0026#39;); // Sync roles (removes old roles, adds new ones) $user-\u0026gt;syncRoles([\u0026#39;editor\u0026#39;]); Check if a user has a role:\nif ($user-\u0026gt;hasRole(\u0026#39;admin\u0026#39;)) { // User is admin } // Check for any of these roles if ($user-\u0026gt;hasAnyRole([\u0026#39;admin\u0026#39;, \u0026#39;editor\u0026#39;])) { // User is admin OR editor } // Check for all roles if ($user-\u0026gt;hasAllRoles([\u0026#39;writer\u0026#39;, \u0026#39;editor\u0026#39;])) { // User is both writer AND editor } // Get all roles $roles = $user-\u0026gt;getRoleNames(); // Returns collection of role names Assign permissions to users You can assign permissions directly to users (bypassing roles):\n// Give permission directly $user-\u0026gt;givePermissionTo(\u0026#39;edit-post\u0026#39;); // Give multiple permissions $user-\u0026gt;givePermissionTo([\u0026#39;edit-post\u0026#39;, \u0026#39;delete-post\u0026#39;]); // Revoke permission $user-\u0026gt;revokePermissionTo(\u0026#39;delete-post\u0026#39;); // Sync permissions $user-\u0026gt;syncPermissions([\u0026#39;edit-post\u0026#39;, \u0026#39;publish-post\u0026#39;]); Check permissions:\nif ($user-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;)) { // User can edit posts } // Check for any permission if ($user-\u0026gt;hasAnyPermission([\u0026#39;edit-post\u0026#39;, \u0026#39;delete-post\u0026#39;])) { // User has at least one of these permissions } // Using Laravel\u0026#39;s can() method (works with Spatie) if ($user-\u0026gt;can(\u0026#39;edit-post\u0026#39;)) { // User can edit posts } Direct permissions are useful for one-off access. For example, giving a specific user temporary admin access without making them an admin.\nProtect routes with middleware Spatie provides middleware to protect routes by role or permission.\nRegister middleware aliases in bootstrap/app.php (Laravel 11) or app/Http/Kernel.php (Laravel 10):\nFor Laravel 11:\n-\u0026gt;withMiddleware(function (Middleware $middleware) { $middleware-\u0026gt;alias([ \u0026#39;role\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\RoleMiddleware::class, \u0026#39;permission\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\PermissionMiddleware::class, \u0026#39;role_or_permission\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\RoleOrPermissionMiddleware::class, ]); }) For Laravel 10 and below, add to $middlewareAliases in app/Http/Kernel.php:\nprotected $middlewareAliases = [ // ... \u0026#39;role\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\RoleMiddleware::class, \u0026#39;permission\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\PermissionMiddleware::class, \u0026#39;role_or_permission\u0026#39; =\u0026gt; \\Spatie\\Permission\\Middleware\\RoleOrPermissionMiddleware::class, ]; Protect routes:\n// Only admins can access Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;role:admin\u0026#39;])-\u0026gt;group(function () { Route::get(\u0026#39;/admin/users\u0026#39;, [UserController::class, \u0026#39;index\u0026#39;]); Route::post(\u0026#39;/admin/users\u0026#39;, [UserController::class, \u0026#39;store\u0026#39;]); }); // Multiple roles (user needs ANY of these roles) Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;role:admin|editor\u0026#39;])-\u0026gt;group(function () { Route::get(\u0026#39;/posts/pending\u0026#39;, [PostController::class, \u0026#39;pending\u0026#39;]); }); // Require permission Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;permission:edit-post\u0026#39;])-\u0026gt;group(function () { Route::put(\u0026#39;/posts/{post}\u0026#39;, [PostController::class, \u0026#39;update\u0026#39;]); }); // Multiple permissions (user needs ALL of these) Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;permission:edit-post,publish-post\u0026#39;])-\u0026gt;group(function () { Route::post(\u0026#39;/posts/{post}/publish\u0026#39;, [PostController::class, \u0026#39;publish\u0026#39;]); }); // Role OR permission Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;role_or_permission:admin|edit-post\u0026#39;])-\u0026gt;group(function () { Route::get(\u0026#39;/posts/{post}/edit\u0026#39;, [PostController::class, \u0026#39;edit\u0026#39;]); }); If a user doesn\u0026rsquo;t have the required role or permission, they get a 403 Forbidden error.\nCheck permissions in controllers Check permissions programmatically in controller methods:\npublic function update(Request $request, Post $post) { // Throw 403 if user doesn\u0026#39;t have permission abort_unless(auth()-\u0026gt;user()-\u0026gt;can(\u0026#39;edit-post\u0026#39;), 403); // Or use authorize helper $this-\u0026gt;authorize(\u0026#39;edit-post\u0026#39;); // Or check manually if (!auth()-\u0026gt;user()-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;)) { return response()-\u0026gt;json([\u0026#39;error\u0026#39; =\u0026gt; \u0026#39;Unauthorized\u0026#39;], 403); } $post-\u0026gt;update($request-\u0026gt;validated()); return response()-\u0026gt;json($post); } Using authorize() is cleaner and works with Laravel\u0026rsquo;s authorization system.\nUse Blade directives in views Show or hide UI elements based on roles and permissions:\n@role(\u0026#39;admin\u0026#39;) \u0026lt;a href=\u0026#34;/admin\u0026#34;\u0026gt;Admin Panel\u0026lt;/a\u0026gt; @endrole @hasrole(\u0026#39;admin|editor\u0026#39;) \u0026lt;button\u0026gt;Edit Content\u0026lt;/button\u0026gt; @endhasrole @hasanyrole(\u0026#39;admin|editor|writer\u0026#39;) \u0026lt;a href=\u0026#34;/posts/create\u0026#34;\u0026gt;Create Post\u0026lt;/a\u0026gt; @endhasanyrole @permission(\u0026#39;delete-post\u0026#39;) \u0026lt;button\u0026gt;Delete Post\u0026lt;/button\u0026gt; @endpermission @haspermission(\u0026#39;edit-post|delete-post\u0026#39;) \u0026lt;div class=\u0026#34;post-actions\u0026#34;\u0026gt;...\u0026lt;/div\u0026gt; @endhaspermission @unlessrole(\u0026#39;admin\u0026#39;) \u0026lt;p\u0026gt;You need admin access\u0026lt;/p\u0026gt; @endunlessrole You can also use Laravel\u0026rsquo;s built-in @can directive (works with Spatie):\n@can(\u0026#39;edit-post\u0026#39;) \u0026lt;button\u0026gt;Edit\u0026lt;/button\u0026gt; @endcan @cannot(\u0026#39;delete-post\u0026#39;) \u0026lt;p\u0026gt;You cannot delete this post\u0026lt;/p\u0026gt; @endcannot Combine roles and direct permissions Users can have both roles and direct permissions. Spatie checks both:\n$user-\u0026gt;assignRole(\u0026#39;writer\u0026#39;); $user-\u0026gt;givePermissionTo(\u0026#39;manage-users\u0026#39;); // Extra permission not in writer role // Returns true if user has writer role OR the specific permission if ($user-\u0026gt;hasPermissionTo(\u0026#39;manage-users\u0026#39;)) { // True - user has direct permission } if ($user-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;)) { // True - user has this through writer role } Get all permissions (from roles AND direct assignments):\n$permissions = $user-\u0026gt;getAllPermissions(); // Collection of permission models $permissionNames = $user-\u0026gt;getPermissionNames(); // Collection of permission names This is useful when you need to give specific users extra permissions without creating new roles.\nProtect API routes For APIs protected with Sanctum:\nRoute::middleware([\u0026#39;auth:sanctum\u0026#39;, \u0026#39;permission:edit-post\u0026#39;])-\u0026gt;group(function () { Route::put(\u0026#39;/api/posts/{post}\u0026#39;, [PostController::class, \u0026#39;update\u0026#39;]); }); In API controllers, return JSON errors:\npublic function destroy(Post $post) { if (!auth()-\u0026gt;user()-\u0026gt;hasPermissionTo(\u0026#39;delete-post\u0026#39;)) { return response()-\u0026gt;json([ \u0026#39;message\u0026#39; =\u0026gt; \u0026#39;You do not have permission to delete posts\u0026#39; ], 403); } $post-\u0026gt;delete(); return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Post deleted\u0026#39;]); } Combine Sanctum token abilities with Spatie permissions for double protection. See: Laravel API Authentication with Sanctum .\nSuper admin bypass Grant super admins all permissions automatically with a Gate::before callback.\nAdd to app/Providers/AuthServiceProvider.php:\nuse Illuminate\\Support\\Facades\\Gate; public function boot() { Gate::before(function ($user, $ability) { return $user-\u0026gt;hasRole(\u0026#39;super-admin\u0026#39;) ? true : null; }); } Return true to grant access, null to continue normal permission checks. Now any user with the super-admin role passes all permission checks.\nCreate the super-admin role in your seeder:\n$superAdminRole = Role::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;super-admin\u0026#39;]); $superAdminRole-\u0026gt;givePermissionTo(Permission::all()); Create a role and permission manager UI Build an admin interface to manage roles and permissions without touching code:\n// routes/web.php Route::middleware([\u0026#39;auth\u0026#39;, \u0026#39;role:admin\u0026#39;])-\u0026gt;prefix(\u0026#39;admin\u0026#39;)-\u0026gt;group(function () { Route::resource(\u0026#39;roles\u0026#39;, RoleController::class); Route::resource(\u0026#39;permissions\u0026#39;, PermissionController::class); Route::post(\u0026#39;roles/{role}/permissions\u0026#39;, [RoleController::class, \u0026#39;attachPermissions\u0026#39;]); }); Example controller:\nuse Spatie\\Permission\\Models\\Role; use Spatie\\Permission\\Models\\Permission; class RoleController extends Controller { public function index() { $roles = Role::with(\u0026#39;permissions\u0026#39;)-\u0026gt;get(); return view(\u0026#39;admin.roles.index\u0026#39;, compact(\u0026#39;roles\u0026#39;)); } public function store(Request $request) { $validated = $request-\u0026gt;validate([ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;required|unique:roles,name\u0026#39;, ]); Role::create($validated); return redirect()-\u0026gt;route(\u0026#39;roles.index\u0026#39;)-\u0026gt;with(\u0026#39;success\u0026#39;, \u0026#39;Role created\u0026#39;); } public function attachPermissions(Request $request, Role $role) { $validated = $request-\u0026gt;validate([ \u0026#39;permissions\u0026#39; =\u0026gt; \u0026#39;required|array\u0026#39;, \u0026#39;permissions.*\u0026#39; =\u0026gt; \u0026#39;exists:permissions,name\u0026#39;, ]); $role-\u0026gt;syncPermissions($validated[\u0026#39;permissions\u0026#39;]); return back()-\u0026gt;with(\u0026#39;success\u0026#39;, \u0026#39;Permissions updated\u0026#39;); } } This lets non-developers manage access control through an admin panel.\nHandle multiple guards If you use multiple guards (web and api), specify the guard:\n// Create permission for specific guard Permission::create([\u0026#39;name\u0026#39; =\u0026gt; \u0026#39;edit-post\u0026#39;, \u0026#39;guard_name\u0026#39; =\u0026gt; \u0026#39;api\u0026#39;]); // Assign role with specific guard $user-\u0026gt;assignRole(\u0026#39;admin\u0026#39;, \u0026#39;api\u0026#39;); // Check permission with guard $user-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;, \u0026#39;api\u0026#39;); Spatie defaults to the default guard in config/auth.php. Only specify guards if you use multiple authentication systems.\nCache permissions for performance Spatie caches roles and permissions automatically. When you change them, clear the cache:\nphp artisan permission:cache-reset Or in code:\napp()[\\Spatie\\Permission\\PermissionRegistrar::class]-\u0026gt;forgetCachedPermissions(); For high-traffic apps, use Redis for caching. Set CACHE_DRIVER=redis in .env.\nTo manually cache after changes:\n$role-\u0026gt;givePermissionTo(\u0026#39;edit-post\u0026#39;); app()[\\Spatie\\Permission\\PermissionRegistrar::class]-\u0026gt;forgetCachedPermissions(); This ensures users see permission changes immediately.\nTesting RBAC Test your authorization logic:\nuse App\\Models\\User; use Spatie\\Permission\\Models\\Role; use Spatie\\Permission\\Models\\Permission; public function test_admin_can_access_admin_panel() { $admin = User::factory()-\u0026gt;create(); $admin-\u0026gt;assignRole(\u0026#39;admin\u0026#39;); $response = $this-\u0026gt;actingAs($admin)-\u0026gt;get(\u0026#39;/admin/users\u0026#39;); $response-\u0026gt;assertOk(); } public function test_writer_cannot_delete_posts() { $writer = User::factory()-\u0026gt;create(); $writer-\u0026gt;assignRole(\u0026#39;writer\u0026#39;); $response = $this-\u0026gt;actingAs($writer)-\u0026gt;delete(\u0026#39;/posts/1\u0026#39;); $response-\u0026gt;assertForbidden(); } public function test_user_with_permission_can_edit_post() { $user = User::factory()-\u0026gt;create(); $user-\u0026gt;givePermissionTo(\u0026#39;edit-post\u0026#39;); $this-\u0026gt;assertTrue($user-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;)); } public function test_super_admin_bypasses_all_checks() { $superAdmin = User::factory()-\u0026gt;create(); $superAdmin-\u0026gt;assignRole(\u0026#39;super-admin\u0026#39;); // Should pass even without specific permission $this-\u0026gt;assertTrue($superAdmin-\u0026gt;can(\u0026#39;any-random-permission\u0026#39;)); } Test both route protection and permission checks in your logic.\nCommon mistakes to avoid Don\u0026rsquo;t check roles when you should check permissions:\n// Bad - checking role if ($user-\u0026gt;hasRole(\u0026#39;admin\u0026#39;)) { $post-\u0026gt;delete(); } // Good - check permission if ($user-\u0026gt;hasPermissionTo(\u0026#39;delete-post\u0026#39;)) { $post-\u0026gt;delete(); } Roles can change. Permissions describe what users can actually do.\nDon\u0026rsquo;t forget to reset cache after changing permissions in production:\n$role-\u0026gt;givePermissionTo(\u0026#39;new-permission\u0026#39;); app()[\\Spatie\\Permission\\PermissionRegistrar::class]-\u0026gt;forgetCachedPermissions(); Don\u0026rsquo;t assign permissions one by one in loops:\n// Bad - runs many queries foreach ($permissions as $permission) { $role-\u0026gt;givePermissionTo($permission); } // Good - runs one query $role-\u0026gt;givePermissionTo($permissions); Don\u0026rsquo;t create too many granular permissions. edit-post is better than edit-post-title and edit-post-content. Keep permissions at a reasonable level of detail.\nPermissions vs Policies Use Spatie permissions for role-based access (can this user type do this action). Use Laravel policies for resource-based access (can this user edit this specific post).\nCombine both:\n// In PostPolicy public function update(User $user, Post $post) { // Check permission AND ownership return $user-\u0026gt;hasPermissionTo(\u0026#39;edit-post\u0026#39;) \u0026amp;\u0026amp; $user-\u0026gt;id === $post-\u0026gt;user_id; } Spatie handles who can perform an action type. Policies handle who can perform an action on a specific resource.\nFor resource ownership, see Laravel\u0026rsquo;s policy documentation. For role-based access, use Spatie Permission.\nSummary Spatie Permission makes RBAC simple in Laravel. Install the package, create roles and permissions in seeders, assign them to users, and protect routes with middleware.\nUse role and permission middleware on routes, check permissions in controllers with hasPermissionTo(), and show/hide UI with Blade directives. Build an admin panel to let non-developers manage access.\nCache permissions with Redis in production and reset cache when changing permissions. Test your authorization logic to ensure users only access what they should.\nFor more on securing Laravel apps, see Laravel Security Best Practices for Production and Laravel 2FA Implementation .\n","href":"/2025/10/how-to-implement-rbac-laravel-spatie-permission.html","title":"How to Implement Role-Based Access Control (RBAC) in Laravel with Spatie Permission"},{"content":"A good API lets clients filter, sort, and paginate through data. Nobody wants to download 10,000 records just to find a few items. This guide shows you how to add pagination, filtering, sorting, and search to your Laravel REST API.\nYou\u0026rsquo;ll learn Laravel\u0026rsquo;s built-in pagination methods, how to add query parameters for filters and sorting, search across multiple columns, cursor pagination for large datasets, and API resources for formatting responses. We\u0026rsquo;ll also cover performance tips and common mistakes.\nWhy pagination and filtering matter for APIs Without pagination, your API returns all records at once. This wastes bandwidth, slows response times, and crashes clients when datasets grow large.\nYour API should let clients:\nRequest specific page sizes (?per_page=20) Navigate through pages (?page=2) Filter by fields (?status=active\u0026amp;category=books) Sort results (?sort=-created_at for descending) Search across columns (?search=laravel) This keeps responses fast and gives clients control over the data they receive.\nBasic pagination with paginate() Laravel\u0026rsquo;s paginate() method handles pagination automatically:\nuse App\\Models\\Product; use Illuminate\\Http\\Request; public function index(Request $request) { $perPage = $request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15); $products = Product::paginate($perPage); return response()-\u0026gt;json($products); } Clients call: GET /products?page=2\u0026amp;per_page=20\nThe response includes pagination metadata:\n{ \u0026#34;data\u0026#34;: [...], \u0026#34;links\u0026#34;: { \u0026#34;first\u0026#34;: \u0026#34;http://api.example.com/products?page=1\u0026#34;, \u0026#34;last\u0026#34;: \u0026#34;http://api.example.com/products?page=10\u0026#34;, \u0026#34;prev\u0026#34;: \u0026#34;http://api.example.com/products?page=1\u0026#34;, \u0026#34;next\u0026#34;: \u0026#34;http://api.example.com/products?page=3\u0026#34; }, \u0026#34;meta\u0026#34;: { \u0026#34;current_page\u0026#34;: 2, \u0026#34;from\u0026#34;: 16, \u0026#34;last_page\u0026#34;: 10, \u0026#34;per_page\u0026#34;: 15, \u0026#34;to\u0026#34;: 30, \u0026#34;total\u0026#34;: 150 } } Always set a reasonable default (10-20) and a maximum limit to prevent abuse:\n$perPage = min($request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15), 100); Simple pagination for infinite scroll If you don\u0026rsquo;t need page numbers or total count, use simplePaginate():\n$products = Product::simplePaginate($perPage); This skips the COUNT query, which speeds up responses on large tables. The response only includes prev and next links, no total count.\nUse this for mobile apps with infinite scroll or feeds where users rarely jump to specific pages.\nCursor pagination for large datasets Cursor pagination uses WHERE clauses instead of OFFSET, which performs better on millions of records:\n$products = Product::orderBy(\u0026#39;id\u0026#39;)-\u0026gt;cursorPaginate($perPage); Request: GET /products?cursor=eyJpZCI6MTAwfQ\nCursors encode the position in the dataset. Clients can\u0026rsquo;t jump to arbitrary pages, only move forward/backward. This is perfect for:\nTime-series data (feeds, logs, events) Real-time data where new items appear Tables with millions of rows where OFFSET is slow The tradeoff: users can\u0026rsquo;t jump to page 50 directly.\nAdd filtering with query parameters Let clients filter by specific fields:\npublic function index(Request $request) { $query = Product::query(); if ($request-\u0026gt;has(\u0026#39;status\u0026#39;)) { $query-\u0026gt;where(\u0026#39;status\u0026#39;, $request-\u0026gt;status); } if ($request-\u0026gt;has(\u0026#39;category\u0026#39;)) { $query-\u0026gt;where(\u0026#39;category\u0026#39;, $request-\u0026gt;category); } if ($request-\u0026gt;has(\u0026#39;min_price\u0026#39;)) { $query-\u0026gt;where(\u0026#39;price\u0026#39;, \u0026#39;\u0026gt;=\u0026#39;, $request-\u0026gt;min_price); } if ($request-\u0026gt;has(\u0026#39;max_price\u0026#39;)) { $query-\u0026gt;where(\u0026#39;price\u0026#39;, \u0026#39;\u0026lt;=\u0026#39;, $request-\u0026gt;max_price); } $products = $query-\u0026gt;paginate($request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15)); return response()-\u0026gt;json($products); } Request: GET /products?status=active\u0026amp;category=electronics\u0026amp;min_price=100\u0026amp;page=1\nThis builds the WHERE clauses dynamically based on query parameters.\nValidate filter parameters Users might send invalid data. Validate before querying:\n$validated = $request-\u0026gt;validate([ \u0026#39;status\u0026#39; =\u0026gt; \u0026#39;sometimes|in:active,inactive,draft\u0026#39;, \u0026#39;category\u0026#39; =\u0026gt; \u0026#39;sometimes|string|max:50\u0026#39;, \u0026#39;min_price\u0026#39; =\u0026gt; \u0026#39;sometimes|numeric|min:0\u0026#39;, \u0026#39;max_price\u0026#39; =\u0026gt; \u0026#39;sometimes|numeric|min:0\u0026#39;, \u0026#39;per_page\u0026#39; =\u0026gt; \u0026#39;sometimes|integer|min:1|max:100\u0026#39;, \u0026#39;page\u0026#39; =\u0026gt; \u0026#39;sometimes|integer|min:1\u0026#39;, ]); $query = Product::query(); if (isset($validated[\u0026#39;status\u0026#39;])) { $query-\u0026gt;where(\u0026#39;status\u0026#39;, $validated[\u0026#39;status\u0026#39;]); } // ... rest of filters This prevents SQL errors and ensures clean data.\nImplement sorting Let clients sort by any column:\n$sortBy = $request-\u0026gt;input(\u0026#39;sort\u0026#39;, \u0026#39;created_at\u0026#39;); $sortDirection = \u0026#39;asc\u0026#39;; // Support descending sort with minus prefix: ?sort=-price if (str_starts_with($sortBy, \u0026#39;-\u0026#39;)) { $sortDirection = \u0026#39;desc\u0026#39;; $sortBy = substr($sortBy, 1); } // Whitelist sortable columns $allowedSorts = [\u0026#39;name\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;created_at\u0026#39;, \u0026#39;updated_at\u0026#39;]; if (in_array($sortBy, $allowedSorts)) { $query-\u0026gt;orderBy($sortBy, $sortDirection); } Request: GET /products?sort=-price (descending by price)\nAlways whitelist sortable columns to prevent sorting by sensitive fields or SQL injection.\nAdd search across multiple columns Simple search with OR conditions:\nif ($request-\u0026gt;has(\u0026#39;search\u0026#39;)) { $search = $request-\u0026gt;search; $query-\u0026gt;where(function($q) use ($search) { $q-\u0026gt;where(\u0026#39;name\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$search}%\u0026#34;) -\u0026gt;orWhere(\u0026#39;description\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$search}%\u0026#34;) -\u0026gt;orWhere(\u0026#39;sku\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$search}%\u0026#34;); }); } Laravel 11 adds whereAny() for cleaner syntax:\nif ($request-\u0026gt;has(\u0026#39;search\u0026#39;)) { $query-\u0026gt;whereAny( [\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;sku\u0026#39;], \u0026#39;LIKE\u0026#39;, \u0026#34;%{$request-\u0026gt;search}%\u0026#34; ); } For large datasets, use full-text search indexes:\nALTER TABLE products ADD FULLTEXT(name, description); if ($request-\u0026gt;has(\u0026#39;search\u0026#39;)) { $query-\u0026gt;whereRaw( \u0026#34;MATCH(name, description) AGAINST(? IN BOOLEAN MODE)\u0026#34;, [$request-\u0026gt;search] ); } Full-text search is much faster than LIKE on large tables.\nPreserve filters across pagination When users click \u0026ldquo;next page,\u0026rdquo; filters and sorting should persist. Use appends():\n$products = $query-\u0026gt;paginate($perPage); $products-\u0026gt;appends($request-\u0026gt;query()); return response()-\u0026gt;json($products); This includes all query parameters in pagination links:\n\u0026#34;next\u0026#34;: \u0026#34;http://api.example.com/products?status=active\u0026amp;sort=-price\u0026amp;page=3\u0026#34; Clients maintain their filters when navigating pages.\nUse API resources for clean responses Laravel API Resources format your data:\nphp artisan make:resource ProductResource \u0026lt;?php namespace App\\Http\\Resources; use Illuminate\\Http\\Resources\\Json\\JsonResource; class ProductResource extends JsonResource { public function toArray($request) { return [ \u0026#39;id\u0026#39; =\u0026gt; $this-\u0026gt;id, \u0026#39;name\u0026#39; =\u0026gt; $this-\u0026gt;name, \u0026#39;price\u0026#39; =\u0026gt; number_format($this-\u0026gt;price, 2), \u0026#39;status\u0026#39; =\u0026gt; $this-\u0026gt;status, \u0026#39;category\u0026#39; =\u0026gt; $this-\u0026gt;category, \u0026#39;created_at\u0026#39; =\u0026gt; $this-\u0026gt;created_at-\u0026gt;toISOString(), ]; } } Use it in your controller:\nuse App\\Http\\Resources\\ProductResource; $products = $query-\u0026gt;paginate($perPage); return ProductResource::collection($products); Resources automatically handle pagination meta and preserve query parameters.\nCreate a resource collection for custom meta For custom pagination metadata:\nphp artisan make:resource ProductCollection --collection \u0026lt;?php namespace App\\Http\\Resources; use Illuminate\\Http\\Resources\\Json\\ResourceCollection; class ProductCollection extends ResourceCollection { public function toArray($request) { return [ \u0026#39;data\u0026#39; =\u0026gt; $this-\u0026gt;collection, \u0026#39;meta\u0026#39; =\u0026gt; [ \u0026#39;total\u0026#39; =\u0026gt; $this-\u0026gt;total(), \u0026#39;count\u0026#39; =\u0026gt; $this-\u0026gt;count(), \u0026#39;per_page\u0026#39; =\u0026gt; $this-\u0026gt;perPage(), \u0026#39;current_page\u0026#39; =\u0026gt; $this-\u0026gt;currentPage(), \u0026#39;total_pages\u0026#39; =\u0026gt; $this-\u0026gt;lastPage(), ], \u0026#39;links\u0026#39; =\u0026gt; [ \u0026#39;self\u0026#39; =\u0026gt; $request-\u0026gt;url(), \u0026#39;first\u0026#39; =\u0026gt; $this-\u0026gt;url(1), \u0026#39;last\u0026#39; =\u0026gt; $this-\u0026gt;url($this-\u0026gt;lastPage()), \u0026#39;prev\u0026#39; =\u0026gt; $this-\u0026gt;previousPageUrl(), \u0026#39;next\u0026#39; =\u0026gt; $this-\u0026gt;nextPageUrl(), ], ]; } } Use it:\nreturn new ProductCollection($products); Build a flexible filter class For complex filtering logic, create a dedicated filter class:\n\u0026lt;?php namespace App\\Filters; use Illuminate\\Http\\Request; class ProductFilter { protected $request; protected $query; protected $filters = [\u0026#39;status\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;minPrice\u0026#39;, \u0026#39;maxPrice\u0026#39;, \u0026#39;search\u0026#39;]; public function __construct(Request $request) { $this-\u0026gt;request = $request; } public function apply($query) { $this-\u0026gt;query = $query; foreach ($this-\u0026gt;filters as $filter) { if ($this-\u0026gt;request-\u0026gt;has($this-\u0026gt;getFilterKey($filter))) { $this-\u0026gt;$filter($this-\u0026gt;request-\u0026gt;input($this-\u0026gt;getFilterKey($filter))); } } return $this-\u0026gt;query; } protected function status($value) { $this-\u0026gt;query-\u0026gt;where(\u0026#39;status\u0026#39;, $value); } protected function category($value) { $this-\u0026gt;query-\u0026gt;where(\u0026#39;category\u0026#39;, $value); } protected function minPrice($value) { $this-\u0026gt;query-\u0026gt;where(\u0026#39;price\u0026#39;, \u0026#39;\u0026gt;=\u0026#39;, $value); } protected function maxPrice($value) { $this-\u0026gt;query-\u0026gt;where(\u0026#39;price\u0026#39;, \u0026#39;\u0026lt;=\u0026#39;, $value); } protected function search($value) { $this-\u0026gt;query-\u0026gt;whereAny([\u0026#39;name\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;sku\u0026#39;], \u0026#39;LIKE\u0026#39;, \u0026#34;%{$value}%\u0026#34;); } protected function getFilterKey($name) { return strtolower(preg_replace(\u0026#39;/(?\u0026lt;!^)[A-Z]/\u0026#39;, \u0026#39;_$0\u0026#39;, $name)); } } Use in controller:\nuse App\\Filters\\ProductFilter; public function index(Request $request, ProductFilter $filter) { $query = Product::query(); $filter-\u0026gt;apply($query); $products = $query-\u0026gt;paginate($request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15)); return ProductResource::collection($products); } This keeps your controller clean and makes filters reusable.\nUse Spatie Query Builder package For production APIs, use Spatie\u0026rsquo;s Laravel Query Builder package:\ncomposer require spatie/laravel-query-builder use Spatie\\QueryBuilder\\QueryBuilder; public function index(Request $request) { $products = QueryBuilder::for(Product::class) -\u0026gt;allowedFilters([\u0026#39;status\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;name\u0026#39;]) -\u0026gt;allowedSorts([\u0026#39;name\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;created_at\u0026#39;]) -\u0026gt;defaultSort(\u0026#39;-created_at\u0026#39;) -\u0026gt;paginate($request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15)); return ProductResource::collection($products); } Request: GET /products?filter[status]=active\u0026amp;filter[name]=phone\u0026amp;sort=-price\u0026amp;page=2\nSpatie Query Builder handles:\nFiltering with filter[field]=value syntax Sorting with sort=field or sort=-field Including relationships with include=category,tags Selecting fields with fields[products]=id,name,price It validates allowed filters and sorts automatically, preventing unauthorized queries.\nAdd relationship filtering Filter by related models:\nQueryBuilder::for(Product::class) -\u0026gt;allowedFilters([ \u0026#39;status\u0026#39;, AllowedFilter::exact(\u0026#39;category_id\u0026#39;), AllowedFilter::scope(\u0026#39;has_reviews\u0026#39;), ]) -\u0026gt;allowedIncludes([\u0026#39;category\u0026#39;, \u0026#39;reviews\u0026#39;]) -\u0026gt;paginate($perPage); Define a scope in your model:\npublic function scopeHasReviews($query) { return $query-\u0026gt;has(\u0026#39;reviews\u0026#39;); } Request: GET /products?filter[has_reviews]=true\u0026amp;include=reviews\nImplement date range filtering Filter by date ranges:\nuse Spatie\\QueryBuilder\\AllowedFilter; use Carbon\\Carbon; QueryBuilder::for(Product::class) -\u0026gt;allowedFilters([ AllowedFilter::callback(\u0026#39;created_from\u0026#39;, function ($query, $value) { $query-\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;=\u0026#39;, Carbon::parse($value)); }), AllowedFilter::callback(\u0026#39;created_to\u0026#39;, function ($query, $value) { $query-\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026lt;=\u0026#39;, Carbon::parse($value)-\u0026gt;endOfDay()); }), ]) -\u0026gt;paginate($perPage); Request: GET /products?filter[created_from]=2025-01-01\u0026amp;filter[created_to]=2025-01-31\nPerformance tips Add indexes on filtered and sorted columns:\nSchema::table(\u0026#39;products\u0026#39;, function (Blueprint $table) { $table-\u0026gt;index([\u0026#39;status\u0026#39;, \u0026#39;created_at\u0026#39;]); $table-\u0026gt;index(\u0026#39;category\u0026#39;); $table-\u0026gt;index(\u0026#39;price\u0026#39;); }); Composite indexes work best when columns match your WHERE + ORDER BY clauses.\nAvoid N+1 queries by eager loading relationships:\n$products = QueryBuilder::for(Product::class) -\u0026gt;with([\u0026#39;category\u0026#39;, \u0026#39;tags\u0026#39;]) -\u0026gt;allowedFilters([\u0026#39;status\u0026#39;]) -\u0026gt;paginate($perPage); Use select() to fetch only needed columns:\n$products = Product::select([\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;status\u0026#39;]) -\u0026gt;where(\u0026#39;status\u0026#39;, \u0026#39;active\u0026#39;) -\u0026gt;paginate($perPage); For very large datasets, consider caching:\n$cacheKey = \u0026#39;products_\u0026#39; . md5(json_encode($request-\u0026gt;query())); $products = Cache::remember($cacheKey, 300, function () use ($query, $perPage) { return $query-\u0026gt;paginate($perPage); }); Cache for 5 minutes. Invalidate when products change.\nHandle empty results gracefully Return consistent JSON for empty results:\n$products = $query-\u0026gt;paginate($perPage); if ($products-\u0026gt;isEmpty()) { return response()-\u0026gt;json([ \u0026#39;data\u0026#39; =\u0026gt; [], \u0026#39;message\u0026#39; =\u0026gt; \u0026#39;No products found matching your criteria\u0026#39;, \u0026#39;meta\u0026#39; =\u0026gt; [ \u0026#39;total\u0026#39; =\u0026gt; 0, \u0026#39;current_page\u0026#39; =\u0026gt; 1, ], ]); } return ProductResource::collection($products); Or use resources which handle empty collections automatically:\nreturn ProductResource::collection($products); // Returns {\u0026#34;data\u0026#34;: []} if empty Document your API filters Document all available filters, sorts, and pagination parameters in your API documentation:\nGET /api/products Query Parameters: - page (integer): Page number (default: 1) - per_page (integer): Items per page (default: 15, max: 100) - status (string): Filter by status (active, inactive, draft) - category (string): Filter by category - min_price (number): Minimum price - max_price (number): Maximum price - search (string): Search in name, description, SKU - sort (string): Sort field (name, price, created_at). Prefix with - for descending. Example: GET /api/products?status=active\u0026amp;sort=-price\u0026amp;per_page=20\u0026amp;page=2 Use tools like OpenAPI/Swagger or API Blueprint to generate interactive docs.\nTesting pagination and filters Write tests to ensure filters work correctly:\npublic function test_can_filter_products_by_status() { Product::factory()-\u0026gt;create([\u0026#39;status\u0026#39; =\u0026gt; \u0026#39;active\u0026#39;]); Product::factory()-\u0026gt;create([\u0026#39;status\u0026#39; =\u0026gt; \u0026#39;inactive\u0026#39;]); $response = $this-\u0026gt;getJson(\u0026#39;/api/products?status=active\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJsonCount(1, \u0026#39;data\u0026#39;) -\u0026gt;assertJsonPath(\u0026#39;data.0.status\u0026#39;, \u0026#39;active\u0026#39;); } public function test_can_sort_products_by_price() { Product::factory()-\u0026gt;create([\u0026#39;price\u0026#39; =\u0026gt; 100]); Product::factory()-\u0026gt;create([\u0026#39;price\u0026#39; =\u0026gt; 50]); Product::factory()-\u0026gt;create([\u0026#39;price\u0026#39; =\u0026gt; 200]); $response = $this-\u0026gt;getJson(\u0026#39;/api/products?sort=-price\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJsonPath(\u0026#39;data.0.price\u0026#39;, \u0026#39;200.00\u0026#39;) -\u0026gt;assertJsonPath(\u0026#39;data.2.price\u0026#39;, \u0026#39;50.00\u0026#39;); } public function test_pagination_respects_per_page_limit() { Product::factory()-\u0026gt;count(150)-\u0026gt;create(); $response = $this-\u0026gt;getJson(\u0026#39;/api/products?per_page=20\u0026#39;); $response-\u0026gt;assertOk() -\u0026gt;assertJsonCount(20, \u0026#39;data\u0026#39;) -\u0026gt;assertJsonPath(\u0026#39;meta.per_page\u0026#39;, 20); } Test edge cases like invalid filters, exceeding max per_page, and empty results.\nSecurity considerations Always validate and whitelist filter parameters. Don\u0026rsquo;t let users filter by any column:\n$allowedFilters = [\u0026#39;status\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;price\u0026#39;]; if (!in_array($request-\u0026gt;filter_field, $allowedFilters)) { return response()-\u0026gt;json([\u0026#39;error\u0026#39; =\u0026gt; \u0026#39;Invalid filter field\u0026#39;], 422); } Sanitize search input to prevent SQL injection:\n$search = strip_tags($request-\u0026gt;search); $query-\u0026gt;where(\u0026#39;name\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$search}%\u0026#34;); Better yet, use parameter binding which Laravel does automatically:\n$query-\u0026gt;where(\u0026#39;name\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$request-\u0026gt;search}%\u0026#34;); // Laravel binds this safely Rate limit API endpoints to prevent abuse:\nRoute::middleware(\u0026#39;throttle:60,1\u0026#39;)-\u0026gt;group(function () { Route::get(\u0026#39;/products\u0026#39;, [ProductController::class, \u0026#39;index\u0026#39;]); }); See: Laravel API Authentication with Sanctum for securing your API.\nCommon mistakes to avoid Don\u0026rsquo;t filter after fetching all records:\n// Bad - loads all products into memory $products = Product::all(); $filtered = $products-\u0026gt;where(\u0026#39;status\u0026#39;, \u0026#39;active\u0026#39;); Always filter at the database level:\n// Good - database does the filtering $products = Product::where(\u0026#39;status\u0026#39;, \u0026#39;active\u0026#39;)-\u0026gt;paginate($perPage); Don\u0026rsquo;t forget to set max per_page:\n// Bad - user can request 1 million items $products = Product::paginate($request-\u0026gt;per_page); // Good - cap at 100 $perPage = min($request-\u0026gt;input(\u0026#39;per_page\u0026#39;, 15), 100); Don\u0026rsquo;t expose sensitive columns in filters or sorts. Whitelist allowed fields.\nDon\u0026rsquo;t run COUNT queries on every request for simple pagination. Use simplePaginate() when total count isn\u0026rsquo;t needed.\nSummary API pagination and filtering let clients control what data they get. Use paginate() for standard pagination, simplePaginate() for infinite scroll, and cursorPaginate() for large datasets.\nAdd query parameters for filtering, sorting, and searching. Validate all input and whitelist which filters clients can use. Format responses with API resources.\nFor production, Spatie Query Builder handles complex filtering with clean syntax. Add indexes on columns you filter and sort by. Document all parameters and test edge cases.\nThese patterns let you build flexible APIs that scale and give clients exactly what they need. For more, check out Laravel Security Best Practices for Production and Laravel Performance Optimization .\n","href":"/2025/10/how-to-create-restful-api-pagination-filtering-sorting-laravel.html","title":"How to Create RESTful API Pagination and Filtering in Laravel"},{"content":"Real-time notifications let you push updates to users instantly without page refreshes. Laravel Broadcasting with Pusher makes this easy using WebSockets. When something happens on your server like a new message, order update, or system alert, your frontend gets notified immediately.\nThis guide shows you how to set up Laravel Broadcasting from scratch. We\u0026rsquo;ll install Pusher, create events, configure channels for public and private notifications, set up Laravel Echo on the frontend, and test everything. You\u0026rsquo;ll also learn about presence channels for tracking online users and production deployment tips.\nWhen to use real-time notifications You need real-time notifications when users should see updates immediately without refreshing the page.\nCommon use cases:\nChat applications and messaging systems Notification bells showing new alerts, mentions, or messages Live dashboards with updating metrics or analytics Collaborative tools where multiple users edit the same data Order status updates in e-commerce Social media feeds with new posts or likes Admin panels monitoring system events If updates can wait 30 seconds or a minute, polling might be simpler. If updates must arrive instantly (chat, live auctions, stock tickers), use broadcasting.\nBroadcasting architecture overview Laravel Broadcasting works like this:\nSomething happens in your app (new order, message sent, user registered) You fire an event that implements ShouldBroadcast Laravel queues a job to send the event to Pusher via HTTP API Pusher pushes the event through WebSocket to all connected clients Laravel Echo (JavaScript) receives the event and triggers your callback Your frontend updates the UI The key parts: Events (server-side), Channels (authorization), Pusher (WebSocket infrastructure), and Echo (client library).\nInstall and configure Pusher Sign up for a free account at pusher.com. Create a new app and choose your cluster (closest to your users).\nCopy your credentials from the app dashboard. You\u0026rsquo;ll need:\nApp ID Key Secret Cluster Install the Pusher PHP SDK:\ncomposer require pusher/pusher-php-server Add your Pusher credentials to .env:\nBROADCAST_CONNECTION=pusher PUSHER_APP_ID=your-app-id PUSHER_APP_KEY=your-key PUSHER_APP_SECRET=your-secret PUSHER_APP_CLUSTER=mt1 PUSHER_SCHEME=https Open config/broadcasting.php and verify the Pusher config uses your env variables:\n\u0026#39;pusher\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;pusher\u0026#39;, \u0026#39;key\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_APP_KEY\u0026#39;), \u0026#39;secret\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_APP_SECRET\u0026#39;), \u0026#39;app_id\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_APP_ID\u0026#39;), \u0026#39;options\u0026#39; =\u0026gt; [ \u0026#39;cluster\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_APP_CLUSTER\u0026#39;), \u0026#39;host\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_HOST\u0026#39;) ?: \u0026#39;api-\u0026#39;.env(\u0026#39;PUSHER_APP_CLUSTER\u0026#39;, \u0026#39;mt1\u0026#39;).\u0026#39;.pusher.com\u0026#39;, \u0026#39;port\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_PORT\u0026#39;, 443), \u0026#39;scheme\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_SCHEME\u0026#39;, \u0026#39;https\u0026#39;), \u0026#39;encrypted\u0026#39; =\u0026gt; true, \u0026#39;useTLS\u0026#39; =\u0026gt; env(\u0026#39;PUSHER_SCHEME\u0026#39;, \u0026#39;https\u0026#39;) === \u0026#39;https\u0026#39;, ], ], Uncomment App\\Providers\\BroadcastServiceProvider::class in config/app.php (Laravel 10 and below) or it auto-registers in Laravel 11+.\nCreate a broadcastable event Generate an event:\nphp artisan make:event OrderShipped Edit app/Events/OrderShipped.php to implement ShouldBroadcast:\n\u0026lt;?php namespace App\\Events; use App\\Models\\Order; use Illuminate\\Broadcasting\\Channel; use Illuminate\\Broadcasting\\InteractsWithSockets; use Illuminate\\Contracts\\Broadcasting\\ShouldBroadcast; use Illuminate\\Foundation\\Events\\Dispatchable; use Illuminate\\Queue\\SerializesModels; class OrderShipped implements ShouldBroadcast { use Dispatchable, InteractsWithSockets, SerializesModels; public function __construct(public Order $order) { } public function broadcastOn() { return new Channel(\u0026#39;orders\u0026#39;); } public function broadcastWith() { return [ \u0026#39;id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;id, \u0026#39;status\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;status, \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;user_id, ]; } } broadcastOn() returns the channel(s) to broadcast on. broadcastWith() defines the data sent to clients. Only include data the frontend needs.\nTrigger the event anywhere in your app:\nuse App\\Events\\OrderShipped; $order = Order::find(1); event(new OrderShipped($order)); Laravel queues this event automatically if you have ShouldBroadcast. Make sure your queue worker is running:\nphp artisan queue:work Set up Laravel Echo on the frontend Install Laravel Echo and Pusher JavaScript SDK:\nnpm install --save-dev laravel-echo pusher-js Configure Echo in resources/js/bootstrap.js or app.js:\nimport Echo from \u0026#39;laravel-echo\u0026#39;; import Pusher from \u0026#39;pusher-js\u0026#39;; window.Pusher = Pusher; window.Echo = new Echo({ broadcaster: \u0026#39;pusher\u0026#39;, key: import.meta.env.VITE_PUSHER_APP_KEY, cluster: import.meta.env.VITE_PUSHER_APP_CLUSTER, forceTLS: true }); Add Pusher credentials to .env for Vite:\nVITE_PUSHER_APP_KEY=\u0026#34;${PUSHER_APP_KEY}\u0026#34; VITE_PUSHER_APP_CLUSTER=\u0026#34;${PUSHER_APP_CLUSTER}\u0026#34; Restart Vite:\nnpm run dev Listen for events on the frontend In your JavaScript (Vue, React, or vanilla JS):\nEcho.channel(\u0026#39;orders\u0026#39;) .listen(\u0026#39;OrderShipped\u0026#39;, (e) =\u0026gt; { console.log(\u0026#39;Order shipped:\u0026#39;, e); // Update UI: show notification, update order status, etc. showNotification(`Order #${e.id} shipped!`); }); The first parameter to .listen() is the event class name without the namespace. Laravel uses the class name by default. To customize, add broadcastAs() to your event:\npublic function broadcastAs() { return \u0026#39;order.shipped\u0026#39;; } Then listen for it:\nEcho.channel(\u0026#39;orders\u0026#39;).listen(\u0026#39;.order.shipped\u0026#39;, (e) =\u0026gt; { // Note the dot prefix for custom event names }); Private channels for user-specific notifications Public channels let anyone subscribe. Private channels require authentication.\nCreate a notification event for a specific user:\nphp artisan make:event NewMessage \u0026lt;?php namespace App\\Events; use App\\Models\\Message; use Illuminate\\Broadcasting\\PrivateChannel; use Illuminate\\Contracts\\Broadcasting\\ShouldBroadcast; use Illuminate\\Foundation\\Events\\Dispatchable; use Illuminate\\Queue\\SerializesModels; class NewMessage implements ShouldBroadcast { use Dispatchable, SerializesModels; public function __construct(public Message $message) { } public function broadcastOn() { return new PrivateChannel(\u0026#39;user.\u0026#39; . $this-\u0026gt;message-\u0026gt;recipient_id); } public function broadcastWith() { return [ \u0026#39;id\u0026#39; =\u0026gt; $this-\u0026gt;message-\u0026gt;id, \u0026#39;from\u0026#39; =\u0026gt; $this-\u0026gt;message-\u0026gt;sender-\u0026gt;name, \u0026#39;text\u0026#39; =\u0026gt; $this-\u0026gt;message-\u0026gt;text, ]; } } Define authorization in routes/channels.php:\nuse Illuminate\\Support\\Facades\\Broadcast; Broadcast::channel(\u0026#39;user.{userId}\u0026#39;, function ($user, $userId) { return (int) $user-\u0026gt;id === (int) $userId; }); This ensures users can only listen to their own private channel.\nMake sure authentication routes are enabled in routes/web.php:\nBroadcast::routes([\u0026#39;middleware\u0026#39; =\u0026gt; [\u0026#39;web\u0026#39;, \u0026#39;auth\u0026#39;]]); On the frontend, listen to the private channel:\nEcho.private(`user.${userId}`) .listen(\u0026#39;NewMessage\u0026#39;, (e) =\u0026gt; { console.log(\u0026#39;New message:\u0026#39;, e); displayMessage(e.from, e.text); }); Laravel Echo automatically calls /broadcasting/auth to authorize the subscription. If the user isn\u0026rsquo;t authenticated or authorized, the subscription fails.\nPresence channels for online users Presence channels are private channels that track who\u0026rsquo;s subscribed. Use them for online user lists, typing indicators, or collaborative features.\nCreate a chat room event:\n\u0026lt;?php namespace App\\Events; use Illuminate\\Broadcasting\\PresenceChannel; use Illuminate\\Contracts\\Broadcasting\\ShouldBroadcast; class MessageSent implements ShouldBroadcast { public function __construct(public string $roomId, public string $message) { } public function broadcastOn() { return new PresenceChannel(\u0026#39;chat.\u0026#39; . $this-\u0026gt;roomId); } public function broadcastWith() { return [\u0026#39;message\u0026#39; =\u0026gt; $this-\u0026gt;message]; } } Authorize in routes/channels.php:\nBroadcast::channel(\u0026#39;chat.{roomId}\u0026#39;, function ($user, $roomId) { // Check if user can access this room if ($user-\u0026gt;canAccessRoom($roomId)) { return [\u0026#39;id\u0026#39; =\u0026gt; $user-\u0026gt;id, \u0026#39;name\u0026#39; =\u0026gt; $user-\u0026gt;name]; } }); The returned array is user info shared with all subscribers.\nOn the frontend:\nEcho.join(`chat.${roomId}`) .here((users) =\u0026gt; { // users is an array of everyone currently in the channel console.log(\u0026#39;Currently online:\u0026#39;, users); updateOnlineList(users); }) .joining((user) =\u0026gt; { // Someone new joined console.log(user.name + \u0026#39; joined\u0026#39;); addUserToList(user); }) .leaving((user) =\u0026gt; { // Someone left console.log(user.name + \u0026#39; left\u0026#39;); removeUserFromList(user); }) .listen(\u0026#39;MessageSent\u0026#39;, (e) =\u0026gt; { console.log(\u0026#39;New message:\u0026#39;, e.message); }); .here() fires when you first join and shows who\u0026rsquo;s already there. .joining() fires when someone new joins. .leaving() fires when someone disconnects.\nCustomize event data with broadcastWith By default, Laravel broadcasts all public properties of your event. To control exactly what gets sent:\npublic function broadcastWith() { return [ \u0026#39;order_id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;id, \u0026#39;total\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;total, \u0026#39;items_count\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;items-\u0026gt;count(), \u0026#39;customer_name\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;user-\u0026gt;name, ]; } Only send data the frontend needs. Don\u0026rsquo;t expose sensitive fields like internal IDs, API keys, or raw model data.\nQueue broadcasts for performance Always use ShouldBroadcast, not ShouldBroadcastNow. Broadcasting synchronously adds 100-300ms to your response time because it waits for the Pusher HTTP API call.\nWith ShouldBroadcast, Laravel queues the broadcast job. Make sure you have a queue worker running:\nphp artisan queue:work --tries=3 In production, use Supervisor to keep the queue worker running. See: Laravel Queue Jobs and Background Processing .\nTest broadcasts with Pusher Debug Console Open your Pusher dashboard and go to the Debug Console tab. Fire an event in your Laravel app:\nevent(new OrderShipped($order)); You should see the event appear in the Debug Console in real-time. If not:\nCheck your queue worker is running Verify .env credentials match your Pusher app Check BROADCAST_CONNECTION=pusher in .env Look for errors in storage/logs/laravel.log You can also trigger test events from the Pusher console to verify your frontend is listening correctly.\nHandle authentication for Echo Laravel Echo needs to authenticate before subscribing to private or presence channels.\nEcho sends a POST request to /broadcasting/auth with the channel name. Laravel checks your channels.php authorization logic and returns the auth signature if authorized.\nMake sure your frontend has a valid session cookie. For SPAs on different domains, configure CORS and credentials:\nIn config/cors.php:\n\u0026#39;paths\u0026#39; =\u0026gt; [\u0026#39;api/*\u0026#39;, \u0026#39;broadcasting/auth\u0026#39;], \u0026#39;supports_credentials\u0026#39; =\u0026gt; true, In your Echo config:\nwindow.Echo = new Echo({ broadcaster: \u0026#39;pusher\u0026#39;, key: import.meta.env.VITE_PUSHER_APP_KEY, cluster: import.meta.env.VITE_PUSHER_APP_CLUSTER, forceTLS: true, authEndpoint: \u0026#39;https://your-api.com/broadcasting/auth\u0026#39;, auth: { headers: { Authorization: \u0026#39;Bearer \u0026#39; + token, // if using Sanctum tokens } } }); For Sanctum token auth in SPAs, see: Laravel API Authentication with Sanctum .\nBroadcast to multiple channels Return an array of channels from broadcastOn():\npublic function broadcastOn() { return [ new Channel(\u0026#39;orders\u0026#39;), new PrivateChannel(\u0026#39;user.\u0026#39; . $this-\u0026gt;order-\u0026gt;user_id), new PrivateChannel(\u0026#39;admin-dashboard\u0026#39;), ]; } This event goes to all three channels. Useful when the same event affects multiple audiences (public feed, private user notification, admin log).\nConditionally broadcast events Only broadcast when certain conditions are met:\npublic function broadcastWhen() { return $this-\u0026gt;order-\u0026gt;status === \u0026#39;shipped\u0026#39;; } If broadcastWhen() returns false, Laravel skips broadcasting. Use this to avoid sending unnecessary events.\nClient-side event broadcasting Pusher lets clients trigger events directly without hitting your server. This is useful for typing indicators or cursor positions.\nEnable client events in your Pusher app settings (under App Settings \u0026gt; Enable client events).\nAuthorize the channel in channels.php:\nBroadcast::channel(\u0026#39;chat.{roomId}\u0026#39;, function ($user, $roomId) { return [\u0026#39;id\u0026#39; =\u0026gt; $user-\u0026gt;id, \u0026#39;name\u0026#39; =\u0026gt; $user-\u0026gt;name]; }); On the frontend, trigger client events with a client- prefix:\nEcho.private(`chat.${roomId}`) .listenForWhisper(\u0026#39;typing\u0026#39;, (e) =\u0026gt; { console.log(e.name + \u0026#39; is typing...\u0026#39;); }); // Trigger a whisper event Echo.private(`chat.${roomId}`) .whisper(\u0026#39;typing\u0026#39;, { name: userName }); Whispers (client events) only work on private and presence channels for security. They bypass your server entirely, so they\u0026rsquo;re very fast but you can\u0026rsquo;t validate or log them server-side.\nProduction deployment checklist Use HTTPS in production. Pusher requires forceTLS: true for security.\nSet up a queue worker with Supervisor to process broadcast jobs reliably:\n[program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /path/to/artisan queue:work --sleep=3 --tries=3 --max-time=3600 autostart=true autorestart=true numprocs=2 Monitor your Pusher usage in the dashboard. The free tier allows 100 concurrent connections and 200k messages per day. If you exceed this, either upgrade or switch to Laravel Reverb (self-hosted, no limits).\nCache your config in production to avoid reading .env on every request:\nphp artisan config:cache Test reconnection behavior. Disable network for 10 seconds then re-enable. Echo should reconnect automatically. If not, check your connection state handling.\nRate limit broadcasting in your controllers to prevent abuse. If a user can trigger events (like sending messages), throttle the endpoint:\nRoute::post(\u0026#39;/messages\u0026#39;, [MessageController::class, \u0026#39;store\u0026#39;]) -\u0026gt;middleware(\u0026#39;throttle:20,1\u0026#39;); // 20 messages per minute Log broadcast errors. If Pusher is down or credentials are wrong, your queue jobs will fail. Monitor storage/logs/laravel.log or use error tracking: Laravel Production Monitoring and Error Tracking .\nAlternative: Laravel Reverb (self-hosted) Laravel 11 introduced Reverb, a first-party WebSocket server you can self-host. It\u0026rsquo;s free, fast, and eliminates per-message costs.\nInstall Reverb:\ncomposer require laravel/reverb php artisan reverb:install Start the Reverb server:\nphp artisan reverb:start Change .env:\nBROADCAST_CONNECTION=reverb REVERB_HOST=0.0.0.0 REVERB_PORT=8080 Update Echo config:\nwindow.Echo = new Echo({ broadcaster: \u0026#39;reverb\u0026#39;, key: import.meta.env.VITE_REVERB_APP_KEY, wsHost: import.meta.env.VITE_REVERB_HOST, wsPort: import.meta.env.VITE_REVERB_PORT, forceTLS: false, enabledTransports: [\u0026#39;ws\u0026#39;, \u0026#39;wss\u0026#39;], }); Reverb works identically to Pusher from your application code. Only the infrastructure changes. Use Reverb if you want full control and no usage limits. Use Pusher if you prefer managed infrastructure.\nFor production Reverb, run it behind a reverse proxy (Nginx) with SSL. See the official docs for deployment details.\nTroubleshooting common issues Events not appearing in Pusher Debug Console:\nCheck queue worker is running: php artisan queue:work Verify .env credentials match Pusher dashboard Check BROADCAST_CONNECTION=pusher in .env Look for failed jobs in failed_jobs table: php artisan queue:failed Clear config cache: php artisan config:clear Frontend not receiving events:\nCheck browser console for Echo connection errors Verify Echo is connecting to the right cluster (check Pusher dashboard Connection tab for active connections) Make sure channel name matches exactly (case-sensitive) For private channels, ensure /broadcasting/auth returns 200 (check Network tab) Check if the event class name matches what you\u0026rsquo;re listening for Private channel authorization fails:\nUser must be authenticated before subscribing Check Broadcast::routes() is called with auth middleware For SPAs, verify CORS is configured and supports_credentials is true Check the channel authorization callback in channels.php returns truthy value Look at /broadcasting/auth response in Network tab for error messages Connection drops frequently:\nPusher free tier has connection time limits (1 day) Check if network conditions are unstable Implement reconnection handling and state management Consider using Laravel Reverb for self-hosted stability High Pusher message count:\nYou\u0026rsquo;re broadcasting too often. Throttle events or batch updates. Avoid broadcasting on every model update. Use observers carefully. Check if you\u0026rsquo;re broadcasting to too many channels in broadcastOn() Switch to Reverb or upgrade Pusher plan Security considerations Never broadcast sensitive data like passwords, API keys, or personal information. Use broadcastWith() to whitelist exposed fields.\nAlways use private channels for user-specific data. Public channels are visible to anyone who knows the channel name.\nValidate all data before broadcasting. If users trigger events (chat messages), sanitize input to prevent XSS.\nRate limit endpoints that trigger broadcasts. A malicious user could spam events to exhaust your Pusher quota.\nUse HTTPS and forceTLS: true in production. Pusher doesn\u0026rsquo;t allow insecure connections on production clusters anyway.\nDon\u0026rsquo;t trust client events (whispers) for critical actions. They bypass server validation. Only use them for non-critical UI updates like typing indicators.\nRotate your Pusher secret if it\u0026rsquo;s exposed. Generate a new app key in Pusher dashboard and update .env.\nSummary Laravel Broadcasting with Pusher gives you real-time notifications with minimal code. Install Pusher, create events that implement ShouldBroadcast, define channels and authorization, and set up Laravel Echo on the frontend.\nUse public channels for general updates, private channels for user-specific data, and presence channels to track who\u0026rsquo;s online. Queue all broadcasts for performance and test with Pusher\u0026rsquo;s Debug Console.\nFor production, monitor usage, set up queue workers with Supervisor, use HTTPS, and consider Laravel Reverb for self-hosted infrastructure. Always validate authorization for private channels and never broadcast sensitive data.\nWith these practices, you can build real-time features like notifications, chat, live dashboards, or collaborative tools that update instantly without polling. For more on securing Laravel applications, see Laravel Security Best Practices for Production .\n","href":"/2025/10/how-to-build-real-time-notifications-laravel-broadcasting-pusher.html","title":"How to Build Real-Time Notifications with Laravel Broadcasting and Pusher"},{"content":"Two-factor authentication (2FA) adds an extra layer of security to your Laravel application. Users need both their password and a time-based code from an authenticator app to log in. Laravel Fortify handles all the backend logic, and you build the frontend however you want.\nThis guide walks through the complete setup: installing Fortify, enabling 2FA, generating QR codes for Google Authenticator, managing recovery codes, confirming codes before activation to prevent lockouts, and testing everything. We\u0026rsquo;ll also cover security practices and common problems you might run into.\nWhen to use 2FA You should use 2FA for applications that handle sensitive data like financial systems, healthcare records, admin panels, or HR systems. If someone breaking into an account would cause real damage, you need 2FA.\nIt\u0026rsquo;s required for applications with privileged access, multi-tenant SaaS platforms, or when compliance standards like PCI-DSS, HIPAA, or SOC 2 demand it.\nFor low-risk applications it\u0026rsquo;s optional, but offering 2FA to users who want extra security is always good practice.\nLaravel Fortify provides TOTP-based 2FA that works with Google Authenticator, Authy, 1Password, and any other app that follows RFC 6238.\nInstall Laravel Fortify Laravel Fortify is a headless authentication backend. Install it via Composer:\ncomposer require laravel/fortify Publish Fortify\u0026rsquo;s resources and run migrations:\nphp artisan vendor:publish --provider=\u0026#34;Laravel\\Fortify\\FortifyServiceProvider\u0026#34; php artisan migrate This creates the config/fortify.php configuration file, app/Actions/Fortify action classes, app/Providers/FortifyServiceProvider.php, and adds columns to the users table for 2FA (two_factor_secret, two_factor_recovery_codes, two_factor_confirmed_at).\nRegister the service provider in config/app.php (Laravel 10 and below) or it will auto-register in Laravel 11+:\n\u0026#39;providers\u0026#39; =\u0026gt; [ // ... App\\Providers\\FortifyServiceProvider::class, ], Enable 2FA in Fortify configuration Open config/fortify.php and enable the two-factor authentication feature:\n\u0026#39;features\u0026#39; =\u0026gt; [ Features::registration(), Features::resetPasswords(), Features::emailVerification(), Features::updateProfileInformation(), Features::updatePasswords(), Features::twoFactorAuthentication([ \u0026#39;confirm\u0026#39; =\u0026gt; true, \u0026#39;confirmPassword\u0026#39; =\u0026gt; true, ]), ], Key options:\n'confirm' =\u0026gt; true: Users must enter a valid TOTP code before 2FA is fully enabled. This prevents lockouts if they scan the QR code wrong. 'confirmPassword' =\u0026gt; true: Users must confirm their password before enabling or disabling 2FA. Add the TwoFactorAuthenticatable trait to User model Update your app/Models/User.php to use the TwoFactorAuthenticatable trait:\n\u0026lt;?php namespace App\\Models; use Illuminate\\Database\\Eloquent\\Factories\\HasFactory; use Illuminate\\Foundation\\Auth\\User as Authenticatable; use Illuminate\\Notifications\\Notifiable; use Laravel\\Fortify\\TwoFactorAuthenticatable; use Laravel\\Sanctum\\HasApiTokens; class User extends Authenticatable { use HasApiTokens, HasFactory, Notifiable, TwoFactorAuthenticatable; // ... } This trait adds methods like twoFactorQrCodeSvg(), recoveryCodes(), and database columns for the 2FA secret and recovery codes.\nFortify routes overview Fortify automatically registers these routes when 2FA is enabled:\nPOST /user/two-factor-authentication \u0026ndash; Enable 2FA (generates secret and recovery codes) DELETE /user/two-factor-authentication \u0026ndash; Disable 2FA GET /user/two-factor-qr-code \u0026ndash; Get QR code SVG for scanning GET /user/two-factor-recovery-codes \u0026ndash; Get recovery codes as JSON POST /user/two-factor-recovery-codes \u0026ndash; Regenerate recovery codes POST /user/confirmed-two-factor-authentication \u0026ndash; Confirm 2FA by validating a code All routes require authentication and are under the web middleware group. Password confirmation may be required based on your config.\nEnable 2FA: Generate secret and QR code When a user wants to enable 2FA, make a POST request to /user/two-factor-authentication. Fortify generates a secret and recovery codes and stores them encrypted in the database.\nFrontend flow (example with Axios):\n// Enable 2FA await axios.post(\u0026#39;/user/two-factor-authentication\u0026#39;); // Fetch the QR code SVG const qrResponse = await axios.get(\u0026#39;/user/two-factor-qr-code\u0026#39;); document.getElementById(\u0026#39;qr-code\u0026#39;).innerHTML = qrResponse.data.svg; // Fetch recovery codes const recoveryResponse = await axios.get(\u0026#39;/user/two-factor-recovery-codes\u0026#39;); displayRecoveryCodes(recoveryResponse.data); Controller example (custom UI):\nuse Illuminate\\Http\\Request; use Laravel\\Fortify\\Actions\\EnableTwoFactorAuthentication; public function enable(Request $request, EnableTwoFactorAuthentication $enable) { $enable($request-\u0026gt;user()); return response()-\u0026gt;json([ \u0026#39;qr_code\u0026#39; =\u0026gt; $request-\u0026gt;user()-\u0026gt;twoFactorQrCodeSvg(), \u0026#39;recovery_codes\u0026#39; =\u0026gt; json_decode(decrypt($request-\u0026gt;user()-\u0026gt;two_factor_recovery_codes), true), ]); } Show the QR code and recovery codes to the user. The QR code is an SVG you can embed directly in your HTML. Display the recovery codes clearly and tell users to save them somewhere safe.\nConfirm 2FA before fully enabling With 'confirm' =\u0026gt; true, users must enter a valid TOTP code from their authenticator app before 2FA is fully active. This ensures the QR code was scanned correctly.\nConfirmation request:\nawait axios.post(\u0026#39;/user/confirmed-two-factor-authentication\u0026#39;, { code: \u0026#39;123456\u0026#39; // 6-digit code from authenticator app }); If the code is valid, Fortify sets the two_factor_confirmed_at timestamp, and 2FA is now active. If invalid, the endpoint returns a 422 error.\nCustom controller example:\nuse Laravel\\Fortify\\Actions\\ConfirmTwoFactorAuthentication; public function confirm(Request $request, ConfirmTwoFactorAuthentication $confirm) { $confirmed = $confirm($request-\u0026gt;user(), $request-\u0026gt;code); if (! $confirmed) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Invalid code\u0026#39;], 422); } return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;2FA enabled successfully\u0026#39;]); } Only show the \u0026ldquo;2FA enabled\u0026rdquo; success message after confirmation. Until then, keep showing the QR code and the input field for the code.\nLogin flow with 2FA enabled Once 2FA is enabled, the login process changes:\nUser submits email and password to /login. If credentials are valid and 2FA is enabled, Laravel redirects to /two-factor-challenge (or returns a JSON response indicating 2FA is required). User submits a 6-digit TOTP code or a recovery code to /two-factor-challenge. If code is valid, user is authenticated and session starts. Two-factor challenge endpoint:\nPOST /two-factor-challenge { \u0026#34;code\u0026#34;: \u0026#34;123456\u0026#34; // TOTP code from authenticator app // OR \u0026#34;recovery_code\u0026#34;: \u0026#34;abcd-efgh-ijkl\u0026#34; // One-time recovery code } Customize the challenge view:\nFortify looks for a two-factor-challenge view. Create resources/views/auth/two-factor-challenge.blade.php:\n\u0026lt;form method=\u0026#34;POST\u0026#34; action=\u0026#34;/two-factor-challenge\u0026#34;\u0026gt; @csrf \u0026lt;label\u0026gt;Enter authentication code\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;code\u0026#34; placeholder=\u0026#34;123456\u0026#34; autofocus\u0026gt; \u0026lt;p\u0026gt;Or enter a recovery code:\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; name=\u0026#34;recovery_code\u0026#34; placeholder=\u0026#34;abcd-efgh-ijkl\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Verify\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; Register the view in FortifyServiceProvider:\nuse Laravel\\Fortify\\Fortify; public function boot() { Fortify::twoFactorChallengeView(fn() =\u0026gt; view(\u0026#39;auth.two-factor-challenge\u0026#39;)); } Recovery codes: Backup access Recovery codes are one-time-use codes that allow users to log in if they lose access to their authenticator app. Fortify generates 8 recovery codes by default when 2FA is enabled.\nShow recovery codes after enabling 2FA:\nconst codes = await axios.get(\u0026#39;/user/two-factor-recovery-codes\u0026#39;); codes.data.forEach(code =\u0026gt; console.log(code)); Regenerate recovery codes (invalidates old codes):\nawait axios.post(\u0026#39;/user/two-factor-recovery-codes\u0026#39;); const newCodes = await axios.get(\u0026#39;/user/two-factor-recovery-codes\u0026#39;); Best practices:\nMake users confirm they\u0026rsquo;ve saved the codes before closing the setup dialog. Use a checkbox or an \u0026ldquo;I\u0026rsquo;ve saved these\u0026rdquo; button. Show recovery codes only once during setup, unless the user regenerates them later. Recovery codes are stored hashed in the database. Fortify does this automatically. Log when recovery codes are used so you can monitor for suspicious activity. Disable 2FA Users can disable 2FA by making a DELETE request:\nawait axios.delete(\u0026#39;/user/two-factor-authentication\u0026#39;); This clears the two_factor_secret, two_factor_recovery_codes, and two_factor_confirmed_at columns.\nYou should require password confirmation before disabling 2FA. Fortify does this automatically if you set 'confirmPassword' =\u0026gt; true in the config.\nCustomize 2FA secret length and algorithm Fortify uses the pragmarx/google2fa package. Default secret length is 32 characters (160 bits of entropy). You can customize this by binding a custom Google2FA instance:\nuse PragmaRX\\Google2FA\\Google2FA; // In a service provider $this-\u0026gt;app-\u0026gt;singleton(Google2FA::class, function () { $google2fa = new Google2FA(); $google2fa-\u0026gt;setSecretLength(64); // Increase to 64 characters return $google2fa; }); Most authenticator apps support the default length. Only customize this if you have specific security requirements.\nTesting the 2FA flow Write feature tests to ensure 2FA works end-to-end:\nuse App\\Models\\User; use Laravel\\Fortify\\Features; use PragmaRX\\Google2FA\\Google2FA; public function test_user_can_enable_two_factor_authentication() { if (! Features::enabled(Features::twoFactorAuthentication())) { $this-\u0026gt;markTestSkipped(\u0026#39;Two factor authentication is not enabled.\u0026#39;); } $user = User::factory()-\u0026gt;create(); $this-\u0026gt;actingAs($user); $response = $this-\u0026gt;post(\u0026#39;/user/two-factor-authentication\u0026#39;); $response-\u0026gt;assertSessionHasNoErrors(); $user-\u0026gt;refresh(); $this-\u0026gt;assertNotNull($user-\u0026gt;two_factor_secret); $this-\u0026gt;assertNotNull($user-\u0026gt;two_factor_recovery_codes); } public function test_user_must_confirm_two_factor_before_it_is_active() { $user = User::factory()-\u0026gt;create(); $this-\u0026gt;actingAs($user); $this-\u0026gt;post(\u0026#39;/user/two-factor-authentication\u0026#39;); $user-\u0026gt;refresh(); $google2fa = new Google2FA(); $secret = decrypt($user-\u0026gt;two_factor_secret); $validCode = $google2fa-\u0026gt;getCurrentOtp($secret); $response = $this-\u0026gt;post(\u0026#39;/user/confirmed-two-factor-authentication\u0026#39;, [ \u0026#39;code\u0026#39; =\u0026gt; $validCode, ]); $user-\u0026gt;refresh(); $this-\u0026gt;assertNotNull($user-\u0026gt;two_factor_confirmed_at); } public function test_user_can_login_with_two_factor_code() { $user = User::factory()-\u0026gt;create([\u0026#39;password\u0026#39; =\u0026gt; bcrypt(\u0026#39;password\u0026#39;)]); $this-\u0026gt;actingAs($user); $this-\u0026gt;post(\u0026#39;/user/two-factor-authentication\u0026#39;); $google2fa = new Google2FA(); $secret = decrypt($user-\u0026gt;two_factor_secret); $validCode = $google2fa-\u0026gt;getCurrentOtp($secret); $this-\u0026gt;post(\u0026#39;/user/confirmed-two-factor-authentication\u0026#39;, [\u0026#39;code\u0026#39; =\u0026gt; $validCode]); $this-\u0026gt;post(\u0026#39;/logout\u0026#39;); $this-\u0026gt;post(\u0026#39;/login\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;password\u0026#39;, ]); $response = $this-\u0026gt;post(\u0026#39;/two-factor-challenge\u0026#39;, [ \u0026#39;code\u0026#39; =\u0026gt; $google2fa-\u0026gt;getCurrentOtp($secret), ]); $response-\u0026gt;assertRedirect(\u0026#39;/dashboard\u0026#39;); // or wherever you redirect after login $this-\u0026gt;assertAuthenticatedAs($user); } public function test_user_can_login_with_recovery_code() { $user = User::factory()-\u0026gt;create([\u0026#39;password\u0026#39; =\u0026gt; bcrypt(\u0026#39;password\u0026#39;)]); $this-\u0026gt;actingAs($user); $this-\u0026gt;post(\u0026#39;/user/two-factor-authentication\u0026#39;); $recoveryCodes = json_decode(decrypt($user-\u0026gt;fresh()-\u0026gt;two_factor_recovery_codes), true); $validRecoveryCode = $recoveryCodes[0]; $google2fa = new Google2FA(); $secret = decrypt($user-\u0026gt;two_factor_secret); $validCode = $google2fa-\u0026gt;getCurrentOtp($secret); $this-\u0026gt;post(\u0026#39;/user/confirmed-two-factor-authentication\u0026#39;, [\u0026#39;code\u0026#39; =\u0026gt; $validCode]); $this-\u0026gt;post(\u0026#39;/logout\u0026#39;); $this-\u0026gt;post(\u0026#39;/login\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;password\u0026#39;, ]); $response = $this-\u0026gt;post(\u0026#39;/two-factor-challenge\u0026#39;, [ \u0026#39;recovery_code\u0026#39; =\u0026gt; $validRecoveryCode, ]); $response-\u0026gt;assertRedirect(\u0026#39;/dashboard\u0026#39;); $this-\u0026gt;assertAuthenticatedAs($user); // Recovery code should be used and no longer valid $user-\u0026gt;refresh(); $updatedCodes = json_decode(decrypt($user-\u0026gt;two_factor_recovery_codes), true); $this-\u0026gt;assertNotContains($validRecoveryCode, $updatedCodes); } Implementing 2FA for API / Sanctum token authentication Fortify\u0026rsquo;s 2FA is designed for session-based auth. For API authentication with Sanctum, you need a custom flow:\nUser logs in with credentials via API and receives a temporary session or token. Check if $user-\u0026gt;two_factor_secret exists. If yes, return a response indicating 2FA is required (e.g., { \u0026quot;two_factor_required\u0026quot;: true }). Client prompts for TOTP code and sends it in a second request. Validate the code using Google2FA::verifyKey(). If valid, issue a full Sanctum token with appropriate abilities. Example controller:\nuse PragmaRX\\Google2FA\\Google2FA; use Illuminate\\Support\\Facades\\Hash; public function login(Request $request) { $request-\u0026gt;validate([ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;required\u0026#39;, ]); $user = User::where(\u0026#39;email\u0026#39;, $request-\u0026gt;email)-\u0026gt;first(); if (! $user || ! Hash::check($request-\u0026gt;password, $user-\u0026gt;password)) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Invalid credentials\u0026#39;], 422); } if ($user-\u0026gt;two_factor_secret) { // Store user ID in session or return a temporary token for 2FA step session([\u0026#39;2fa_user_id\u0026#39; =\u0026gt; $user-\u0026gt;id]); return response()-\u0026gt;json([\u0026#39;two_factor_required\u0026#39; =\u0026gt; true]); } $token = $user-\u0026gt;createToken(\u0026#39;api-token\u0026#39;)-\u0026gt;plainTextToken; return response()-\u0026gt;json([\u0026#39;token\u0026#39; =\u0026gt; $token]); } public function verifyTwoFactor(Request $request, Google2FA $google2fa) { $request-\u0026gt;validate([\u0026#39;code\u0026#39; =\u0026gt; \u0026#39;required|digits:6\u0026#39;]); $userId = session(\u0026#39;2fa_user_id\u0026#39;); if (! $userId) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Unauthorized\u0026#39;], 401); } $user = User::findOrFail($userId); $secret = decrypt($user-\u0026gt;two_factor_secret); $valid = $google2fa-\u0026gt;verifyKey($secret, $request-\u0026gt;code); if (! $valid) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Invalid code\u0026#39;], 422); } session()-\u0026gt;forget(\u0026#39;2fa_user_id\u0026#39;); $token = $user-\u0026gt;createToken(\u0026#39;api-token\u0026#39;)-\u0026gt;plainTextToken; return response()-\u0026gt;json([\u0026#39;token\u0026#39; =\u0026gt; $token]); } This is a basic example. For production, use encrypted temporary tokens instead of sessions, add rate limiting on 2FA attempts, and use token abilities to mark tokens as 2FA-verified.\nSecurity best practices for production Always use HTTPS in production. TOTP secrets and recovery codes should never be sent over unencrypted connections.\nFortify encrypts two_factor_secret and two_factor_recovery_codes automatically using Laravel\u0026rsquo;s encryption. Make sure your APP_KEY is secure and never commit it to version control.\nAdd throttle middleware to /two-factor-challenge to prevent brute-force attacks. Limit it to 5 attempts per minute per user:\nRoute::post(\u0026#39;/two-factor-challenge\u0026#39;, [...])-\u0026gt;middleware(\u0026#39;throttle:5,1\u0026#39;); Fortify stores recovery codes hashed. When users enter a recovery code, it\u0026rsquo;s checked against the hash and then removed from the list.\nLog when 2FA is enabled, disabled, or when recovery codes are used. Set up alerts for suspicious activity like multiple failed 2FA attempts or 2FA being disabled from a new location.\nGive users clear instructions on setting up 2FA, saving recovery codes, and what to do if they lose access. A help page with screenshots is useful.\nLet users contact support to verify their identity via email or phone if they get locked out. Document your account recovery process.\nKeep laravel/fortify and pragmarx/google2fa updated for security patches.\nTroubleshooting common issues \u0026ldquo;Invalid authentication code\u0026rdquo; during confirmation:\nCheck that the user\u0026rsquo;s device time is synced. TOTP depends on accurate time. Even a 30-second drift will break the codes. Make sure the secret decrypts correctly. If APP_KEY changed after you generated the secret, decryption fails. Verify the QR code has the right secret. Print decrypt($user-\u0026gt;two_factor_secret) and manually generate a code with Google Authenticator to test. Users locked out after enabling 2FA:\nTest the recovery code flow before you deploy 2FA. Make sure the confirmation step works. Don\u0026rsquo;t mark 2FA as enabled until two_factor_confirmed_at is set. Give administrators a way to disable 2FA via CLI: php artisan tinker then User::find($id)-\u0026gt;update(['two_factor_secret' =\u0026gt; null]); QR code not displaying:\nCheck that the /user/two-factor-qr-code route works and returns SVG content. If your frontend and backend are on different domains, check CORS settings. See: Laravel API Authentication with Sanctum . Make sure the twoFactorQrCodeSvg() method exists on the User model. Check that the TwoFactorAuthenticatable trait is used. 2FA works locally but fails in production:\nCheck that APP_KEY is the same in both environments. Different keys break encryption. Clear the config cache: php artisan config:clear \u0026amp;\u0026amp; php artisan config:cache. Make sure these database columns exist: two_factor_secret, two_factor_recovery_codes, two_factor_confirmed_at. Re-run migrations if they\u0026rsquo;re missing. Verify session and cookie settings for HTTPS: Fixing Laravel Session and Cache Issues . High server load during TOTP validation:\nTOTP validation doesn\u0026rsquo;t use much CPU, but if you have millions of requests, cache failed attempts to avoid repeated database lookups. Index the two_factor_secret column if you query it often, though it\u0026rsquo;s usually only checked once per login. Advanced: Custom 2FA challenges and UI Fortify is headless, so you have full control over the UI. Build a custom React, Vue, or mobile app frontend:\nReact example (2FA setup component):\nfunction Enable2FA() { const [qrCode, setQrCode] = useState(\u0026#39;\u0026#39;); const [recoveryCodes, setRecoveryCodes] = useState([]); const [code, setCode] = useState(\u0026#39;\u0026#39;); const enable = async () =\u0026gt; { await axios.post(\u0026#39;/user/two-factor-authentication\u0026#39;); const qr = await axios.get(\u0026#39;/user/two-factor-qr-code\u0026#39;); const codes = await axios.get(\u0026#39;/user/two-factor-recovery-codes\u0026#39;); setQrCode(qr.data.svg); setRecoveryCodes(codes.data); }; const confirm = async () =\u0026gt; { try { await axios.post(\u0026#39;/user/confirmed-two-factor-authentication\u0026#39;, { code }); alert(\u0026#39;2FA enabled successfully!\u0026#39;); } catch (err) { alert(\u0026#39;Invalid code\u0026#39;); } }; return ( \u0026lt;div\u0026gt; \u0026lt;button onClick={enable}\u0026gt;Enable 2FA\u0026lt;/button\u0026gt; {qrCode \u0026amp;\u0026amp; ( \u0026lt;\u0026gt; \u0026lt;div dangerouslySetInnerHTML={{ __html: qrCode }} /\u0026gt; \u0026lt;input value={code} onChange={(e) =\u0026gt; setCode(e.target.value)} placeholder=\u0026#34;Enter code\u0026#34; /\u0026gt; \u0026lt;button onClick={confirm}\u0026gt;Confirm\u0026lt;/button\u0026gt; \u0026lt;h3\u0026gt;Recovery Codes (save these!):\u0026lt;/h3\u0026gt; \u0026lt;ul\u0026gt;{recoveryCodes.map((c) =\u0026gt; \u0026lt;li key={c}\u0026gt;{c}\u0026lt;/li\u0026gt;)}\u0026lt;/ul\u0026gt; \u0026lt;/\u0026gt; )} \u0026lt;/div\u0026gt; ); } For a full SPA setup with Inertia.js, see: Laravel Integration with React and Vue .\nAlternative packages and approaches spatie/laravel-one-time-passwords (released May 2025): A newer package with flexible OTP generation for email, SMS, or TOTP. Good if you need custom OTP delivery methods beyond TOTP.\npragmarx/google2fa-laravel: Direct Google2FA integration without Fortify. Use this if you want full control over all 2FA logic and don\u0026rsquo;t need Fortify\u0026rsquo;s other features.\nLaravel Jetstream/Breeze: Pre-built 2FA UI using Fortify. Fastest setup for new projects but less flexible than using Fortify alone.\nFor most projects, Fortify gives you the best balance of flexibility and built-in security.\nSummary Laravel Fortify makes TOTP-based 2FA simple to set up. Install the package, enable the feature, add the TwoFactorAuthenticatable trait to your User model, and build your frontend to work with Fortify\u0026rsquo;s routes.\nAlways require code confirmation before enabling 2FA to prevent lockouts. Generate and display recovery codes, and enforce password confirmation for 2FA changes.\nAdd rate limiting on the challenge endpoint, monitor 2FA events, and teach users to save their recovery codes. Test the complete flow including login with TOTP codes and recovery codes. Make sure your production environment uses HTTPS with proper encryption.\nFollowing these practices gives you a secure, user-friendly multi-factor authentication system that reduces account takeover risk. For more security layers, check out Laravel Security Best Practices for Production and Laravel Production Monitoring and Error Tracking .\n","href":"/2025/10/laravel-two-factor-authentication-2fa-fortify.html","title":"How to Implement Multi-Factor Authentication (2FA) in Laravel with Laravel Fortify"},{"content":"It\u0026rsquo;s 3 AM and your phone won\u0026rsquo;t stop buzzing. Your API just got hammered with 10,000 requests in 30 seconds. Users uploading images, generating PDF reports, sending welcome emails, processing credit cards - all at once. Your server\u0026rsquo;s trying to handle everything synchronously and it\u0026rsquo;s dying. Response times creep from 200ms to 15 seconds. Timeout errors everywhere. Your monitoring dashboard looks like a Christmas tree, but red instead of green.\nI\u0026rsquo;ve been there. It\u0026rsquo;s not fun.\nHere\u0026rsquo;s what changed everything for me: message queues. Instead of trying to do everything right now, you queue it up and process it when you can. Your API responds in milliseconds. Workers in the background handle the heavy lifting at whatever pace they can sustain. RabbitMQ is the tool that made this click for me.\nIn this guide, I\u0026rsquo;ll walk you through everything I learned about RabbitMQ and Go. We\u0026rsquo;ll start simple with basic message sending, then build up to work queues, pub/sub patterns, sophisticated routing, and all the production stuff like reconnection logic and error handling. By the end, you\u0026rsquo;ll know how to build systems that don\u0026rsquo;t fall over when things get busy.\nUnderstanding RabbitMQ and Message Queuing Think about how you\u0026rsquo;d handle a busy restaurant. When customers place orders, the waiter doesn\u0026rsquo;t run back to the kitchen, watch the chef cook everything, then deliver it to the table. That would be insane. Instead, orders go to the kitchen queue. The chef processes them as fast as possible. The waiter moves on to the next customer.\nMessage queues work the same way for services. Service A drops a message in the queue and keeps going. Service B picks it up when ready. Nobody\u0026rsquo;s waiting around, tapping their fingers.\nRabbitMQ is the kitchen manager in this analogy. It\u0026rsquo;s a message broker running AMQP (Advanced Message Queuing Protocol). Think of it as a standalone server sitting between your services, taking messages from producers and delivering them to consumers based on whatever rules you set up.\nHere\u0026rsquo;s the vocabulary you need to know:\nProducers are services that send messages. They drop stuff into exchanges without caring who\u0026rsquo;s listening on the other end.\nExchanges are like post offices. They receive messages and figure out which queues should get them. Different exchange types have different routing strategies - we\u0026rsquo;ll cover those later.\nQueues are mailboxes. They hold messages in order until someone\u0026rsquo;s ready to process them. You can make them durable so messages survive server crashes.\nConsumers grab messages from queues and do the actual work. You can run multiple consumers on the same queue to split the load.\nBindings are the rules connecting exchanges to queues. They define which messages end up where.\nSo why use RabbitMQ? A few good reasons:\nYour services can scale independently without being tied together. Traffic spikes get queued instead of crashing everything. You can spread work across as many workers as you need. Services can restart without losing messages. Plus you get sophisticated routing patterns that would be painful to build yourself.\nUse message queues when you\u0026rsquo;re doing background jobs (emails, reports, image processing), building microservices that don\u0026rsquo;t need instant responses, creating event-driven systems where multiple services react to the same thing, balancing load across workers, or dealing with flaky networks and services that go down sometimes.\nPrerequisites and Setup Let\u0026rsquo;s get RabbitMQ running on your machine. Pick whichever method works best for you:\nInstalling RabbitMQ macOS:\nbrew install rabbitmq brew services start rabbitmq Ubuntu/Debian:\nsudo apt-get update sudo apt-get install rabbitmq-server sudo systemctl start rabbitmq sudo systemctl enable rabbitmq Docker (recommended for development):\ndocker run -d --name rabbitmq \\ -p 5672:5672 \\ -p 15672:15672 \\ rabbitmq:3-management The management plugin provides a web UI at http://localhost:15672 (username: guest, password: guest).\nWindows:\nDownload the installer from rabbitmq.com/download.html and follow the installation wizard.\nVerify Installation Check RabbitMQ is running:\nsudo rabbitmqctl status Or visit the management UI at http://localhost:15672.\nGo RabbitMQ Client Create a new Go project:\nmkdir rabbitmq-tutorial cd rabbitmq-tutorial go mod init github.com/yourusername/rabbitmq-tutorial Install the RabbitMQ Go client:\ngo get github.com/rabbitmq/amqp091-go This is the official Go client for RabbitMQ supporting AMQP 0.9.1.\nBasic Producer - Sending Messages Time to send our first message. This is the \u0026ldquo;Hello World\u0026rdquo; of message queues - simple but it teaches you the fundamentals.\n// producer/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { // Connect to RabbitMQ conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect to RabbitMQ: %v\u0026#34;, err) } defer conn.Close() // Create a channel ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open a channel: %v\u0026#34;, err) } defer ch.Close() // Declare a queue queue, err := ch.QueueDeclare( \u0026#34;hello\u0026#34;, // queue name false, // durable false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Prepare message body := \u0026#34;Hello, RabbitMQ!\u0026#34; ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() // Publish message err = ch.PublishWithContext( ctx, \u0026#34;\u0026#34;, // exchange queue.Name, // routing key (queue name) false, // mandatory false, // immediate amqp.Publishing{ ContentType: \u0026#34;text/plain\u0026#34;, Body: []byte(body), }, ) if err != nil { log.Fatalf(\u0026#34;Failed to publish message: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sent message: %s\\n\u0026#34;, body) } Run the producer:\ngo run producer/main.go Run this and you\u0026rsquo;ve just sent your first message through RabbitMQ. Pretty straightforward, right?\nHere\u0026rsquo;s what\u0026rsquo;s happening under the hood:\namqp.Dial() connects to RabbitMQ using the AMQP URL format. The default credentials are guest/guest for local development.\nconn.Channel() creates a channel. Most operations happen on channels, not connections. One connection can have many channels.\nQueueDeclare() creates a queue if it doesn\u0026rsquo;t exist. The parameters control queue behavior - we\u0026rsquo;re using a simple non-durable queue for now.\nPublishWithContext() sends the message. Empty exchange means the default exchange, which routes to queues by name.\nBasic Consumer - Receiving Messages Sending messages is only half the story. Now let\u0026rsquo;s build something to actually receive and process them.\n// consumer/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { // Connect to RabbitMQ conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect to RabbitMQ: %v\u0026#34;, err) } defer conn.Close() // Create a channel ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open a channel: %v\u0026#34;, err) } defer ch.Close() // Declare the same queue queue, err := ch.QueueDeclare( \u0026#34;hello\u0026#34;, false, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Register a consumer msgs, err := ch.Consume( queue.Name, // queue \u0026#34;\u0026#34;, // consumer tag true, // auto-ack false, // exclusive false, // no-local false, // no-wait nil, // args ) if err != nil { log.Fatalf(\u0026#34;Failed to register consumer: %v\u0026#34;, err) } // Block and wait for messages forever := make(chan bool) go func() { for msg := range msgs { fmt.Printf(\u0026#34;Received message: %s\\n\u0026#34;, msg.Body) } }() fmt.Println(\u0026#34;Waiting for messages. Press CTRL+C to exit.\u0026#34;) \u0026lt;-forever } Run the consumer in a separate terminal:\ngo run consumer/main.go Keep the consumer running, then fire up the producer again in another terminal. Watch the magic happen - the consumer picks up the message instantly.\nWhat\u0026rsquo;s going on here:\nThe consumer also declares the queue. Queue declarations are idempotent - if the queue exists with the same configuration, nothing happens.\nConsume() registers this application as a consumer. It returns a channel that delivers messages.\nauto-ack: true means messages are automatically acknowledged when delivered. We\u0026rsquo;ll improve this later.\nThe goroutine processes messages as they arrive. The channel blocks until new messages appear.\nWork Queues - Distributing Tasks Now we\u0026rsquo;re getting to the good stuff. Work queues let you spread heavy tasks across multiple workers. Got a thousand images to resize? Spin up five workers and knock them out five times faster.\nProducer with Task Distribution // producer_tasks/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() // Declare durable queue queue, err := ch.QueueDeclare( \u0026#34;task_queue\u0026#34;, // name true, // durable (survives broker restart) false, // delete when unused false, // exclusive false, // no-wait nil, // arguments ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Get message from command line args or use default body := \u0026#34;Task with default complexity\u0026#34; if len(os.Args) \u0026gt; 1 { body = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() err = ch.PublishWithContext( ctx, \u0026#34;\u0026#34;, queue.Name, false, false, amqp.Publishing{ DeliveryMode: amqp.Persistent, // Persist message to disk ContentType: \u0026#34;text/plain\u0026#34;, Body: []byte(body), }, ) if err != nil { log.Fatalf(\u0026#34;Failed to publish: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sent task: %s\\n\u0026#34;, body) } Notice what we changed here:\ndurable: true means the queue survives restarts. Your messages won\u0026rsquo;t vanish if RabbitMQ crashes.\nDeliveryMode: amqp.Persistent writes messages to disk. This is important - without it, a crash loses everything in flight.\nWorker with Fair Dispatch // worker/main.go package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() queue, err := ch.QueueDeclare( \u0026#34;task_queue\u0026#34;, true, // durable false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Set QoS to process one message at a time err = ch.Qos( 1, // prefetch count 0, // prefetch size false, // global ) if err != nil { log.Fatalf(\u0026#34;Failed to set QoS: %v\u0026#34;, err) } msgs, err := ch.Consume( queue.Name, \u0026#34;\u0026#34;, false, // manual ack false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to register consumer: %v\u0026#34;, err) } forever := make(chan bool) go func() { for msg := range msgs { fmt.Printf(\u0026#34;Received task: %s\\n\u0026#34;, msg.Body) // Simulate work (each dot = 1 second) dotCount := bytes.Count(msg.Body, []byte(\u0026#34;.\u0026#34;)) duration := time.Duration(dotCount) * time.Second time.Sleep(duration) fmt.Println(\u0026#34;Task completed\u0026#34;) // Manually acknowledge msg.Ack(false) } }() fmt.Println(\u0026#34;Worker waiting for tasks. Press CTRL+C to exit.\u0026#34;) \u0026lt;-forever } Three critical improvements here:\nQos(1, 0, false) tells RabbitMQ \u0026ldquo;only give me one message at a time.\u0026rdquo; Without this, fast workers sit idle while slow ones get buried. Fair dispatch keeps everyone working efficiently.\nauto-ack: false turns off automatic acknowledgment. Now messages only disappear after you explicitly confirm you handled them.\nmsg.Ack(false) confirms \u0026ldquo;yeah, I processed this successfully.\u0026rdquo; If your worker crashes before acking, RabbitMQ sends that message to another worker. No lost work.\nTesting work distribution:\nRun multiple workers in different terminals:\n# Terminal 1 go run worker/main.go # Terminal 2 go run worker/main.go # Terminal 3 go run worker/main.go Send tasks with different complexities:\ngo run producer_tasks/main.go \u0026#34;Task one.\u0026#34; go run producer_tasks/main.go \u0026#34;Task two....\u0026#34; go run producer_tasks/main.go \u0026#34;Task three.\u0026#34; go run producer_tasks/main.go \u0026#34;Task four......\u0026#34; Watch what happens - tasks get distributed evenly across your workers. Each dot simulates one second of work, so \u0026ldquo;Task four\u0026hellip;\u0026hellip;\u0026rdquo; takes six seconds. The worker that finishes first grabs the next task. Beautiful.\nPublish/Subscribe Pattern Sometimes you need to broadcast the same message to everyone. Think logging systems where you want to save logs to a file AND send them to a monitoring service AND display them in the console. That\u0026rsquo;s pub/sub.\nPublisher with Fanout Exchange // publisher/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() // Declare fanout exchange err = ch.ExchangeDeclare( \u0026#34;logs\u0026#34;, // name \u0026#34;fanout\u0026#34;, // type true, // durable false, // auto-deleted false, // internal false, // no-wait nil, // arguments ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } body := \u0026#34;Info: Something happened\u0026#34; if len(os.Args) \u0026gt; 1 { body = strings.Join(os.Args[1:], \u0026#34; \u0026#34;) } ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() err = ch.PublishWithContext( ctx, \u0026#34;logs\u0026#34;, // exchange \u0026#34;\u0026#34;, // routing key (ignored for fanout) false, false, amqp.Publishing{ ContentType: \u0026#34;text/plain\u0026#34;, Body: []byte(body), }, ) if err != nil { log.Fatalf(\u0026#34;Failed to publish: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Published log: %s\\n\u0026#34;, body) } The fanout exchange is like yelling in a crowded room - everyone hears it. The routing key doesn\u0026rsquo;t matter here.\nSubscriber // subscriber/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() err = ch.ExchangeDeclare( \u0026#34;logs\u0026#34;, \u0026#34;fanout\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } // Declare exclusive queue (auto-delete when consumer disconnects) queue, err := ch.QueueDeclare( \u0026#34;\u0026#34;, // empty name = random queue name false, // durable false, // delete when unused true, // exclusive false, // no-wait nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Bind queue to exchange err = ch.QueueBind( queue.Name, // queue name \u0026#34;\u0026#34;, // routing key \u0026#34;logs\u0026#34;, // exchange false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to bind queue: %v\u0026#34;, err) } msgs, err := ch.Consume( queue.Name, \u0026#34;\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to register consumer: %v\u0026#34;, err) } forever := make(chan bool) go func() { for msg := range msgs { fmt.Printf(\u0026#34;Received log: %s\\n\u0026#34;, msg.Body) } }() fmt.Println(\u0026#34;Waiting for logs. Press CTRL+C to exit.\u0026#34;) \u0026lt;-forever } Testing pub/sub:\nRun multiple subscribers in different terminals:\n# Terminal 1 go run subscriber/main.go # Terminal 2 go run subscriber/main.go # Terminal 3 go run subscriber/main.go Publish a message:\ngo run publisher/main.go \u0026#34;Error: Database connection failed\u0026#34; Boom - all three subscribers get the exact same message. Perfect for when multiple services need to know about the same event.\nRouting with Direct Exchange Fanout is great, but sometimes you want selective delivery. Maybe you only want errors going to your pager, while warnings go to logs. Direct exchanges let you route based on exact key matches.\nProducer with Routing // emit_log_direct/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() // Declare direct exchange err = ch.ExchangeDeclare( \u0026#34;logs_direct\u0026#34;, // name \u0026#34;direct\u0026#34;, // type true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } // Get severity and message from args severity := \u0026#34;info\u0026#34; if len(os.Args) \u0026gt; 1 { severity = os.Args[1] } body := \u0026#34;Default log message\u0026#34; if len(os.Args) \u0026gt; 2 { body = strings.Join(os.Args[2:], \u0026#34; \u0026#34;) } ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() err = ch.PublishWithContext( ctx, \u0026#34;logs_direct\u0026#34;, // exchange severity, // routing key false, false, amqp.Publishing{ ContentType: \u0026#34;text/plain\u0026#34;, Body: []byte(body), }, ) if err != nil { log.Fatalf(\u0026#34;Failed to publish: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sent [%s] %s\\n\u0026#34;, severity, body) } Consumer with Selective Routing // receive_logs_direct/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() err = ch.ExchangeDeclare( \u0026#34;logs_direct\u0026#34;, \u0026#34;direct\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } queue, err := ch.QueueDeclare( \u0026#34;\u0026#34;, false, false, true, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Get severities from command line args severities := os.Args[1:] if len(severities) == 0 { log.Printf(\u0026#34;Usage: %s [info] [warning] [error]\u0026#34;, os.Args[0]) os.Exit(1) } // Bind queue for each severity for _, severity := range severities { err = ch.QueueBind( queue.Name, severity, // routing key \u0026#34;logs_direct\u0026#34;, // exchange false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to bind queue: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Bound to severity: %s\\n\u0026#34;, severity) } msgs, err := ch.Consume( queue.Name, \u0026#34;\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to register consumer: %v\u0026#34;, err) } forever := make(chan bool) go func() { for msg := range msgs { fmt.Printf(\u0026#34;[%s] %s\\n\u0026#34;, msg.RoutingKey, msg.Body) } }() fmt.Println(\u0026#34;Waiting for logs. Press CTRL+C to exit.\u0026#34;) \u0026lt;-forever } Testing routing:\nStart a consumer that only receives errors:\ngo run receive_logs_direct/main.go error Start another consumer for warnings and errors:\ngo run receive_logs_direct/main.go warning error Send different log levels:\ngo run emit_log_direct/main.go info \u0026#34;Application started\u0026#34; go run emit_log_direct/main.go warning \u0026#34;CPU usage high\u0026#34; go run emit_log_direct/main.go error \u0026#34;Database connection failed\u0026#34; The first consumer only catches the error. The second one sees both warnings and errors. Super useful for setting up different alert levels.\nTopic Exchange for Pattern Matching Direct routing works, but what if you want more flexibility? Topics let you use wildcards for pattern matching. Way more powerful.\nPublisher with Topics // emit_log_topic/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() // Declare topic exchange err = ch.ExchangeDeclare( \u0026#34;logs_topic\u0026#34;, // name \u0026#34;topic\u0026#34;, // type true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } // Get routing key and message routingKey := \u0026#34;anonymous.info\u0026#34; if len(os.Args) \u0026gt; 1 { routingKey = os.Args[1] } body := \u0026#34;Default message\u0026#34; if len(os.Args) \u0026gt; 2 { body = strings.Join(os.Args[2:], \u0026#34; \u0026#34;) } ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() err = ch.PublishWithContext( ctx, \u0026#34;logs_topic\u0026#34;, routingKey, false, false, amqp.Publishing{ ContentType: \u0026#34;text/plain\u0026#34;, Body: []byte(body), }, ) if err != nil { log.Fatalf(\u0026#34;Failed to publish: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Sent [%s] %s\\n\u0026#34;, routingKey, body) } Subscriber with Pattern Matching // receive_logs_topic/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) func main() { conn, err := amqp.Dial(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() ch, err := conn.Channel() if err != nil { log.Fatalf(\u0026#34;Failed to open channel: %v\u0026#34;, err) } defer ch.Close() err = ch.ExchangeDeclare( \u0026#34;logs_topic\u0026#34;, \u0026#34;topic\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare exchange: %v\u0026#34;, err) } queue, err := ch.QueueDeclare( \u0026#34;\u0026#34;, false, false, true, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to declare queue: %v\u0026#34;, err) } // Get binding keys from args bindingKeys := os.Args[1:] if len(bindingKeys) == 0 { log.Printf(\u0026#34;Usage: %s [binding_key]...\u0026#34;, os.Args[0]) log.Printf(\u0026#34;Example patterns:\u0026#34;) log.Printf(\u0026#34; *.critical - all critical logs\u0026#34;) log.Printf(\u0026#34; kern.* - all kernel logs\u0026#34;) log.Printf(\u0026#34; *.*.error - all error logs from any facility\u0026#34;) os.Exit(1) } // Bind queue with each pattern for _, key := range bindingKeys { err = ch.QueueBind( queue.Name, key, \u0026#34;logs_topic\u0026#34;, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to bind queue: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Bound with pattern: %s\\n\u0026#34;, key) } msgs, err := ch.Consume( queue.Name, \u0026#34;\u0026#34;, true, false, false, false, nil, ) if err != nil { log.Fatalf(\u0026#34;Failed to register consumer: %v\u0026#34;, err) } forever := make(chan bool) go func() { for msg := range msgs { fmt.Printf(\u0026#34;[%s] %s\\n\u0026#34;, msg.RoutingKey, msg.Body) } }() fmt.Println(\u0026#34;Waiting for logs. Press CTRL+C to exit.\u0026#34;) \u0026lt;-forever } Topic routing rules:\n* (star) matches exactly one word. # (hash) matches zero or more words.\nWords are separated by dots.\nExamples:\nRouting key kern.critical matches pattern kern.* and *.critical and #.\nRouting key kern.info.security matches kern.# and #.security and kern.*.security.\nTesting topics:\nReceive all kernel logs:\ngo run receive_logs_topic/main.go \u0026#34;kern.*\u0026#34; Receive all critical logs:\ngo run receive_logs_topic/main.go \u0026#34;*.critical\u0026#34; Receive all logs:\ngo run receive_logs_topic/main.go \u0026#34;#\u0026#34; Send various log types:\ngo run emit_log_topic/main.go \u0026#34;kern.critical\u0026#34; \u0026#34;Kernel panic\u0026#34; go run emit_log_topic/main.go \u0026#34;kern.info\u0026#34; \u0026#34;System booted\u0026#34; go run emit_log_topic/main.go \u0026#34;app.critical\u0026#34; \u0026#34;Out of memory\u0026#34; Connection Management and Error Handling Here\u0026rsquo;s where things get real. In production, connections drop. Networks hiccup. RabbitMQ restarts for updates. Your code needs to handle all of this gracefully without losing messages.\n// pkg/rabbitmq/connection.go package rabbitmq import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) type Connection struct { conn *amqp.Connection channel *amqp.Channel url string done chan bool } func NewConnection(url string) (*Connection, error) { c := \u0026amp;Connection{ url: url, done: make(chan bool), } err := c.connect() if err != nil { return nil, err } go c.handleReconnect() return c, nil } func (c *Connection) connect() error { var err error c.conn, err = amqp.Dial(c.url) if err != nil { return fmt.Errorf(\u0026#34;failed to connect: %w\u0026#34;, err) } c.channel, err = c.conn.Channel() if err != nil { c.conn.Close() return fmt.Errorf(\u0026#34;failed to open channel: %w\u0026#34;, err) } log.Println(\u0026#34;Connected to RabbitMQ\u0026#34;) return nil } func (c *Connection) handleReconnect() { for { select { case \u0026lt;-c.done: return case err := \u0026lt;-c.conn.NotifyClose(make(chan *amqp.Error)): if err != nil { log.Printf(\u0026#34;Connection closed: %v\u0026#34;, err) c.reconnect() } } } } func (c *Connection) reconnect() { for { log.Println(\u0026#34;Attempting to reconnect...\u0026#34;) err := c.connect() if err == nil { return } log.Printf(\u0026#34;Reconnection failed: %v. Retrying in 5 seconds...\u0026#34;, err) time.Sleep(5 * time.Second) } } func (c *Connection) Channel() *amqp.Channel { return c.channel } func (c *Connection) Close() { close(c.done) if c.channel != nil { c.channel.Close() } if c.conn != nil { c.conn.Close() } } Usage:\n// main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/yourusername/rabbitmq-tutorial/pkg/rabbitmq\u0026#34; ) func main() { conn, err := rabbitmq.NewConnection(\u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to establish connection: %v\u0026#34;, err) } defer conn.Close() // Use conn.Channel() for operations ch := conn.Channel() // Your application logic here time.Sleep(60 * time.Second) } This handles disconnections automatically. Network blip? No problem. RabbitMQ restart? It\u0026rsquo;ll reconnect and keep going. You won\u0026rsquo;t lose messages.\nPublisher Confirms for Reliability Want to be absolutely sure your messages made it to RabbitMQ? Publisher confirms give you that guarantee. RabbitMQ sends back a confirmation for each message.\n// pkg/rabbitmq/reliable_publisher.go package rabbitmq import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) type ReliablePublisher struct { channel *amqp.Channel } func NewReliablePublisher(channel *amqp.Channel) (*ReliablePublisher, error) { // Enable publisher confirms err := channel.Confirm(false) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to enable confirms: %w\u0026#34;, err) } return \u0026amp;ReliablePublisher{ channel: channel, }, nil } func (p *ReliablePublisher) Publish(ctx context.Context, exchange, routingKey string, body []byte) error { // Create confirm channel confirms := p.channel.NotifyPublish(make(chan amqp.Confirmation, 1)) err := p.channel.PublishWithContext( ctx, exchange, routingKey, false, false, amqp.Publishing{ DeliveryMode: amqp.Persistent, ContentType: \u0026#34;application/json\u0026#34;, Body: body, }, ) if err != nil { return fmt.Errorf(\u0026#34;failed to publish: %w\u0026#34;, err) } // Wait for confirmation select { case confirm := \u0026lt;-confirms: if !confirm.Ack { return fmt.Errorf(\u0026#34;message not acknowledged by broker\u0026#34;) } log.Println(\u0026#34;Message confirmed by broker\u0026#34;) return nil case \u0026lt;-ctx.Done(): return ctx.Err() case \u0026lt;-time.After(5 * time.Second): return fmt.Errorf(\u0026#34;confirmation timeout\u0026#34;) } } Usage:\npublisher, err := NewReliablePublisher(ch) if err != nil { log.Fatal(err) } ctx := context.Background() err = publisher.Publish(ctx, \u0026#34;exchange\u0026#34;, \u0026#34;routing.key\u0026#34;, []byte(\u0026#34;message\u0026#34;)) if err != nil { log.Printf(\u0026#34;Publish failed: %v\u0026#34;, err) } Consumer with Retry Logic Processing fails sometimes. Database is down, external API times out, whatever. You don\u0026rsquo;t want to lose the message, but you also don\u0026rsquo;t want to retry forever. Here\u0026rsquo;s how to handle it properly.\n// pkg/rabbitmq/reliable_consumer.go package rabbitmq import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; amqp \u0026#34;github.com/rabbitmq/amqp091-go\u0026#34; ) type MessageHandler func([]byte) error type ReliableConsumer struct { channel *amqp.Channel queueName string handler MessageHandler maxRetries int retryDelay time.Duration } func NewReliableConsumer( channel *amqp.Channel, queueName string, handler MessageHandler, ) *ReliableConsumer { return \u0026amp;ReliableConsumer{ channel: channel, queueName: queueName, handler: handler, maxRetries: 3, retryDelay: 5 * time.Second, } } func (c *ReliableConsumer) Start() error { // Declare dead letter exchange err := c.channel.ExchangeDeclare( \u0026#34;dlx\u0026#34;, \u0026#34;direct\u0026#34;, true, false, false, false, nil, ) if err != nil { return fmt.Errorf(\u0026#34;failed to declare DLX: %w\u0026#34;, err) } // Declare dead letter queue _, err = c.channel.QueueDeclare( \u0026#34;dlq\u0026#34;, true, false, false, false, nil, ) if err != nil { return fmt.Errorf(\u0026#34;failed to declare DLQ: %w\u0026#34;, err) } // Bind DLQ to DLX err = c.channel.QueueBind(\u0026#34;dlq\u0026#34;, c.queueName, \u0026#34;dlx\u0026#34;, false, nil) if err != nil { return fmt.Errorf(\u0026#34;failed to bind DLQ: %w\u0026#34;, err) } // Declare main queue with DLX _, err = c.channel.QueueDeclare( c.queueName, true, false, false, false, amqp.Table{ \u0026#34;x-dead-letter-exchange\u0026#34;: \u0026#34;dlx\u0026#34;, }, ) if err != nil { return fmt.Errorf(\u0026#34;failed to declare queue: %w\u0026#34;, err) } // Set QoS err = c.channel.Qos(1, 0, false) if err != nil { return fmt.Errorf(\u0026#34;failed to set QoS: %w\u0026#34;, err) } // Start consuming msgs, err := c.channel.Consume( c.queueName, \u0026#34;\u0026#34;, false, // manual ack false, false, false, nil, ) if err != nil { return fmt.Errorf(\u0026#34;failed to start consuming: %w\u0026#34;, err) } go c.handleMessages(msgs) return nil } func (c *ReliableConsumer) handleMessages(msgs \u0026lt;-chan amqp.Delivery) { for msg := range msgs { retries := c.getRetryCount(msg) err := c.handler(msg.Body) if err != nil { log.Printf(\u0026#34;Handler error: %v\u0026#34;, err) if retries \u0026lt; c.maxRetries { // Reject with requeue for retry log.Printf(\u0026#34;Retry %d/%d\u0026#34;, retries+1, c.maxRetries) msg.Nack(false, true) time.Sleep(c.retryDelay) } else { // Max retries exceeded, send to DLQ log.Printf(\u0026#34;Max retries exceeded, sending to DLQ\u0026#34;) msg.Nack(false, false) } } else { // Success msg.Ack(false) } } } func (c *ReliableConsumer) getRetryCount(msg amqp.Delivery) int { if msg.Headers == nil { return 0 } if count, ok := msg.Headers[\u0026#34;x-retry-count\u0026#34;].(int); ok { return count } return 0 } Production Best Practices Connection Pooling For high-throughput applications, maintain a pool of connections:\ntype ConnectionPool struct { connections []*Connection current int mu sync.Mutex } func NewConnectionPool(url string, size int) (*ConnectionPool, error) { pool := \u0026amp;ConnectionPool{ connections: make([]*Connection, size), } for i := 0; i \u0026lt; size; i++ { conn, err := NewConnection(url) if err != nil { return nil, err } pool.connections[i] = conn } return pool, nil } func (p *ConnectionPool) Get() *Connection { p.mu.Lock() defer p.mu.Unlock() conn := p.connections[p.current] p.current = (p.current + 1) % len(p.connections) return conn } Monitoring and Metrics Track queue depth, consumer count, and message rates:\nfunc (c *Connection) GetQueueInfo(queueName string) (*QueueInfo, error) { queue, err := c.channel.QueueInspect(queueName) if err != nil { return nil, err } return \u0026amp;QueueInfo{ Messages: queue.Messages, Consumers: queue.Consumers, Name: queue.Name, }, nil } type QueueInfo struct { Messages int Consumers int Name string } Configuration Management Use environment variables for configuration:\ntype Config struct { URL string Exchange string QueueName string RoutingKey string PrefetchCount int ReconnectDelay time.Duration } func LoadConfig() *Config { return \u0026amp;Config{ URL: getEnv(\u0026#34;RABBITMQ_URL\u0026#34;, \u0026#34;amqp://guest:guest@localhost:5672/\u0026#34;), Exchange: getEnv(\u0026#34;RABBITMQ_EXCHANGE\u0026#34;, \u0026#34;default\u0026#34;), QueueName: getEnv(\u0026#34;RABBITMQ_QUEUE\u0026#34;, \u0026#34;default_queue\u0026#34;), RoutingKey: getEnv(\u0026#34;RABBITMQ_ROUTING_KEY\u0026#34;, \u0026#34;\u0026#34;), PrefetchCount: getEnvInt(\u0026#34;RABBITMQ_PREFETCH\u0026#34;, 1), ReconnectDelay: getEnvDuration(\u0026#34;RABBITMQ_RECONNECT_DELAY\u0026#34;, 5*time.Second), } } func getEnv(key, defaultValue string) string { if value := os.Getenv(key); value != \u0026#34;\u0026#34; { return value } return defaultValue } Security Considerations Use TLS for production:\nconn, err := amqp.DialTLS(\u0026#34;amqps://user:pass@hostname:5671/\u0026#34;, \u0026amp;tls.Config{ MinVersion: tls.VersionTLS12, }) Create dedicated users with limited permissions instead of using the guest account.\nPerformance Optimization Batch messages when possible:\nfunc (p *Publisher) PublishBatch(messages []Message) error { for _, msg := range messages { err := p.channel.PublishWithContext( context.Background(), p.exchange, msg.RoutingKey, false, false, amqp.Publishing{ Body: msg.Body, }, ) if err != nil { return err } } return nil } Use connection and channel pooling for concurrent publishing.\nIntegration with Other Go Features Combine RabbitMQ with other patterns for robust applications.\nFor background job processing, use RabbitMQ with Asynq and Redis for complementary features.\nStore message metadata in PostgreSQL or MongoDB for audit trails.\nUse structured logging with slog to track message processing.\nImplement rate limiting for message publishing.\nSecure message endpoints with JWT authentication .\nWrapping Up RabbitMQ completely changed how I build distributed systems. Before message queues, everything was tightly coupled - one service called another, waited for a response, hoped nothing broke. Painful. With RabbitMQ, services barely know each other exist. They drop messages in queues and move on. Way more resilient.\nWe covered a lot of ground here. Basic producers and consumers, work queues for distributing load, pub/sub for broadcasting, routing patterns for selective delivery, connection management that handles failures, and all the production stuff like publisher confirms and retry logic. That\u0026rsquo;s your foundation right there.\nOne thing to keep in mind: message queues mean eventual consistency. Work happens asynchronously. Data doesn\u0026rsquo;t update instantly everywhere. That takes some getting used to if you\u0026rsquo;re coming from a synchronous world, but once it clicks, you\u0026rsquo;ll never want to go back. Systems that handle failures gracefully and scale horizontally are worth the mental shift.\nFor production, watch your queue depths - if they keep growing, you need more workers. Set up dead letter queues for messages that fail repeatedly. Use publisher confirms when you can\u0026rsquo;t afford to lose data. Pool connections for high throughput. These aren\u0026rsquo;t optional niceties - they\u0026rsquo;re what keep your messaging infrastructure solid when things get busy.\nThe best way to really learn this stuff is to build something real with it. Pick a feature in your application that\u0026rsquo;s slow or blocks - image processing, report generation, email sending, whatever. Throw it behind RabbitMQ and see how much better it feels. You\u0026rsquo;ll be hooked.\n","href":"/2025/10/how-to-implement-message-queuing-with-rabbitmq-in-go.html","title":"How to Implement Message Queuing with RabbitMQ in Go"},{"content":"AI applications are everywhere now. Chatbots answering customer questions, code assistants writing functions, content generators creating blog posts, search systems understanding natural language. If you\u0026rsquo;re building with Go, you need to know how to tap into these language models without fighting with complicated Python libraries or rewriting your entire stack.\nThe good news: integrating AI into Go applications is straightforward once you understand the patterns. You have two main paths - cloud APIs like OpenAI for maximum quality and scale, or local models with Ollama for privacy and cost control. Sometimes you want both.\nThis guide shows you how to build real AI features in Go. We\u0026rsquo;ll start with OpenAI integration for GPT-4 and embeddings, move to running local models with Ollama, build streaming chat interfaces, implement semantic search with RAG (Retrieval-Augmented Generation), and cover all the production concerns like rate limiting, error handling, and cost optimization.\nUnderstanding LLMs and Go Integration Large Language Models (LLMs) are neural networks trained on massive text datasets. They predict the next token (word or subword) based on previous context, which lets them generate human-like text, answer questions, write code, and more.\nOpenAI provides cloud-hosted models like GPT-4, GPT-3.5 Turbo, and text embedding models. You send HTTP requests to their API with your prompt, they run inference on their servers, and stream back responses. Simple, scalable, but costs money and requires internet connectivity.\nOllama lets you run open-source models like Llama 2, Mistral, and CodeLlama locally on your machine. Download models once, run inference locally without API costs. Perfect for development, privacy-sensitive applications, or high-volume use cases where API costs add up.\nWhen to use each:\nOpenAI works best for production applications needing best-in-class quality, scale beyond your infrastructure, features like function calling and vision, or when you need embeddings for semantic search. The API is reliable, fast, and keeps improving.\nOllama shines for local development without API costs, applications requiring data privacy, high-volume batch processing where API costs would be prohibitive, or environments without reliable internet. Performance depends on your hardware - good GPU helps significantly.\nPrerequisites and Setup Before we start coding, set up your environment.\nInstalling Dependencies Create a new Go project:\nmkdir ai-go-tutorial cd ai-go-tutorial go mod init github.com/yourusername/ai-go-tutorial Install the OpenAI Go SDK:\ngo get github.com/sashabaranov/go-openai For Ollama, install the Ollama application first:\n# macOS brew install ollama # Linux curl -fsSL https://ollama.com/install.sh | sh # Windows - download from ollama.com Install the Ollama Go client:\ngo get github.com/ollama/ollama/api OpenAI API Setup Sign up at platform.openai.com and create an API key from the API keys section. Never commit API keys to version control.\nSet your API key as an environment variable:\nexport OPENAI_API_KEY=sk-your-api-key-here For production, use secrets management like AWS Secrets Manager, HashiCorp Vault, or environment variables in your deployment platform.\nOllama Setup Start the Ollama service:\nollama serve Download models you want to use:\n# Download Llama 2 (4GB) ollama pull llama2 # Download Mistral (4GB) ollama pull mistral # Download CodeLlama for code tasks ollama pull codellama # Download embedding model ollama pull nomic-embed-text Models download to ~/.ollama/models by default. Verify installation:\nollama list OpenAI Integration Basics Let\u0026rsquo;s start with the simplest possible integration - sending a prompt to GPT-4 and getting a response.\n// main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) func main() { apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) if apiKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;OPENAI_API_KEY environment variable not set\u0026#34;) } client := openai.NewClient(apiKey) resp, err := client.CreateChatCompletion( context.Background(), openai.ChatCompletionRequest{ Model: openai.GPT4TurboPreview, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: \u0026#34;Explain concurrency in Go in one paragraph\u0026#34;, }, }, }, ) if err != nil { log.Fatalf(\u0026#34;ChatCompletion error: %v\u0026#34;, err) } fmt.Println(resp.Choices[0].Message.Content) } Run this:\ngo run main.go You\u0026rsquo;ll get a detailed explanation of Go concurrency from GPT-4. The response comes back in seconds.\nUnderstanding the Request Structure Let\u0026rsquo;s break down what\u0026rsquo;s happening:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type ChatService struct { client *openai.Client } func NewChatService(apiKey string) *ChatService { return \u0026amp;ChatService{ client: openai.NewClient(apiKey), } } func (s *ChatService) Chat(ctx context.Context, model string, messages []openai.ChatCompletionMessage) (string, error) { resp, err := s.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: model, Messages: messages, Temperature: 0.7, MaxTokens: 1000, }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;chat completion failed: %w\u0026#34;, err) } if len(resp.Choices) == 0 { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;no response choices returned\u0026#34;) } return resp.Choices[0].Message.Content, nil } func main() { service := NewChatService(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) messages := []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleSystem, Content: \u0026#34;You are a helpful Go programming expert\u0026#34;, }, { Role: openai.ChatMessageRoleUser, Content: \u0026#34;How do channels work in Go?\u0026#34;, }, } response, err := service.Chat(context.Background(), openai.GPT4TurboPreview, messages) if err != nil { log.Fatalf(\u0026#34;Error: %v\u0026#34;, err) } fmt.Println(\u0026#34;AI Response:\u0026#34;) fmt.Println(response) } Model specifies which AI model to use. GPT-4 Turbo is the latest and best, GPT-3.5 Turbo is faster and cheaper.\nMessages is a conversation history. System messages set behavior, user messages are prompts, assistant messages are previous AI responses.\nTemperature controls randomness (0-2). Lower values (0.2) give focused, deterministic responses. Higher values (0.8) give creative, varied responses.\nMaxTokens limits response length. One token is roughly 4 characters in English. Set this to prevent unexpectedly long (expensive) responses.\nBuilding a Conversational Chat Real chat applications maintain conversation history:\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type ConversationManager struct { client *openai.Client messages []openai.ChatCompletionMessage model string } func NewConversationManager(apiKey string, systemPrompt string) *ConversationManager { messages := []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleSystem, Content: systemPrompt, }, } return \u0026amp;ConversationManager{ client: openai.NewClient(apiKey), messages: messages, model: openai.GPT4TurboPreview, } } func (cm *ConversationManager) SendMessage(ctx context.Context, userMessage string) (string, error) { // Add user message to history cm.messages = append(cm.messages, openai.ChatCompletionMessage{ Role: openai.ChatMessageRoleUser, Content: userMessage, }) // Get AI response resp, err := cm.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: cm.model, Messages: cm.messages, }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;completion error: %w\u0026#34;, err) } assistantMessage := resp.Choices[0].Message.Content // Add assistant response to history cm.messages = append(cm.messages, openai.ChatCompletionMessage{ Role: openai.ChatMessageRoleAssistant, Content: assistantMessage, }) return assistantMessage, nil } func (cm *ConversationManager) GetMessageCount() int { return len(cm.messages) } func main() { apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) if apiKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;OPENAI_API_KEY not set\u0026#34;) } conversation := NewConversationManager( apiKey, \u0026#34;You are a helpful assistant that explains Go programming concepts clearly.\u0026#34;, ) fmt.Println(\u0026#34;Chat with AI (type \u0026#39;quit\u0026#39; to exit)\u0026#34;) fmt.Println(\u0026#34;----------------------------------------\u0026#34;) scanner := bufio.NewScanner(os.Stdin) for { fmt.Print(\u0026#34;You: \u0026#34;) if !scanner.Scan() { break } userInput := strings.TrimSpace(scanner.Text()) if userInput == \u0026#34;\u0026#34; { continue } if userInput == \u0026#34;quit\u0026#34; { fmt.Println(\u0026#34;Goodbye!\u0026#34;) break } response, err := conversation.SendMessage(context.Background(), userInput) if err != nil { log.Printf(\u0026#34;Error: %v\u0026#34;, err) continue } fmt.Printf(\u0026#34;AI: %s\\n\\n\u0026#34;, response) } } This maintains full conversation context. The AI remembers what you discussed earlier, so follow-up questions work naturally. Watch token usage though - long conversations accumulate tokens quickly.\nStreaming Responses for Better UX Nobody wants to stare at a blank screen for 10 seconds waiting for a response. Streaming shows text as it generates, giving users immediate feedback.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) func streamChat(client *openai.Client, prompt string) error { ctx := context.Background() req := openai.ChatCompletionRequest{ Model: openai.GPT4TurboPreview, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: prompt, }, }, Stream: true, } stream, err := client.CreateChatCompletionStream(ctx, req) if err != nil { return fmt.Errorf(\u0026#34;stream creation failed: %w\u0026#34;, err) } defer stream.Close() fmt.Print(\u0026#34;AI: \u0026#34;) for { response, err := stream.Recv() if errors.Is(err, io.EOF) { fmt.Println(\u0026#34;\\n\u0026#34;) return nil } if err != nil { return fmt.Errorf(\u0026#34;stream error: %w\u0026#34;, err) } if len(response.Choices) \u0026gt; 0 { fmt.Print(response.Choices[0].Delta.Content) } } } func main() { client := openai.NewClient(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) err := streamChat(client, \u0026#34;Write a haiku about Go programming\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Error: %v\u0026#34;, err) } } Run this and you\u0026rsquo;ll see the response appear word by word, just like ChatGPT\u0026rsquo;s interface. Much better UX than waiting for everything.\nStreaming to Web Clients with SSE For web applications, use Server-Sent Events to push chunks to browsers:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) func streamHandler(w http.ResponseWriter, r *http.Request) { prompt := r.URL.Query().Get(\u0026#34;prompt\u0026#34;) if prompt == \u0026#34;\u0026#34; { http.Error(w, \u0026#34;prompt parameter required\u0026#34;, http.StatusBadRequest) return } // Set headers for SSE w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/event-stream\u0026#34;) w.Header().Set(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache\u0026#34;) w.Header().Set(\u0026#34;Connection\u0026#34;, \u0026#34;keep-alive\u0026#34;) client := openai.NewClient(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) req := openai.ChatCompletionRequest{ Model: openai.GPT35Turbo, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: prompt, }, }, Stream: true, } stream, err := client.CreateChatCompletionStream(context.Background(), req) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } defer stream.Close() flusher, ok := w.(http.Flusher) if !ok { http.Error(w, \u0026#34;Streaming unsupported\u0026#34;, http.StatusInternalServerError) return } for { response, err := stream.Recv() if errors.Is(err, io.EOF) { fmt.Fprintf(w, \u0026#34;data: [DONE]\\n\\n\u0026#34;) flusher.Flush() return } if err != nil { log.Printf(\u0026#34;Stream error: %v\u0026#34;, err) return } if len(response.Choices) \u0026gt; 0 { content := response.Choices[0].Delta.Content fmt.Fprintf(w, \u0026#34;data: %s\\n\\n\u0026#34;, content) flusher.Flush() } } } func main() { http.HandleFunc(\u0026#34;/stream\u0026#34;, streamHandler) fmt.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Test it:\ncurl \u0026#34;http://localhost:8080/stream?prompt=Explain%20goroutines\u0026#34; You\u0026rsquo;ll see chunks arrive in real-time. Frontend code can consume this with EventSource API.\nOllama Integration for Local AI Running models locally with Ollama gives you zero API costs and complete privacy. The API is similar to OpenAI\u0026rsquo;s.\nBasic Ollama Chat package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/ollama/ollama/api\u0026#34; ) func main() { client, err := api.ClientFromEnvironment() if err != nil { log.Fatal(err) } req := \u0026amp;api.ChatRequest{ Model: \u0026#34;llama2\u0026#34;, Messages: []api.Message{ { Role: \u0026#34;user\u0026#34;, Content: \u0026#34;Explain goroutines in one paragraph\u0026#34;, }, }, } ctx := context.Background() err = client.Chat(ctx, req, func(resp api.ChatResponse) error { fmt.Print(resp.Message.Content) return nil }) if err != nil { log.Fatal(err) } fmt.Println() } This uses Llama 2 running locally. First run will be slow as it loads the model into memory. Subsequent requests are faster.\nOllama Chat Service Create a reusable service:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/ollama/ollama/api\u0026#34; ) type OllamaService struct { client *api.Client model string } func NewOllamaService(model string) (*OllamaService, error) { client, err := api.ClientFromEnvironment() if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create client: %w\u0026#34;, err) } return \u0026amp;OllamaService{ client: client, model: model, }, nil } func (s *OllamaService) Chat(ctx context.Context, prompt string) (string, error) { var fullResponse string req := \u0026amp;api.ChatRequest{ Model: s.model, Messages: []api.Message{ { Role: \u0026#34;user\u0026#34;, Content: prompt, }, }, } err := s.client.Chat(ctx, req, func(resp api.ChatResponse) error { fullResponse += resp.Message.Content return nil }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;chat failed: %w\u0026#34;, err) } return fullResponse, nil } func (s *OllamaService) ChatWithHistory(ctx context.Context, messages []api.Message) (string, error) { var fullResponse string req := \u0026amp;api.ChatRequest{ Model: s.model, Messages: messages, } err := s.client.Chat(ctx, req, func(resp api.ChatResponse) error { fullResponse += resp.Message.Content return nil }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;chat failed: %w\u0026#34;, err) } return fullResponse, nil } func main() { service, err := NewOllamaService(\u0026#34;llama2\u0026#34;) if err != nil { log.Fatal(err) } response, err := service.Chat(context.Background(), \u0026#34;What is the main benefit of using Go?\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(response) } Comparing OpenAI and Ollama Performance Here\u0026rsquo;s a service that tries OpenAI first, falls back to Ollama:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/ollama/ollama/api\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type AIService struct { openaiClient *openai.Client ollamaClient *api.Client useOpenAI bool } func NewAIService(useOpenAI bool) (*AIService, error) { service := \u0026amp;AIService{ useOpenAI: useOpenAI, } if useOpenAI { apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) if apiKey == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;OPENAI_API_KEY not set\u0026#34;) } service.openaiClient = openai.NewClient(apiKey) } ollamaClient, err := api.ClientFromEnvironment() if err != nil { return nil, fmt.Errorf(\u0026#34;ollama client failed: %w\u0026#34;, err) } service.ollamaClient = ollamaClient return service, nil } func (s *AIService) Generate(ctx context.Context, prompt string) (string, time.Duration, error) { start := time.Now() if s.useOpenAI { response, err := s.generateWithOpenAI(ctx, prompt) return response, time.Since(start), err } response, err := s.generateWithOllama(ctx, prompt) return response, time.Since(start), err } func (s *AIService) generateWithOpenAI(ctx context.Context, prompt string) (string, error) { resp, err := s.openaiClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: openai.GPT35Turbo, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: prompt, }, }, }) if err != nil { return \u0026#34;\u0026#34;, err } return resp.Choices[0].Message.Content, nil } func (s *AIService) generateWithOllama(ctx context.Context, prompt string) (string, error) { var fullResponse string req := \u0026amp;api.ChatRequest{ Model: \u0026#34;llama2\u0026#34;, Messages: []api.Message{ { Role: \u0026#34;user\u0026#34;, Content: prompt, }, }, } err := s.ollamaClient.Chat(ctx, req, func(resp api.ChatResponse) error { fullResponse += resp.Message.Content return nil }) return fullResponse, err } func main() { prompt := \u0026#34;Write a function in Go that reverses a string\u0026#34; // Test OpenAI openaiService, _ := NewAIService(true) response, duration, err := openaiService.Generate(context.Background(), prompt) if err != nil { log.Printf(\u0026#34;OpenAI error: %v\u0026#34;, err) } else { fmt.Printf(\u0026#34;OpenAI (%v):\\n%s\\n\\n\u0026#34;, duration, response) } // Test Ollama ollamaService, _ := NewAIService(false) response, duration, err = ollamaService.Generate(context.Background(), prompt) if err != nil { log.Printf(\u0026#34;Ollama error: %v\u0026#34;, err) } else { fmt.Printf(\u0026#34;Ollama (%v):\\n%s\\n\\n\u0026#34;, duration, response) } } This lets you compare response quality and speed between providers.\nBuilding a RAG System with Embeddings RAG (Retrieval-Augmented Generation) lets AI answer questions using your specific documents. Instead of hallucinating, the AI pulls relevant info from your knowledge base.\nThe process:\nSplit documents into chunks Generate embeddings (vector representations) for each chunk Store embeddings in a vector database When user asks a question, find similar chunks Include relevant chunks in the AI prompt AI answers based on provided context Generating Embeddings with OpenAI package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type EmbeddingService struct { client *openai.Client } func NewEmbeddingService(apiKey string) *EmbeddingService { return \u0026amp;EmbeddingService{ client: openai.NewClient(apiKey), } } func (s *EmbeddingService) GenerateEmbedding(ctx context.Context, text string) ([]float32, error) { req := openai.EmbeddingRequest{ Input: []string{text}, Model: openai.AdaEmbeddingV2, } resp, err := s.client.CreateEmbeddings(ctx, req) if err != nil { return nil, fmt.Errorf(\u0026#34;embedding generation failed: %w\u0026#34;, err) } if len(resp.Data) == 0 { return nil, fmt.Errorf(\u0026#34;no embeddings returned\u0026#34;) } return resp.Data[0].Embedding, nil } func (s *EmbeddingService) GenerateBatchEmbeddings(ctx context.Context, texts []string) ([][]float32, error) { req := openai.EmbeddingRequest{ Input: texts, Model: openai.AdaEmbeddingV2, } resp, err := s.client.CreateEmbeddings(ctx, req) if err != nil { return nil, fmt.Errorf(\u0026#34;batch embedding failed: %w\u0026#34;, err) } embeddings := make([][]float32, len(resp.Data)) for i, data := range resp.Data { embeddings[i] = data.Embedding } return embeddings, nil } func main() { service := NewEmbeddingService(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) texts := []string{ \u0026#34;Go is a statically typed, compiled programming language\u0026#34;, \u0026#34;Goroutines are lightweight threads managed by the Go runtime\u0026#34;, \u0026#34;Channels are the pipes that connect concurrent goroutines\u0026#34;, } embeddings, err := service.GenerateBatchEmbeddings(context.Background(), texts) if err != nil { log.Fatal(err) } for i, emb := range embeddings { fmt.Printf(\u0026#34;Text %d: %d dimensions\\n\u0026#34;, i+1, len(emb)) fmt.Printf(\u0026#34;First 5 values: %v\\n\\n\u0026#34;, emb[:5]) } } Embeddings are 1536-dimensional vectors representing semantic meaning. Similar texts have similar vectors.\nSimple In-Memory Vector Search Before using a real vector database, here\u0026rsquo;s a simple similarity search:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;math\u0026#34; \u0026#34;os\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type Document struct { Text string Embedding []float32 } type VectorStore struct { documents []Document embedSvc *EmbeddingService } func NewVectorStore(embedSvc *EmbeddingService) *VectorStore { return \u0026amp;VectorStore{ documents: make([]Document, 0), embedSvc: embedSvc, } } func (vs *VectorStore) AddDocument(ctx context.Context, text string) error { embedding, err := vs.embedSvc.GenerateEmbedding(ctx, text) if err != nil { return err } vs.documents = append(vs.documents, Document{ Text: text, Embedding: embedding, }) return nil } func cosineSimilarity(a, b []float32) float32 { var dotProduct, magA, magB float32 for i := range a { dotProduct += a[i] * b[i] magA += a[i] * a[i] magB += b[i] * b[i] } if magA == 0 || magB == 0 { return 0 } return dotProduct / (float32(math.Sqrt(float64(magA))) * float32(math.Sqrt(float64(magB)))) } func (vs *VectorStore) Search(ctx context.Context, query string, topK int) ([]Document, error) { queryEmbedding, err := vs.embedSvc.GenerateEmbedding(ctx, query) if err != nil { return nil, err } type scored struct { doc Document score float32 } scores := make([]scored, len(vs.documents)) for i, doc := range vs.documents { similarity := cosineSimilarity(queryEmbedding, doc.Embedding) scores[i] = scored{doc: doc, score: similarity} } sort.Slice(scores, func(i, j int) bool { return scores[i].score \u0026gt; scores[j].score }) if topK \u0026gt; len(scores) { topK = len(scores) } results := make([]Document, topK) for i := 0; i \u0026lt; topK; i++ { results[i] = scores[i].doc } return results, nil } func main() { embedSvc := NewEmbeddingService(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) vectorStore := NewVectorStore(embedSvc) ctx := context.Background() // Add documents documents := []string{ \u0026#34;Go was created at Google by Robert Griesemer, Rob Pike, and Ken Thompson\u0026#34;, \u0026#34;Goroutines are functions that run concurrently with other functions\u0026#34;, \u0026#34;Go\u0026#39;s garbage collector uses a concurrent mark-and-sweep algorithm\u0026#34;, \u0026#34;Channels provide a way for goroutines to communicate with each other\u0026#34;, \u0026#34;The Go compiler produces statically linked binaries with no external dependencies\u0026#34;, } fmt.Println(\u0026#34;Adding documents to vector store...\u0026#34;) for _, doc := range documents { if err := vectorStore.AddDocument(ctx, doc); err != nil { log.Fatal(err) } } // Search query := \u0026#34;How do goroutines communicate?\u0026#34; results, err := vectorStore.Search(ctx, query, 2) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;\\nQuery: %s\\n\u0026#34;, query) fmt.Println(\u0026#34;\\nTop results:\u0026#34;) for i, result := range results { fmt.Printf(\u0026#34;%d. %s\\n\u0026#34;, i+1, result.Text) } } This finds the most relevant documents for a query. In production, use a proper vector database like Qdrant, Milvus, or Pinecone.\nComplete RAG Implementation Combine retrieval with generation:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type RAGSystem struct { vectorStore *VectorStore chatClient *openai.Client } func NewRAGSystem(apiKey string) (*RAGSystem, error) { embedSvc := NewEmbeddingService(apiKey) vectorStore := NewVectorStore(embedSvc) return \u0026amp;RAGSystem{ vectorStore: vectorStore, chatClient: openai.NewClient(apiKey), }, nil } func (rag *RAGSystem) AddKnowledge(ctx context.Context, texts []string) error { for _, text := range texts { if err := rag.vectorStore.AddDocument(ctx, text); err != nil { return fmt.Errorf(\u0026#34;failed to add document: %w\u0026#34;, err) } } return nil } func (rag *RAGSystem) Ask(ctx context.Context, question string) (string, error) { // Retrieve relevant documents relevantDocs, err := rag.vectorStore.Search(ctx, question, 3) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;search failed: %w\u0026#34;, err) } // Build context from retrieved documents var contextParts []string for _, doc := range relevantDocs { contextParts = append(contextParts, doc.Text) } context := strings.Join(contextParts, \u0026#34;\\n\\n\u0026#34;) // Create prompt with context prompt := fmt.Sprintf(`Answer the question based on the following context. If the answer is not in the context, say \u0026#34;I don\u0026#39;t have enough information to answer that.\u0026#34; Context: %s Question: %s Answer:`, context, question) // Get AI response resp, err := rag.chatClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: openai.GPT4TurboPreview, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: prompt, }, }, Temperature: 0.3, }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;chat completion failed: %w\u0026#34;, err) } return resp.Choices[0].Message.Content, nil } func main() { apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;) if apiKey == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;OPENAI_API_KEY not set\u0026#34;) } rag, err := NewRAGSystem(apiKey) if err != nil { log.Fatal(err) } ctx := context.Background() // Add knowledge base knowledge := []string{ \u0026#34;BuanaCoding is a programming tutorial website focused on Go, PHP, and JavaScript.\u0026#34;, \u0026#34;The site was created by Wiku Karno in 2024.\u0026#34;, \u0026#34;BuanaCoding provides tutorials on REST APIs, authentication, databases, and deployment.\u0026#34;, \u0026#34;All tutorials include complete code examples and production best practices.\u0026#34;, \u0026#34;The site uses Hugo static site generator and is deployed on GitHub Pages.\u0026#34;, } fmt.Println(\u0026#34;Building knowledge base...\u0026#34;) if err := rag.AddKnowledge(ctx, knowledge); err != nil { log.Fatal(err) } // Ask questions questions := []string{ \u0026#34;Who created BuanaCoding?\u0026#34;, \u0026#34;What programming languages does BuanaCoding cover?\u0026#34;, \u0026#34;What is the capital of France?\u0026#34;, // Not in knowledge base } for _, question := range questions { fmt.Printf(\u0026#34;\\nQuestion: %s\\n\u0026#34;, question) answer, err := rag.Ask(ctx, question) if err != nil { log.Printf(\u0026#34;Error: %v\u0026#34;, err) continue } fmt.Printf(\u0026#34;Answer: %s\\n\u0026#34;, answer) } } The RAG system retrieves relevant context before answering. This prevents hallucination and grounds answers in your actual documents.\nProduction Considerations Development demos are fun, but production AI apps need proper error handling, rate limiting, monitoring, and cost control.\nRate Limiting and Retry Logic package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type RateLimitedClient struct { client *openai.Client maxRetries int baseDelay time.Duration } func NewRateLimitedClient(apiKey string) *RateLimitedClient { return \u0026amp;RateLimitedClient{ client: openai.NewClient(apiKey), maxRetries: 3, baseDelay: time.Second, } } func (rlc *RateLimitedClient) ChatWithRetry(ctx context.Context, req openai.ChatCompletionRequest) (*openai.ChatCompletionResponse, error) { var lastErr error for attempt := 0; attempt \u0026lt; rlc.maxRetries; attempt++ { resp, err := rlc.client.CreateChatCompletion(ctx, req) if err == nil { return \u0026amp;resp, nil } lastErr = err // Check if it\u0026#39;s a rate limit error if isRateLimitError(err) { delay := rlc.baseDelay * time.Duration(1\u0026lt;\u0026lt;uint(attempt)) fmt.Printf(\u0026#34;Rate limited, retrying in %v...\\n\u0026#34;, delay) time.Sleep(delay) continue } // For other errors, don\u0026#39;t retry return nil, err } return nil, fmt.Errorf(\u0026#34;max retries exceeded: %w\u0026#34;, lastErr) } func isRateLimitError(err error) bool { // Check error message for rate limit indicators errMsg := err.Error() return contains(errMsg, \u0026#34;rate limit\u0026#34;) || contains(errMsg, \u0026#34;429\u0026#34;) } func contains(s, substr string) bool { return len(s) \u0026gt;= len(substr) \u0026amp;\u0026amp; (s == substr || len(s) \u0026gt; len(substr) \u0026amp;\u0026amp; (s[:len(substr)] == substr || s[len(s)-len(substr):] == substr || containsInner(s, substr))) } func containsInner(s, substr string) bool { for i := 0; i \u0026lt;= len(s)-len(substr); i++ { if s[i:i+len(substr)] == substr { return true } } return false } func main() { client := NewRateLimitedClient(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;)) req := openai.ChatCompletionRequest{ Model: openai.GPT35Turbo, Messages: []openai.ChatCompletionMessage{ { Role: openai.ChatMessageRoleUser, Content: \u0026#34;Hello!\u0026#34;, }, }, } resp, err := client.ChatWithRetry(context.Background(), req) if err != nil { log.Fatalf(\u0026#34;Request failed: %v\u0026#34;, err) } fmt.Println(resp.Choices[0].Message.Content) } Cost Tracking and Token Counting package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type CostTracker struct { client *openai.Client totalTokens int totalCost float64 mu sync.Mutex inputPricePerK float64 outputPricePerK float64 } func NewCostTracker(apiKey string, model string) *CostTracker { ct := \u0026amp;CostTracker{ client: openai.NewClient(apiKey), } // Set prices based on model switch model { case openai.GPT4TurboPreview: ct.inputPricePerK = 0.01 ct.outputPricePerK = 0.03 case openai.GPT35Turbo: ct.inputPricePerK = 0.0005 ct.outputPricePerK = 0.0015 default: ct.inputPricePerK = 0.001 ct.outputPricePerK = 0.002 } return ct } func (ct *CostTracker) Chat(ctx context.Context, req openai.ChatCompletionRequest) (string, error) { resp, err := ct.client.CreateChatCompletion(ctx, req) if err != nil { return \u0026#34;\u0026#34;, err } // Track usage ct.mu.Lock() inputTokens := resp.Usage.PromptTokens outputTokens := resp.Usage.CompletionTokens ct.totalTokens += resp.Usage.TotalTokens inputCost := float64(inputTokens) / 1000.0 * ct.inputPricePerK outputCost := float64(outputTokens) / 1000.0 * ct.outputPricePerK ct.totalCost += inputCost + outputCost ct.mu.Unlock() return resp.Choices[0].Message.Content, nil } func (ct *CostTracker) GetStats() (int, float64) { ct.mu.Lock() defer ct.mu.Unlock() return ct.totalTokens, ct.totalCost } func main() { tracker := NewCostTracker(os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;), openai.GPT4TurboPreview) questions := []string{ \u0026#34;What is Go?\u0026#34;, \u0026#34;Explain goroutines\u0026#34;, \u0026#34;How do channels work?\u0026#34;, } for _, q := range questions { req := openai.ChatCompletionRequest{ Model: openai.GPT4TurboPreview, Messages: []openai.ChatCompletionMessage{ {Role: openai.ChatMessageRoleUser, Content: q}, }, } answer, err := tracker.Chat(context.Background(), req) if err != nil { log.Printf(\u0026#34;Error: %v\u0026#34;, err) continue } fmt.Printf(\u0026#34;Q: %s\\nA: %s\\n\\n\u0026#34;, q, answer) } tokens, cost := tracker.GetStats() fmt.Printf(\u0026#34;Total tokens used: %d\\n\u0026#34;, tokens) fmt.Printf(\u0026#34;Total cost: $%.4f\\n\u0026#34;, cost) } This tracks API costs in real-time, essential for production applications with budgets.\nError Handling and Graceful Degradation package main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/ollama/ollama/api\u0026#34; \u0026#34;github.com/sashabaranov/go-openai\u0026#34; ) type ResilientAI struct { openaiClient *openai.Client ollamaClient *api.Client useOpenAI bool timeout time.Duration } func NewResilientAI() (*ResilientAI, error) { rai := \u0026amp;ResilientAI{ timeout: 30 * time.Second, } // Try OpenAI first if apiKey := os.Getenv(\u0026#34;OPENAI_API_KEY\u0026#34;); apiKey != \u0026#34;\u0026#34; { rai.openaiClient = openai.NewClient(apiKey) rai.useOpenAI = true } // Set up Ollama as fallback ollamaClient, err := api.ClientFromEnvironment() if err == nil { rai.ollamaClient = ollamaClient } if rai.openaiClient == nil \u0026amp;\u0026amp; rai.ollamaClient == nil { return nil, errors.New(\u0026#34;no AI backends available\u0026#34;) } return rai, nil } func (rai *ResilientAI) Generate(ctx context.Context, prompt string) (string, error) { ctx, cancel := context.WithTimeout(ctx, rai.timeout) defer cancel() // Try OpenAI first if available if rai.useOpenAI \u0026amp;\u0026amp; rai.openaiClient != nil { resp, err := rai.tryOpenAI(ctx, prompt) if err == nil { return resp, nil } log.Printf(\u0026#34;OpenAI failed: %v, falling back to Ollama\u0026#34;, err) } // Fall back to Ollama if rai.ollamaClient != nil { resp, err := rai.tryOllama(ctx, prompt) if err == nil { return resp, nil } log.Printf(\u0026#34;Ollama failed: %v\u0026#34;, err) } return \u0026#34;\u0026#34;, errors.New(\u0026#34;all AI backends failed\u0026#34;) } func (rai *ResilientAI) tryOpenAI(ctx context.Context, prompt string) (string, error) { resp, err := rai.openaiClient.CreateChatCompletion(ctx, openai.ChatCompletionRequest{ Model: openai.GPT35Turbo, Messages: []openai.ChatCompletionMessage{ {Role: openai.ChatMessageRoleUser, Content: prompt}, }, }) if err != nil { return \u0026#34;\u0026#34;, err } return resp.Choices[0].Message.Content, nil } func (rai *ResilientAI) tryOllama(ctx context.Context, prompt string) (string, error) { var response string req := \u0026amp;api.ChatRequest{ Model: \u0026#34;llama2\u0026#34;, Messages: []api.Message{ {Role: \u0026#34;user\u0026#34;, Content: prompt}, }, } err := rai.ollamaClient.Chat(ctx, req, func(resp api.ChatResponse) error { response += resp.Message.Content return nil }) return response, err } func main() { ai, err := NewResilientAI() if err != nil { log.Fatal(err) } response, err := ai.Generate(context.Background(), \u0026#34;Explain Go interfaces\u0026#34;) if err != nil { log.Fatalf(\u0026#34;All backends failed: %v\u0026#34;, err) } fmt.Println(response) } This automatically falls back from OpenAI to Ollama if the cloud service is down or rate-limited.\nWrapping Up Building AI features in Go is straightforward once you understand the APIs. OpenAI gives you best-in-class quality through simple HTTP calls. Ollama lets you run models locally for privacy and cost savings. Both integrate cleanly into Go applications.\nThe patterns covered here - streaming responses, conversation management, RAG systems, error handling - apply to most AI features you\u0026rsquo;ll build. Start simple with basic completions, add streaming for better UX, implement RAG when you need document-grounded answers, and layer in production concerns like rate limiting and cost tracking.\nFor production applications, use OpenAI for quality and reliability, implement proper error handling and retries, track token usage and costs, consider Ollama for high-volume or privacy-sensitive workloads, and monitor AI performance and accuracy over time.\nTo build complete AI-powered applications, combine these patterns with other Go features. Use Redis for caching AI responses to reduce API costs, implement rate limiting to prevent API abuse, add JWT authentication to secure your AI endpoints, and use WebSockets for real-time streaming chat interfaces. For storing conversation history and user data, check out our guides on working with PostgreSQL or MongoDB .\nThe AI landscape moves fast, but the fundamentals stay stable. Understanding embeddings, prompts, and API patterns gives you a foundation that works regardless of which models you use. Build iteratively, measure results, and keep improving based on real user feedback.\n","href":"/2025/10/how-to-build-ai-llm-applications-in-go-openai-and-ollama-integration.html","title":"How to Build AI/LLM Applications in Go - OpenAI and Ollama Integration"},{"content":"File uploads seem simple until you deploy to production. Users upload 500MB videos that crash your server. Someone uploads a PHP file disguised as an image and compromises your system. Filenames with path traversal characters like ../../etc/passwd expose sensitive data. What started as a basic feature becomes a security nightmare.\nThis guide demonstrates how to handle file uploads securely in Go applications. You\u0026rsquo;ll learn to parse multipart form data correctly, validate file types using magic number detection, enforce size limits that protect server resources, sanitize filenames to prevent attacks, store files securely with proper permissions, and implement production-ready patterns that scale.\nUnderstanding Multipart Form Data HTTP file uploads use multipart/form-data encoding. This format splits the request body into parts, each containing a file or form field. The browser sets the Content-Type header to multipart/form-data; boundary=----WebKitFormBoundary... where the boundary separates parts.\nGo\u0026rsquo;s net/http package provides methods to parse multipart forms automatically. The ParseMultipartForm method reads the request body, splits it by boundaries, and makes files accessible through FormFile or MultipartForm.\nUnderstanding limits is critical. ParseMultipartForm accepts a maxMemory parameter. Files smaller than this stay in memory. Larger files spill to temporary disk storage. Set this based on expected file sizes and available memory.\nRequest timeouts prevent slow uploads from tying up resources. Configure http.Server with appropriate ReadTimeout and WriteTimeout values. For large files, use longer timeouts but implement progress tracking to detect stalled uploads.\nBasic File Upload Handler Create a simple file upload endpoint that receives and saves files.\n// main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; ) func uploadHandler(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } const maxUploadSize = 10 \u0026lt;\u0026lt; 20 // 10 MB r.Body = http.MaxBytesReader(w, r.Body, maxUploadSize) if err := r.ParseMultipartForm(maxUploadSize); err != nil { http.Error(w, \u0026#34;File too large\u0026#34;, http.StatusBadRequest) return } file, header, err := r.FormFile(\u0026#34;file\u0026#34;) if err != nil { http.Error(w, \u0026#34;Error retrieving file\u0026#34;, http.StatusBadRequest) return } defer file.Close() uploadDir := \u0026#34;./uploads\u0026#34; if err := os.MkdirAll(uploadDir, 0755); err != nil { http.Error(w, \u0026#34;Failed to create upload directory\u0026#34;, http.StatusInternalServerError) return } dst, err := os.Create(filepath.Join(uploadDir, header.Filename)) if err != nil { http.Error(w, \u0026#34;Failed to create file\u0026#34;, http.StatusInternalServerError) return } defer dst.Close() if _, err := io.Copy(dst, file); err != nil { http.Error(w, \u0026#34;Failed to save file\u0026#34;, http.StatusInternalServerError) return } fmt.Fprintf(w, \u0026#34;File uploaded successfully: %s\\n\u0026#34;, header.Filename) } func main() { http.HandleFunc(\u0026#34;/upload\u0026#34;, uploadHandler) log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatal(err) } } This basic handler uses MaxBytesReader to enforce size limits before parsing, creates the uploads directory if missing, and streams the file to disk using io.Copy.\nTest with curl:\ncurl -F \u0026#34;file=@image.jpg\u0026#34; http://localhost:8080/upload Implementing File Type Validation Never trust the Content-Type header or file extension. Attackers easily manipulate these. Validate file types by reading magic numbers (file signatures).\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) var allowedMimeTypes = map[string]bool{ \u0026#34;image/jpeg\u0026#34;: true, \u0026#34;image/png\u0026#34;: true, \u0026#34;image/gif\u0026#34;: true, \u0026#34;image/webp\u0026#34;: true, \u0026#34;application/pdf\u0026#34;: true, } func detectContentType(file []byte) string { return http.DetectContentType(file) } func validateFileType(file []byte) error { contentType := detectContentType(file) if !allowedMimeTypes[contentType] { return fmt.Errorf(\u0026#34;file type not allowed: %s\u0026#34;, contentType) } return nil } func uploadHandlerWithValidation(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } const maxUploadSize = 10 \u0026lt;\u0026lt; 20 r.Body = http.MaxBytesReader(w, r.Body, maxUploadSize) if err := r.ParseMultipartForm(maxUploadSize); err != nil { http.Error(w, \u0026#34;File too large\u0026#34;, http.StatusBadRequest) return } file, header, err := r.FormFile(\u0026#34;file\u0026#34;) if err != nil { http.Error(w, \u0026#34;Error retrieving file\u0026#34;, http.StatusBadRequest) return } defer file.Close() buffer := make([]byte, 512) _, err = file.Read(buffer) if err != nil { http.Error(w, \u0026#34;Error reading file\u0026#34;, http.StatusBadRequest) return } if err := validateFileType(buffer); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } file.Seek(0, 0) uploadDir := \u0026#34;./uploads\u0026#34; os.MkdirAll(uploadDir, 0755) dst, err := os.Create(filepath.Join(uploadDir, header.Filename)) if err != nil { http.Error(w, \u0026#34;Failed to create file\u0026#34;, http.StatusInternalServerError) return } defer dst.Close() if _, err := io.Copy(dst, file); err != nil { http.Error(w, \u0026#34;Failed to save file\u0026#34;, http.StatusInternalServerError) return } fmt.Fprintf(w, \u0026#34;File uploaded successfully: %s\\n\u0026#34;, header.Filename) } The code reads the first 512 bytes to detect content type using http.DetectContentType, validates against whitelist, then seeks back to position 0 to copy the full file.\nSanitizing Filenames User-provided filenames can contain path traversal attempts or special characters that cause issues.\npackage main import ( \u0026#34;path/filepath\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;strings\u0026#34; ) var filenameRegex = regexp.MustCompile(`[^a-zA-Z0-9._-]`) func sanitizeFilename(filename string) string { filename = filepath.Base(filename) filename = filenameRegex.ReplaceAllString(filename, \u0026#34;_\u0026#34;) filename = strings.Trim(filename, \u0026#34;._-\u0026#34;) if filename == \u0026#34;\u0026#34; { filename = \u0026#34;unnamed\u0026#34; } return filename } filepath.Base removes directory components, the regex replaces unsafe characters, and trimming removes leading/trailing special chars.\nUse sanitized filenames:\nsafeFilename := sanitizeFilename(header.Filename) dst, err := os.Create(filepath.Join(uploadDir, safeFilename)) Generating Unique Filenames Using original filenames causes collisions. Generate unique names while preserving extensions.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; ) func generateUniqueFilename(originalFilename string) string { ext := filepath.Ext(originalFilename) uniqueID := uuid.New().String() timestamp := time.Now().Unix() return fmt.Sprintf(\u0026#34;%d_%s%s\u0026#34;, timestamp, uniqueID, ext) } func uploadHandlerWithUniqueNames(w http.ResponseWriter, r *http.Request) { // ... previous validation code ... originalFilename := sanitizeFilename(header.Filename) uniqueFilename := generateUniqueFilename(originalFilename) dst, err := os.Create(filepath.Join(uploadDir, uniqueFilename)) if err != nil { http.Error(w, \u0026#34;Failed to create file\u0026#34;, http.StatusInternalServerError) return } defer dst.Close() if _, err := io.Copy(dst, file); err != nil { http.Error(w, \u0026#34;Failed to save file\u0026#34;, http.StatusInternalServerError) return } fmt.Fprintf(w, \u0026#34;File uploaded: %s (saved as: %s)\\n\u0026#34;, originalFilename, uniqueFilename) } Install UUID library:\ngo get github.com/google/uuid Handling Multiple File Uploads Accept multiple files in a single request.\nfunc multipleUploadHandler(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } const maxUploadSize = 50 \u0026lt;\u0026lt; 20 // 50 MB total r.Body = http.MaxBytesReader(w, r.Body, maxUploadSize) if err := r.ParseMultipartForm(maxUploadSize); err != nil { http.Error(w, \u0026#34;Request too large\u0026#34;, http.StatusBadRequest) return } files := r.MultipartForm.File[\u0026#34;files\u0026#34;] if len(files) == 0 { http.Error(w, \u0026#34;No files uploaded\u0026#34;, http.StatusBadRequest) return } uploadDir := \u0026#34;./uploads\u0026#34; os.MkdirAll(uploadDir, 0755) uploadedFiles := []string{} for _, fileHeader := range files { file, err := fileHeader.Open() if err != nil { http.Error(w, \u0026#34;Error opening file\u0026#34;, http.StatusBadRequest) return } defer file.Close() buffer := make([]byte, 512) if _, err := file.Read(buffer); err != nil { http.Error(w, \u0026#34;Error reading file\u0026#34;, http.StatusBadRequest) return } if err := validateFileType(buffer); err != nil { http.Error(w, fmt.Sprintf(\u0026#34;Invalid file type: %s\u0026#34;, err), http.StatusBadRequest) return } file.Seek(0, 0) uniqueFilename := generateUniqueFilename(fileHeader.Filename) dst, err := os.Create(filepath.Join(uploadDir, uniqueFilename)) if err != nil { http.Error(w, \u0026#34;Failed to create file\u0026#34;, http.StatusInternalServerError) return } defer dst.Close() if _, err := io.Copy(dst, file); err != nil { http.Error(w, \u0026#34;Failed to save file\u0026#34;, http.StatusInternalServerError) return } uploadedFiles = append(uploadedFiles, uniqueFilename) } fmt.Fprintf(w, \u0026#34;Uploaded %d files: %v\\n\u0026#34;, len(uploadedFiles), uploadedFiles) } Test with multiple files:\ncurl -F \u0026#34;files=@image1.jpg\u0026#34; -F \u0026#34;files=@image2.png\u0026#34; http://localhost:8080/upload-multiple Validating Image Dimensions Prevent decompression bombs and enforce size limits for images.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;image\u0026#34; _ \u0026#34;image/gif\u0026#34; _ \u0026#34;image/jpeg\u0026#34; _ \u0026#34;image/png\u0026#34; \u0026#34;io\u0026#34; ) const ( maxImageWidth = 4096 maxImageHeight = 4096 ) func validateImageDimensions(file io.Reader) error { config, _, err := image.DecodeConfig(file) if err != nil { return fmt.Errorf(\u0026#34;invalid image: %w\u0026#34;, err) } if config.Width \u0026gt; maxImageWidth || config.Height \u0026gt; maxImageHeight { return fmt.Errorf(\u0026#34;image dimensions too large: %dx%d (max: %dx%d)\u0026#34;, config.Width, config.Height, maxImageWidth, maxImageHeight) } if config.Width \u0026lt; 1 || config.Height \u0026lt; 1 { return fmt.Errorf(\u0026#34;invalid image dimensions\u0026#34;) } return nil } func imageUploadHandler(w http.ResponseWriter, r *http.Request) { // ... previous validation code ... if err := validateFileType(buffer); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } file.Seek(0, 0) if err := validateImageDimensions(file); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } file.Seek(0, 0) // ... save file ... } Implementing File Upload Service Create a reusable service for file uploads with proper structure.\n// internal/upload/service.go package upload import ( \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;mime/multipart\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; ) type UploadConfig struct { MaxFileSize int64 AllowedMimeTypes map[string]bool UploadDir string MaxImageWidth int MaxImageHeight int } type FileInfo struct { OriginalName string SavedName string Size int64 ContentType string UploadedAt time.Time } type UploadService struct { config UploadConfig } func NewUploadService(config UploadConfig) *UploadService { return \u0026amp;UploadService{config: config} } func (s *UploadService) ValidateAndSaveFile(fileHeader *multipart.FileHeader) (*FileInfo, error) { file, err := fileHeader.Open() if err != nil { return nil, fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() if fileHeader.Size \u0026gt; s.config.MaxFileSize { return nil, fmt.Errorf(\u0026#34;file size exceeds limit\u0026#34;) } buffer := make([]byte, 512) if _, err := file.Read(buffer); err != nil { return nil, fmt.Errorf(\u0026#34;failed to read file: %w\u0026#34;, err) } contentType := http.DetectContentType(buffer) if !s.config.AllowedMimeTypes[contentType] { return nil, fmt.Errorf(\u0026#34;file type not allowed: %s\u0026#34;, contentType) } file.Seek(0, 0) if err := os.MkdirAll(s.config.UploadDir, 0755); err != nil { return nil, fmt.Errorf(\u0026#34;failed to create upload directory: %w\u0026#34;, err) } ext := filepath.Ext(fileHeader.Filename) savedName := fmt.Sprintf(\u0026#34;%d_%s%s\u0026#34;, time.Now().Unix(), uuid.New().String(), ext) filePath := filepath.Join(s.config.UploadDir, savedName) dst, err := os.Create(filePath) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create file: %w\u0026#34;, err) } defer dst.Close() size, err := io.Copy(dst, file) if err != nil { os.Remove(filePath) return nil, fmt.Errorf(\u0026#34;failed to save file: %w\u0026#34;, err) } return \u0026amp;FileInfo{ OriginalName: fileHeader.Filename, SavedName: savedName, Size: size, ContentType: contentType, UploadedAt: time.Now(), }, nil } func (s *UploadService) DeleteFile(filename string) error { filePath := filepath.Join(s.config.UploadDir, filename) if !filepath.HasPrefix(filePath, s.config.UploadDir) { return fmt.Errorf(\u0026#34;invalid file path\u0026#34;) } return os.Remove(filePath) } Use the service in handlers:\n// main.go package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;yourapp/internal/upload\u0026#34; ) func main() { uploadService := upload.NewUploadService(upload.UploadConfig{ MaxFileSize: 10 \u0026lt;\u0026lt; 20, AllowedMimeTypes: map[string]bool{ \u0026#34;image/jpeg\u0026#34;: true, \u0026#34;image/png\u0026#34;: true, \u0026#34;image/gif\u0026#34;: true, }, UploadDir: \u0026#34;./uploads\u0026#34;, MaxImageWidth: 4096, MaxImageHeight: 4096, }) http.HandleFunc(\u0026#34;/upload\u0026#34;, func(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } r.Body = http.MaxBytesReader(w, r.Body, uploadService.config.MaxFileSize) if err := r.ParseMultipartForm(uploadService.config.MaxFileSize); err != nil { http.Error(w, \u0026#34;File too large\u0026#34;, http.StatusBadRequest) return } file, header, err := r.FormFile(\u0026#34;file\u0026#34;) if err != nil { http.Error(w, \u0026#34;Error retrieving file\u0026#34;, http.StatusBadRequest) return } file.Close() fileInfo, err := uploadService.ValidateAndSaveFile(header) if err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(fileInfo) }) log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Storing File Metadata in Database Track uploaded files in a database for management and access control.\n// internal/models/file.go package models import ( \u0026#34;time\u0026#34; ) type UploadedFile struct { ID int64 `json:\u0026#34;id\u0026#34;` UserID int64 `json:\u0026#34;user_id\u0026#34;` OriginalName string `json:\u0026#34;original_name\u0026#34;` SavedName string `json:\u0026#34;saved_name\u0026#34;` FilePath string `json:\u0026#34;file_path\u0026#34;` FileSize int64 `json:\u0026#34;file_size\u0026#34;` ContentType string `json:\u0026#34;content_type\u0026#34;` UploadedAt time.Time `json:\u0026#34;uploaded_at\u0026#34;` } // internal/repository/file_repository.go package repository import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;yourapp/internal/models\u0026#34; ) type FileRepository struct { db *sql.DB } func NewFileRepository(db *sql.DB) *FileRepository { return \u0026amp;FileRepository{db: db} } func (r *FileRepository) Create(ctx context.Context, file *models.UploadedFile) error { query := ` INSERT INTO uploaded_files (user_id, original_name, saved_name, file_path, file_size, content_type, uploaded_at) VALUES (?, ?, ?, ?, ?, ?, ?) ` result, err := r.db.ExecContext(ctx, query, file.UserID, file.OriginalName, file.SavedName, file.FilePath, file.FileSize, file.ContentType, file.UploadedAt, ) if err != nil { return err } id, err := result.LastInsertId() if err != nil { return err } file.ID = id return nil } func (r *FileRepository) FindByUserID(ctx context.Context, userID int64) ([]*models.UploadedFile, error) { query := ` SELECT id, user_id, original_name, saved_name, file_path, file_size, content_type, uploaded_at FROM uploaded_files WHERE user_id = ? ORDER BY uploaded_at DESC ` rows, err := r.db.QueryContext(ctx, query, userID) if err != nil { return nil, err } defer rows.Close() var files []*models.UploadedFile for rows.Next() { var file models.UploadedFile err := rows.Scan( \u0026amp;file.ID, \u0026amp;file.UserID, \u0026amp;file.OriginalName, \u0026amp;file.SavedName, \u0026amp;file.FilePath, \u0026amp;file.FileSize, \u0026amp;file.ContentType, \u0026amp;file.UploadedAt, ) if err != nil { return nil, err } files = append(files, \u0026amp;file) } return files, rows.Err() } func (r *FileRepository) Delete(ctx context.Context, id, userID int64) error { query := `DELETE FROM uploaded_files WHERE id = ? AND user_id = ?` _, err := r.db.ExecContext(ctx, query, id, userID) return err } Database schema:\nCREATE TABLE uploaded_files ( id BIGINT AUTO_INCREMENT PRIMARY KEY, user_id BIGINT NOT NULL, original_name VARCHAR(255) NOT NULL, saved_name VARCHAR(255) NOT NULL UNIQUE, file_path VARCHAR(512) NOT NULL, file_size BIGINT NOT NULL, content_type VARCHAR(100) NOT NULL, uploaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, INDEX idx_user_id (user_id), INDEX idx_uploaded_at (uploaded_at) ); Serving Uploaded Files Securely Never serve files directly from the upload directory. Implement access control and content disposition headers.\nfunc downloadHandler(w http.ResponseWriter, r *http.Request) { fileID := r.URL.Query().Get(\u0026#34;id\u0026#34;) userID := getUserIDFromSession(r) // Implement based on your auth fileRepo := repository.NewFileRepository(db) file, err := fileRepo.FindByID(r.Context(), fileID) if err != nil { http.Error(w, \u0026#34;File not found\u0026#34;, http.StatusNotFound) return } if file.UserID != userID { http.Error(w, \u0026#34;Unauthorized\u0026#34;, http.StatusForbidden) return } filePath := filepath.Join(uploadDir, file.SavedName) f, err := os.Open(filePath) if err != nil { http.Error(w, \u0026#34;File not found\u0026#34;, http.StatusNotFound) return } defer f.Close() w.Header().Set(\u0026#34;Content-Type\u0026#34;, file.ContentType) w.Header().Set(\u0026#34;Content-Disposition\u0026#34;, fmt.Sprintf(\u0026#34;attachment; filename=\\\u0026#34;%s\\\u0026#34;\u0026#34;, file.OriginalName)) http.ServeContent(w, r, file.OriginalName, file.UploadedAt, f) } Production Best Practices Store files outside the web root to prevent direct access. Configure your upload directory with restricted permissions (0700 or 0755).\nImplement file cleanup for abandoned uploads. Track upload sessions and delete files not confirmed within a timeframe.\nUse object storage (S3, Google Cloud Storage) for production scalability instead of local filesystem.\nMonitor disk usage and implement quotas per user to prevent abuse.\nLog upload attempts including rejected files for security monitoring.\nImplement virus scanning for user-generated content using ClamAV or similar tools.\nUse CDN for serving files to reduce server load and improve performance.\nConclusion File uploads introduce security risks that require careful handling. Validate file types using magic numbers, not extensions. Enforce size limits at multiple levels. Sanitize filenames to prevent path traversal. Generate unique filenames to avoid collisions.\nThe patterns demonstrated here - multipart form parsing, type validation, secure storage, and database tracking - create production-ready upload systems that protect against common attacks while providing good user experience.\nRemember that file uploads are a primary attack vector. Apply defense in depth by implementing multiple validation layers. Monitor upload activity for suspicious patterns. Keep upload functionality isolated with proper access controls.\nStart with basic validation and gradually add features like image processing, virus scanning, and CDN integration as needs grow. Each security layer added makes the system more resistant to attacks while maintaining usability.\n","href":"/2025/10/how-to-handle-file-uploads-in-go-validation-storage-and-security.html","title":"How to Handle File Uploads in Go - Validation, Storage, and Security"},{"content":"Manual deployments are error-prone and time-consuming. You make a change, run tests locally, build the binary, SSH into servers, copy files, restart services, and hope nothing breaks. Multiply this by ten deployments per day and you\u0026rsquo;ve wasted hours on repetitive tasks that should be automated.\nThis guide demonstrates how to implement CI/CD (Continuous Integration/Continuous Deployment) for Go applications using GitHub Actions. You\u0026rsquo;ll learn to create automated workflows that test code on every push, build optimized binaries for multiple platforms, deploy Docker containers automatically, cache dependencies for faster builds, and apply production-ready practices that catch bugs before users do.\nUnderstanding CI/CD for Go Applications Continuous Integration automatically tests and builds your code whenever changes occur. Developers push code to Git, GitHub Actions runs tests, and the team gets immediate feedback about code quality. CI catches integration issues early when they\u0026rsquo;re cheap to fix.\nContinuous Deployment takes CI further by automatically deploying tested code to production. When tests pass, GitHub Actions builds the application, creates Docker images, and deploys to servers - all without manual intervention. CD enables multiple deployments per day safely.\nGo applications benefit particularly from CI/CD because Go\u0026rsquo;s fast compilation and simple dependency management make automated builds quick. A typical Go CI/CD pipeline completes in 2-5 minutes, providing rapid feedback. Go\u0026rsquo;s static binaries simplify deployment - no runtime dependencies to manage.\nGitHub Actions provides the automation platform. It runs workflows triggered by Git events (push, pull request, release), executes jobs on GitHub-hosted or self-hosted runners, supports matrix builds for testing multiple configurations, and integrates with GitHub\u0026rsquo;s ecosystem. For public repositories, GitHub Actions is free with unlimited minutes.\nSetting Up Your First GitHub Actions Workflow GitHub Actions workflows live in .github/workflows/ directory as YAML files. Each file defines a workflow with jobs and steps that run when triggered.\nCreate the basic directory structure:\nmkdir -p .github/workflows Create a simple workflow file .github/workflows/ci.yml:\nname: CI on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; - name: Run tests run: go test -v ./... This workflow triggers on pushes to main/develop branches and pull requests to main. It checks out code, sets up Go 1.21, and runs all tests.\nCommit and push this file:\ngit add .github/workflows/ci.yml git commit -m \u0026#34;Add CI workflow\u0026#34; git push GitHub Actions automatically detects the workflow and runs it. View results in the \u0026ldquo;Actions\u0026rdquo; tab of your GitHub repository.\nRunning Tests with Coverage Expand the test job to include coverage reporting and race detection.\nname: Test on: push: branches: [ main ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true - name: Run tests with coverage run: | go test -v -race -coverprofile=coverage.out -covermode=atomic ./... - name: Upload coverage to Codecov uses: codecov/codecov-action@v3 with: file: ./coverage.out flags: unittests name: codecov-umbrella The -race flag detects data races, -coverprofile generates coverage data, and codecov-action uploads results to Codecov for tracking over time.\nTesting Multiple Go Versions with Matrix Strategy Test against multiple Go versions to ensure compatibility.\nname: Test Matrix on: push: branches: [ main ] pull_request: branches: [ main ] jobs: test: strategy: matrix: go-version: [\u0026#39;1.20\u0026#39;, \u0026#39;1.21\u0026#39;, \u0026#39;1.22\u0026#39;] os: [ubuntu-latest, macos-latest, windows-latest] runs-on: ${{ matrix.os }} steps: - uses: actions/checkout@v4 - name: Set up Go ${{ matrix.go-version }} uses: actions/setup-go@v5 with: go-version: ${{ matrix.go-version }} cache: true - name: Run tests run: go test -v ./... - name: Run tests with race detector (Linux/macOS only) if: runner.os != \u0026#39;Windows\u0026#39; run: go test -race -v ./... This creates 9 jobs (3 Go versions × 3 operating systems), running in parallel. Matrix builds verify your code works across different environments.\nImplementing Linting and Code Quality Checks Add automated linting to enforce code standards.\nname: Lint on: push: branches: [ main ] pull_request: branches: [ main ] jobs: golangci-lint: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true - name: Run golangci-lint uses: golangci/golangci-lint-action@v3 with: version: latest args: --timeout=5m gofmt: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; - name: Check formatting run: | if [ \u0026#34;$(gofmt -s -l . | wc -l)\u0026#34; -gt 0 ]; then echo \u0026#34;The following files are not formatted:\u0026#34; gofmt -s -l . exit 1 fi govet: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true - name: Run go vet run: go vet ./... golangci-lint runs multiple linters in parallel, gofmt ensures consistent formatting, and go vet catches common mistakes.\nBuilding Binaries for Multiple Platforms Create optimized binaries for different operating systems and architectures.\nname: Build on: push: branches: [ main ] tags: - \u0026#39;v*\u0026#39; jobs: build: runs-on: ubuntu-latest strategy: matrix: goos: [linux, darwin, windows] goarch: [amd64, arm64] exclude: - goos: windows goarch: arm64 steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true - name: Build binary env: GOOS: ${{ matrix.goos }} GOARCH: ${{ matrix.goarch }} CGO_ENABLED: 0 run: | OUTPUT_NAME=myapp-${{ matrix.goos }}-${{ matrix.goarch }} if [ \u0026#34;${{ matrix.goos }}\u0026#34; == \u0026#34;windows\u0026#34; ]; then OUTPUT_NAME=\u0026#34;${OUTPUT_NAME}.exe\u0026#34; fi go build -ldflags=\u0026#34;-s -w\u0026#34; -o build/${OUTPUT_NAME} ./cmd/myapp - name: Upload artifacts uses: actions/upload-artifact@v3 with: name: binaries path: build/ This builds for Linux, macOS, and Windows on both x64 and ARM64 architectures (excluding Windows ARM64). Binaries are uploaded as artifacts for download or use in later jobs.\nCaching Go Modules for Faster Builds Module caching dramatically reduces workflow time.\nname: CI with Cache on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true # Automatically caches Go modules - name: Download dependencies run: go mod download - name: Run tests run: go test -v ./... The cache: true option in actions/setup-go automatically caches ~/go/pkg/mod and ~/.cache/go-build. Cache keys use go.sum hash, so cache invalidates when dependencies change.\nFirst build: 2-3 minutes. Cached builds: 30-60 seconds.\nBuilding and Pushing Docker Images Automate Docker image creation and registry pushing.\nname: Docker Build and Push on: push: branches: [ main ] tags: - \u0026#39;v*\u0026#39; env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: docker: runs-on: ubuntu-latest permissions: contents: read packages: write steps: - uses: actions/checkout@v4 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v3 - name: Log in to Container Registry uses: docker/login-action@v3 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata id: meta uses: docker/metadata-action@v5 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} tags: | type=ref,event=branch type=ref,event=pr type=semver,pattern={{version}} type=semver,pattern={{major}}.{{minor}} type=sha,prefix={{branch}}- - name: Build and push uses: docker/build-push-action@v5 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} labels: ${{ steps.meta.outputs.labels }} cache-from: type=gha cache-to: type=gha,mode=max This builds Docker images using buildx (supports multi-platform), pushes to GitHub Container Registry (ghcr.io), tags images based on branch/tag/commit, and caches layers between builds using GitHub Actions cache.\nDeploying to Production with SSH Automate deployment to VPS or dedicated servers via SSH.\nname: Deploy on: push: branches: [ main ] jobs: deploy: runs-on: ubuntu-latest environment: production steps: - name: Deploy to server uses: appleboy/ssh-action@v1.0.0 with: host: ${{ secrets.SERVER_HOST }} username: ${{ secrets.SERVER_USER }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | cd /opt/myapp git pull origin main go build -o myapp ./cmd/myapp sudo systemctl restart myapp Add secrets in repository settings (Settings -\u0026gt; Secrets and variables -\u0026gt; Actions):\nSERVER_HOST: Your server IP or domain SERVER_USER: SSH username SSH_PRIVATE_KEY: Private SSH key for authentication Deploying Docker Containers Deploy Docker images to production servers.\nname: Deploy Docker on: push: branches: [ main ] jobs: deploy: runs-on: ubuntu-latest environment: production steps: - name: Deploy to server uses: appleboy/ssh-action@v1.0.0 with: host: ${{ secrets.SERVER_HOST }} username: ${{ secrets.SERVER_USER }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | cd /opt/myapp docker pull ghcr.io/${{ github.repository }}:main docker-compose down docker-compose up -d docker image prune -f This pulls the latest Docker image, restarts containers using Docker Compose, and cleans up old images.\nDeploying to Kubernetes Automate Kubernetes deployments with kubectl.\nname: Deploy to Kubernetes on: push: branches: [ main ] jobs: deploy: runs-on: ubuntu-latest environment: production steps: - uses: actions/checkout@v4 - name: Configure kubectl uses: azure/k8s-set-context@v3 with: method: kubeconfig kubeconfig: ${{ secrets.KUBE_CONFIG }} - name: Deploy to cluster run: | kubectl set image deployment/myapp \\ myapp=ghcr.io/${{ github.repository }}:${{ github.sha }} kubectl rollout status deployment/myapp Store kubeconfig in KUBE_CONFIG secret. This updates deployment with new image and waits for rollout to complete.\nComplete Production Workflow Combine all concepts into a production-ready workflow.\nname: Production CI/CD on: push: branches: [ main, develop ] pull_request: branches: [ main ] release: types: [published] env: GO_VERSION: \u0026#39;1.21\u0026#39; REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: test: name: Test runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: ${{ env.GO_VERSION }} cache: true - name: Run tests run: go test -v -race -coverprofile=coverage.out ./... - name: Upload coverage uses: codecov/codecov-action@v3 with: file: ./coverage.out lint: name: Lint runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: ${{ env.GO_VERSION }} cache: true - name: Run golangci-lint uses: golangci/golangci-lint-action@v3 with: version: latest build: name: Build needs: [test, lint] runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: ${{ env.GO_VERSION }} cache: true - name: Build binary run: | CGO_ENABLED=0 GOOS=linux GOARCH=amd64 \\ go build -ldflags=\u0026#34;-s -w\u0026#34; -o myapp ./cmd/myapp - name: Upload binary uses: actions/upload-artifact@v3 with: name: myapp-binary path: myapp docker: name: Docker Build and Push needs: [test, lint] runs-on: ubuntu-latest if: github.event_name != \u0026#39;pull_request\u0026#39; permissions: contents: read packages: write steps: - uses: actions/checkout@v4 - name: Set up Docker Buildx uses: docker/setup-buildx-action@v3 - name: Log in to Registry uses: docker/login-action@v3 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata id: meta uses: docker/metadata-action@v5 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} - name: Build and push uses: docker/build-push-action@v5 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} cache-from: type=gha cache-to: type=gha,mode=max deploy-staging: name: Deploy to Staging needs: [docker] runs-on: ubuntu-latest if: github.ref == \u0026#39;refs/heads/develop\u0026#39; environment: staging steps: - name: Deploy via SSH uses: appleboy/ssh-action@v1.0.0 with: host: ${{ secrets.STAGING_HOST }} username: ${{ secrets.SERVER_USER }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | cd /opt/myapp docker-compose pull docker-compose up -d deploy-production: name: Deploy to Production needs: [docker] runs-on: ubuntu-latest if: github.ref == \u0026#39;refs/heads/main\u0026#39; environment: production steps: - name: Deploy via SSH uses: appleboy/ssh-action@v1.0.0 with: host: ${{ secrets.PRODUCTION_HOST }} username: ${{ secrets.SERVER_USER }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | cd /opt/myapp docker-compose pull docker-compose up -d - name: Health check run: | sleep 10 curl -f https://myapp.com/health || exit 1 This workflow:\nRuns tests and linting in parallel Builds binary and Docker image after tests pass Deploys to staging from develop branch Deploys to production from main branch Verifies deployment with health check Using GitHub Environments for Deployment Protection GitHub Environments add protection rules and secrets per environment.\nCreate environments in repository settings (Settings -\u0026gt; Environments):\nStaging Environment:\nNo protection rules Secrets: STAGING_HOST Production Environment:\nRequired reviewers: Add team members who must approve deployments Wait timer: Optional delay before deployment Secrets: PRODUCTION_HOST Update workflow to use environments:\ndeploy-production: name: Deploy to Production needs: [docker] runs-on: ubuntu-latest environment: name: production url: https://myapp.com steps: # Deployment steps Deployments to production now require manual approval, adding safety to your pipeline.\nImplementing Rollback Strategy Add rollback capability for failed deployments.\nname: Rollback on: workflow_dispatch: inputs: version: description: \u0026#39;Version to rollback to\u0026#39; required: true type: string jobs: rollback: runs-on: ubuntu-latest environment: production steps: - name: Rollback via SSH uses: appleboy/ssh-action@v1.0.0 with: host: ${{ secrets.SERVER_HOST }} username: ${{ secrets.SERVER_USER }} key: ${{ secrets.SSH_PRIVATE_KEY }} script: | cd /opt/myapp docker pull ghcr.io/${{ github.repository }}:${{ inputs.version }} docker-compose down docker-compose up -d Trigger manually from Actions tab by selecting workflow and providing version tag.\nMonitoring Workflow Performance Track workflow execution times and identify bottlenecks.\nAdd timing information to jobs:\njobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Go uses: actions/setup-go@v5 with: go-version: \u0026#39;1.21\u0026#39; cache: true - name: Run tests with timing run: | START_TIME=$(date +%s) go test -v ./... END_TIME=$(date +%s) echo \u0026#34;Tests took $((END_TIME - START_TIME)) seconds\u0026#34; GitHub Actions automatically tracks total workflow time in the Actions tab. Use this data to optimize slow steps.\nBest Practices for Go CI/CD Keep workflows fast by caching dependencies, running jobs in parallel, and using matrix builds efficiently. Workflows under 5 minutes provide quick feedback.\nFail fast by running cheap checks (linting, formatting) before expensive ones (tests, builds). Place critical jobs early so failures stop the workflow quickly.\nUse meaningful job and step names that clearly describe what\u0026rsquo;s happening. Good names make debugging failed workflows easier.\nSeparate concerns by splitting workflows into multiple files: test.yml for testing, build.yml for building, deploy.yml for deployment. This makes workflows easier to understand and maintain.\nSecure secrets properly by storing them in GitHub Secrets, never committing them to code, and limiting access to production secrets to specific branches or environments.\nDocument workflows with comments explaining complex steps or non-obvious logic. Future maintainers will thank you.\nIntegration with Other Tools Combine CI/CD with other development tools for complete automation.\nUse database migrations in deployment workflows to automatically update schemas.\nIntegrate mock testing to ensure complete test coverage in CI.\nDeploy containerized applications built in your workflow.\nConclusion CI/CD changes Go application development from manual, error-prone processes to automated, reliable workflows. GitHub Actions provides free, easy-to-configure automation that tests every change, builds optimized binaries, and deploys to production without manual intervention.\nThe patterns demonstrated here - automated testing with coverage, multi-platform builds, Docker integration, staged deployments, and environment protection - create production-ready pipelines that catch bugs early and deploy confidently. Start with basic testing workflows and gradually add complexity as needs grow.\nRemember that CI/CD is an investment that pays dividends quickly. Initial setup takes hours, but saves countless hours preventing bugs and automating repetitive tasks. Every team member benefits from faster feedback, fewer integration issues, and reliable deployments.\nBuild your CI/CD pipeline incrementally: start with automated tests, add linting, implement building, and finally automate deployment. Each step improves development workflow and code quality, making the next step easier to justify and implement.\n","href":"/2025/10/how-to-implement-cicd-for-go-applications-with-github-actions.html","title":"How to Implement CI/CD for Go Applications with GitHub Actions"},{"content":"MySQL remains one of the most popular relational databases for web applications. Go provides excellent MySQL support through the database/sql package and MySQL driver. Understanding connection pooling and transactions is critical for building production-ready applications that handle concurrent users efficiently while maintaining data consistency.\nThis guide demonstrates how to work with MySQL in Go effectively. You\u0026rsquo;ll learn to connect to MySQL with proper driver configuration, implement CRUD operations with prepared statements, configure connection pooling for optimal performance, handle transactions correctly to maintain data integrity, manage NULL values and error conditions, and apply production best practices that scale.\nUnderstanding Go\u0026rsquo;s database/sql Package Go\u0026rsquo;s database/sql package provides a generic interface for working with SQL databases. It handles connection pooling automatically, supports prepared statements, and provides transaction management. The package defines the interface while specific drivers implement MySQL protocol details.\nThe database/sql design separates interface from implementation. Your code imports database/sql for types and methods, while the MySQL driver registers itself during initialization. This separation allows switching databases by changing the driver import and connection string without modifying query code.\nConnection pooling happens transparently. When you call db.Query(), the pool provides an available connection, executes the query, and returns the connection to the pool. You don\u0026rsquo;t manually manage connections - the pool handles creation, reuse, and cleanup based on configuration.\nThe package is safe for concurrent use. Multiple goroutines can execute queries simultaneously using the same db instance. The pool manages connection distribution across goroutines, making it suitable for web servers handling many concurrent requests.\nInstalling MySQL Driver and Dependencies Install the Go MySQL driver that implements database/sql interfaces.\ngo get -u github.com/go-sql-driver/mysql The go-sql-driver/mysql package is the most widely used MySQL driver for Go, supporting MySQL 5.5+, MariaDB, and Amazon Aurora. It implements the database/sql/driver interface and handles MySQL wire protocol details.\nCreate a new Go module if starting a fresh project:\nmkdir mysql-example cd mysql-example go mod init mysql-example go get github.com/go-sql-driver/mysql The driver registers itself with database/sql through an init function. Import it with a blank identifier to execute initialization:\nimport ( \u0026#34;database/sql\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) The underscore import runs the driver\u0026rsquo;s init function which registers \u0026ldquo;mysql\u0026rdquo; with database/sql without directly using exported identifiers from the driver package.\nConnecting to MySQL with Connection Pooling Create a database connection with the MySQL driver using a Data Source Name (DSN) connection string.\n// main.go package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) type Config struct { Host string Port string User string Password string Database string } func NewDB(cfg Config) (*sql.DB, error) { dsn := fmt.Sprintf(\u0026#34;%s:%s@tcp(%s:%s)/%s?parseTime=true\u0026amp;timeout=10s\u0026#34;, cfg.User, cfg.Password, cfg.Host, cfg.Port, cfg.Database, ) db, err := sql.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to open database: %w\u0026#34;, err) } db.SetMaxOpenConns(25) db.SetMaxIdleConns(5) db.SetConnMaxLifetime(5 * time.Minute) db.SetConnMaxIdleTime(10 * time.Minute) if err := db.Ping(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to ping database: %w\u0026#34;, err) } return db, nil } func main() { config := Config{ Host: \u0026#34;localhost\u0026#34;, Port: \u0026#34;3306\u0026#34;, User: \u0026#34;root\u0026#34;, Password: \u0026#34;password\u0026#34;, Database: \u0026#34;testdb\u0026#34;, } db, err := NewDB(config) if err != nil { log.Fatalf(\u0026#34;Database connection failed: %v\u0026#34;, err) } defer db.Close() fmt.Println(\u0026#34;Successfully connected to MySQL!\u0026#34;) } The DSN includes critical parameters: parseTime=true converts MySQL DATETIME and TIMESTAMP to Go time.Time, while timeout=10s sets connection timeout preventing indefinite hangs.\nConnection pool configuration controls resource usage:\nSetMaxOpenConns(25): Maximum connections to MySQL (prevents exhausting server connections) SetMaxIdleConns(5): Idle connections kept ready (reduces connection overhead) SetConnMaxLifetime(5min): Recycles connections periodically (prevents stale connections) SetConnMaxIdleTime(10min): Closes idle connections after inactivity (frees resources) The db.Ping() call verifies connectivity immediately. Without this, sql.Open() defers connection until first query, delaying error detection.\nCreating Tables and Defining Models Define database schema and corresponding Go structs for type-safe operations.\n// models.go package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID int64 `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt sql.NullTime `json:\u0026#34;updated_at\u0026#34;` } type Product struct { ID int64 `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Description sql.NullString `json:\u0026#34;description\u0026#34;` Price float64 `json:\u0026#34;price\u0026#34;` Stock int `json:\u0026#34;stock\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } Create tables in MySQL:\nCREATE TABLE users ( id BIGINT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) NOT NULL UNIQUE, name VARCHAR(255) NOT NULL, age INT NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP NULL ON UPDATE CURRENT_TIMESTAMP ); CREATE TABLE products ( id BIGINT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(255) NOT NULL, description TEXT, price DECIMAL(10, 2) NOT NULL, stock INT NOT NULL DEFAULT 0, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, INDEX idx_name (name) ); CREATE TABLE orders ( id BIGINT AUTO_INCREMENT PRIMARY KEY, user_id BIGINT NOT NULL, product_id BIGINT NOT NULL, quantity INT NOT NULL, total_price DECIMAL(10, 2) NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id), FOREIGN KEY (product_id) REFERENCES products(id) ); The sql.NullTime and sql.NullString types handle nullable columns. NULL values can\u0026rsquo;t map to regular Go types, requiring special handling.\nImplementing CRUD Operations Create a repository pattern for database operations with proper error handling.\n// repository.go package main import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type UserRepository struct { db *sql.DB } func NewUserRepository(db *sql.DB) *UserRepository { return \u0026amp;UserRepository{db: db} } func (r *UserRepository) Create(ctx context.Context, user *User) error { query := ` INSERT INTO users (email, name, age, created_at) VALUES (?, ?, ?, ?) ` result, err := r.db.ExecContext(ctx, query, user.Email, user.Name, user.Age, time.Now()) if err != nil { return fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } id, err := result.LastInsertId() if err != nil { return fmt.Errorf(\u0026#34;failed to get last insert id: %w\u0026#34;, err) } user.ID = id return nil } func (r *UserRepository) FindByID(ctx context.Context, id int64) (*User, error) { query := ` SELECT id, email, name, age, created_at, updated_at FROM users WHERE id = ? ` var user User err := r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Age, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to find user: %w\u0026#34;, err) } return \u0026amp;user, nil } func (r *UserRepository) FindByEmail(ctx context.Context, email string) (*User, error) { query := ` SELECT id, email, name, age, created_at, updated_at FROM users WHERE email = ? ` var user User err := r.db.QueryRowContext(ctx, query, email).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Age, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to find user: %w\u0026#34;, err) } return \u0026amp;user, nil } func (r *UserRepository) FindAll(ctx context.Context, limit, offset int) ([]*User, error) { query := ` SELECT id, email, name, age, created_at, updated_at FROM users ORDER BY created_at DESC LIMIT ? OFFSET ? ` rows, err := r.db.QueryContext(ctx, query, limit, offset) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to query users: %w\u0026#34;, err) } defer rows.Close() var users []*User for rows.Next() { var user User err := rows.Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Age, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to scan user: %w\u0026#34;, err) } users = append(users, \u0026amp;user) } if err := rows.Err(); err != nil { return nil, fmt.Errorf(\u0026#34;row iteration error: %w\u0026#34;, err) } return users, nil } func (r *UserRepository) Update(ctx context.Context, user *User) error { query := ` UPDATE users SET email = ?, name = ?, age = ?, updated_at = ? WHERE id = ? ` result, err := r.db.ExecContext(ctx, query, user.Email, user.Name, user.Age, time.Now(), user.ID) if err != nil { return fmt.Errorf(\u0026#34;failed to update user: %w\u0026#34;, err) } rowsAffected, err := result.RowsAffected() if err != nil { return fmt.Errorf(\u0026#34;failed to get rows affected: %w\u0026#34;, err) } if rowsAffected == 0 { return fmt.Errorf(\u0026#34;user not found\u0026#34;) } return nil } func (r *UserRepository) Delete(ctx context.Context, id int64) error { query := `DELETE FROM users WHERE id = ?` result, err := r.db.ExecContext(ctx, query, id) if err != nil { return fmt.Errorf(\u0026#34;failed to delete user: %w\u0026#34;, err) } rowsAffected, err := result.RowsAffected() if err != nil { return fmt.Errorf(\u0026#34;failed to get rows affected: %w\u0026#34;, err) } if rowsAffected == 0 { return fmt.Errorf(\u0026#34;user not found\u0026#34;) } return nil } The ExecContext method executes queries that modify data, returning sql.Result with LastInsertId() and RowsAffected(). QueryRowContext retrieves single rows, while QueryContext handles multiple rows requiring iteration.\nAlways defer rows.Close() after Query() to release the connection back to the pool. Forgetting this causes connection leaks that exhaust the pool.\nCheck sql.ErrNoRows explicitly when expecting results. This error indicates no matching rows, distinct from connection or syntax errors.\nUsing Prepared Statements for Better Performance Prepared statements optimize repeated queries by parsing SQL once and executing with different parameters multiple times.\nfunc (r *UserRepository) BatchCreate(ctx context.Context, users []*User) error { stmt, err := r.db.PrepareContext(ctx, ` INSERT INTO users (email, name, age, created_at) VALUES (?, ?, ?, ?) `) if err != nil { return fmt.Errorf(\u0026#34;failed to prepare statement: %w\u0026#34;, err) } defer stmt.Close() for _, user := range users { result, err := stmt.ExecContext(ctx, user.Email, user.Name, user.Age, time.Now()) if err != nil { return fmt.Errorf(\u0026#34;failed to insert user %s: %w\u0026#34;, user.Email, err) } id, err := result.LastInsertId() if err != nil { return fmt.Errorf(\u0026#34;failed to get last insert id: %w\u0026#34;, err) } user.ID = id } return nil } func (r *UserRepository) FindMultipleByIDs(ctx context.Context, ids []int64) ([]*User, error) { if len(ids) == 0 { return []*User{}, nil } placeholders := \u0026#34;\u0026#34; args := make([]interface{}, len(ids)) for i, id := range ids { if i \u0026gt; 0 { placeholders += \u0026#34;, \u0026#34; } placeholders += \u0026#34;?\u0026#34; args[i] = id } query := fmt.Sprintf(` SELECT id, email, name, age, created_at, updated_at FROM users WHERE id IN (%s) `, placeholders) rows, err := r.db.QueryContext(ctx, query, args...) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to query users: %w\u0026#34;, err) } defer rows.Close() var users []*User for rows.Next() { var user User err := rows.Scan(\u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Age, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to scan user: %w\u0026#34;, err) } users = append(users, \u0026amp;user) } return users, rows.Err() } Prepared statements reduce parsing overhead for queries executed repeatedly. The database parses SQL once, caches the execution plan, and reuses it for subsequent executions with different parameters.\nAlways close prepared statements with defer stmt.Close() to free server resources. Unclosed statements accumulate and can exhaust server limits.\nImplementing Transactions for Data Consistency Transactions ensure multiple operations succeed together or fail together, maintaining data integrity across related operations.\ntype OrderRepository struct { db *sql.DB } func NewOrderRepository(db *sql.DB) *OrderRepository { return \u0026amp;OrderRepository{db: db} } func (r *OrderRepository) CreateOrder(ctx context.Context, userID, productID int64, quantity int) (*Order, error) { tx, err := r.db.BeginTx(ctx, nil) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to begin transaction: %w\u0026#34;, err) } defer tx.Rollback() var product Product query := `SELECT id, name, price, stock FROM products WHERE id = ? FOR UPDATE` err = tx.QueryRowContext(ctx, query, productID).Scan( \u0026amp;product.ID, \u0026amp;product.Name, \u0026amp;product.Price, \u0026amp;product.Stock, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;product not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to query product: %w\u0026#34;, err) } if product.Stock \u0026lt; quantity { return nil, fmt.Errorf(\u0026#34;insufficient stock: have %d, need %d\u0026#34;, product.Stock, quantity) } updateStock := `UPDATE products SET stock = stock - ? WHERE id = ?` _, err = tx.ExecContext(ctx, updateStock, quantity, productID) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to update stock: %w\u0026#34;, err) } totalPrice := product.Price * float64(quantity) insertOrder := ` INSERT INTO orders (user_id, product_id, quantity, total_price, created_at) VALUES (?, ?, ?, ?, ?) ` result, err := tx.ExecContext(ctx, insertOrder, userID, productID, quantity, totalPrice, time.Now()) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create order: %w\u0026#34;, err) } orderID, err := result.LastInsertId() if err != nil { return nil, fmt.Errorf(\u0026#34;failed to get order id: %w\u0026#34;, err) } if err := tx.Commit(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to commit transaction: %w\u0026#34;, err) } order := \u0026amp;Order{ ID: orderID, UserID: userID, ProductID: productID, Quantity: quantity, TotalPrice: totalPrice, CreatedAt: time.Now(), } return order, nil } type Order struct { ID int64 `json:\u0026#34;id\u0026#34;` UserID int64 `json:\u0026#34;user_id\u0026#34;` ProductID int64 `json:\u0026#34;product_id\u0026#34;` Quantity int `json:\u0026#34;quantity\u0026#34;` TotalPrice float64 `json:\u0026#34;total_price\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } The defer tx.Rollback() pattern ensures rollback on error or panic. After successful tx.Commit(), Rollback becomes a no-op, making this pattern safe.\nThe FOR UPDATE clause locks selected rows, preventing concurrent modifications during the transaction. This prevents race conditions where multiple orders could deplete stock below zero.\nKeep transactions short. Long-running transactions hold locks that block other queries, degrading application performance. Execute only necessary operations within transactions, performing independent queries outside.\nHandling Errors and NULL Values Handle MySQL-specific errors and NULL values correctly for reliable applications.\nimport ( \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) func (r *UserRepository) CreateWithErrorHandling(ctx context.Context, user *User) error { query := `INSERT INTO users (email, name, age, created_at) VALUES (?, ?, ?, ?)` result, err := r.db.ExecContext(ctx, query, user.Email, user.Name, user.Age, time.Now()) if err != nil { if mysqlErr, ok := err.(*mysql.MySQLError); ok { switch mysqlErr.Number { case 1062: return fmt.Errorf(\u0026#34;email already exists: %s\u0026#34;, user.Email) case 1452: return fmt.Errorf(\u0026#34;foreign key constraint failed\u0026#34;) default: return fmt.Errorf(\u0026#34;mysql error %d: %w\u0026#34;, mysqlErr.Number, err) } } return fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } id, err := result.LastInsertId() if err != nil { return fmt.Errorf(\u0026#34;failed to get last insert id: %w\u0026#34;, err) } user.ID = id return nil } func (r *UserRepository) FindWithNullHandling(ctx context.Context, id int64) (*User, error) { query := `SELECT id, email, name, age, created_at, updated_at FROM users WHERE id = ?` var user User var updatedAt sql.NullTime err := r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Age, \u0026amp;user.CreatedAt, \u0026amp;updatedAt, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to find user: %w\u0026#34;, err) } user.UpdatedAt = updatedAt return \u0026amp;user, nil } MySQL error 1062 indicates duplicate key violations, while 1452 signals foreign key constraint failures. Type-assert errors to *mysql.MySQLError for specific error codes.\nUse sql.Null* types for nullable columns: sql.NullString, sql.NullInt64, sql.NullBool, sql.NullFloat64, sql.NullTime. These types have Valid and Value fields. Check Valid before accessing Value to determine if the database value was NULL.\nMonitoring Connection Pool Statistics Monitor pool health to identify configuration issues and optimize resource usage.\nfunc MonitorConnectionPool(db *sql.DB) { stats := db.Stats() fmt.Printf(\u0026#34;Open Connections: %d\\n\u0026#34;, stats.OpenConnections) fmt.Printf(\u0026#34;In Use: %d\\n\u0026#34;, stats.InUse) fmt.Printf(\u0026#34;Idle: %d\\n\u0026#34;, stats.Idle) fmt.Printf(\u0026#34;Wait Count: %d\\n\u0026#34;, stats.WaitCount) fmt.Printf(\u0026#34;Wait Duration: %s\\n\u0026#34;, stats.WaitDuration) fmt.Printf(\u0026#34;Max Idle Closed: %d\\n\u0026#34;, stats.MaxIdleClosed) fmt.Printf(\u0026#34;Max Idle Time Closed: %d\\n\u0026#34;, stats.MaxIdleTimeClosed) fmt.Printf(\u0026#34;Max Lifetime Closed: %d\\n\u0026#34;, stats.MaxLifetimeClosed) } func HealthCheck(db *sql.DB) error { ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second) defer cancel() if err := db.PingContext(ctx); err != nil { return fmt.Errorf(\u0026#34;database health check failed: %w\u0026#34;, err) } stats := db.Stats() if stats.OpenConnections \u0026gt;= 20 { return fmt.Errorf(\u0026#34;connection pool nearly exhausted: %d connections\u0026#34;, stats.OpenConnections) } return nil } Key metrics:\nOpenConnections: Current open connections InUse: Connections actively executing queries Idle: Available connections in pool WaitCount: Times queries waited for available connection WaitDuration: Total time spent waiting for connections High WaitCount and WaitDuration indicate insufficient pool size. Increase MaxOpenConns to provide more connections.\nProduction Best Practices Apply these patterns for reliable production MySQL applications.\ntype Database struct { *sql.DB } func NewDatabase(dsn string) (*Database, error) { db, err := sql.Open(\u0026#34;mysql\u0026#34;, dsn) if err != nil { return nil, err } db.SetMaxOpenConns(25) db.SetMaxIdleConns(5) db.SetConnMaxLifetime(5 * time.Minute) db.SetConnMaxIdleTime(10 * time.Minute) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := db.PingContext(ctx); err != nil { return nil, fmt.Errorf(\u0026#34;failed to ping database: %w\u0026#34;, err) } return \u0026amp;Database{db}, nil } func (db *Database) QueryWithRetry(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error) { var rows *sql.Rows var err error for i := 0; i \u0026lt; 3; i++ { rows, err = db.QueryContext(ctx, query, args...) if err == nil { return rows, nil } if mysqlErr, ok := err.(*mysql.MySQLError); ok { if mysqlErr.Number == 1213 { time.Sleep(time.Duration(i*100) * time.Millisecond) continue } } return nil, err } return nil, fmt.Errorf(\u0026#34;query failed after retries: %w\u0026#34;, err) } func (db *Database) Close() error { return db.DB.Close() } Use a single global database instance shared across the application. The sql.DB type is safe for concurrent use by multiple goroutines.\nAlways use context with timeout for database operations. This prevents queries from hanging indefinitely and allows graceful cancellation.\nImplement retry logic for transient errors like deadlocks (MySQL error 1213). Use exponential backoff between retries to reduce contention.\nLog slow queries to identify optimization opportunities. Add middleware that logs queries exceeding a threshold duration.\nTesting Database Code Write tests that use real MySQL or test doubles for repository testing.\n// repository_test.go package main import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/DATA-DOG/go-sqlmock\u0026#34; ) func TestUserRepository_Create(t *testing.T) { db, mock, err := sqlmock.New() if err != nil { t.Fatalf(\u0026#34;failed to create mock: %v\u0026#34;, err) } defer db.Close() repo := NewUserRepository(db) user := \u0026amp;User{ Email: \u0026#34;test@example.com\u0026#34;, Name: \u0026#34;Test User\u0026#34;, Age: 25, } mock.ExpectExec(\u0026#34;INSERT INTO users\u0026#34;). WithArgs(user.Email, user.Name, user.Age, sqlmock.AnyArg()). WillReturnResult(sqlmock.NewResult(1, 1)) err = repo.Create(context.Background(), user) if err != nil { t.Errorf(\u0026#34;Create failed: %v\u0026#34;, err) } if user.ID != 1 { t.Errorf(\u0026#34;expected ID 1, got %d\u0026#34;, user.ID) } if err := mock.ExpectationsWereMet(); err != nil { t.Errorf(\u0026#34;unmet expectations: %v\u0026#34;, err) } } The sqlmock library provides a mock database driver for testing without real MySQL. It verifies queries match expectations and returns configured results.\nFor integration tests, use a real MySQL instance with test data. Container tools like Docker make this straightforward.\nConclusion Working with MySQL in Go through database/sql provides control and performance for production applications. Proper connection pooling configuration prevents resource exhaustion while maintaining efficiency. Transactions ensure data consistency across related operations, critical for business logic integrity.\nUnderstanding the distinction between Query, QueryRow, Exec, and Prepare enables writing correct database code. Handle NULL values explicitly with sql.Null types or pointers. Monitor connection pool statistics to identify configuration issues and optimize resource usage.\nThe patterns demonstrated here apply to any Go application using MySQL, from REST APIs to background workers. Apply connection pooling best practices, use transactions for multi-step operations, implement proper error handling, and monitor pool health in production. These foundations create reliable database layers that scale with your application.\nFor PostgreSQL applications, similar patterns apply with driver-specific differences covered in our PostgreSQL connection guide . Combine database operations with proper testing patterns to ensure correctness.\n","href":"/2025/10/how-to-work-with-mysql-in-go-connection-pooling-and-transactions.html","title":"How to Work with MySQL in Go - Connection Pooling and Transactions Guide"},{"content":"Testing individual units of code in isolation is critical for building reliable software. When your code depends on databases, external APIs, or other services, testing becomes complex and slow. Mock testing solves this by replacing real dependencies with controlled fake implementations, allowing you to test your code quickly without external systems.\nThis guide demonstrates how to implement effective mock testing in Go using Testify and Mockery. You\u0026rsquo;ll learn to design testable code with interfaces, generate mocks automatically with Mockery, write assertions with Testify, verify method calls and return values, implement table-driven mock tests, and follow best practices that create maintainable test suites without over-mocking.\nUnderstanding Mock Testing and When to Use It Mock testing replaces real dependencies with fake objects that simulate behavior during tests. When your UserService needs a database, instead of connecting to PostgreSQL, you provide a mock that returns predefined data. This isolates the test to only the UserService logic, making tests faster and more focused.\nMocks serve two purposes: they provide controlled responses to method calls, and they verify that your code calls dependencies correctly. A mock database can return specific test data and confirm that your code executed the right query with correct parameters. This dual nature makes mocks effective for testing both behavior and interactions.\nUse mocks for external boundaries where your application interacts with systems you don\u0026rsquo;t control. Database connections, HTTP clients, cloud service SDKs, and file system operations should be mocked in unit tests. These dependencies are slow, require setup, and introduce non-determinism that makes tests flaky.\nDon\u0026rsquo;t mock everything. Use real implementations for simple types, your own business logic, and internal functions. Over-mocking creates tests coupled to implementation details that break during refactoring. The goal is testing behavior, not implementation, so mock at system boundaries and use real code internally.\nInstalling Testify and Mockery Install the Testify testing toolkit which provides assertions, mocking capabilities, and test utilities.\ngo get github.com/stretchr/testify Testify includes several useful packages:\ntestify/assert: Assertion functions for tests testify/require: Assertions that stop test on failure testify/mock: Manual mocking framework testify/suite: Test suite support Install Mockery to automatically generate mock implementations from interfaces.\ngo install github.com/vektra/mockery/v2@latest Verify Mockery installed correctly:\nmockery --version Create a .mockery.yaml configuration file in your project root to configure mock generation behavior.\nwith-expecter: true dir: \u0026#34;mocks\u0026#34; outpkg: mocks filename: \u0026#34;mock_{{.InterfaceName}}.go\u0026#34; all: true keeptree: false This configuration generates mocks with expecter pattern (type-safe expectations), places them in a mocks directory, and processes all interfaces in the project.\nDesigning Testable Code with Interfaces Design code around interfaces to enable dependency injection and mocking. Define what your code needs, not how it\u0026rsquo;s implemented.\n// internal/repository/user.go package repository import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID int `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type UserRepository interface { FindByID(ctx context.Context, id int) (*User, error) FindByEmail(ctx context.Context, email string) (*User, error) Create(ctx context.Context, user *User) error Update(ctx context.Context, user *User) error Delete(ctx context.Context, id int) error } The interface defines the contract without specifying implementation. This allows testing code that depends on UserRepository without a real database.\nImplement the interface with a real database adapter for production use.\n// internal/repository/postgres_user_repository.go package repository import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; ) type PostgresUserRepository struct { db *sql.DB } func NewPostgresUserRepository(db *sql.DB) *PostgresUserRepository { return \u0026amp;PostgresUserRepository{db: db} } func (r *PostgresUserRepository) FindByID(ctx context.Context, id int) (*User, error) { query := \u0026#34;SELECT id, email, name, created_at FROM users WHERE id = $1\u0026#34; var user User err := r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.CreatedAt, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to find user: %w\u0026#34;, err) } return \u0026amp;user, nil } func (r *PostgresUserRepository) FindByEmail(ctx context.Context, email string) (*User, error) { query := \u0026#34;SELECT id, email, name, created_at FROM users WHERE email = $1\u0026#34; var user User err := r.db.QueryRowContext(ctx, query, email).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.CreatedAt, ) if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } if err != nil { return nil, fmt.Errorf(\u0026#34;failed to find user: %w\u0026#34;, err) } return \u0026amp;user, nil } func (r *PostgresUserRepository) Create(ctx context.Context, user *User) error { query := \u0026#34;INSERT INTO users (email, name, created_at) VALUES ($1, $2, $3) RETURNING id\u0026#34; err := r.db.QueryRowContext(ctx, query, user.Email, user.Name, time.Now()).Scan(\u0026amp;user.ID) if err != nil { return fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } return nil } func (r *PostgresUserRepository) Update(ctx context.Context, user *User) error { query := \u0026#34;UPDATE users SET email = $1, name = $2 WHERE id = $3\u0026#34; _, err := r.db.ExecContext(ctx, query, user.Email, user.Name, user.ID) if err != nil { return fmt.Errorf(\u0026#34;failed to update user: %w\u0026#34;, err) } return nil } func (r *PostgresUserRepository) Delete(ctx context.Context, id int) error { query := \u0026#34;DELETE FROM users WHERE id = $1\u0026#34; _, err := r.db.ExecContext(ctx, query, id) if err != nil { return fmt.Errorf(\u0026#34;failed to delete user: %w\u0026#34;, err) } return nil } Create a service that depends on the interface, not the concrete implementation. This enables injecting mocks during testing.\n// internal/service/user_service.go package service import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;yourapp/internal/repository\u0026#34; ) type UserService struct { repo repository.UserRepository } func NewUserService(repo repository.UserRepository) *UserService { return \u0026amp;UserService{repo: repo} } func (s *UserService) GetUser(ctx context.Context, id int) (*repository.User, error) { if id \u0026lt;= 0 { return nil, fmt.Errorf(\u0026#34;invalid user ID\u0026#34;) } user, err := s.repo.FindByID(ctx, id) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to get user: %w\u0026#34;, err) } return user, nil } func (s *UserService) RegisterUser(ctx context.Context, email, name string) (*repository.User, error) { email = strings.TrimSpace(strings.ToLower(email)) name = strings.TrimSpace(name) if email == \u0026#34;\u0026#34; || name == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;email and name are required\u0026#34;) } if !strings.Contains(email, \u0026#34;@\u0026#34;) { return nil, fmt.Errorf(\u0026#34;invalid email format\u0026#34;) } existingUser, err := s.repo.FindByEmail(ctx, email) if err == nil \u0026amp;\u0026amp; existingUser != nil { return nil, fmt.Errorf(\u0026#34;email already exists\u0026#34;) } user := \u0026amp;repository.User{ Email: email, Name: name, } if err := s.repo.Create(ctx, user); err != nil { return nil, fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } return user, nil } func (s *UserService) UpdateUserEmail(ctx context.Context, userID int, newEmail string) error { newEmail = strings.TrimSpace(strings.ToLower(newEmail)) if !strings.Contains(newEmail, \u0026#34;@\u0026#34;) { return fmt.Errorf(\u0026#34;invalid email format\u0026#34;) } user, err := s.repo.FindByID(ctx, userID) if err != nil { return fmt.Errorf(\u0026#34;user not found: %w\u0026#34;, err) } existingUser, err := s.repo.FindByEmail(ctx, newEmail) if err == nil \u0026amp;\u0026amp; existingUser != nil \u0026amp;\u0026amp; existingUser.ID != userID { return fmt.Errorf(\u0026#34;email already in use\u0026#34;) } user.Email = newEmail if err := s.repo.Update(ctx, user); err != nil { return fmt.Errorf(\u0026#34;failed to update user: %w\u0026#34;, err) } return nil } func (s *UserService) DeleteUser(ctx context.Context, userID int) error { if userID \u0026lt;= 0 { return fmt.Errorf(\u0026#34;invalid user ID\u0026#34;) } if err := s.repo.Delete(ctx, userID); err != nil { return fmt.Errorf(\u0026#34;failed to delete user: %w\u0026#34;, err) } return nil } UserService depends on the UserRepository interface, not PostgresUserRepository. Production code injects the real implementation, while tests inject mocks.\nGenerating Mocks with Mockery Run Mockery to generate mock implementations for all interfaces in your project.\nmockery Mockery scans your code for interfaces and generates mock implementations in the mocks directory. For the UserRepository interface, it creates mock_UserRepository.go.\n// mocks/mock_UserRepository.go package mocks import ( \u0026#34;context\u0026#34; \u0026#34;yourapp/internal/repository\u0026#34; \u0026#34;github.com/stretchr/testify/mock\u0026#34; ) type MockUserRepository struct { mock.Mock } func (m *MockUserRepository) FindByID(ctx context.Context, id int) (*repository.User, error) { args := m.Called(ctx, id) if args.Get(0) == nil { return nil, args.Error(1) } return args.Get(0).(*repository.User), args.Error(1) } func (m *MockUserRepository) FindByEmail(ctx context.Context, email string) (*repository.User, error) { args := m.Called(ctx, email) if args.Get(0) == nil { return nil, args.Error(1) } return args.Get(0).(*repository.User), args.Error(1) } func (m *MockUserRepository) Create(ctx context.Context, user *repository.User) error { args := m.Called(ctx, user) return args.Error(0) } func (m *MockUserRepository) Update(ctx context.Context, user *repository.User) error { args := m.Called(ctx, user) return args.Error(0) } func (m *MockUserRepository) Delete(ctx context.Context, id int) error { args := m.Called(ctx, id) return args.Error(0) } The generated mock implements UserRepository and tracks method calls. The Called method records invocations and returns configured values.\nGenerate mocks for specific interfaces instead of all:\nmockery --name UserRepository Regenerate mocks when interfaces change:\nmockery --all Add mock generation to your Makefile for convenient updates:\n.PHONY: mocks mocks: mockery --all Writing Tests with Testify Assertions Create tests that inject mocks and verify behavior using Testify assertions.\n// internal/service/user_service_test.go package service import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; \u0026#34;yourapp/internal/repository\u0026#34; \u0026#34;yourapp/mocks\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;github.com/stretchr/testify/mock\u0026#34; ) func TestUserService_GetUser_Success(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() expectedUser := \u0026amp;repository.User{ ID: 1, Email: \u0026#34;test@example.com\u0026#34;, Name: \u0026#34;Test User\u0026#34;, CreatedAt: time.Now(), } mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 1).Return(expectedUser, nil) user, err := service.GetUser(ctx, 1) assert.NoError(t, err) assert.NotNil(t, user) assert.Equal(t, expectedUser.ID, user.ID) assert.Equal(t, expectedUser.Email, user.Email) assert.Equal(t, expectedUser.Name, user.Name) mockRepo.AssertExpectations(t) } func TestUserService_GetUser_NotFound(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 999).Return(nil, errors.New(\u0026#34;user not found\u0026#34;)) user, err := service.GetUser(ctx, 999) assert.Error(t, err) assert.Nil(t, user) assert.Contains(t, err.Error(), \u0026#34;failed to get user\u0026#34;) mockRepo.AssertExpectations(t) } func TestUserService_GetUser_InvalidID(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() user, err := service.GetUser(ctx, 0) assert.Error(t, err) assert.Nil(t, user) assert.Contains(t, err.Error(), \u0026#34;invalid user ID\u0026#34;) mockRepo.AssertNotCalled(t, \u0026#34;FindByID\u0026#34;) } The test creates a mock repository, configures expected method calls with On, executes the service method, and verifies results with assertions. AssertExpectations confirms all expected calls occurred.\nTestify provides many assertion functions:\nassert.Equal(t, expected, actual): Values must be equal assert.NotEqual(t, unexpected, actual): Values must differ assert.NoError(t, err): No error occurred assert.Error(t, err): Error occurred assert.Nil(t, value): Value is nil assert.NotNil(t, value): Value is not nil assert.True(t, condition): Condition is true assert.False(t, condition): Condition is false assert.Contains(t, haystack, needle): String/slice contains value assert.Len(t, list, length): Collection has specific length Use require instead of assert when subsequent code depends on the assertion. Require stops the test immediately on failure, preventing panics from nil dereferences.\nfunc TestUserService_GetUser_WithRequire(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() expectedUser := \u0026amp;repository.User{ID: 1, Email: \u0026#34;test@example.com\u0026#34;, Name: \u0026#34;Test User\u0026#34;} mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 1).Return(expectedUser, nil) user, err := service.GetUser(ctx, 1) require.NoError(t, err) require.NotNil(t, user) assert.Equal(t, expectedUser.Email, user.Email) mockRepo.AssertExpectations(t) } Implementing Table-Driven Mock Tests Table-driven tests handle multiple scenarios with different mock behaviors efficiently.\nfunc TestUserService_RegisterUser(t *testing.T) { tests := []struct { name string email string userName string setupMock func(*mocks.MockUserRepository) expectError bool errorContains string }{ { name: \u0026#34;successful registration\u0026#34;, email: \u0026#34;new@example.com\u0026#34;, userName: \u0026#34;New User\u0026#34;, setupMock: func(m *mocks.MockUserRepository) { m.On(\u0026#34;FindByEmail\u0026#34;, mock.Anything, \u0026#34;new@example.com\u0026#34;).Return(nil, errors.New(\u0026#34;not found\u0026#34;)) m.On(\u0026#34;Create\u0026#34;, mock.Anything, mock.AnythingOfType(\u0026#34;*repository.User\u0026#34;)).Return(nil) }, expectError: false, }, { name: \u0026#34;email already exists\u0026#34;, email: \u0026#34;existing@example.com\u0026#34;, userName: \u0026#34;Existing User\u0026#34;, setupMock: func(m *mocks.MockUserRepository) { existingUser := \u0026amp;repository.User{ID: 1, Email: \u0026#34;existing@example.com\u0026#34;} m.On(\u0026#34;FindByEmail\u0026#34;, mock.Anything, \u0026#34;existing@example.com\u0026#34;).Return(existingUser, nil) }, expectError: true, errorContains: \u0026#34;email already exists\u0026#34;, }, { name: \u0026#34;empty email\u0026#34;, email: \u0026#34;\u0026#34;, userName: \u0026#34;User\u0026#34;, setupMock: func(m *mocks.MockUserRepository) {}, expectError: true, errorContains: \u0026#34;email and name are required\u0026#34;, }, { name: \u0026#34;invalid email format\u0026#34;, email: \u0026#34;invalid-email\u0026#34;, userName: \u0026#34;User\u0026#34;, setupMock: func(m *mocks.MockUserRepository) {}, expectError: true, errorContains: \u0026#34;invalid email format\u0026#34;, }, { name: \u0026#34;database error on create\u0026#34;, email: \u0026#34;test@example.com\u0026#34;, userName: \u0026#34;Test User\u0026#34;, setupMock: func(m *mocks.MockUserRepository) { m.On(\u0026#34;FindByEmail\u0026#34;, mock.Anything, \u0026#34;test@example.com\u0026#34;).Return(nil, errors.New(\u0026#34;not found\u0026#34;)) m.On(\u0026#34;Create\u0026#34;, mock.Anything, mock.AnythingOfType(\u0026#34;*repository.User\u0026#34;)).Return(errors.New(\u0026#34;database error\u0026#34;)) }, expectError: true, errorContains: \u0026#34;failed to create user\u0026#34;, }, } for _, tt := range tests { t.Run(tt.name, func(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) tt.setupMock(mockRepo) user, err := service.RegisterUser(context.Background(), tt.email, tt.userName) if tt.expectError { assert.Error(t, err) assert.Nil(t, user) if tt.errorContains != \u0026#34;\u0026#34; { assert.Contains(t, err.Error(), tt.errorContains) } } else { assert.NoError(t, err) assert.NotNil(t, user) assert.Equal(t, tt.email, user.Email) assert.Equal(t, tt.userName, user.Name) } mockRepo.AssertExpectations(t) }) } } Each test case defines a scenario with specific inputs and mock behavior. The setupMock function configures the mock for that scenario, isolating test setup from execution.\nVerifying Method Call Arguments Use argument matchers to verify methods receive correct parameters without specifying exact values.\nfunc TestUserService_UpdateUserEmail(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() existingUser := \u0026amp;repository.User{ ID: 1, Email: \u0026#34;old@example.com\u0026#34;, Name: \u0026#34;Test User\u0026#34;, } mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 1).Return(existingUser, nil) mockRepo.On(\u0026#34;FindByEmail\u0026#34;, ctx, \u0026#34;new@example.com\u0026#34;).Return(nil, errors.New(\u0026#34;not found\u0026#34;)) mockRepo.On(\u0026#34;Update\u0026#34;, ctx, mock.MatchedBy(func(u *repository.User) bool { return u.ID == 1 \u0026amp;\u0026amp; u.Email == \u0026#34;new@example.com\u0026#34; })).Return(nil) err := service.UpdateUserEmail(ctx, 1, \u0026#34;new@example.com\u0026#34;) assert.NoError(t, err) mockRepo.AssertExpectations(t) } mock.MatchedBy takes a function that validates argument properties. This verifies the service passes a user with correct ID and updated email without requiring exact object matching.\nCommon argument matchers:\nmock.Anything: Accepts any value mock.AnythingOfType(\u0026quot;string\u0026quot;): Accepts any value of specific type mock.MatchedBy(func(x Type) bool { ... }): Custom validation function Verify specific method calls occurred:\nfunc TestUserService_DeleteUser(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() mockRepo.On(\u0026#34;Delete\u0026#34;, ctx, 1).Return(nil) err := service.DeleteUser(ctx, 1) assert.NoError(t, err) mockRepo.AssertCalled(t, \u0026#34;Delete\u0026#34;, ctx, 1) } AssertCalled verifies the method was called with specific arguments, useful when you want to confirm interactions without setting up expectations.\nHandling Multiple Return Values and Errors Configure mocks to return different values based on arguments or call count.\nfunc TestUserService_RetryLogic(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 1).Return(nil, errors.New(\u0026#34;temporary error\u0026#34;)).Once() mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, 1).Return(\u0026amp;repository.User{ID: 1, Email: \u0026#34;test@example.com\u0026#34;}, nil).Once() _, err := service.GetUser(ctx, 1) assert.Error(t, err) user, err := service.GetUser(ctx, 1) assert.NoError(t, err) assert.NotNil(t, user) mockRepo.AssertExpectations(t) } The first call returns an error, the second succeeds. Once() specifies the expectation applies to one call, allowing different behaviors for subsequent calls.\nReturn values based on input:\nfunc TestUserService_DynamicMockBehavior(t *testing.T) { mockRepo := new(mocks.MockUserRepository) service := NewUserService(mockRepo) ctx := context.Background() mockRepo.On(\u0026#34;FindByID\u0026#34;, ctx, mock.AnythingOfType(\u0026#34;int\u0026#34;)).Return( func(ctx context.Context, id int) *repository.User { if id == 1 { return \u0026amp;repository.User{ID: 1, Email: \u0026#34;user1@example.com\u0026#34;} } return nil }, func(ctx context.Context, id int) error { if id == 1 { return nil } return errors.New(\u0026#34;not found\u0026#34;) }, ) user1, err := service.GetUser(ctx, 1) assert.NoError(t, err) assert.Equal(t, \u0026#34;user1@example.com\u0026#34;, user1.Email) user2, err := service.GetUser(ctx, 2) assert.Error(t, err) assert.Nil(t, user2) } Functions as return values enable dynamic behavior based on arguments, useful for testing edge cases without creating multiple expectations.\nTesting with Multiple Dependencies Services often depend on multiple interfaces. Mock each dependency independently.\ntype NotificationService interface { SendEmail(ctx context.Context, to, subject, body string) error } type UserService struct { repo repository.UserRepository notification NotificationService } func NewUserService(repo repository.UserRepository, notification NotificationService) *UserService { return \u0026amp;UserService{ repo: repo, notification: notification, } } func (s *UserService) RegisterUserWithNotification(ctx context.Context, email, name string) (*repository.User, error) { user, err := s.RegisterUser(ctx, email, name) if err != nil { return nil, err } err = s.notification.SendEmail(ctx, email, \u0026#34;Welcome!\u0026#34;, \u0026#34;Thanks for registering\u0026#34;) if err != nil { return user, fmt.Errorf(\u0026#34;user created but notification failed: %w\u0026#34;, err) } return user, nil } Test with multiple mocks:\nfunc TestUserService_RegisterUserWithNotification(t *testing.T) { mockRepo := new(mocks.MockUserRepository) mockNotification := new(mocks.MockNotificationService) service := NewUserService(mockRepo, mockNotification) ctx := context.Background() mockRepo.On(\u0026#34;FindByEmail\u0026#34;, ctx, \u0026#34;test@example.com\u0026#34;).Return(nil, errors.New(\u0026#34;not found\u0026#34;)) mockRepo.On(\u0026#34;Create\u0026#34;, ctx, mock.AnythingOfType(\u0026#34;*repository.User\u0026#34;)).Return(nil) mockNotification.On(\u0026#34;SendEmail\u0026#34;, ctx, \u0026#34;test@example.com\u0026#34;, \u0026#34;Welcome!\u0026#34;, \u0026#34;Thanks for registering\u0026#34;).Return(nil) user, err := service.RegisterUserWithNotification(ctx, \u0026#34;test@example.com\u0026#34;, \u0026#34;Test User\u0026#34;) assert.NoError(t, err) assert.NotNil(t, user) mockRepo.AssertExpectations(t) mockNotification.AssertExpectations(t) } Each mock tracks its own expectations independently. This tests the integration between service and multiple dependencies while keeping tests isolated.\nBest Practices for Mock Testing Keep mocks simple and focused. Each test should verify one behavior with minimal mock setup. Complex mocks indicate the code under test does too much.\nDon\u0026rsquo;t over-specify implementation details. Mock the interface contract, not internal implementation. Tests should pass after refactoring if behavior stays the same.\nUse table-driven tests for multiple scenarios. This reduces duplication and makes adding test cases straightforward.\nClean up mocks properly:\nfunc TestUserService_WithCleanup(t *testing.T) { mockRepo := new(mocks.MockUserRepository) t.Cleanup(func() { mockRepo.AssertExpectations(t) }) service := NewUserService(mockRepo) mockRepo.On(\u0026#34;FindByID\u0026#34;, mock.Anything, 1).Return(\u0026amp;repository.User{ID: 1}, nil) user, err := service.GetUser(context.Background(), 1) assert.NoError(t, err) assert.NotNil(t, user) } t.Cleanup ensures expectations are verified even if the test panics or returns early.\nGenerate mocks instead of writing manually. Mockery keeps mocks synchronized with interface changes and reduces maintenance burden.\nBalance unit tests with mocks against integration tests with real dependencies. Unit tests verify logic quickly, integration tests ensure components work together correctly. Most projects need both for good coverage.\nAvoid mocking standard library types. Use real implementations like bytes.Buffer for io.Writer or httptest.NewRecorder for HTTP handlers. These are designed for testing and work better than mocks.\nCommon Pitfalls and How to Avoid Them Forgetting AssertExpectations causes tests to pass even when expected calls don\u0026rsquo;t occur. Always call it at the end of tests or in t.Cleanup.\nLoop variable capture in table-driven tests causes all subtests to use the last value:\nfor _, tt := range tests { tt := tt t.Run(tt.name, func(t *testing.T) { // Use tt here }) } The tt := tt line captures the loop variable for each iteration.\nMocking too much creates brittle tests coupled to implementation. Mock external boundaries, use real code internally.\nNot using mock.Anything for context arguments makes tests verbose:\nmockRepo.On(\u0026#34;FindByID\u0026#34;, mock.Anything, 1).Return(user, nil) Context values rarely matter for business logic tests, so mock.Anything simplifies setup.\nParallel tests with shared mocks cause race conditions. Each test needs its own mock instance:\nt.Run(\u0026#34;test\u0026#34;, func(t *testing.T) { t.Parallel() mockRepo := new(mocks.MockUserRepository) // Use mockRepo }) Integrating Mock Tests into CI/CD Run tests with coverage reporting in CI pipelines:\ngo test ./... -cover -coverprofile=coverage.out go tool cover -html=coverage.out -o coverage.html Configure GitHub Actions to run tests on every push:\nname: Tests on: [push, pull_request] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-go@v4 with: go-version: \u0026#39;1.21\u0026#39; - name: Install dependencies run: go mod download - name: Generate mocks run: | go install github.com/vektra/mockery/v2@latest mockery --all - name: Run tests run: go test ./... -v -cover This ensures mocks stay current and tests pass before merging code.\nConclusion Mock testing enables fast, reliable unit tests by isolating code from external dependencies. Testify provides assertions and mocking capabilities that make tests readable and maintainable, while Mockery generates mock implementations automatically, reducing boilerplate and keeping mocks synchronized with interfaces.\nDesigning code around interfaces and dependency injection creates testable architecture that separates concerns and enables mock injection during tests. Mock external boundaries like databases and APIs while using real implementations for business logic, balancing isolation with realistic testing.\nRemember that mocking is a tool, not a goal. Tests should verify behavior, not implementation details. Combine unit tests with mocks against integration tests with real dependencies for complete coverage. This approach catches logic bugs early through fast unit tests while ensuring components integrate correctly through slower but more realistic integration tests.\nThe patterns demonstrated here work for any Go application, from web services to CLI tools. Apply these techniques to your projects, generate mocks for your interfaces, write clear assertions, and build test suites that give confidence in your code without slowing development.\n","href":"/2025/10/how-to-use-mock-testing-in-go-with-testify-and-mockery.html","title":"How to Use Mock Testing in Go with Testify and Mockery - Complete Guide"},{"content":"Web applications need to remember users across multiple requests. When a user logs in, adds items to a shopping cart, or sets preferences, the application must maintain this state throughout their session. HTTP\u0026rsquo;s stateless nature makes this challenging, but proper session management solves this problem by storing user state securely on the server while using cookies to track users across requests.\nThis complete guide demonstrates how to implement production-ready session management in Go using cookies and Redis. You\u0026rsquo;ll learn to create secure sessions, store data efficiently in Redis, implement session middleware, handle authentication flows, prevent common security vulnerabilities, and build scalable session systems that work across multiple server instances.\nUnderstanding Session Management Architecture Session management works through a combination of client-side cookies and server-side storage. When a user first visits your application, the server generates a unique session ID, stores it in a cookie, and creates a corresponding session record in Redis. On subsequent requests, the browser sends the session ID cookie, allowing the server to retrieve the user\u0026rsquo;s session data from Redis.\nThe session ID acts as a lookup key that connects the browser to server-side session data. This separation keeps sensitive information on the server while providing a convenient way to track users. The session ID must be cryptographically random to prevent guessing attacks, and cookies must be configured with security flags to prevent various attack vectors.\nRedis serves as the ideal session store for production applications. Its in-memory architecture provides microsecond-level access times, automatic expiration handles session cleanup, and distributed setup enables horizontal scaling. Unlike database-backed sessions that add query overhead, Redis delivers consistent performance even with thousands of concurrent sessions.\nCookie-based session tracking works universally across all browsers and requires no special client-side code. The browser automatically sends cookies with each request to the same domain, making session management transparent to your application logic. This approach integrates smoothly with server-side rendering, APIs, and modern frontend frameworks.\nSetting Up the Project and Dependencies Start by creating a new Go project and installing the required dependencies. You\u0026rsquo;ll need the Gin web framework for HTTP handling, go-redis for session storage, and gorilla/securecookie for secure cookie management.\nmkdir session-management-example cd session-management-example go mod init session-management-example go get github.com/gin-gonic/gin go get github.com/redis/go-redis/v9 go get github.com/gorilla/securecookie go get github.com/google/uuid Create a structured project layout that separates concerns and makes the codebase maintainable as it grows.\nmkdir -p cmd/server mkdir -p internal/{config,session,middleware,handlers} mkdir -p pkg/redis Configure environment variables for Redis connection and cookie secrets. Never hardcode secrets in your application code.\n# .env REDIS_HOST=localhost REDIS_PORT=6379 REDIS_PASSWORD= REDIS_DB=0 COOKIE_HASH_KEY=change-this-to-64-random-bytes-in-production COOKIE_BLOCK_KEY=change-this-to-32-random-bytes-in-production SESSION_MAX_AGE=3600 SESSION_NAME=session_id The cookie hash key signs cookies to prevent tampering, while the block key encrypts cookie contents. Both should be cryptographically random bytes generated specifically for your application. The hash key must be 32 or 64 bytes, and the block key must be 16, 24, or 32 bytes for AES encryption.\nConfiguring Redis Connection Create a Redis client with connection pooling and proper timeout configurations. This client will handle all session storage operations efficiently.\n// pkg/redis/client.go package redis import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type Config struct { Host string Port string Password string DB int PoolSize int MinIdleConns int DialTimeout time.Duration ReadTimeout time.Duration WriteTimeout time.Duration } type Client struct { rdb *redis.Client } func NewClient(cfg *Config) (*Client, error) { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, cfg.Host, cfg.Port), Password: cfg.Password, DB: cfg.DB, PoolSize: cfg.PoolSize, MinIdleConns: cfg.MinIdleConns, DialTimeout: cfg.DialTimeout, ReadTimeout: cfg.ReadTimeout, WriteTimeout: cfg.WriteTimeout, }) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := rdb.Ping(ctx).Err(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to connect to Redis: %w\u0026#34;, err) } return \u0026amp;Client{rdb: rdb}, nil } func (c *Client) GetClient() *redis.Client { return c.rdb } func (c *Client) Close() error { return c.rdb.Close() } func (c *Client) HealthCheck(ctx context.Context) error { return c.rdb.Ping(ctx).Err() } Connection pooling reuses TCP connections across requests, dramatically improving performance. Configure the pool size based on your expected concurrent users and available Redis connections. The default of 10 per CPU core works well for most applications.\nCreating the Session Manager Build a session manager that handles all session operations including creation, retrieval, updates, and deletion. This abstraction provides a clean interface for working with sessions throughout your application.\n// internal/session/manager.go package session import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) var ( ErrSessionNotFound = errors.New(\u0026#34;session not found\u0026#34;) ErrInvalidSession = errors.New(\u0026#34;invalid session\u0026#34;) ) type Session struct { ID string `json:\u0026#34;id\u0026#34;` UserID int `json:\u0026#34;user_id,omitempty\u0026#34;` Data map[string]interface{} `json:\u0026#34;data\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` ExpiresAt time.Time `json:\u0026#34;expires_at\u0026#34;` LastAccess time.Time `json:\u0026#34;last_access\u0026#34;` } type Manager struct { client *redis.Client prefix string maxAge time.Duration } func NewManager(client *redis.Client, prefix string, maxAge time.Duration) *Manager { return \u0026amp;Manager{ client: client, prefix: prefix, maxAge: maxAge, } } func (m *Manager) Create(ctx context.Context) (*Session, error) { sessionID := uuid.New().String() now := time.Now() session := \u0026amp;Session{ ID: sessionID, Data: make(map[string]interface{}), CreatedAt: now, ExpiresAt: now.Add(m.maxAge), LastAccess: now, } if err := m.Save(ctx, session); err != nil { return nil, fmt.Errorf(\u0026#34;failed to create session: %w\u0026#34;, err) } return session, nil } func (m *Manager) Get(ctx context.Context, sessionID string) (*Session, error) { key := m.keyFor(sessionID) data, err := m.client.Get(ctx, key).Bytes() if err != nil { if err == redis.Nil { return nil, ErrSessionNotFound } return nil, fmt.Errorf(\u0026#34;failed to get session: %w\u0026#34;, err) } var session Session if err := json.Unmarshal(data, \u0026amp;session); err != nil { return nil, fmt.Errorf(\u0026#34;failed to unmarshal session: %w\u0026#34;, err) } if time.Now().After(session.ExpiresAt) { m.Destroy(ctx, sessionID) return nil, ErrSessionNotFound } return \u0026amp;session, nil } func (m *Manager) Save(ctx context.Context, session *Session) error { key := m.keyFor(session.ID) data, err := json.Marshal(session) if err != nil { return fmt.Errorf(\u0026#34;failed to marshal session: %w\u0026#34;, err) } ttl := time.Until(session.ExpiresAt) if ttl \u0026lt;= 0 { return errors.New(\u0026#34;session already expired\u0026#34;) } if err := m.client.Set(ctx, key, data, ttl).Err(); err != nil { return fmt.Errorf(\u0026#34;failed to save session: %w\u0026#34;, err) } return nil } func (m *Manager) Destroy(ctx context.Context, sessionID string) error { key := m.keyFor(sessionID) return m.client.Del(ctx, key).Err() } func (m *Manager) Refresh(ctx context.Context, session *Session) error { now := time.Now() session.LastAccess = now session.ExpiresAt = now.Add(m.maxAge) return m.Save(ctx, session) } func (m *Manager) Exists(ctx context.Context, sessionID string) (bool, error) { key := m.keyFor(sessionID) count, err := m.client.Exists(ctx, key).Result() if err != nil { return false, err } return count \u0026gt; 0, nil } func (m *Manager) keyFor(sessionID string) string { return fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, m.prefix, sessionID) } func (m *Manager) SetUserID(session *Session, userID int) { session.UserID = userID } func (m *Manager) GetUserID(session *Session) (int, bool) { if session.UserID == 0 { return 0, false } return session.UserID, true } func (m *Manager) Set(session *Session, key string, value interface{}) { session.Data[key] = value } func (m *Manager) Get(session *Session, key string) (interface{}, bool) { value, exists := session.Data[key] return value, exists } func (m *Manager) Delete(session *Session, key string) { delete(session.Data, key) } The session manager uses UUID v4 for session IDs, providing 122 bits of randomness that make guessing attacks computationally infeasible. JSON serialization enables storing complex data structures while remaining human-readable for debugging. Automatic expiration through Redis TTL ensures old sessions don\u0026rsquo;t accumulate and consume memory.\nImplementing Secure Cookie Handling Create a cookie service that handles secure cookie operations with encryption and signature verification. This prevents cookie tampering and ensures session integrity.\n// internal/session/cookie.go package session import ( \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gorilla/securecookie\u0026#34; ) type CookieManager struct { sc *securecookie.SecureCookie name string maxAge int secure bool httpOnly bool sameSite http.SameSite domain string path string } type CookieConfig struct { Name string HashKey []byte BlockKey []byte MaxAge int Secure bool HttpOnly bool SameSite http.SameSite Domain string Path string } func NewCookieManager(cfg *CookieConfig) *CookieManager { return \u0026amp;CookieManager{ sc: securecookie.New(cfg.HashKey, cfg.BlockKey), name: cfg.Name, maxAge: cfg.MaxAge, secure: cfg.Secure, httpOnly: cfg.HttpOnly, sameSite: cfg.SameSite, domain: cfg.Domain, path: cfg.Path, } } func (cm *CookieManager) Set(w http.ResponseWriter, sessionID string) error { encoded, err := cm.sc.Encode(cm.name, sessionID) if err != nil { return err } cookie := \u0026amp;http.Cookie{ Name: cm.name, Value: encoded, Path: cm.path, Domain: cm.domain, MaxAge: cm.maxAge, Secure: cm.secure, HttpOnly: cm.httpOnly, SameSite: cm.sameSite, } http.SetCookie(w, cookie) return nil } func (cm *CookieManager) Get(r *http.Request) (string, error) { cookie, err := r.Cookie(cm.name) if err != nil { return \u0026#34;\u0026#34;, err } var sessionID string if err := cm.sc.Decode(cm.name, cookie.Value, \u0026amp;sessionID); err != nil { return \u0026#34;\u0026#34;, err } return sessionID, nil } func (cm *CookieManager) Delete(w http.ResponseWriter) { cookie := \u0026amp;http.Cookie{ Name: cm.name, Value: \u0026#34;\u0026#34;, Path: cm.path, Domain: cm.domain, MaxAge: -1, Secure: cm.secure, HttpOnly: cm.httpOnly, SameSite: cm.sameSite, } http.SetCookie(w, cookie) } The HttpOnly flag prevents JavaScript access to cookies, protecting against XSS attacks. The Secure flag ensures cookies only transmit over HTTPS, preventing man-in-the-middle attacks. SameSite=Strict prevents CSRF attacks by blocking cross-site cookie transmission. Together, these flags create defense-in-depth security for session cookies.\nBuilding Session Middleware Create middleware that automatically loads sessions from cookies, makes them available to request handlers, and saves changes after request processing completes.\n// internal/middleware/session.go package middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;session-management-example/internal/session\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) const SessionKey = \u0026#34;session\u0026#34; func SessionMiddleware(manager *session.Manager, cookieManager *session.CookieManager) gin.HandlerFunc { return func(c *gin.Context) { sessionID, err := cookieManager.Get(c.Request) var sess *session.Session if err == nil \u0026amp;\u0026amp; sessionID != \u0026#34;\u0026#34; { sess, err = manager.Get(c.Request.Context(), sessionID) if err != nil \u0026amp;\u0026amp; err != session.ErrSessionNotFound { c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;failed to load session\u0026#34;, }) c.Abort() return } } if sess == nil { sess, err = manager.Create(c.Request.Context()) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;failed to create session\u0026#34;, }) c.Abort() return } if err := cookieManager.Set(c.Writer, sess.ID); err != nil { c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;failed to set session cookie\u0026#34;, }) c.Abort() return } } else { if err := manager.Refresh(c.Request.Context(), sess); err != nil { c.JSON(http.StatusInternalServerError, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;failed to refresh session\u0026#34;, }) c.Abort() return } } c.Set(SessionKey, sess) c.Next() updatedSession, exists := c.Get(SessionKey) if exists { if s, ok := updatedSession.(*session.Session); ok { manager.Save(c.Request.Context(), s) } } } } func RequireAuth() gin.HandlerFunc { return func(c *gin.Context) { sess := GetSession(c) if sess == nil { c.JSON(http.StatusUnauthorized, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;authentication required\u0026#34;, }) c.Abort() return } userID, authenticated := sess.UserID, sess.UserID != 0 if !authenticated { c.JSON(http.StatusUnauthorized, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;authentication required\u0026#34;, }) c.Abort() return } c.Set(\u0026#34;user_id\u0026#34;, userID) c.Next() } } func GetSession(c *gin.Context) *session.Session { value, exists := c.Get(SessionKey) if !exists { return nil } sess, ok := value.(*session.Session) if !ok { return nil } return sess } The middleware implements sliding expiration by refreshing the session TTL on each request. This keeps active users logged in while allowing inactive sessions to expire. The pattern integrates cleanly with authentication handlers and provides transparent session access throughout the request lifecycle.\nImplementing Authentication Handlers Create handlers for login, logout, and profile access that demonstrate session-based authentication flows.\n// internal/handlers/auth.go package handlers import ( \u0026#34;net/http\u0026#34; \u0026#34;session-management-example/internal/middleware\u0026#34; \u0026#34;session-management-example/internal/session\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; ) type AuthHandler struct { sessionManager *session.Manager cookieManager *session.CookieManager users map[string]*User } type User struct { ID int `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Password string `json:\u0026#34;-\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type LoginRequest struct { Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } type RegisterRequest struct { Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required,min=6\u0026#34;` Name string `json:\u0026#34;name\u0026#34; binding:\u0026#34;required\u0026#34;` } func NewAuthHandler(sessionManager *session.Manager, cookieManager *session.CookieManager) *AuthHandler { handler := \u0026amp;AuthHandler{ sessionManager: sessionManager, cookieManager: cookieManager, users: make(map[string]*User), } hashedPassword, _ := bcrypt.GenerateFromPassword([]byte(\u0026#34;password123\u0026#34;), bcrypt.DefaultCost) handler.users[\u0026#34;demo@example.com\u0026#34;] = \u0026amp;User{ ID: 1, Email: \u0026#34;demo@example.com\u0026#34;, Password: string(hashedPassword), Name: \u0026#34;Demo User\u0026#34;, } return handler } func (h *AuthHandler) Register(c *gin.Context) { var req RegisterRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } if _, exists := h.users[req.Email]; exists { c.JSON(http.StatusConflict, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user already exists\u0026#34;}) return } hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to hash password\u0026#34;}) return } user := \u0026amp;User{ ID: len(h.users) + 1, Email: req.Email, Password: string(hashedPassword), Name: req.Name, } h.users[req.Email] = user sess := middleware.GetSession(c) sess.UserID = user.ID h.sessionManager.Set(sess, \u0026#34;email\u0026#34;, user.Email) h.sessionManager.Set(sess, \u0026#34;name\u0026#34;, user.Name) if err := h.sessionManager.Save(c.Request.Context(), sess); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to save session\u0026#34;}) return } c.JSON(http.StatusCreated, gin.H{ \u0026#34;user\u0026#34;: gin.H{ \u0026#34;id\u0026#34;: user.ID, \u0026#34;email\u0026#34;: user.Email, \u0026#34;name\u0026#34;: user.Name, }, \u0026#34;message\u0026#34;: \u0026#34;registration successful\u0026#34;, }) } func (h *AuthHandler) Login(c *gin.Context) { var req LoginRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } user, exists := h.users[req.Email] if !exists { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid credentials\u0026#34;}) return } if err := bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(req.Password)); err != nil { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid credentials\u0026#34;}) return } sess := middleware.GetSession(c) sess.UserID = user.ID h.sessionManager.Set(sess, \u0026#34;email\u0026#34;, user.Email) h.sessionManager.Set(sess, \u0026#34;name\u0026#34;, user.Name) if err := h.sessionManager.Save(c.Request.Context(), sess); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to save session\u0026#34;}) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: gin.H{ \u0026#34;id\u0026#34;: user.ID, \u0026#34;email\u0026#34;: user.Email, \u0026#34;name\u0026#34;: user.Name, }, \u0026#34;message\u0026#34;: \u0026#34;login successful\u0026#34;, }) } func (h *AuthHandler) Logout(c *gin.Context) { sess := middleware.GetSession(c) if sess != nil { if err := h.sessionManager.Destroy(c.Request.Context(), sess.ID); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to destroy session\u0026#34;}) return } } h.cookieManager.Delete(c.Writer) c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;logout successful\u0026#34;, }) } func (h *AuthHandler) Profile(c *gin.Context) { sess := middleware.GetSession(c) userID := sess.UserID var user *User for _, u := range h.users { if u.ID == userID { user = u break } } if user == nil { c.JSON(http.StatusNotFound, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user not found\u0026#34;}) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: gin.H{ \u0026#34;id\u0026#34;: user.ID, \u0026#34;email\u0026#34;: user.Email, \u0026#34;name\u0026#34;: user.Name, }, \u0026#34;session\u0026#34;: gin.H{ \u0026#34;id\u0026#34;: sess.ID, \u0026#34;created_at\u0026#34;: sess.CreatedAt, \u0026#34;expires_at\u0026#34;: sess.ExpiresAt, \u0026#34;last_access\u0026#34;: sess.LastAccess, }, }) } func (h *AuthHandler) CheckAuth(c *gin.Context) { sess := middleware.GetSession(c) authenticated := sess != nil \u0026amp;\u0026amp; sess.UserID != 0 c.JSON(http.StatusOK, gin.H{ \u0026#34;authenticated\u0026#34;: authenticated, \u0026#34;user_id\u0026#34;: sess.UserID, }) } Login handlers set the user ID in the session upon successful authentication. This pattern works with any authentication method including passwords, OAuth, or multi-factor authentication. The session stores user identity, eliminating database queries on every request while maintaining security through Redis storage and cookie protection.\nCreating the Main Server Application Wire everything together in the main server that configures dependencies, sets up routes, and starts the application.\n// cmd/server/main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; \u0026#34;session-management-example/internal/handlers\u0026#34; \u0026#34;session-management-example/internal/middleware\u0026#34; \u0026#34;session-management-example/internal/session\u0026#34; redisclient \u0026#34;session-management-example/pkg/redis\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; ) func main() { if err := godotenv.Load(); err != nil { log.Println(\u0026#34;No .env file found, using environment variables\u0026#34;) } redisConfig := \u0026amp;redisclient.Config{ Host: getEnv(\u0026#34;REDIS_HOST\u0026#34;, \u0026#34;localhost\u0026#34;), Port: getEnv(\u0026#34;REDIS_PORT\u0026#34;, \u0026#34;6379\u0026#34;), Password: getEnv(\u0026#34;REDIS_PASSWORD\u0026#34;, \u0026#34;\u0026#34;), DB: getEnvAsInt(\u0026#34;REDIS_DB\u0026#34;, 0), PoolSize: getEnvAsInt(\u0026#34;REDIS_POOL_SIZE\u0026#34;, 10), MinIdleConns: getEnvAsInt(\u0026#34;REDIS_MIN_IDLE\u0026#34;, 5), DialTimeout: 5 * time.Second, ReadTimeout: 3 * time.Second, WriteTimeout: 3 * time.Second, } redisClient, err := redisclient.NewClient(redisConfig) if err != nil { log.Fatalf(\u0026#34;Failed to connect to Redis: %v\u0026#34;, err) } defer redisClient.Close() sessionMaxAge := getEnvAsInt(\u0026#34;SESSION_MAX_AGE\u0026#34;, 3600) sessionManager := session.NewManager( redisClient.GetClient(), \u0026#34;session\u0026#34;, time.Duration(sessionMaxAge)*time.Second, ) hashKey := []byte(getEnv(\u0026#34;COOKIE_HASH_KEY\u0026#34;, \u0026#34;very-secret-hash-key-32-bytes!!\u0026#34;)) blockKey := []byte(getEnv(\u0026#34;COOKIE_BLOCK_KEY\u0026#34;, \u0026#34;secret-block-key-16-bytes!\u0026#34;)) cookieConfig := \u0026amp;session.CookieConfig{ Name: getEnv(\u0026#34;SESSION_NAME\u0026#34;, \u0026#34;session_id\u0026#34;), HashKey: hashKey, BlockKey: blockKey, MaxAge: sessionMaxAge, Secure: getEnv(\u0026#34;COOKIE_SECURE\u0026#34;, \u0026#34;false\u0026#34;) == \u0026#34;true\u0026#34;, HttpOnly: true, SameSite: http.SameSiteStrictMode, Domain: getEnv(\u0026#34;COOKIE_DOMAIN\u0026#34;, \u0026#34;\u0026#34;), Path: \u0026#34;/\u0026#34;, } cookieManager := session.NewCookieManager(cookieConfig) router := gin.Default() router.Use(middleware.SessionMiddleware(sessionManager, cookieManager)) authHandler := handlers.NewAuthHandler(sessionManager, cookieManager) router.POST(\u0026#34;/api/auth/register\u0026#34;, authHandler.Register) router.POST(\u0026#34;/api/auth/login\u0026#34;, authHandler.Login) router.POST(\u0026#34;/api/auth/logout\u0026#34;, authHandler.Logout) router.GET(\u0026#34;/api/auth/check\u0026#34;, authHandler.CheckAuth) protected := router.Group(\u0026#34;/api\u0026#34;) protected.Use(middleware.RequireAuth()) { protected.GET(\u0026#34;/profile\u0026#34;, authHandler.Profile) protected.GET(\u0026#34;/dashboard\u0026#34;, func(c *gin.Context) { sess := middleware.GetSession(c) c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Welcome to your dashboard\u0026#34;, \u0026#34;user_id\u0026#34;: sess.UserID, \u0026#34;session\u0026#34;: gin.H{ \u0026#34;created_at\u0026#34;: sess.CreatedAt, \u0026#34;expires_at\u0026#34;: sess.ExpiresAt, \u0026#34;last_access\u0026#34;: sess.LastAccess, }, }) }) } router.GET(\u0026#34;/api/health\u0026#34;, func(c *gin.Context) { if err := redisClient.HealthCheck(c.Request.Context()); err != nil { c.JSON(http.StatusServiceUnavailable, gin.H{ \u0026#34;status\u0026#34;: \u0026#34;unhealthy\u0026#34;, \u0026#34;redis\u0026#34;: \u0026#34;disconnected\u0026#34;, }) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;redis\u0026#34;: \u0026#34;connected\u0026#34;, }) }) port := getEnv(\u0026#34;PORT\u0026#34;, \u0026#34;8080\u0026#34;) log.Printf(\u0026#34;Server starting on port %s\u0026#34;, port) if err := router.Run(\u0026#34;:\u0026#34; + port); err != nil { log.Fatalf(\u0026#34;Failed to start server: %v\u0026#34;, err) } } func getEnv(key, defaultValue string) string { if value := os.Getenv(key); value != \u0026#34;\u0026#34; { return value } return defaultValue } func getEnvAsInt(key string, defaultValue int) int { if value := os.Getenv(key); value != \u0026#34;\u0026#34; { if intValue, err := strconv.Atoi(value); err == nil { return intValue } } return defaultValue } The server configures secure defaults for production use while allowing environment-based customization. Session middleware applies globally, providing transparent session access to all routes. Protected routes use the RequireAuth middleware to enforce authentication.\nTesting the Session System Test your session management implementation by making requests to the authentication endpoints and verifying session behavior.\n# Register a new user curl -X POST http://localhost:8080/api/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Test User\u0026#34; }\u0026#39; \\ -c cookies.txt # Check authentication status curl -X GET http://localhost:8080/api/auth/check \\ -b cookies.txt # Access protected profile endpoint curl -X GET http://localhost:8080/api/profile \\ -b cookies.txt # Login with existing user curl -X POST http://localhost:8080/api/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;demo@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password123\u0026#34; }\u0026#39; \\ -c cookies.txt # Access dashboard curl -X GET http://localhost:8080/api/dashboard \\ -b cookies.txt # Logout curl -X POST http://localhost:8080/api/auth/logout \\ -b cookies.txt \\ -c cookies.txt # Try accessing protected route after logout (should fail) curl -X GET http://localhost:8080/api/profile \\ -b cookies.txt The -c cookies.txt flag saves cookies to a file, while -b cookies.txt sends cookies from that file. This simulates browser behavior where cookies persist across requests.\nImplementing Session Flash Messages Flash messages store one-time notifications that display once and then disappear. This pattern works perfectly for success messages, error notifications, and form validation feedback.\n// internal/session/flash.go package session const ( FlashSuccess = \u0026#34;flash_success\u0026#34; FlashError = \u0026#34;flash_error\u0026#34; FlashWarning = \u0026#34;flash_warning\u0026#34; FlashInfo = \u0026#34;flash_info\u0026#34; ) func SetFlash(sess *Session, flashType string, message string) { if sess.Data == nil { sess.Data = make(map[string]interface{}) } sess.Data[flashType] = message } func GetFlash(sess *Session, flashType string) (string, bool) { if sess.Data == nil { return \u0026#34;\u0026#34;, false } message, exists := sess.Data[flashType] if !exists { return \u0026#34;\u0026#34;, false } delete(sess.Data, flashType) if str, ok := message.(string); ok { return str, true } return \u0026#34;\u0026#34;, false } func GetAllFlashes(sess *Session) map[string]string { flashes := make(map[string]string) flashTypes := []string{FlashSuccess, FlashError, FlashWarning, FlashInfo} for _, flashType := range flashTypes { if message, exists := GetFlash(sess, flashType); exists { flashes[flashType] = message } } return flashes } Flash messages automatically delete after retrieval, ensuring they only display once. This prevents duplicate notifications when users refresh pages or navigate back.\nAdvanced Session Security Patterns Implement session fixation prevention by regenerating session IDs after authentication state changes. This prevents attackers from hijacking sessions by pre-setting session IDs.\nfunc (m *Manager) Regenerate(ctx context.Context, oldSession *Session) (*Session, error) { newSessionID := uuid.New().String() now := time.Now() newSession := \u0026amp;Session{ ID: newSessionID, UserID: oldSession.UserID, Data: oldSession.Data, CreatedAt: now, ExpiresAt: now.Add(m.maxAge), LastAccess: now, } if err := m.Save(ctx, newSession); err != nil { return nil, err } if err := m.Destroy(ctx, oldSession.ID); err != nil { return newSession, nil } return newSession, nil } Call session regeneration after login, privilege escalation, or any security-sensitive operation. Update the cookie with the new session ID to maintain user context while preventing session fixation attacks.\nImplement concurrent session tracking to prevent account sharing or detect compromised credentials. Store session metadata including IP addresses, user agents, and creation times.\ntype SessionMetadata struct { IPAddress string `json:\u0026#34;ip_address\u0026#34;` UserAgent string `json:\u0026#34;user_agent\u0026#34;` Location string `json:\u0026#34;location,omitempty\u0026#34;` DeviceType string `json:\u0026#34;device_type,omitempty\u0026#34;` } func (m *Manager) CreateWithMetadata(ctx context.Context, metadata *SessionMetadata) (*Session, error) { sess, err := m.Create(ctx) if err != nil { return nil, err } sess.Data[\u0026#34;metadata\u0026#34;] = metadata if err := m.Save(ctx, sess); err != nil { return nil, err } return sess, nil } func (m *Manager) GetUserSessions(ctx context.Context, userID int) ([]*Session, error) { pattern := fmt.Sprintf(\u0026#34;%s:*\u0026#34;, m.prefix) keys, err := m.client.Keys(ctx, pattern).Result() if err != nil { return nil, err } var sessions []*Session for _, key := range keys { data, err := m.client.Get(ctx, key).Bytes() if err != nil { continue } var sess Session if err := json.Unmarshal(data, \u0026amp;sess); err != nil { continue } if sess.UserID == userID { sessions = append(sessions, \u0026amp;sess) } } return sessions, nil } Display active sessions to users and allow them to revoke specific sessions. This provides transparency and control over account security.\nHandling Session Edge Cases Handle Redis unavailability gracefully to prevent complete application failure. Implement fallback behavior that allows the application to continue operating with degraded functionality.\nfunc (h *AuthHandler) LoginWithFallback(c *gin.Context) { var req LoginRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } user, valid := h.validateCredentials(req.Email, req.Password) if !valid { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid credentials\u0026#34;}) return } sess := middleware.GetSession(c) if sess != nil { sess.UserID = user.ID if err := h.sessionManager.Save(c.Request.Context(), sess); err != nil { log.Printf(\u0026#34;Failed to save session to Redis: %v\u0026#34;, err) c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: user, \u0026#34;message\u0026#34;: \u0026#34;login successful (session storage degraded)\u0026#34;, \u0026#34;warning\u0026#34;: \u0026#34;some features may be limited\u0026#34;, }) return } } c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: user, \u0026#34;message\u0026#34;: \u0026#34;login successful\u0026#34;, }) } Implement session cleanup for expired sessions to prevent Redis memory exhaustion. While Redis automatically removes expired keys, periodic cleanup handles edge cases and maintains data hygiene.\nfunc (m *Manager) CleanupExpiredSessions(ctx context.Context) error { pattern := fmt.Sprintf(\u0026#34;%s:*\u0026#34;, m.prefix) keys, err := m.client.Keys(ctx, pattern).Result() if err != nil { return err } var expiredCount int for _, key := range keys { data, err := m.client.Get(ctx, key).Bytes() if err != nil { continue } var sess Session if err := json.Unmarshal(data, \u0026amp;sess); err != nil { continue } if time.Now().After(sess.ExpiresAt) { if err := m.client.Del(ctx, key).Err(); err == nil { expiredCount++ } } } log.Printf(\u0026#34;Cleaned up %d expired sessions\u0026#34;, expiredCount) return nil } Run cleanup periodically using a background goroutine or scheduled task. This maintenance ensures consistent behavior and prevents unexpected Redis memory usage spikes.\nProduction Deployment Considerations Configure Redis persistence to survive server restarts without losing active sessions. Use RDB snapshots for periodic backups and AOF (Append-Only File) for maximum durability.\n# redis.conf save 900 1 save 300 10 save 60 10000 appendonly yes appendfsync everysec RDB snapshots create periodic backups at configurable intervals, while AOF logs every write operation for point-in-time recovery. The combination provides both performance and durability.\nImplement Redis Sentinel for high availability in production. Sentinel monitors Redis instances, performs automatic failover during failures, and provides service discovery for clients.\nfunc NewSentinelClient(cfg *Config) (*Client, error) { rdb := redis.NewFailoverClient(\u0026amp;redis.FailoverOptions{ MasterName: cfg.MasterName, SentinelAddrs: cfg.SentinelAddrs, Password: cfg.Password, DB: cfg.DB, PoolSize: cfg.PoolSize, MinIdleConns: cfg.MinIdleConns, DialTimeout: cfg.DialTimeout, ReadTimeout: cfg.ReadTimeout, WriteTimeout: cfg.WriteTimeout, }) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := rdb.Ping(ctx).Err(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to connect to Redis Sentinel: %w\u0026#34;, err) } return \u0026amp;Client{rdb: rdb}, nil } Monitor session metrics to understand usage patterns and identify issues early. Track active sessions, creation rate, expiration rate, and Redis performance metrics.\ntype SessionMetrics struct { ActiveSessions int64 CreatedToday int64 ExpiredToday int64 AverageLifetime time.Duration RedisConnections int } func (m *Manager) GetMetrics(ctx context.Context) (*SessionMetrics, error) { pattern := fmt.Sprintf(\u0026#34;%s:*\u0026#34;, m.prefix) keys, err := m.client.Keys(ctx, pattern).Result() if err != nil { return nil, err } metrics := \u0026amp;SessionMetrics{ ActiveSessions: int64(len(keys)), } stats := m.client.PoolStats() metrics.RedisConnections = int(stats.TotalConns) return metrics, nil } Expose metrics through a monitoring endpoint or integrate with observability platforms like Prometheus, Datadog, or New Relic to track session behavior in production.\nIntegrating with Authentication Systems Combine session management with JWT authentication from our JWT guide to create hybrid systems that use the benefits of both approaches.\nfunc (h *AuthHandler) LoginWithJWT(c *gin.Context) { var req LoginRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } user, valid := h.validateCredentials(req.Email, req.Password) if !valid { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid credentials\u0026#34;}) return } accessToken, err := h.jwtService.GenerateToken(user) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to generate token\u0026#34;}) return } sess := middleware.GetSession(c) sess.UserID = user.ID h.sessionManager.Set(sess, \u0026#34;jwt\u0026#34;, accessToken) if err := h.sessionManager.Save(c.Request.Context(), sess); err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to save session\u0026#34;}) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: user, \u0026#34;access_token\u0026#34;: accessToken, \u0026#34;session_id\u0026#34;: sess.ID, }) } Store JWT tokens in sessions for server-side rendering applications while still enabling API authentication. This approach provides flexibility for different client types while maintaining centralized session management.\nUse Redis caching patterns from our Redis caching guide to optimize session-related database queries and improve overall application performance.\nConclusion Session management forms the backbone of user authentication and state management in web applications. The combination of secure cookies and Redis storage provides a reliable, scalable solution that handles millions of concurrent sessions while maintaining excellent performance and security. The patterns demonstrated in this guide create production-ready session systems that work reliably across distributed architectures.\nProper cookie configuration with HttpOnly, Secure, and SameSite flags protects against common web vulnerabilities including XSS and CSRF attacks. Redis provides fast session access with automatic expiration, eliminating manual cleanup and scaling horizontally across multiple instances. Session regeneration prevents fixation attacks while sliding expiration balances security with user experience.\nRemember that session management integrates with broader application architecture including authentication systems , caching strategies , and database connections . Monitor session metrics in production, implement proper Redis high availability, and regularly review security configurations to maintain reliable session management as your application grows and threat models evolve.\n","href":"/2025/10/how-to-implement-session-management-in-go-cookies-and-redis.html","title":"How to Implement Session Management in Go - Cookies and Redis Tutorial"},{"content":"Ever wanted to build a real-time chat app, live notification system, or multiplayer game? WebSocket is your answer. Unlike regular HTTP where clients have to constantly ask \u0026ldquo;got any updates?\u0026rdquo;, WebSocket keeps a persistent connection open so the server can push data whenever it wants. No more polling, no more delays\u0026ndash;just instant, bidirectional communication.\nIn this tutorial, we\u0026rsquo;re building a production-ready chat application from scratch using Go and the gorilla/websocket package. By the end, you\u0026rsquo;ll have a working chat app where multiple users can send messages in real-time.\nHere\u0026rsquo;s what we\u0026rsquo;ll cover:\nUnderstanding WebSocket and when you actually need it Building your first WebSocket server Managing multiple connections with goroutines and channels The Hub pattern for broadcasting messages Production-ready features (auth, rate limiting, graceful shutdown) Deployment tips and security best practices What is WebSocket? Think of WebSocket as a phone call, while HTTP is like sending letters back and forth. With HTTP, the client always has to initiate a request\u0026ndash;\u0026ldquo;Hey server, got anything new?\u0026rdquo; With WebSocket, once the connection is established, both sides can send data anytime. No more asking. Just instant communication.\nWebSocket uses a single, long-lived TCP connection that stays open. This is way more efficient than opening a new HTTP connection for every update.\nWhen to use WebSocket:\nReal-time chat apps (we\u0026rsquo;re building one today!) Live notifications and alerts Collaborative editing (Google Docs style) Live dashboards showing metrics Multiplayer games Trading platforms that need instant price updates IoT devices sending/receiving data When you DON\u0026rsquo;T need WebSocket:\nBuilding a regular REST API - stick with HTTP Updates happen every few minutes - polling works fine Serving static content SEO matters - search engines can\u0026rsquo;t crawl WebSocket content Installing gorilla/websocket First, install the gorilla/websocket package:\ngo get github.com/gorilla/websocket Create a new project:\nmkdir websocket-chat cd websocket-chat go mod init websocket-chat Building a Basic WebSocket Server Let\u0026rsquo;s start simple. Here\u0026rsquo;s an echo server that just bounces back whatever you send it:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/gorilla/websocket\u0026#34; ) // Upgrader upgrades HTTP connections to WebSocket var upgrader = websocket.Upgrader{ ReadBufferSize: 1024, WriteBufferSize: 1024, // Allow all origins for development (restrict in production!) CheckOrigin: func(r *http.Request) bool { return true }, } func handleWebSocket(w http.ResponseWriter, r *http.Request) { // Upgrade HTTP connection to WebSocket conn, err := upgrader.Upgrade(w, r, nil) if err != nil { log.Println(\u0026#34;Upgrade error:\u0026#34;, err) return } defer conn.Close() log.Println(\u0026#34;Client connected\u0026#34;) // Read messages in a loop for { messageType, message, err := conn.ReadMessage() if err != nil { log.Println(\u0026#34;Read error:\u0026#34;, err) break } log.Printf(\u0026#34;Received: %s\u0026#34;, message) // Echo the message back err = conn.WriteMessage(messageType, message) if err != nil { log.Println(\u0026#34;Write error:\u0026#34;, err) break } } log.Println(\u0026#34;Client disconnected\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/ws\u0026#34;, handleWebSocket) log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatal(err) } } What\u0026rsquo;s happening here:\nupgrader.Upgrade() converts a regular HTTP connection to WebSocket conn.ReadMessage() waits for incoming messages (it blocks the goroutine) conn.WriteMessage() sends data back to the client CheckOrigin checks where requests come from - set to allow all for now, but lock this down in production! Creating a WebSocket Client (HTML/JavaScript) Create a simple HTML client to test the server:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;WebSocket Chat\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;WebSocket Echo Test\u0026lt;/h1\u0026gt; \u0026lt;input id=\u0026#34;messageInput\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Type a message\u0026#34; /\u0026gt; \u0026lt;button onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;div id=\u0026#34;messages\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; const ws = new WebSocket(\u0026#39;ws://localhost:8080/ws\u0026#39;); ws.onopen = () =\u0026gt; { console.log(\u0026#39;Connected to server\u0026#39;); addMessage(\u0026#39;Connected to server\u0026#39;, \u0026#39;system\u0026#39;); }; ws.onmessage = (event) =\u0026gt; { console.log(\u0026#39;Received:\u0026#39;, event.data); addMessage(event.data, \u0026#39;received\u0026#39;); }; ws.onclose = () =\u0026gt; { console.log(\u0026#39;Disconnected from server\u0026#39;); addMessage(\u0026#39;Disconnected from server\u0026#39;, \u0026#39;system\u0026#39;); }; ws.onerror = (error) =\u0026gt; { console.error(\u0026#39;WebSocket error:\u0026#39;, error); }; function sendMessage() { const input = document.getElementById(\u0026#39;messageInput\u0026#39;); const message = input.value; if (message) { ws.send(message); addMessage(message, \u0026#39;sent\u0026#39;); input.value = \u0026#39;\u0026#39;; } } function addMessage(text, type) { const messagesDiv = document.getElementById(\u0026#39;messages\u0026#39;); const messageEl = document.createElement(\u0026#39;div\u0026#39;); messageEl.style.color = type === \u0026#39;system\u0026#39; ? \u0026#39;gray\u0026#39; : (type === \u0026#39;sent\u0026#39; ? \u0026#39;blue\u0026#39; : \u0026#39;green\u0026#39;); messageEl.textContent = text; messagesDiv.appendChild(messageEl); } // Allow Enter key to send message document.getElementById(\u0026#39;messageInput\u0026#39;).addEventListener(\u0026#39;keypress\u0026#39;, (e) =\u0026gt; { if (e.key === \u0026#39;Enter\u0026#39;) sendMessage(); }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Save this as index.html, fire up your Go server (go run .), and open the HTML in your browser. Type a message, hit send, and watch it bounce right back. Pretty cool, right?\nBuilding a Real-time Chat Application The echo server is cute, but let\u0026rsquo;s build something real. We need to handle multiple users chatting at the same time, which means managing lots of connections and broadcasting messages to everyone.\nArchitecture Overview We\u0026rsquo;re using the Hub pattern. Think of it as a chat room manager:\nHub - The brain. It tracks all connected users and routes messages Client - Wraps each WebSocket connection with read/write logic Broadcasting - When one user sends a message, the Hub pushes it to everyone This pattern leverages Go\u0026rsquo;s goroutines and channels beautifully. Each client gets two goroutines\u0026ndash;one for reading, one for writing. They communicate with the Hub through channels.\nThe Hub (Connection Manager) // hub.go package main import ( \u0026#34;log\u0026#34; \u0026#34;sync\u0026#34; ) type Hub struct { // Registered clients clients map[*Client]bool // Inbound messages from clients broadcast chan []byte // Register requests from clients register chan *Client // Unregister requests from clients unregister chan *Client // Mutex for thread-safe operations (learn more: https://www.buanacoding.com/2025/10/synchronizing-goroutines-in-go-using-syncmutex-and-synconce.html) mu sync.RWMutex } func NewHub() *Hub { return \u0026amp;Hub{ clients: make(map[*Client]bool), broadcast: make(chan []byte, 256), register: make(chan *Client), unregister: make(chan *Client), } } func (h *Hub) Run() { for { select { case client := \u0026lt;-h.register: h.mu.Lock() h.clients[client] = true h.mu.Unlock() log.Printf(\u0026#34;Client registered. Total clients: %d\u0026#34;, len(h.clients)) case client := \u0026lt;-h.unregister: h.mu.Lock() if _, ok := h.clients[client]; ok { delete(h.clients, client) close(client.send) log.Printf(\u0026#34;Client unregistered. Total clients: %d\u0026#34;, len(h.clients)) } h.mu.Unlock() case message := \u0026lt;-h.broadcast: h.mu.RLock() for client := range h.clients { select { case client.send \u0026lt;- message: // Message sent successfully default: // Client\u0026#39;s send channel is full, close it close(client.send) delete(h.clients, client) } } h.mu.RUnlock() } } } The Client Wrapper // client.go package main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gorilla/websocket\u0026#34; ) const ( // Time allowed to write a message to the peer writeWait = 10 * time.Second // Time allowed to read the next pong message from the peer pongWait = 60 * time.Second // Send pings to peer with this period (must be less than pongWait) pingPeriod = (pongWait * 9) / 10 // Maximum message size allowed from peer maxMessageSize = 512 ) type Client struct { hub *Hub conn *websocket.Conn send chan []byte } func NewClient(hub *Hub, conn *websocket.Conn) *Client { return \u0026amp;Client{ hub: hub, conn: conn, send: make(chan []byte, 256), } } // readPump pumps messages from the WebSocket connection to the hub func (c *Client) readPump() { defer func() { c.hub.unregister \u0026lt;- c c.conn.Close() }() c.conn.SetReadDeadline(time.Now().Add(pongWait)) c.conn.SetPongHandler(func(string) error { c.conn.SetReadDeadline(time.Now().Add(pongWait)) return nil }) c.conn.SetReadLimit(maxMessageSize) for { _, message, err := c.conn.ReadMessage() if err != nil { if websocket.IsUnexpectedCloseError(err, websocket.CloseGoingAway, websocket.CloseAbnormalClosure) { log.Printf(\u0026#34;error: %v\u0026#34;, err) } break } // Broadcast the message to all clients c.hub.broadcast \u0026lt;- message } } // writePump pumps messages from the hub to the WebSocket connection func (c *Client) writePump() { ticker := time.NewTicker(pingPeriod) defer func() { ticker.Stop() c.conn.Close() }() for { select { case message, ok := \u0026lt;-c.send: c.conn.SetWriteDeadline(time.Now().Add(writeWait)) if !ok { // The hub closed the channel c.conn.WriteMessage(websocket.CloseMessage, []byte{}) return } w, err := c.conn.NextWriter(websocket.TextMessage) if err != nil { return } w.Write(message) // Add queued messages to the current message n := len(c.send) for i := 0; i \u0026lt; n; i++ { w.Write([]byte{\u0026#39;\\n\u0026#39;}) w.Write(\u0026lt;-c.send) } if err := w.Close(); err != nil { return } case \u0026lt;-ticker.C: c.conn.SetWriteDeadline(time.Now().Add(writeWait)) if err := c.conn.WriteMessage(websocket.PingMessage, nil); err != nil { return } } } } The Main Server // main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/gorilla/websocket\u0026#34; ) var upgrader = websocket.Upgrader{ ReadBufferSize: 1024, WriteBufferSize: 1024, CheckOrigin: func(r *http.Request) bool { // In production, validate the origin properly return true }, } func serveWs(hub *Hub, w http.ResponseWriter, r *http.Request) { conn, err := upgrader.Upgrade(w, r, nil) if err != nil { log.Println(err) return } client := NewClient(hub, conn) client.hub.register \u0026lt;- client // Start goroutines for reading and writing go client.writePump() go client.readPump() } func main() { hub := NewHub() go hub.Run() http.HandleFunc(\u0026#34;/ws\u0026#34;, func(w http.ResponseWriter, r *http.Request) { serveWs(hub, w, r) }) // Serve static files (your HTML/CSS/JS) http.Handle(\u0026#34;/\u0026#34;, http.FileServer(http.Dir(\u0026#34;./static\u0026#34;))) log.Println(\u0026#34;Chat server starting on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatal(\u0026#34;ListenAndServe:\u0026#34;, err) } } Enhanced Chat Client (HTML) Create static/index.html:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Real-time Chat\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; * { margin: 0; padding: 0; box-sizing: border-box; } body { font-family: Arial, sans-serif; background: #f0f0f0; } .chat-container { max-width: 800px; margin: 20px auto; background: white; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); overflow: hidden; } .chat-header { background: #0084ff; color: white; padding: 20px; text-align: center; } #messages { height: 400px; overflow-y: auto; padding: 20px; display: flex; flex-direction: column; } .message { margin-bottom: 10px; padding: 10px 15px; border-radius: 20px; max-width: 70%; word-wrap: break-word; } .message.sent { background: #0084ff; color: white; align-self: flex-end; } .message.received { background: #e4e6eb; color: black; align-self: flex-start; } .message.system { background: #ffeaa7; color: #333; align-self: center; font-size: 12px; } .chat-input { display: flex; padding: 20px; border-top: 1px solid #ddd; } #messageInput { flex: 1; padding: 12px; border: 1px solid #ddd; border-radius: 25px; outline: none; font-size: 14px; } #sendBtn { margin-left: 10px; padding: 12px 30px; background: #0084ff; color: white; border: none; border-radius: 25px; cursor: pointer; font-size: 14px; } #sendBtn:hover { background: #0073e6; } #sendBtn:disabled { background: #ccc; cursor: not-allowed; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;chat-container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;chat-header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Real-time Chat\u0026lt;/h1\u0026gt; \u0026lt;p id=\u0026#34;status\u0026#34;\u0026gt;Connecting...\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026#34;messages\u0026#34;\u0026gt;\u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;chat-input\u0026#34;\u0026gt; \u0026lt;input id=\u0026#34;messageInput\u0026#34; type=\u0026#34;text\u0026#34; placeholder=\u0026#34;Type a message...\u0026#34; disabled /\u0026gt; \u0026lt;button id=\u0026#34;sendBtn\u0026#34; disabled\u0026gt;Send\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; let ws; const messagesDiv = document.getElementById(\u0026#39;messages\u0026#39;); const messageInput = document.getElementById(\u0026#39;messageInput\u0026#39;); const sendBtn = document.getElementById(\u0026#39;sendBtn\u0026#39;); const statusEl = document.getElementById(\u0026#39;status\u0026#39;); function connect() { ws = new WebSocket(\u0026#39;ws://localhost:8080/ws\u0026#39;); ws.onopen = () =\u0026gt; { console.log(\u0026#39;Connected\u0026#39;); statusEl.textContent = \u0026#39;Connected\u0026#39;; statusEl.style.color = \u0026#39;#00ff00\u0026#39;; messageInput.disabled = false; sendBtn.disabled = false; addMessage(\u0026#39;You joined the chat\u0026#39;, \u0026#39;system\u0026#39;); }; ws.onmessage = (event) =\u0026gt; { addMessage(event.data, \u0026#39;received\u0026#39;); }; ws.onclose = () =\u0026gt; { console.log(\u0026#39;Disconnected\u0026#39;); statusEl.textContent = \u0026#39;Disconnected - Reconnecting...\u0026#39;; statusEl.style.color = \u0026#39;#ff0000\u0026#39;; messageInput.disabled = true; sendBtn.disabled = true; addMessage(\u0026#39;Connection lost. Reconnecting...\u0026#39;, \u0026#39;system\u0026#39;); // Reconnect after 3 seconds setTimeout(connect, 3000); }; ws.onerror = (error) =\u0026gt; { console.error(\u0026#39;WebSocket error:\u0026#39;, error); }; } function sendMessage() { const message = messageInput.value.trim(); if (message \u0026amp;\u0026amp; ws.readyState === WebSocket.OPEN) { ws.send(message); addMessage(message, \u0026#39;sent\u0026#39;); messageInput.value = \u0026#39;\u0026#39;; } } function addMessage(text, type) { const messageEl = document.createElement(\u0026#39;div\u0026#39;); messageEl.className = `message ${type}`; messageEl.textContent = text; messagesDiv.appendChild(messageEl); messagesDiv.scrollTop = messagesDiv.scrollHeight; } sendBtn.addEventListener(\u0026#39;click\u0026#39;, sendMessage); messageInput.addEventListener(\u0026#39;keypress\u0026#39;, (e) =\u0026gt; { if (e.key === \u0026#39;Enter\u0026#39;) sendMessage(); }); // Connect on page load connect(); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Running the Chat Application Time to see it in action:\nSave all three Go files (hub.go, client.go, main.go) in your project root Create a static/ folder and drop the index.html in there Run the server: go run . Open http://localhost:8080 in multiple browser tabs/windows Start chatting! Messages appear instantly across all tabs Try opening it on your phone too (use your local IP like 192.168.1.x:8080). Watch messages fly between devices in real-time. That\u0026rsquo;s the power of WebSocket!\nProduction Best Practices 1. Origin Validation In production, restrict allowed origins:\nvar upgrader = websocket.Upgrader{ CheckOrigin: func(r *http.Request) bool { origin := r.Header.Get(\u0026#34;Origin\u0026#34;) return origin == \u0026#34;https://yourdomain.com\u0026#34; }, } 2. Authentication Never trust anonymous connections in production. Validate JWT tokens before the upgrade:\nfunc serveWs(hub *Hub, w http.ResponseWriter, r *http.Request) { // Validate JWT token from query param or cookie token := r.URL.Query().Get(\u0026#34;token\u0026#34;) if !validateToken(token) { http.Error(w, \u0026#34;Unauthorized\u0026#34;, http.StatusUnauthorized) return } conn, err := upgrader.Upgrade(w, r, nil) // ... rest of the code } You can also check auth via cookies or custom headers. The key is: authenticate BEFORE upgrading to WebSocket, not after.\n3. Rate Limiting Someone will try to spam your chat. Guaranteed. Add simple rate limiting to keep things civil:\ntype Client struct { // ... existing fields lastMessageTime time.Time messageCount int } func (c *Client) readPump() { // ... setup code for { _, message, err := c.conn.ReadMessage() if err != nil { break } // Rate limiting: max 10 messages per second now := time.Now() if now.Sub(c.lastMessageTime) \u0026lt; time.Second { c.messageCount++ if c.messageCount \u0026gt; 10 { log.Println(\u0026#34;Rate limit exceeded\u0026#34;) continue } } else { c.messageCount = 0 c.lastMessageTime = now } c.hub.broadcast \u0026lt;- message } } 4. Message Size Limits We already set maxMessageSize = 512 bytes earlier. Adjust this based on your use case. Text chat? 512 is plenty. Sharing code snippets? Bump it to 4096 or higher. Just don\u0026rsquo;t allow unlimited sizes\u0026ndash;someone will send the entire Bee Movie script and crash your server.\n5. Graceful Shutdown When deploying updates, don\u0026rsquo;t just kill the server. Give active connections time to close properly. Here\u0026rsquo;s how to handle shutdown signals gracefully using context :\nfunc main() { hub := NewHub() go hub.Run() // ... setup routes server := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, } // Handle shutdown signals go func() { sigint := make(chan os.Signal, 1) signal.Notify(sigint, os.Interrupt, syscall.SIGTERM) \u0026lt;-sigint log.Println(\u0026#34;Shutting down server...\u0026#34;) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := server.Shutdown(ctx); err != nil { log.Printf(\u0026#34;Server shutdown error: %v\u0026#34;, err) } }() log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := server.ListenAndServe(); err != http.ErrServerClosed { log.Fatal(err) } } 6. TLS/WSS in Production Never run WebSocket over plain ws:// in production. Always use secure WebSocket (wss://) with TLS certificates:\n// Use Let\u0026#39;s Encrypt certificates log.Fatal(http.ListenAndServeTLS(\u0026#34;:443\u0026#34;, \u0026#34;cert.pem\u0026#34;, \u0026#34;key.pem\u0026#34;, nil)) Update your client to use wss://:\nconst ws = new WebSocket(\u0026#39;wss://yourdomain.com/ws\u0026#39;); Without TLS, anyone on the network can read your chat messages. Not cool.\nTesting the Application Testing WebSocket is tricky since it involves persistent connections. Here\u0026rsquo;s a basic test to get you started:\n// hub_test.go package main import ( \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; ) func TestHubBroadcast(t *testing.T) { hub := NewHub() go hub.Run() // Simulate message broadcast testMessage := []byte(\u0026#34;Hello, World!\u0026#34;) hub.broadcast \u0026lt;- testMessage // Give time for processing time.Sleep(100 * time.Millisecond) // In real tests, you\u0026#39;d verify client received the message // This is a simplified example } For more thorough testing, check out Testing in Go and use the httptest package to simulate WebSocket upgrades.\nMonitoring and Debugging Want to know how many people are connected? Add a simple stats endpoint:\nfunc (h *Hub) GetStats() map[string]int { h.mu.RLock() defer h.mu.RUnlock() return map[string]int{ \u0026#34;clients\u0026#34;: len(h.clients), } } // Add an HTTP endpoint for stats http.HandleFunc(\u0026#34;/stats\u0026#34;, func(w http.ResponseWriter, r *http.Request) { stats := hub.GetStats() json.NewEncoder(w).Encode(stats) }) Hit /stats and you\u0026rsquo;ll get JSON like {\u0026quot;clients\u0026quot;: 42}. In production, pipe this to your monitoring system (Prometheus, Datadog, whatever you use). Track connection counts, message rates, and errors. If you see connections spiking or messages slowing down, you\u0026rsquo;ll know before users start complaining.\nConclusion You just built a real-time chat app from scratch! The Hub pattern keeps things organized, goroutines and channels handle the concurrency, and with a few production tweaks, this code is ready for the real world.\nWhat you learned:\nWebSocket basics and when to use them Building servers with gorilla/websocket The Hub pattern for managing multiple connections Using goroutines for concurrent read/write operations Production essentials: auth, rate limiting, graceful shutdown Security with origin validation and TLS/WSS Take it further:\nAdd user authentication and private rooms Store message history in a database like MongoDB or PostgreSQL Scale horizontally with Redis Pub/Sub Show who\u0026rsquo;s online (presence detection) Add typing indicators (\u0026ldquo;User is typing\u0026hellip;\u0026rdquo;) Support file/image sharing Go and WebSocket make a killer combo for real-time apps. Now go build something awesome!\n","href":"/2025/10/how-to-build-websocket-applications-in-go-real-time-chat.html","title":"How to Build WebSocket Applications in Go - Real-time Chat Example"},{"content":"Working with MongoDB in Go is straightforward once you understand the official driver’s patterns: always use contexts , define strong models with bson tags (and clean JSON handling ), ensure indexes, and wrap database calls behind a repository (see project structure best practices ). In this tutorial, you’ll build a complete CRUD flow using idiomatic Go and production-friendly practices.\nWhat you’ll learn:\nInstall and initialize the official driver Connect to MongoDB with timeouts and pooling Design models with bson and json tags Create necessary indexes programmatically Implement Create, Read, Update, Delete operations Add projections, filtering, pagination, and error handling Structure your code for maintainability Prerequisites:\nGo 1.21+ A running MongoDB instance (local Docker or Atlas URI) If you plan to expose a REST API on top of this repository, consider our production-ready Gin guide or the standard library approach in net/http REST tutorial .\nInstall the driver:\ngo get go.mongodb.org/mongo-driver/mongo@latest go get go.mongodb.org/mongo-driver/bson@latest Project layout (minimal example):\ninternal/ db/ mongo.go # client init and helpers user/ model.go # User model repo.go # CRUD repository cmd/ server/ main.go # wire everything together Connect with context and pooling (internal/db/mongo.go):\npackage db import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo/options\u0026#34; ) type MongoConfig struct { URI string Database string Timeout time.Duration MaxPool uint64 } func NewClient(cfg MongoConfig) (*mongo.Client, error) { ctx, cancel := context.WithTimeout(context.Background(), cfg.Timeout) defer cancel() clientOpts := options.Client().ApplyURI(cfg.URI) if cfg.MaxPool \u0026gt; 0 { clientOpts.SetMaxPoolSize(cfg.MaxPool) } client, err := mongo.Connect(ctx, clientOpts) if err != nil { return nil, err } // Verify connection if err := client.Ping(ctx, nil); err != nil { _ = client.Disconnect(context.Background()) return nil, err } return client, nil } Model with bson and json tags (internal/user/model.go):\npackage user import \u0026#34;time\u0026#34; type User struct { ID string `bson:\u0026#34;_id,omitempty\u0026#34; json:\u0026#34;id\u0026#34;` Email string `bson:\u0026#34;email\u0026#34; json:\u0026#34;email\u0026#34;` Name string `bson:\u0026#34;name\u0026#34; json:\u0026#34;name\u0026#34;` CreatedAt time.Time `bson:\u0026#34;created_at\u0026#34; json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `bson:\u0026#34;updated_at\u0026#34; json:\u0026#34;updated_at\u0026#34;` } type CreateUserInput struct { Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } type UpdateUserInput struct { Name string `json:\u0026#34;name\u0026#34;` } New to JSON encoding/decoding and tags in Go? See: Working with JSON in Go .\nRepository interface and implementation (internal/user/repo.go):\npackage user import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;time\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/bson/primitive\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo/options\u0026#34; ) var ErrNotFound = errors.New(\u0026#34;user not found\u0026#34;) type Repository interface { Create(ctx context.Context, in CreateUserInput) (User, error) GetByID(ctx context.Context, id string) (User, error) List(ctx context.Context, limit, skip int64) ([]User, error) UpdateName(ctx context.Context, id string, name string) (User, error) Delete(ctx context.Context, id string) error EnsureIndexes(ctx context.Context) error } type MongoRepo struct { c *mongo.Collection } func NewMongoRepo(db *mongo.Database) *MongoRepo { return \u0026amp;MongoRepo{c: db.Collection(\u0026#34;users\u0026#34;)} } func (r *MongoRepo) EnsureIndexes(ctx context.Context) error { // Unique index on email and a TTL-friendly created_at sorter idxs := []mongo.IndexModel{ { Keys: bson.D{{Key: \u0026#34;email\u0026#34;, Value: 1}}, Options: options.Index().SetUnique(true).SetName(\u0026#34;uniq_email\u0026#34;), }, { Keys: bson.D{{Key: \u0026#34;created_at\u0026#34;, Value: -1}}, Options: options.Index().SetName(\u0026#34;created_at_desc\u0026#34;), }, } _, err := r.c.Indexes().CreateMany(ctx, idxs) return err } func (r *MongoRepo) Create(ctx context.Context, in CreateUserInput) (User, error) { now := time.Now().UTC() doc := User{ Email: in.Email, Name: in.Name, CreatedAt: now, UpdatedAt: now, } res, err := r.c.InsertOne(ctx, doc) if err != nil { return User{}, err } if oid, ok := res.InsertedID.(primitive.ObjectID); ok { doc.ID = oid.Hex() } return doc, nil } func (r *MongoRepo) GetByID(ctx context.Context, id string) (User, error) { oid, err := primitive.ObjectIDFromHex(id) if err != nil { return User{}, ErrNotFound } var out User if err := r.c.FindOne(ctx, bson.M{\u0026#34;_id\u0026#34;: oid}).Decode(\u0026amp;out); err != nil { if errors.Is(err, mongo.ErrNoDocuments) { return User{}, ErrNotFound } return User{}, err } return out, nil } func (r *MongoRepo) List(ctx context.Context, limit, skip int64) ([]User, error) { opts := options.Find().SetLimit(limit).SetSkip(skip).SetSort(bson.D{{Key: \u0026#34;created_at\u0026#34;, Value: -1}}) cur, err := r.c.Find(ctx, bson.M{}, opts) if err != nil { return nil, err } defer cur.Close(ctx) var users []User for cur.Next(ctx) { var u User if err := cur.Decode(\u0026amp;u); err != nil { return nil, err } users = append(users, u) } return users, cur.Err() } func (r *MongoRepo) UpdateName(ctx context.Context, id string, name string) (User, error) { oid, err := primitive.ObjectIDFromHex(id) if err != nil { return User{}, ErrNotFound } upd := bson.M{\u0026#34;$set\u0026#34;: bson.M{\u0026#34;name\u0026#34;: name, \u0026#34;updated_at\u0026#34;: time.Now().UTC()}} opts := options.FindOneAndUpdate().SetReturnDocument(options.After) var out User if err := r.c.FindOneAndUpdate(ctx, bson.M{\u0026#34;_id\u0026#34;: oid}, upd, opts).Decode(\u0026amp;out); err != nil { if errors.Is(err, mongo.ErrNoDocuments) { return User{}, ErrNotFound } return User{}, err } return out, nil } func (r *MongoRepo) Delete(ctx context.Context, id string) error { oid, err := primitive.ObjectIDFromHex(id) if err != nil { return ErrNotFound } res, err := r.c.DeleteOne(ctx, bson.M{\u0026#34;_id\u0026#34;: oid}) if err != nil { return err } if res.DeletedCount == 0 { return ErrNotFound } return nil } Wire up in main.go with contexts and timeouts:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;yourapp/internal/db\u0026#34; \u0026#34;yourapp/internal/user\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; ) func main() { cfg := db.MongoConfig{ URI: getEnv(\u0026#34;MONGODB_URI\u0026#34;, \u0026#34;mongodb://localhost:27017\u0026#34;), Database: getEnv(\u0026#34;MONGODB_DB\u0026#34;, \u0026#34;appdb\u0026#34;), Timeout: 10 * time.Second, MaxPool: 50, } client, err := db.NewClient(cfg) if err != nil { log.Fatalf(\u0026#34;mongo: %v\u0026#34;, err) } defer func() { _ = client.Disconnect(context.Background()) }() database := client.Database(cfg.Database) repo := user.NewMongoRepo(database) // Ensure indexes at startup if err := repo.EnsureIndexes(context.Background()); err != nil { log.Fatalf(\u0026#34;ensure indexes: %v\u0026#34;, err) } // Demo workflow with context timeouts ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() u, err := repo.Create(ctx, user.CreateUserInput{Email: \u0026#34;jane@site.com\u0026#34;, Name: \u0026#34;Jane\u0026#34;}) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;created:\u0026#34;, u.ID) got, _ := repo.GetByID(ctx, u.ID) fmt.Println(\u0026#34;fetched:\u0026#34;, got.Email, got.Name) updated, _ := repo.UpdateName(ctx, u.ID, \u0026#34;Jane Doe\u0026#34;) fmt.Println(\u0026#34;updated:\u0026#34;, updated.Name) list, _ := repo.List(ctx, 10, 0) fmt.Println(\u0026#34;total in page:\u0026#34;, len(list)) _ = repo.Delete(ctx, u.ID) fmt.Println(\u0026#34;deleted:\u0026#34;, u.ID) } func getEnv(k, def string) string { if v := getenv(k); v != \u0026#34;\u0026#34; { return v }; return def } // replace with os.Getenv in real code func getenv(k string) string { return \u0026#34;\u0026#34; } To containerize and deploy this service, follow: How to Containerize and Deploy Go Apps with Docker .\nReading documents with projections and filters\n// Find users created in the last 7 days, only return email and name sevenDays := time.Now().UTC().Add(-7 * 24 * time.Hour) filter := bson.M{\u0026#34;created_at\u0026#34;: bson.M{\u0026#34;$gte\u0026#34;: sevenDays}} opts := options.Find().SetProjection(bson.M{\u0026#34;email\u0026#34;: 1, \u0026#34;name\u0026#34;: 1}) cur, err := repo.c.Find(ctx, filter, opts) Updating multiple documents\n// Add a prefix to all names that are empty filter := bson.M{\u0026#34;name\u0026#34;: bson.M{\u0026#34;$eq\u0026#34;: \u0026#34;\u0026#34;}} update := bson.M{\u0026#34;$set\u0026#34;: bson.M{\u0026#34;name\u0026#34;: \u0026#34;User\u0026#34;}} res, err := repo.c.UpdateMany(ctx, filter, update) Deleting by filter\n// Delete accounts with a specific domain (be careful!) filter := bson.M{\u0026#34;email\u0026#34;: bson.M{\u0026#34;$regex\u0026#34;: \u0026#34;@example.com$\u0026#34;}} res, err := repo.c.DeleteMany(ctx, filter) Transactions and sessions (brief)\nMongoDB supports multi-document ACID transactions on replica sets and sharded clusters. With the Go driver, you run code inside a session callback. Keep transactions short, avoid long-running operations, and set timeouts.\n// Example outline; ensure your deployment supports transactions func runInTxn(client *mongo.Client, cb func(mongo.SessionContext) error) error { sess, err := client.StartSession() if err != nil { return err } defer sess.EndSession(context.Background()) return mongo.WithSession(context.Background(), sess, func(sc mongo.SessionContext) error { return sc.StartTransaction(); }) } Error handling and common pitfalls\nAlways check mongo.ErrNoDocuments to distinguish “not found” from real errors. Use context.WithTimeout for every operation; surface deadline exceeded errors to callers. Unique constraints belong to indexes, not only application logic. Validate inputs at boundaries and sanitize regex filters from user input. Avoid unbounded Find calls; always use limit and prefer stable sorts. For a deeper dive into idiomatic error patterns, read: Error Handling in Go .\nPagination strategies\nOffset-based: use Find with limit and skip plus a sort field (e.g., created_at desc). Simple and good for small pages. Cursor-based: store the last seen _id and query {\u0026quot;_id\u0026quot;: {\u0026quot;$lt\u0026quot;: lastID}} with the same sort. This scales better for large collections. Need to investigate query performance and memory usage? Try our profiling guide: Profile and Optimize Go Apps with pprof .\nTesting tips\nUse a dedicated test database and clean up with DeleteMany after each test. For fast unit tests, abstract the repository and mock it; reserve integration tests for verifying real behavior. Learn the testing fundamentals and patterns here: Testing in Go with the testing package .\nFolder structure recommendations\nKeep driver-specific logic in the repository package; expose an interface. Pass contexts from the HTTP layer down to repositories; the caller owns timeouts. Centralize client initialization and ensure indexes at startup. Security and configuration\nKeep the connection string in environment variables (e.g., MONGODB_URI). Use different databases for dev/test/prod. Restrict network access and credentials; prefer least-privilege users. Wrap-up\nYou implemented a complete CRUD workflow in Go with the official MongoDB driver, added indexes, and covered pagination, projections, and robust error handling. From here, you can extend the repository with compound indexes, unique constraints on multiple fields, soft deletes, or aggregation pipelines for analytics. Keep contexts and timeouts at the top of your mind\u0026ndash;those two practices alone go a long way toward reliable production services.\nBuilding an API that needs authentication and rate control? Continue with JWT authentication in Go and API rate limiting in Go .\n","href":"/2025/10/how-to-work-with-mongodb-in-go-complete-crud-tutorial.html","title":"How to Work with MongoDB in Go - Complete CRUD Tutorial"},{"content":"Deploying Go applications used to mean SSH into servers, copying binaries, managing dependencies, and praying everything works. Different machines had different library versions. Production behaved differently than development. Debugging deployment issues wasted hours.\nWhat is Docker for Go? Docker is a containerization platform that packages your Go application and all its dependencies into a portable container image. Instead of installing Go and dependencies on every server, Docker bundles everything your app needs into a container that runs identically everywhere - from your laptop to production servers.\nContainers solve the deployment problem. Build once, run anywhere. Your Go app runs the same on MacOS, Linux, and Windows. Development matches production. Scaling means starting more containers, not configuring more servers.\nThis guide covers everything - from basic Dockerfiles to production-ready multi-stage builds, Docker Compose for multi-service apps, optimization techniques, and deployment strategies. You\u0026rsquo;ll learn how to build images under 20MB, deploy to Kubernetes, and follow security best practices.\nWhy Docker for Go Applications Go compiles to static binaries. You might wonder why bother with Docker. Just copy the binary to a server and run it, right?\nThat works until it doesn\u0026rsquo;t. I deployed a Go API this way once. Worked perfectly locally. On the production server, it crashed with \u0026ldquo;library not found\u0026rdquo; errors. The server had a different glibc version. Took me three hours to debug.\nWhat Docker provides for Go:\nConsistency - Your app runs identically in development, testing, staging, and production. No environment-specific bugs.\nDependencies - Go might be statically compiled, but what about PostgreSQL, Redis, or other services? Docker bundles everything.\nIsolation - Multiple apps run on one server without conflicts. Each container has its own filesystem and network.\nPortability - Build once, deploy anywhere. AWS, Google Cloud, your own servers - same container works everywhere.\nScaling - Need more capacity? Start more containers. Kubernetes and other orchestrators make this automatic.\nVersion control - Docker images are versioned. Rollback to previous versions in seconds if something breaks.\nDevelopment parity - Developers run the exact same environment as production. \u0026ldquo;Works on my machine\u0026rdquo; becomes \u0026ldquo;works in the container.\u0026rdquo;\nFor Go applications specifically, Docker shines because Go\u0026rsquo;s small binary size means tiny container images. A Go app container can be 10-20MB compared to 500MB+ for Node.js or Python apps.\nUnderstanding Docker Basics Before diving into Go-specific containers, understand Docker fundamentals.\nDocker Image - A read-only template containing your application code, dependencies, and filesystem. Think of it as a snapshot of your application and environment.\nContainer - A running instance of an image. Like how a process is a running instance of a program, a container is a running instance of an image.\nDockerfile - A text file with instructions to build a Docker image. Defines base image, dependencies, files to copy, commands to run, and how to start your app.\nDocker Registry - A repository for storing and distributing images. Docker Hub is the default public registry. You can also run private registries.\nDocker Compose - A tool for defining and running multi-container applications. Perfect for local development with multiple services.\nThe typical workflow:\nWrite a Dockerfile Build an image: docker build Run a container: docker run Push to registry: docker push Deploy anywhere: Pull image and run Docker uses layers. Each instruction in a Dockerfile creates a layer. Layers are cached and reused, making subsequent builds fast.\nCreating Your First Go Dockerfile Let\u0026rsquo;s start with a simple Go web server and containerize it.\nSample Go application (main.go):\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; ) func handler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello from Go in Docker!\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, handler) log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { log.Fatal(err) } } Basic Dockerfile:\n# Use official Go image FROM golang:1.21-alpine # Set working directory WORKDIR /app # Copy source code COPY . . # Build the application RUN go build -o main . # Expose port EXPOSE 8080 # Run the application CMD [\u0026#34;./main\u0026#34;] Build and run:\n# Build image docker build -t mygoapp . # Run container docker run -p 8080:8080 mygoapp # Test curl http://localhost:8080 # Hello from Go in Docker! This works, but the image is huge - around 300MB. Most of that is the Go toolchain and build dependencies we don\u0026rsquo;t need at runtime.\nMulti-Stage Builds for Go Multi-stage builds solve the bloat problem. Build in one image, run in another.\nOptimized Dockerfile:\n# Build stage FROM golang:1.21-alpine AS builder WORKDIR /app # Copy go mod files COPY go.mod go.sum ./ # Download dependencies RUN go mod download # Copy source code COPY . . # Build with optimizations RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags=\u0026#34;-w -s\u0026#34; -o main . # Runtime stage FROM alpine:latest # Install ca-certificates for HTTPS RUN apk --no-cache add ca-certificates WORKDIR /root/ # Copy binary from builder COPY --from=builder /app/main . EXPOSE 8080 CMD [\u0026#34;./main\u0026#34;] Size difference:\nSingle-stage build: 300MB Multi-stage build: 15MB 20x smaller! What changed:\nBuilder stage - Uses full golang image with all build tools. Compiles the application.\nRuntime stage - Uses tiny alpine image. Only copies the compiled binary.\nCGO_ENABLED=0 - Produces a fully static binary with no C library dependencies.\n-ldflags=\u0026quot;-w -s\u0026quot; - Strips debug information and symbol tables. Smaller binary, harder to debug but fine for production.\nThe final image only contains the binary and minimal OS utilities. Everything else stays in the builder stage and gets discarded.\nOptimizing Go Docker Builds Beyond multi-stage builds, several techniques optimize Docker images for Go.\nUsing Distroless Images Google\u0026rsquo;s distroless images are even smaller than alpine and more secure:\n# Build stage FROM golang:1.21 AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -ldflags=\u0026#34;-w -s\u0026#34; -o main . # Runtime stage with distroless FROM gcr.io/distroless/static:nonroot COPY --from=builder /app/main /main EXPOSE 8080 ENTRYPOINT [\u0026#34;/main\u0026#34;] Distroless images contain only your app and runtime dependencies. No shell, no package manager, minimal attack surface. Size: 10-12MB.\nLeveraging Go Modules Cache Cache Go modules between builds for faster iterations:\nFROM golang:1.21-alpine AS builder WORKDIR /app # Copy go.mod and go.sum first COPY go.mod go.sum ./ # Download dependencies (cached layer) RUN go mod download # Copy source code (this layer changes frequently) COPY . . # Build RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-w -s\u0026#34; -o main . FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/main . EXPOSE 8080 CMD [\u0026#34;./main\u0026#34;] By copying go.mod and go.sum before source code, Docker caches the go mod download layer. When you change code, dependencies don\u0026rsquo;t re-download.\nBuild time comparison:\nFirst build: 60 seconds Subsequent builds (code changes only): 5 seconds 12x faster! Using .dockerignore Exclude unnecessary files from the build context:\n.dockerignore:\n# Git files .git .gitignore # IDE files .vscode .idea # Test files *_test.go testdata # Documentation README.md docs # Build artifacts *.exe *.dll *.so *.dylib main # Temporary files tmp *.tmp Smaller build context means faster uploads to Docker daemon and faster builds.\nScratch Image for Ultimate Minimalism For truly static binaries with no external dependencies:\nFROM golang:1.21 AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -a -ldflags=\u0026#34;-w -s\u0026#34; -o main . # Use scratch (empty image) FROM scratch COPY --from=builder /app/main /main EXPOSE 8080 ENTRYPOINT [\u0026#34;/main\u0026#34;] Scratch image is empty - literally nothing except your binary. Final size: 5-8MB depending on your app.\nLimitations: No shell for debugging, no SSL certificates (copy them manually), no timezone data. Only works if your app needs nothing from the OS.\nGo Application with Dependencies Real applications have databases, Redis, and other services. Let\u0026rsquo;s containerize a Go API with PostgreSQL.\nProject structure:\nmyapp/ ├── cmd/ │ └── api/ │ └── main.go ├── internal/ │ ├── database/ │ │ └── db.go │ └── handlers/ │ └── handlers.go ├── go.mod ├── go.sum ├── Dockerfile └── docker-compose.yml main.go:\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) func main() { dbHost := os.Getenv(\u0026#34;DB_HOST\u0026#34;) dbUser := os.Getenv(\u0026#34;DB_USER\u0026#34;) dbPassword := os.Getenv(\u0026#34;DB_PASSWORD\u0026#34;) dbName := os.Getenv(\u0026#34;DB_NAME\u0026#34;) connStr := fmt.Sprintf( \u0026#34;host=%s user=%s password=%s dbname=%s sslmode=disable\u0026#34;, dbHost, dbUser, dbPassword, dbName, ) db, err := sql.Open(\u0026#34;postgres\u0026#34;, connStr) if err != nil { log.Fatal(err) } defer db.Close() if err := db.Ping(); err != nil { log.Fatal(\u0026#34;Cannot connect to database:\u0026#34;, err) } log.Println(\u0026#34;Connected to database\u0026#34;) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { var count int db.QueryRow(\u0026#34;SELECT COUNT(*) FROM users\u0026#34;).Scan(\u0026amp;count) fmt.Fprintf(w, \u0026#34;User count: %d\\n\u0026#34;, count) }) log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Dockerfile:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-w -s\u0026#34; -o main ./cmd/api FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/main . EXPOSE 8080 CMD [\u0026#34;./main\u0026#34;] Build:\ndocker build -t myapp . Run with environment variables:\ndocker run -p 8080:8080 \\ -e DB_HOST=localhost \\ -e DB_USER=postgres \\ -e DB_PASSWORD=secret \\ -e DB_NAME=mydb \\ myapp But this fails because the container can\u0026rsquo;t reach localhost. That\u0026rsquo;s where Docker Compose comes in.\nDocker Compose for Multi-Service Apps Docker Compose manages multiple containers as one application.\ndocker-compose.yml:\nversion: \u0026#39;3.8\u0026#39; services: app: build: . ports: - \u0026#34;8080:8080\u0026#34; environment: - DB_HOST=db - DB_USER=postgres - DB_PASSWORD=secret - DB_NAME=mydb depends_on: - db networks: - app-network db: image: postgres:15-alpine environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=secret - POSTGRES_DB=mydb ports: - \u0026#34;5432:5432\u0026#34; volumes: - postgres-data:/var/lib/postgresql/data networks: - app-network networks: app-network: driver: bridge volumes: postgres-data: Start everything:\ndocker-compose up # Or in detached mode docker-compose up -d # View logs docker-compose logs -f app # Stop everything docker-compose down # Stop and remove volumes docker-compose down -v How it works:\nServices - Defines app and db containers.\nNetworks - Both services join app-network. They can communicate using service names as hostnames (app can reach db at hostname \u0026ldquo;db\u0026rdquo;).\nVolumes - postgres-data persists database data across container restarts.\ndepends_on - Ensures db starts before app.\nDocker Compose automatically handles networking, DNS resolution between containers, and dependency management.\nComplete Stack with Redis Add Redis for caching:\nversion: \u0026#39;3.8\u0026#39; services: app: build: . ports: - \u0026#34;8080:8080\u0026#34; environment: - DB_HOST=db - DB_USER=postgres - DB_PASSWORD=secret - DB_NAME=mydb - REDIS_HOST=redis - REDIS_PORT=6379 depends_on: - db - redis networks: - app-network db: image: postgres:15-alpine environment: - POSTGRES_USER=postgres - POSTGRES_PASSWORD=secret - POSTGRES_DB=mydb volumes: - postgres-data:/var/lib/postgresql/data networks: - app-network redis: image: redis:7-alpine ports: - \u0026#34;6379:6379\u0026#34; networks: - app-network networks: app-network: volumes: postgres-data: Your Go app can now connect to both PostgreSQL (host=db) and Redis (host=redis).\nEnvironment Variables and Configuration Hardcoding config in Docker Compose is bad for production. Use environment files.\n.env:\n# Database DB_HOST=db DB_USER=postgres DB_PASSWORD=supersecretpassword DB_NAME=mydb # Redis REDIS_HOST=redis REDIS_PORT=6379 # App PORT=8080 ENV=development docker-compose.yml:\nversion: \u0026#39;3.8\u0026#39; services: app: build: . ports: - \u0026#34;${PORT}:8080\u0026#34; env_file: - .env depends_on: - db - redis networks: - app-network db: image: postgres:15-alpine environment: - POSTGRES_USER=${DB_USER} - POSTGRES_PASSWORD=${DB_PASSWORD} - POSTGRES_DB=${DB_NAME} volumes: - postgres-data:/var/lib/postgresql/data networks: - app-network redis: image: redis:7-alpine networks: - app-network networks: app-network: volumes: postgres-data: Now change config by editing .env instead of docker-compose.yml.\nProduction secrets: Never commit .env to git. Add to .gitignore:\n.env .env.local .env.production For production, use secret management systems like Docker Secrets, Kubernetes Secrets, AWS Secrets Manager, or HashiCorp Vault.\nHealth Checks and Readiness Probes Production containers need health checks so orchestrators know if they\u0026rsquo;re working.\nAdd health check to Go app:\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) var db *sql.DB func healthHandler(w http.ResponseWriter, r *http.Request) { // Check database connection if err := db.Ping(); err != nil { w.WriteHeader(http.StatusServiceUnavailable) json.NewEncoder(w).Encode(map[string]string{ \u0026#34;status\u0026#34;: \u0026#34;unhealthy\u0026#34;, \u0026#34;error\u0026#34;: err.Error(), }) return } w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{ \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, }) } func main() { // Database setup connStr := fmt.Sprintf( \u0026#34;host=%s user=%s password=%s dbname=%s sslmode=disable\u0026#34;, os.Getenv(\u0026#34;DB_HOST\u0026#34;), os.Getenv(\u0026#34;DB_USER\u0026#34;), os.Getenv(\u0026#34;DB_PASSWORD\u0026#34;), os.Getenv(\u0026#34;DB_NAME\u0026#34;), ) var err error db, err = sql.Open(\u0026#34;postgres\u0026#34;, connStr) if err != nil { log.Fatal(err) } defer db.Close() db.SetMaxOpenConns(25) db.SetMaxIdleConns(5) db.SetConnMaxLifetime(5 * time.Minute) // Wait for database for i := 0; i \u0026lt; 30; i++ { if err := db.Ping(); err == nil { break } log.Println(\u0026#34;Waiting for database...\u0026#34;) time.Sleep(time.Second) } log.Println(\u0026#34;Connected to database\u0026#34;) // Routes http.HandleFunc(\u0026#34;/health\u0026#34;, healthHandler) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello World\u0026#34;) }) log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Update Dockerfile with health check:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-w -s\u0026#34; -o main ./cmd/api FROM alpine:latest RUN apk --no-cache add ca-certificates curl WORKDIR /root/ COPY --from=builder /app/main . EXPOSE 8080 # Health check HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 CMD [\u0026#34;./main\u0026#34;] Docker Compose health check:\nservices: app: build: . ports: - \u0026#34;8080:8080\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34;] interval: 30s timeout: 3s retries: 3 start_period: 5s depends_on: db: condition: service_healthy db: image: postgres:15-alpine healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -U postgres\u0026#34;] interval: 10s timeout: 3s retries: 3 Health checks ensure containers are actually working, not just running. Orchestrators automatically restart unhealthy containers.\nDocker Networking and Service Discovery Containers on the same Docker network can communicate using service names.\nCustom network example:\nversion: \u0026#39;3.8\u0026#39; services: frontend: build: ./frontend ports: - \u0026#34;3000:3000\u0026#34; environment: - API_URL=http://backend:8080 networks: - frontend-network backend: build: ./backend environment: - DB_HOST=database networks: - frontend-network - backend-network database: image: postgres:15-alpine networks: - backend-network networks: frontend-network: backend-network: Frontend connects to backend via http://backend:8080.\nBackend connects to database via database:5432.\nDatabase is isolated from frontend - only backend can access it.\nThis network segmentation improves security. External services can\u0026rsquo;t directly access internal services.\nVolume Management for Persistence Containers are ephemeral. Data disappears when containers stop. Volumes persist data.\nNamed Volumes Docker-managed storage:\nservices: db: image: postgres:15-alpine volumes: - postgres-data:/var/lib/postgresql/data volumes: postgres-data: Data persists across container restarts and removals.\nBind Mounts Mount host directories into containers:\nservices: app: build: . volumes: - ./logs:/app/logs # Host ./logs -\u0026gt; Container /app/logs - ./config:/app/config:ro # Read-only mount Useful for development (live code reload) and accessing logs from host.\ntmpfs Mounts In-memory storage for temporary data:\nservices: app: build: . tmpfs: - /tmp - /app/cache Fast but data is lost when container stops.\nBuilding for Multiple Architectures Build images that run on both AMD64 (Intel/AMD) and ARM64 (Apple Silicon, AWS Graviton):\n# Enable buildx docker buildx create --use # Build for multiple platforms docker buildx build \\ --platform linux/amd64,linux/arm64 \\ -t myapp:latest \\ --push \\ . Or in Dockerfile:\nFROM --platform=$BUILDPLATFORM golang:1.21-alpine AS builder ARG TARGETPLATFORM ARG BUILDPLATFORM ARG TARGETOS ARG TARGETARCH WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=$TARGETOS GOARCH=$TARGETARCH \\ go build -ldflags=\u0026#34;-w -s\u0026#34; -o main . FROM alpine:latest COPY --from=builder /app/main /main CMD [\u0026#34;/main\u0026#34;] This creates a universal image that works on any architecture.\nProduction Deployment Strategies Deploy to VPS with Docker Compose Simple deployment for small to medium apps:\n# On your VPS git clone https://github.com/you/myapp.git cd myapp # Create .env with production secrets nano .env # Start services docker-compose -f docker-compose.prod.yml up -d # View logs docker-compose logs -f # Update (pull new code, rebuild, restart) git pull docker-compose up -d --build docker-compose.prod.yml:\nversion: \u0026#39;3.8\u0026#39; services: app: build: . restart: unless-stopped ports: - \u0026#34;8080:8080\u0026#34; env_file: - .env depends_on: - db - redis logging: driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; db: image: postgres:15-alpine restart: unless-stopped env_file: - .env volumes: - postgres-data:/var/lib/postgresql/data logging: driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; redis: image: redis:7-alpine restart: unless-stopped logging: driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; nginx: image: nginx:alpine restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./nginx.conf:/etc/nginx/nginx.conf - ./ssl:/etc/nginx/ssl depends_on: - app volumes: postgres-data: Deploy to Kubernetes For production scale, use Kubernetes.\nDeployment manifest (deployment.yaml):\napiVersion: apps/v1 kind: Deployment metadata: name: myapp spec: replicas: 3 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp image: myregistry/myapp:latest ports: - containerPort: 8080 env: - name: DB_HOST value: postgres-service - name: DB_USER valueFrom: secretKeyRef: name: db-secret key: username - name: DB_PASSWORD valueFrom: secretKeyRef: name: db-secret key: password resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 5 periodSeconds: 10 readinessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 3 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: myapp-service spec: selector: app: myapp ports: - port: 80 targetPort: 8080 type: LoadBalancer Deploy:\n# Create secret kubectl create secret generic db-secret \\ --from-literal=username=postgres \\ --from-literal=password=supersecret # Deploy application kubectl apply -f deployment.yaml # Check status kubectl get pods kubectl get services # View logs kubectl logs -f deployment/myapp # Scale kubectl scale deployment myapp --replicas=5 Kubernetes handles automatic scaling, load balancing, rolling updates, and self-healing.\nDeploy to Cloud Services AWS ECS (Elastic Container Service):\n# Build and push docker build -t myapp . docker tag myapp:latest \u0026lt;aws-account-id\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/myapp:latest docker push \u0026lt;aws-account-id\u0026gt;.dkr.ecr.us-east-1.amazonaws.com/myapp:latest # Create ECS task definition and service via AWS Console or Terraform Google Cloud Run:\n# Build and deploy in one command gcloud run deploy myapp \\ --source . \\ --platform managed \\ --region us-central1 \\ --allow-unauthenticated Azure Container Instances:\naz container create \\ --resource-group myResourceGroup \\ --name myapp \\ --image myregistry.azurecr.io/myapp:latest \\ --dns-name-label myapp \\ --ports 8080 Cloud services handle infrastructure, scaling, and monitoring for you.\nComparison: Deployment Options Method Best For Complexity Cost Scaling Management Docker Compose on VPS Small apps, 1-5 services Low Low ($5-20/mo) Manual Self-managed Docker Swarm Medium apps, simple orchestration Medium Low-Medium Automatic Self-managed Kubernetes Large apps, microservices High Medium-High Automatic Self or managed AWS ECS AWS ecosystem Medium Medium Automatic Managed Google Cloud Run Serverless containers Low Pay-per-use Automatic Fully managed Azure Container Instances Simple containers Low Pay-per-use Limited Fully managed Heroku Rapid deployment Very Low High Automatic Fully managed Recommendations:\nStart small: Docker Compose on VPS ($10/month DigitalOcean droplet).\nGrowing: Migrate to managed Kubernetes (GKE, EKS, AKS) when you need 10+ services.\nServerless: Google Cloud Run or AWS Fargate for variable traffic.\nEnterprise: Self-managed Kubernetes for full control and cost optimization at scale.\nSecurity Best Practices for Docker Go Apps Run as Non-Root User Never run containers as root:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -ldflags=\u0026#34;-w -s\u0026#34; -o main . FROM alpine:latest RUN apk --no-cache add ca-certificates # Create non-root user RUN addgroup -g 1000 appuser \u0026amp;\u0026amp; \\ adduser -D -u 1000 -G appuser appuser WORKDIR /home/appuser COPY --from=builder /app/main . # Change ownership RUN chown -R appuser:appuser /home/appuser # Switch to non-root user USER appuser EXPOSE 8080 CMD [\u0026#34;./main\u0026#34;] If a vulnerability is exploited, the attacker only has limited user privileges, not root.\nScan Images for Vulnerabilities Use Trivy or other scanners:\n# Install Trivy brew install aquasecurity/trivy/trivy # Scan image trivy image myapp:latest # Fail build on high/critical vulnerabilities trivy image --severity HIGH,CRITICAL --exit-code 1 myapp:latest Integrate into CI/CD:\n# GitHub Actions - name: Run Trivy scanner uses: aquasecurity/trivy-action@master with: image-ref: \u0026#39;myapp:latest\u0026#39; severity: \u0026#39;CRITICAL,HIGH\u0026#39; exit-code: \u0026#39;1\u0026#39; Don\u0026rsquo;t Embed Secrets Never put secrets in Dockerfile or image:\n# WRONG - Secret in image ENV API_KEY=supersecret123 # CORRECT - Pass at runtime ENV API_KEY= Pass secrets via environment variables, Docker secrets, or secret management systems.\nUse Read-Only Filesystem Make containers immutable:\nservices: app: build: . read_only: true tmpfs: - /tmp - /app/cache Prevents attackers from modifying files even if they compromise the container.\nLimit Container Capabilities Drop unnecessary Linux capabilities:\nservices: app: build: . cap_drop: - ALL cap_add: - NET_BIND_SERVICE # Only if binding to port \u0026lt;1024 Reduces attack surface by removing privileges containers rarely need.\nMonitoring and Logging Production containers need observability.\nStructured Logging Log in JSON for easier parsing:\npackage main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) type LogEntry struct { Level string `json:\u0026#34;level\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` Message string `json:\u0026#34;message\u0026#34;` Context map[string]interface{} `json:\u0026#34;context,omitempty\u0026#34;` } func logJSON(level, message string, context map[string]interface{}) { entry := LogEntry{ Level: level, Timestamp: time.Now().UTC(), Message: message, Context: context, } json.NewEncoder(os.Stdout).Encode(entry) } func main() { logJSON(\u0026#34;info\u0026#34;, \u0026#34;Server starting\u0026#34;, map[string]interface{}{ \u0026#34;port\u0026#34;: 8080, }) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { logJSON(\u0026#34;info\u0026#34;, \u0026#34;Request received\u0026#34;, map[string]interface{}{ \u0026#34;method\u0026#34;: r.Method, \u0026#34;path\u0026#34;: r.URL.Path, \u0026#34;ip\u0026#34;: r.RemoteAddr, }) w.Write([]byte(\u0026#34;Hello\u0026#34;)) }) if err := http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil); err != nil { logJSON(\u0026#34;error\u0026#34;, \u0026#34;Server failed\u0026#34;, map[string]interface{}{ \u0026#34;error\u0026#34;: err.Error(), }) os.Exit(1) } } Structured logs work well with log aggregation systems like ELK stack, Loki, or cloud logging.\nContainer Metrics Expose Prometheus metrics:\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) var ( httpRequests = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_requests_total\u0026#34;, Help: \u0026#34;Total HTTP requests\u0026#34;, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;status\u0026#34;}, ) httpDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;http_request_duration_seconds\u0026#34;, Help: \u0026#34;HTTP request duration\u0026#34;, Buckets: prometheus.DefBuckets, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;}, ) ) func init() { prometheus.MustRegister(httpRequests) prometheus.MustRegister(httpDuration) } func main() { http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { timer := prometheus.NewTimer(httpDuration.WithLabelValues(r.Method, r.URL.Path)) defer timer.ObserveDuration() // Handle request w.Write([]byte(\u0026#34;Hello\u0026#34;)) httpRequests.WithLabelValues(r.Method, r.URL.Path, \u0026#34;200\u0026#34;).Inc() }) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } Scrape with Prometheus and visualize in Grafana.\nCI/CD Pipeline for Docker Go Apps Automate build, test, and deployment.\nGitHub Actions example (.github/workflows/docker.yml):\nname: Docker Build and Deploy on: push: branches: [ main ] pull_request: branches: [ main ] env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Go uses: actions/setup-go@v4 with: go-version: \u0026#39;1.21\u0026#39; - name: Run tests run: go test -v ./... build: needs: test runs-on: ubuntu-latest permissions: contents: read packages: write steps: - uses: actions/checkout@v3 - name: Log in to Container Registry uses: docker/login-action@v2 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata id: meta uses: docker/metadata-action@v4 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} - name: Build and push uses: docker/build-push-action@v4 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} labels: ${{ steps.meta.outputs.labels }} - name: Scan image uses: aquasecurity/trivy-action@master with: image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest severity: \u0026#39;CRITICAL,HIGH\u0026#39; deploy: needs: build runs-on: ubuntu-latest if: github.ref == \u0026#39;refs/heads/main\u0026#39; steps: - name: Deploy to production run: | # SSH to server and pull new image # kubectl apply or docker-compose pull echo \u0026#34;Deploy to production\u0026#34; This pipeline:\nRuns tests Builds Docker image Pushes to registry Scans for vulnerabilities Deploys to production (if on main branch) Troubleshooting Common Issues Container exits immediately:\nCheck logs:\ndocker logs \u0026lt;container-id\u0026gt; docker-compose logs app Common cause: Application crashes on startup. Check database connection, missing environment variables.\nCannot connect to database:\nVerify network:\ndocker network ls docker network inspect \u0026lt;network-name\u0026gt; Ensure services are on same network. Use service name as hostname, not localhost.\nImage build is slow:\nUse build cache effectively. Put frequently changing files (source code) after infrequently changing files (dependencies).\nEnable BuildKit:\nexport DOCKER_BUILDKIT=1 docker build . Out of disk space:\nClean up unused images and containers:\ndocker system prune -a docker volume prune Permission denied errors:\nCheck file ownership. Container user might not have permissions. Use non-root user or fix permissions in Dockerfile.\nReal-World Example: Complete API with Docker Let\u0026rsquo;s put everything together. Full stack Go API with PostgreSQL, Redis, and Nginx.\nProject structure:\nmyapi/ ├── cmd/ │ └── api/ │ └── main.go ├── internal/ │ ├── handlers/ │ ├── database/ │ └── cache/ ├── migrations/ │ └── 001_create_users.sql ├── nginx/ │ └── nginx.conf ├── Dockerfile ├── docker-compose.yml ├── docker-compose.prod.yml ├── .dockerignore ├── .env.example └── Makefile Dockerfile:\nFROM golang:1.21-alpine AS builder WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags=\u0026#34;-w -s\u0026#34; -o main ./cmd/api FROM alpine:latest RUN apk --no-cache add ca-certificates curl RUN addgroup -g 1000 appuser \u0026amp;\u0026amp; adduser -D -u 1000 -G appuser appuser WORKDIR /home/appuser COPY --from=builder /app/main . RUN chown appuser:appuser main USER appuser EXPOSE 8080 HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD curl -f http://localhost:8080/health || exit 1 CMD [\u0026#34;./main\u0026#34;] docker-compose.prod.yml:\nversion: \u0026#39;3.8\u0026#39; services: app: build: context: . dockerfile: Dockerfile restart: unless-stopped environment: - DB_HOST=db - REDIS_HOST=redis env_file: - .env depends_on: db: condition: service_healthy redis: condition: service_started networks: - backend logging: driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; db: image: postgres:15-alpine restart: unless-stopped env_file: - .env volumes: - postgres-data:/var/lib/postgresql/data - ./migrations:/docker-entrypoint-initdb.d healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -U $$POSTGRES_USER\u0026#34;] interval: 10s timeout: 5s retries: 5 networks: - backend redis: image: redis:7-alpine restart: unless-stopped command: redis-server --appendonly yes volumes: - redis-data:/data networks: - backend nginx: image: nginx:alpine restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro - ./ssl:/etc/ssl/private:ro depends_on: - app networks: - backend networks: backend: driver: bridge volumes: postgres-data: redis-data: Makefile:\n.PHONY: build run test clean build: docker-compose build run: docker-compose up run-prod: docker-compose -f docker-compose.prod.yml up -d test: go test -v ./... logs: docker-compose logs -f stop: docker-compose down clean: docker-compose down -v docker system prune -f deploy: git pull docker-compose -f docker-compose.prod.yml up -d --build Deploy to production:\nmake deploy This setup is production-ready with health checks, logging, secrets management, and easy deployment.\nIntegration with Other Tools For complete DevOps workflows, combine Docker with other tools.\nIntegrate with database migrations to manage schema changes across environments.\nUse background job processing with Asynq for async task processing in containers.\nImplement monitoring and profiling with pprof to optimize containerized applications.\nWrapping Up Docker transforms Go application deployment from complex server configuration to simple container orchestration. Multi-stage builds create tiny images under 20MB. Docker Compose manages local development with multiple services. Kubernetes and cloud platforms handle production at scale.\nThe key patterns: use multi-stage builds for small images, leverage Go\u0026rsquo;s static compilation for minimal base images, implement health checks for reliability, separate build and runtime concerns, use Docker Compose for local development, and deploy with orchestrators for production.\nStart simple with a basic Dockerfile. Add multi-stage builds when image size matters. Use Docker Compose when you add databases or Redis. Move to Kubernetes or cloud services when you need scaling and high availability.\nDocker makes Go deployment consistent, portable, and scalable. Build once, deploy anywhere, scale automatically. That\u0026rsquo;s the Docker advantage for Go applications .\nFor building complete production systems, check out REST API development with Gin , email integration , and CLI tools with Cobra .\n","href":"/2025/10/how-to-containerize-and-deploy-go-apps-with-docker.html","title":"How to Containerize and Deploy Go Apps with Docker"},{"content":"Your application needs to send emails. Welcome messages after signup, password reset links, order confirmations, notification alerts. Email is still the most reliable way to reach users, but sending emails programmatically is harder than it looks.\nWhat is email sending in Go? Email sending in Go refers to programmatically delivering emails from your application using either SMTP protocol directly or third-party email service APIs like SendGrid and Mailgun. Instead of manually composing and sending emails, your Go code automatically sends transactional emails triggered by user actions.\nYou could use SMTP directly, but deliverability is a nightmare. ISPs block mail from unknown servers. Your emails land in spam. Bounce handling becomes your problem. That\u0026rsquo;s why production applications use email service providers.\nThis guide covers everything - from basic SMTP to production-ready SendGrid and Mailgun integration. You\u0026rsquo;ll learn HTML emails, templates, attachments, error handling, and avoiding spam filters. We\u0026rsquo;ll build real examples that you can deploy to production.\nWhy Email Services Matter Sending email sounds simple. Connect to an SMTP server, send message, done. Reality is different.\nI once built an app that sent password resets using a basic SMTP server. Worked fine in testing. In production, 60% of emails never arrived. Gmail marked them as spam. Outlook blocked them entirely. Users complained they couldn\u0026rsquo;t reset passwords.\nThe problem: deliverability. Major email providers don\u0026rsquo;t trust random SMTP servers. Your emails need proper authentication (SPF, DKIM, DMARC), good sender reputation, and correct headers. Setting this up yourself takes weeks and ongoing maintenance.\nWhat email services handle for you:\nDeliverability infrastructure - Established relationships with ISPs, authenticated domains, good sender reputation.\nBounce management - Automatic handling of bounced emails, maintaining clean sender lists.\nAnalytics - Track opens, clicks, bounces, spam complaints.\nTemplates - Store and manage email templates without code deploys.\nScale - Handle millions of emails without worrying about server capacity.\nCompliance - GDPR compliance, unsubscribe handling, spam regulations.\nUsing an email service costs money but saves time and ensures emails actually get delivered. For production applications, it\u0026rsquo;s not optional.\nUnderstanding Email Delivery Methods Go offers several ways to send emails, each with tradeoffs.\nNet/smtp package - Go\u0026rsquo;s standard library for SMTP. Simple for basic emails but requires managing SMTP servers, authentication, and deliverability yourself. Good for development, risky for production.\nThird-party SMTP services - Use Gmail, Amazon SES, or other SMTP providers. Better deliverability than your own server but still SMTP limitations. You handle connection pooling, retries, and errors.\nEmail service APIs - SendGrid, Mailgun, Postmark use HTTP APIs instead of SMTP. Better error handling, richer features, detailed analytics. Most production apps use these.\nWhen to use each:\nDevelopment - net/smtp with Gmail or Mailtrap for testing.\nSmall projects - SendGrid free tier (100 emails/day).\nProduction apps - SendGrid or Mailgun APIs with proper error handling.\nHigh volume - Mailgun or AWS SES for cost efficiency.\nThis guide focuses on production approaches: basic SMTP for understanding, then SendGrid and Mailgun for real applications.\nSending Email with SMTP in Go Let\u0026rsquo;s start with SMTP using Go\u0026rsquo;s standard library. This teaches fundamentals even if you\u0026rsquo;ll use services later.\nBasic SMTP Example package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/smtp\u0026#34; ) func main() { // SMTP server configuration smtpHost := \u0026#34;smtp.gmail.com\u0026#34; smtpPort := \u0026#34;587\u0026#34; // Sender credentials from := \u0026#34;your-email@gmail.com\u0026#34; password := \u0026#34;your-app-password\u0026#34; // Recipient to := []string{\u0026#34;recipient@example.com\u0026#34;} // Message subject := \u0026#34;Test Email from Go\u0026#34; body := \u0026#34;This is a test email sent from Go using SMTP.\u0026#34; message := []byte(fmt.Sprintf(\u0026#34;Subject: %s\\r\\n\\r\\n%s\u0026#34;, subject, body)) // Authentication auth := smtp.PlainAuth(\u0026#34;\u0026#34;, from, password, smtpHost) // Send email err := smtp.SendMail(smtpHost+\u0026#34;:\u0026#34;+smtpPort, auth, from, to, message) if err != nil { fmt.Println(\u0026#34;Failed to send email:\u0026#34;, err) return } fmt.Println(\u0026#34;Email sent successfully!\u0026#34;) } This works but has problems. The message is plain text only, no proper MIME headers, error handling is basic, and credentials are hardcoded.\nImproved SMTP with Proper Headers package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;os\u0026#34; ) type Email struct { From string To []string Subject string Body string } func (e *Email) BuildMessage() []byte { message := fmt.Sprintf(\u0026#34;From: %s\\r\\n\u0026#34;, e.From) message += fmt.Sprintf(\u0026#34;To: %s\\r\\n\u0026#34;, e.To[0]) message += fmt.Sprintf(\u0026#34;Subject: %s\\r\\n\u0026#34;, e.Subject) message += \u0026#34;MIME-Version: 1.0\\r\\n\u0026#34; message += \u0026#34;Content-Type: text/plain; charset=UTF-8\\r\\n\u0026#34; message += \u0026#34;\\r\\n\u0026#34; message += e.Body return []byte(message) } func SendEmail(email *Email) error { smtpHost := os.Getenv(\u0026#34;SMTP_HOST\u0026#34;) smtpPort := os.Getenv(\u0026#34;SMTP_PORT\u0026#34;) smtpUser := os.Getenv(\u0026#34;SMTP_USER\u0026#34;) smtpPass := os.Getenv(\u0026#34;SMTP_PASS\u0026#34;) if smtpHost == \u0026#34;\u0026#34; || smtpPort == \u0026#34;\u0026#34; { return fmt.Errorf(\u0026#34;SMTP configuration missing\u0026#34;) } auth := smtp.PlainAuth(\u0026#34;\u0026#34;, smtpUser, smtpPass, smtpHost) addr := smtpHost + \u0026#34;:\u0026#34; + smtpPort message := email.BuildMessage() err := smtp.SendMail(addr, auth, email.From, email.To, message) if err != nil { return fmt.Errorf(\u0026#34;failed to send email: %w\u0026#34;, err) } return nil } func main() { email := \u0026amp;Email{ From: \u0026#34;sender@example.com\u0026#34;, To: []string{\u0026#34;recipient@example.com\u0026#34;}, Subject: \u0026#34;Welcome to Our Service\u0026#34;, Body: \u0026#34;Thank you for signing up! We\u0026#39;re excited to have you.\u0026#34;, } if err := SendEmail(email); err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;Email sent successfully!\u0026#34;) } Better. We added proper headers, environment variables for configuration, and error wrapping. But this is still plain text emails.\nSending HTML Emails with SMTP package main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;os\u0026#34; ) type HTMLEmail struct { From string To []string Subject string Body string // HTML content } func (e *HTMLEmail) BuildMessage() []byte { headers := make(map[string]string) headers[\u0026#34;From\u0026#34;] = e.From headers[\u0026#34;To\u0026#34;] = e.To[0] headers[\u0026#34;Subject\u0026#34;] = e.Subject headers[\u0026#34;MIME-Version\u0026#34;] = \u0026#34;1.0\u0026#34; headers[\u0026#34;Content-Type\u0026#34;] = \u0026#34;text/html; charset=UTF-8\u0026#34; message := \u0026#34;\u0026#34; for k, v := range headers { message += fmt.Sprintf(\u0026#34;%s: %s\\r\\n\u0026#34;, k, v) } message += \u0026#34;\\r\\n\u0026#34; + e.Body return []byte(message) } func SendHTMLEmail(email *HTMLEmail) error { smtpHost := os.Getenv(\u0026#34;SMTP_HOST\u0026#34;) smtpPort := os.Getenv(\u0026#34;SMTP_PORT\u0026#34;) smtpUser := os.Getenv(\u0026#34;SMTP_USER\u0026#34;) smtpPass := os.Getenv(\u0026#34;SMTP_PASS\u0026#34;) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, smtpUser, smtpPass, smtpHost) addr := smtpHost + \u0026#34;:\u0026#34; + smtpPort message := email.BuildMessage() return smtp.SendMail(addr, auth, email.From, email.To, message) } func main() { htmlBody := ` \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; } .container { max-width: 600px; margin: 0 auto; padding: 20px; } .header { background-color: #4CAF50; color: white; padding: 20px; text-align: center; } .content { padding: 20px; background-color: #f9f9f9; } .button { background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; display: inline-block; margin: 10px 0; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Welcome to Our Service!\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Hi there,\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Thank you for signing up. We\u0026#39;re excited to have you on board!\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Click the button below to get started:\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://example.com/get-started\u0026#34; class=\u0026#34;button\u0026#34;\u0026gt;Get Started\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;Best regards,\u0026lt;br\u0026gt;The Team\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ` email := \u0026amp;HTMLEmail{ From: \u0026#34;noreply@example.com\u0026#34;, To: []string{\u0026#34;user@example.com\u0026#34;}, Subject: \u0026#34;Welcome to Our Service\u0026#34;, Body: htmlBody, } if err := SendHTMLEmail(email); err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;HTML email sent successfully!\u0026#34;) } Now we\u0026rsquo;re sending styled HTML emails. But there\u0026rsquo;s a problem - what if the recipient\u0026rsquo;s email client doesn\u0026rsquo;t support HTML? We should send both plain text and HTML.\nMultipart Emails (Plain Text + HTML) package main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;mime/multipart\u0026#34; \u0026#34;net/smtp\u0026#34; \u0026#34;net/textproto\u0026#34; \u0026#34;os\u0026#34; ) type MultipartEmail struct { From string To []string Subject string TextBody string HTMLBody string } func (e *MultipartEmail) BuildMessage() ([]byte, error) { buf := bytes.NewBuffer(nil) // Write headers fmt.Fprintf(buf, \u0026#34;From: %s\\r\\n\u0026#34;, e.From) fmt.Fprintf(buf, \u0026#34;To: %s\\r\\n\u0026#34;, e.To[0]) fmt.Fprintf(buf, \u0026#34;Subject: %s\\r\\n\u0026#34;, e.Subject) fmt.Fprintf(buf, \u0026#34;MIME-Version: 1.0\\r\\n\u0026#34;) // Create multipart writer writer := multipart.NewWriter(buf) boundary := writer.Boundary() fmt.Fprintf(buf, \u0026#34;Content-Type: multipart/alternative; boundary=%s\\r\\n\u0026#34;, boundary) fmt.Fprintf(buf, \u0026#34;\\r\\n\u0026#34;) // Plain text part textHeader := textproto.MIMEHeader{} textHeader.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain; charset=UTF-8\u0026#34;) textHeader.Set(\u0026#34;Content-Transfer-Encoding\u0026#34;, \u0026#34;quoted-printable\u0026#34;) textPart, err := writer.CreatePart(textHeader) if err != nil { return nil, err } textPart.Write([]byte(e.TextBody)) // HTML part htmlHeader := textproto.MIMEHeader{} htmlHeader.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html; charset=UTF-8\u0026#34;) htmlHeader.Set(\u0026#34;Content-Transfer-Encoding\u0026#34;, \u0026#34;quoted-printable\u0026#34;) htmlPart, err := writer.CreatePart(htmlHeader) if err != nil { return nil, err } htmlPart.Write([]byte(e.HTMLBody)) writer.Close() return buf.Bytes(), nil } func SendMultipartEmail(email *MultipartEmail) error { smtpHost := os.Getenv(\u0026#34;SMTP_HOST\u0026#34;) smtpPort := os.Getenv(\u0026#34;SMTP_PORT\u0026#34;) smtpUser := os.Getenv(\u0026#34;SMTP_USER\u0026#34;) smtpPass := os.Getenv(\u0026#34;SMTP_PASS\u0026#34;) auth := smtp.PlainAuth(\u0026#34;\u0026#34;, smtpUser, smtpPass, smtpHost) message, err := email.BuildMessage() if err != nil { return fmt.Errorf(\u0026#34;failed to build message: %w\u0026#34;, err) } addr := smtpHost + \u0026#34;:\u0026#34; + smtpPort return smtp.SendMail(addr, auth, email.From, email.To, message) } func main() { email := \u0026amp;MultipartEmail{ From: \u0026#34;noreply@example.com\u0026#34;, To: []string{\u0026#34;user@example.com\u0026#34;}, Subject: \u0026#34;Welcome to Our Service\u0026#34;, TextBody: \u0026#34;Hi there,\\n\\nThank you for signing up!\\n\\nBest regards,\\nThe Team\u0026#34;, HTMLBody: `\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;\u0026lt;h1\u0026gt;Welcome!\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;Thank you for signing up!\u0026lt;/p\u0026gt;\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;`, } if err := SendMultipartEmail(email); err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;Multipart email sent!\u0026#34;) } This sends both plain text and HTML. Email clients show HTML if supported, otherwise fall back to plain text.\nSMTP works, but building MIME messages manually is tedious. Let\u0026rsquo;s look at libraries that make this easier.\nUsing Gomail Library for SMTP Gomail is a popular Go library that simplifies email sending. It handles MIME encoding, attachments, and embedded images.\nInstalling Gomail go get gopkg.in/gomail.v2 Basic Email with Gomail package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;gopkg.in/gomail.v2\u0026#34; ) func main() { m := gomail.NewMessage() // Set headers m.SetHeader(\u0026#34;From\u0026#34;, \u0026#34;sender@example.com\u0026#34;) m.SetHeader(\u0026#34;To\u0026#34;, \u0026#34;recipient@example.com\u0026#34;) m.SetHeader(\u0026#34;Subject\u0026#34;, \u0026#34;Hello from Gomail\u0026#34;) // Set body m.SetBody(\u0026#34;text/html\u0026#34;, \u0026#34;\u0026lt;h1\u0026gt;Hello!\u0026lt;/h1\u0026gt;\u0026lt;p\u0026gt;This is an HTML email.\u0026lt;/p\u0026gt;\u0026#34;) // Alternative plain text m.AddAlternative(\u0026#34;text/plain\u0026#34;, \u0026#34;Hello! This is a plain text fallback.\u0026#34;) // SMTP configuration port, _ := strconv.Atoi(os.Getenv(\u0026#34;SMTP_PORT\u0026#34;)) d := gomail.NewDialer( os.Getenv(\u0026#34;SMTP_HOST\u0026#34;), port, os.Getenv(\u0026#34;SMTP_USER\u0026#34;), os.Getenv(\u0026#34;SMTP_PASS\u0026#34;), ) // Send email if err := d.DialAndSend(m); err != nil { fmt.Println(\u0026#34;Failed to send email:\u0026#34;, err) return } fmt.Println(\u0026#34;Email sent successfully!\u0026#34;) } Much cleaner than manual MIME construction. Gomail handles all the encoding automatically.\nSending Email with Attachments package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;gopkg.in/gomail.v2\u0026#34; ) func SendEmailWithAttachment(to, subject, body, attachmentPath string) error { m := gomail.NewMessage() m.SetHeader(\u0026#34;From\u0026#34;, os.Getenv(\u0026#34;SMTP_USER\u0026#34;)) m.SetHeader(\u0026#34;To\u0026#34;, to) m.SetHeader(\u0026#34;Subject\u0026#34;, subject) m.SetBody(\u0026#34;text/html\u0026#34;, body) // Attach file m.Attach(attachmentPath) port, _ := strconv.Atoi(os.Getenv(\u0026#34;SMTP_PORT\u0026#34;)) d := gomail.NewDialer( os.Getenv(\u0026#34;SMTP_HOST\u0026#34;), port, os.Getenv(\u0026#34;SMTP_USER\u0026#34;), os.Getenv(\u0026#34;SMTP_PASS\u0026#34;), ) return d.DialAndSend(m) } func main() { htmlBody := ` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Invoice Attached\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Please find your invoice attached to this email.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Thank you for your business!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ` err := SendEmailWithAttachment( \u0026#34;customer@example.com\u0026#34;, \u0026#34;Your Invoice\u0026#34;, htmlBody, \u0026#34;./invoice.pdf\u0026#34;, ) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;Email with attachment sent!\u0026#34;) } Gomail makes attachments trivial. Just call Attach() with the file path.\nEmbedded Images in HTML package main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;gopkg.in/gomail.v2\u0026#34; ) func SendEmailWithEmbeddedImage() error { m := gomail.NewMessage() m.SetHeader(\u0026#34;From\u0026#34;, \u0026#34;noreply@example.com\u0026#34;) m.SetHeader(\u0026#34;To\u0026#34;, \u0026#34;user@example.com\u0026#34;) m.SetHeader(\u0026#34;Subject\u0026#34;, \u0026#34;Newsletter with Logo\u0026#34;) // HTML body referencing embedded image htmlBody := ` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;img src=\u0026#34;cid:company-logo\u0026#34; alt=\u0026#34;Company Logo\u0026#34; width=\u0026#34;200\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Monthly Newsletter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Here\u0026#39;s what\u0026#39;s new this month...\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ` m.SetBody(\u0026#34;text/html\u0026#34;, htmlBody) // Embed image with Content-ID m.Embed(\u0026#34;./logo.png\u0026#34;, gomail.Rename(\u0026#34;company-logo\u0026#34;)) port, _ := strconv.Atoi(os.Getenv(\u0026#34;SMTP_PORT\u0026#34;)) d := gomail.NewDialer( os.Getenv(\u0026#34;SMTP_HOST\u0026#34;), port, os.Getenv(\u0026#34;SMTP_USER\u0026#34;), os.Getenv(\u0026#34;SMTP_PASS\u0026#34;), ) return d.DialAndSend(m) } func main() { if err := SendEmailWithEmbeddedImage(); err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;Email with embedded image sent!\u0026#34;) } The cid: reference in HTML links to the embedded image. Email clients display it inline.\nGomail is great for SMTP, but for production we want better deliverability. Let\u0026rsquo;s move to SendGrid.\nSending Emails with SendGrid SendGrid is one of the most popular email services. It has excellent deliverability, generous free tier (100 emails/day), and simple API.\nSetting Up SendGrid Sign up at sendgrid.com Verify your sender email or domain Create an API key in Settings \u0026gt; API Keys Install the Go SDK: go get github.com/sendgrid/sendgrid-go Basic SendGrid Email package main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go/helpers/mail\u0026#34; ) func main() { from := mail.NewEmail(\u0026#34;Your App\u0026#34;, \u0026#34;noreply@yourdomain.com\u0026#34;) to := mail.NewEmail(\u0026#34;User Name\u0026#34;, \u0026#34;user@example.com\u0026#34;) subject := \u0026#34;Welcome to Our Service\u0026#34; plainTextContent := \u0026#34;Thank you for signing up!\u0026#34; htmlContent := \u0026#34;\u0026lt;strong\u0026gt;Thank you for signing up!\u0026lt;/strong\u0026gt;\u0026#34; message := mail.NewSingleEmail(from, subject, to, plainTextContent, htmlContent) client := sendgrid.NewSendClient(os.Getenv(\u0026#34;SENDGRID_API_KEY\u0026#34;)) response, err := client.Send(message) if err != nil { log.Println(\u0026#34;Error sending email:\u0026#34;, err) return } fmt.Printf(\u0026#34;Email sent! Status: %d\\n\u0026#34;, response.StatusCode) fmt.Println(\u0026#34;Response Body:\u0026#34;, response.Body) } SendGrid returns HTTP status codes. 202 means accepted for delivery.\nSendGrid Email Service Package Create a reusable email service:\n// internal/email/sendgrid.go package email import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go/helpers/mail\u0026#34; ) type SendGridService struct { apiKey string fromEmail string fromName string } func NewSendGridService(apiKey, fromEmail, fromName string) *SendGridService { return \u0026amp;SendGridService{ apiKey: apiKey, fromEmail: fromEmail, fromName: fromName, } } func (s *SendGridService) SendWelcomeEmail(toEmail, toName string) error { from := mail.NewEmail(s.fromName, s.fromEmail) to := mail.NewEmail(toName, toEmail) subject := \u0026#34;Welcome to Our Platform\u0026#34; plainText := fmt.Sprintf(\u0026#34;Hi %s,\\n\\nWelcome to our platform!\\n\\nBest regards,\\nThe Team\u0026#34;, toName) html := fmt.Sprintf(` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome, %s!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Thank you for joining our platform.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;We\u0026#39;re excited to have you on board.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://yourapp.com/get-started\u0026#34; style=\u0026#34;background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none;\u0026#34;\u0026gt;Get Started\u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `, toName) message := mail.NewSingleEmail(from, subject, to, plainText, html) client := sendgrid.NewSendClient(s.apiKey) response, err := client.Send(message) if err != nil { return fmt.Errorf(\u0026#34;sendgrid error: %w\u0026#34;, err) } if response.StatusCode \u0026gt;= 400 { return fmt.Errorf(\u0026#34;sendgrid returned status %d: %s\u0026#34;, response.StatusCode, response.Body) } return nil } func (s *SendGridService) SendPasswordReset(toEmail, resetToken string) error { from := mail.NewEmail(s.fromName, s.fromEmail) to := mail.NewEmail(\u0026#34;\u0026#34;, toEmail) subject := \u0026#34;Password Reset Request\u0026#34; resetURL := fmt.Sprintf(\u0026#34;https://yourapp.com/reset-password?token=%s\u0026#34;, resetToken) plainText := fmt.Sprintf(\u0026#34;Click this link to reset your password: %s\\n\\nIf you didn\u0026#39;t request this, ignore this email.\u0026#34;, resetURL) html := fmt.Sprintf(` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Password Reset Request\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;We received a request to reset your password.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Click the button below to reset your password:\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;%s\u0026#34; style=\u0026#34;background-color: #007bff; color: white; padding: 12px 24px; text-decoration: none; display: inline-block;\u0026#34;\u0026gt;Reset Password\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;If you didn\u0026#39;t request this, you can safely ignore this email.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;This link expires in 1 hour.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `, resetURL) message := mail.NewSingleEmail(from, subject, to, plainText, html) client := sendgrid.NewSendClient(s.apiKey) response, err := client.Send(message) if err != nil { return fmt.Errorf(\u0026#34;sendgrid error: %w\u0026#34;, err) } if response.StatusCode \u0026gt;= 400 { return fmt.Errorf(\u0026#34;sendgrid returned status %d: %s\u0026#34;, response.StatusCode, response.Body) } return nil } Usage in your application:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;yourapp/internal/email\u0026#34; ) func main() { emailService := email.NewSendGridService( os.Getenv(\u0026#34;SENDGRID_API_KEY\u0026#34;), \u0026#34;noreply@yourdomain.com\u0026#34;, \u0026#34;Your App Name\u0026#34;, ) // Send welcome email err := emailService.SendWelcomeEmail(\u0026#34;user@example.com\u0026#34;, \u0026#34;John Doe\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Welcome email sent!\u0026#34;) // Send password reset err = emailService.SendPasswordReset(\u0026#34;user@example.com\u0026#34;, \u0026#34;reset-token-123\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Password reset email sent!\u0026#34;) } SendGrid with Templates SendGrid has a powerful template system. Create templates in the SendGrid dashboard, then reference them in code:\npackage email import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go/helpers/mail\u0026#34; ) func (s *SendGridService) SendTemplatedEmail(toEmail, toName, templateID string, data map[string]interface{}) error { from := mail.NewEmail(s.fromName, s.fromEmail) to := mail.NewEmail(toName, toEmail) message := mail.NewV3Mail() message.SetFrom(from) message.SetTemplateID(templateID) personalization := mail.NewPersonalization() personalization.AddTos(to) // Add dynamic template data for key, value := range data { personalization.SetDynamicTemplateData(key, value) } message.AddPersonalizations(personalization) client := sendgrid.NewSendClient(s.apiKey) response, err := client.Send(message) if err != nil { return fmt.Errorf(\u0026#34;sendgrid error: %w\u0026#34;, err) } if response.StatusCode \u0026gt;= 400 { return fmt.Errorf(\u0026#34;sendgrid returned status %d: %s\u0026#34;, response.StatusCode, response.Body) } return nil } Usage:\n// Send using template templateData := map[string]interface{}{ \u0026#34;user_name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;12345\u0026#34;, \u0026#34;total_amount\u0026#34;: \u0026#34;$99.99\u0026#34;, } err := emailService.SendTemplatedEmail( \u0026#34;user@example.com\u0026#34;, \u0026#34;John Doe\u0026#34;, \u0026#34;d-template-id-here\u0026#34;, // Get from SendGrid dashboard templateData, ) Templates separate email design from code. Marketers can update emails without developer involvement.\nSendGrid with Attachments package email import ( \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go\u0026#34; \u0026#34;github.com/sendgrid/sendgrid-go/helpers/mail\u0026#34; ) func (s *SendGridService) SendEmailWithAttachment(toEmail, subject, body, filePath string) error { from := mail.NewEmail(s.fromName, s.fromEmail) to := mail.NewEmail(\u0026#34;\u0026#34;, toEmail) message := mail.NewSingleEmail(from, subject, to, body, body) // Read file fileData, err := os.ReadFile(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to read file: %w\u0026#34;, err) } // Encode to base64 encoded := base64.StdEncoding.EncodeToString(fileData) // Create attachment attachment := mail.NewAttachment() attachment.SetContent(encoded) attachment.SetType(\u0026#34;application/pdf\u0026#34;) attachment.SetFilename(\u0026#34;invoice.pdf\u0026#34;) attachment.SetDisposition(\u0026#34;attachment\u0026#34;) message.AddAttachment(attachment) client := sendgrid.NewSendClient(s.apiKey) response, err := client.Send(message) if err != nil { return fmt.Errorf(\u0026#34;sendgrid error: %w\u0026#34;, err) } if response.StatusCode \u0026gt;= 400 { return fmt.Errorf(\u0026#34;sendgrid returned status %d\u0026#34;, response.StatusCode) } return nil } SendGrid handles attachments well, but watch file sizes. Keep attachments under 10MB for best deliverability.\nSending Emails with Mailgun Mailgun is another excellent email service. It\u0026rsquo;s particularly strong for high-volume sending and has powerful routing features.\nSetting Up Mailgun Sign up at mailgun.com Add and verify your domain Get your API key from Settings Install the Go SDK: go get github.com/mailgun/mailgun-go/v4 Basic Mailgun Email package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/mailgun/mailgun-go/v4\u0026#34; ) func main() { mg := mailgun.NewMailgun( os.Getenv(\u0026#34;MAILGUN_DOMAIN\u0026#34;), os.Getenv(\u0026#34;MAILGUN_API_KEY\u0026#34;), ) sender := \u0026#34;noreply@yourdomain.com\u0026#34; subject := \u0026#34;Welcome to Our Service\u0026#34; body := \u0026#34;Thank you for signing up!\u0026#34; recipient := \u0026#34;user@example.com\u0026#34; message := mg.NewMessage(sender, subject, body, recipient) ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() resp, id, err := mg.Send(ctx, message) if err != nil { log.Fatal(err) } fmt.Printf(\u0026#34;ID: %s Resp: %s\\n\u0026#34;, id, resp) } Mailgun returns a message ID you can use for tracking.\nMailgun Email Service Package // internal/email/mailgun.go package email import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/mailgun/mailgun-go/v4\u0026#34; ) type MailgunService struct { mg *mailgun.MailgunImpl domain string fromEmail string fromName string } func NewMailgunService(domain, apiKey, fromEmail, fromName string) *MailgunService { mg := mailgun.NewMailgun(domain, apiKey) return \u0026amp;MailgunService{ mg: mg, domain: domain, fromEmail: fromEmail, fromName: fromName, } } func (s *MailgunService) SendWelcomeEmail(toEmail, toName string) error { sender := fmt.Sprintf(\u0026#34;%s \u0026lt;%s\u0026gt;\u0026#34;, s.fromName, s.fromEmail) subject := \u0026#34;Welcome to Our Platform\u0026#34; text := fmt.Sprintf(\u0026#34;Hi %s,\\n\\nWelcome to our platform!\\n\\nBest regards,\\nThe Team\u0026#34;, toName) html := fmt.Sprintf(` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome, %s!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Thank you for joining our platform.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;https://yourapp.com/get-started\u0026#34;\u0026gt;Get Started\u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `, toName) message := s.mg.NewMessage(sender, subject, text, toEmail) message.SetHtml(html) ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() _, _, err := s.mg.Send(ctx, message) if err != nil { return fmt.Errorf(\u0026#34;mailgun error: %w\u0026#34;, err) } return nil } func (s *MailgunService) SendPasswordReset(toEmail, resetToken string) error { sender := fmt.Sprintf(\u0026#34;%s \u0026lt;%s\u0026gt;\u0026#34;, s.fromName, s.fromEmail) subject := \u0026#34;Password Reset Request\u0026#34; resetURL := fmt.Sprintf(\u0026#34;https://yourapp.com/reset-password?token=%s\u0026#34;, resetToken) text := fmt.Sprintf(\u0026#34;Click this link to reset your password: %s\u0026#34;, resetURL) html := fmt.Sprintf(` \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h2\u0026gt;Password Reset Request\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Click the link below to reset your password:\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;%s\u0026#34;\u0026gt;Reset Password\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;This link expires in 1 hour.\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; `, resetURL) message := s.mg.NewMessage(sender, subject, text, toEmail) message.SetHtml(html) ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() _, _, err := s.mg.Send(ctx, message) if err != nil { return fmt.Errorf(\u0026#34;mailgun error: %w\u0026#34;, err) } return nil } Mailgun with Tags and Tracking Mailgun lets you tag emails for analytics and add custom variables:\nfunc (s *MailgunService) SendEmailWithTracking(toEmail, subject, body string, tags []string) error { sender := fmt.Sprintf(\u0026#34;%s \u0026lt;%s\u0026gt;\u0026#34;, s.fromName, s.fromEmail) message := s.mg.NewMessage(sender, subject, body, toEmail) // Add tags for tracking for _, tag := range tags { message.AddTag(tag) } // Add custom variables message.AddVariable(\u0026#34;campaign_id\u0026#34;, \u0026#34;summer-2025\u0026#34;) message.AddVariable(\u0026#34;user_type\u0026#34;, \u0026#34;premium\u0026#34;) // Enable click and open tracking message.SetTracking(true) message.SetTrackingClicks(true) message.SetTrackingOpens(true) ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() _, _, err := s.mg.Send(ctx, message) return err } Tags help organize emails in Mailgun analytics. Custom variables let you store metadata with each email.\nMailgun Scheduled Sending Schedule emails for future delivery:\nfunc (s *MailgunService) SendScheduledEmail(toEmail, subject, body string, deliveryTime time.Time) error { sender := fmt.Sprintf(\u0026#34;%s \u0026lt;%s\u0026gt;\u0026#34;, s.fromName, s.fromEmail) message := s.mg.NewMessage(sender, subject, body, toEmail) message.SetDeliveryTime(deliveryTime) ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() _, id, err := s.mg.Send(ctx, message) if err != nil { return fmt.Errorf(\u0026#34;mailgun error: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Scheduled email ID: %s\\n\u0026#34;, id) return nil } Usage:\n// Send email in 24 hours deliveryTime := time.Now().Add(24 * time.Hour) emailService.SendScheduledEmail( \u0026#34;user@example.com\u0026#34;, \u0026#34;Don\u0026#39;t forget!\u0026#34;, \u0026#34;Your trial ends tomorrow.\u0026#34;, deliveryTime, ) Mailgun queues the email and sends it at the specified time.\nEmail Templates in Go Hardcoding HTML in strings is messy. Use Go\u0026rsquo;s html/template package for maintainable email templates.\nCreating Email Templates Create templates/welcome.html:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; } .container { max-width: 600px; margin: 0 auto; } .header { background-color: #4CAF50; color: white; padding: 20px; } .content { padding: 20px; } .button { background-color: #4CAF50; color: white; padding: 10px 20px; text-decoration: none; display: inline-block; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Welcome, {{.Name}}!\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Thank you for signing up for {{.AppName}}.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Your account is ready to use.\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;{{.GetStartedURL}}\u0026#34; class=\u0026#34;button\u0026#34;\u0026gt;Get Started\u0026lt;/a\u0026gt; \u0026lt;p\u0026gt;Best regards,\u0026lt;br\u0026gt;The {{.AppName}} Team\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Template service:\n// internal/email/templates.go package email import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;html/template\u0026#34; \u0026#34;path/filepath\u0026#34; ) type EmailTemplate struct { templatesDir string } func NewEmailTemplate(templatesDir string) *EmailTemplate { return \u0026amp;EmailTemplate{ templatesDir: templatesDir, } } func (et *EmailTemplate) Render(templateName string, data interface{}) (string, error) { templatePath := filepath.Join(et.templatesDir, templateName) tmpl, err := template.ParseFiles(templatePath) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to parse template: %w\u0026#34;, err) } var buf bytes.Buffer if err := tmpl.Execute(\u0026amp;buf, data); err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to execute template: %w\u0026#34;, err) } return buf.String(), nil } Usage:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;yourapp/internal/email\u0026#34; ) func main() { templateService := email.NewEmailTemplate(\u0026#34;./templates\u0026#34;) data := map[string]interface{}{ \u0026#34;Name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;AppName\u0026#34;: \u0026#34;YourApp\u0026#34;, \u0026#34;GetStartedURL\u0026#34;: \u0026#34;https://yourapp.com/dashboard\u0026#34;, } html, err := templateService.Render(\u0026#34;welcome.html\u0026#34;, data) if err != nil { log.Fatal(err) } fmt.Println(html) // Now send this HTML via SendGrid or Mailgun } Templates make emails easier to maintain. Designers can edit HTML without touching Go code.\nTemplate with Partials For shared components like headers and footers:\n\u0026lt;!-- templates/partials/header.html --\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34; style=\u0026#34;background-color: #4CAF50; color: white; padding: 20px;\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{.AppName}}\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- templates/partials/footer.html --\u0026gt; \u0026lt;div class=\u0026#34;footer\u0026#34; style=\u0026#34;padding: 20px; text-align: center; color: #666;\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026amp;copy; 2025 {{.AppName}}. All rights reserved.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;{{.UnsubscribeURL}}\u0026#34;\u0026gt;Unsubscribe\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- templates/newsletter.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{template \u0026#34;header\u0026#34; .}} \u0026lt;div class=\u0026#34;content\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{.Subject}}\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;{{.Message}}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{template \u0026#34;footer\u0026#34; .}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Parse multiple templates:\nfunc (et *EmailTemplate) RenderWithPartials(templateName string, data interface{}) (string, error) { templatePath := filepath.Join(et.templatesDir, templateName) headerPath := filepath.Join(et.templatesDir, \u0026#34;partials\u0026#34;, \u0026#34;header.html\u0026#34;) footerPath := filepath.Join(et.templatesDir, \u0026#34;partials\u0026#34;, \u0026#34;footer.html\u0026#34;) tmpl, err := template.ParseFiles(templatePath, headerPath, footerPath) if err != nil { return \u0026#34;\u0026#34;, err } var buf bytes.Buffer if err := tmpl.Execute(\u0026amp;buf, data); err != nil { return \u0026#34;\u0026#34;, err } return buf.String(), nil } Partials let you reuse email components across multiple templates.\nError Handling and Retries Email sending can fail. Networks timeout, APIs have outages, rate limits trigger. Production apps need robust error handling.\nRetry Logic package email import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type RetryConfig struct { MaxAttempts int InitialDelay time.Duration MaxDelay time.Duration Multiplier float64 } func DefaultRetryConfig() *RetryConfig { return \u0026amp;RetryConfig{ MaxAttempts: 3, InitialDelay: time.Second, MaxDelay: time.Second * 30, Multiplier: 2.0, } } func (s *SendGridService) SendWithRetry(toEmail, subject, body string, config *RetryConfig) error { var lastErr error delay := config.InitialDelay for attempt := 1; attempt \u0026lt;= config.MaxAttempts; attempt++ { err := s.sendEmail(toEmail, subject, body) if err == nil { return nil } lastErr = err if attempt \u0026lt; config.MaxAttempts { time.Sleep(delay) // Exponential backoff delay = time.Duration(float64(delay) * config.Multiplier) if delay \u0026gt; config.MaxDelay { delay = config.MaxDelay } } } return fmt.Errorf(\u0026#34;failed after %d attempts: %w\u0026#34;, config.MaxAttempts, lastErr) } func (s *SendGridService) sendEmail(toEmail, subject, body string) error { // Your SendGrid send logic here return nil } This implements exponential backoff. First retry waits 1 second, second waits 2 seconds, third waits 4 seconds.\nCircuit Breaker Pattern Prevent cascading failures when email service is down:\npackage email import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type CircuitBreaker struct { maxFailures int timeout time.Duration failures int lastFailure time.Time state string // \u0026#34;closed\u0026#34;, \u0026#34;open\u0026#34;, \u0026#34;half-open\u0026#34; mu sync.Mutex } func NewCircuitBreaker(maxFailures int, timeout time.Duration) *CircuitBreaker { return \u0026amp;CircuitBreaker{ maxFailures: maxFailures, timeout: timeout, state: \u0026#34;closed\u0026#34;, } } func (cb *CircuitBreaker) Call(fn func() error) error { cb.mu.Lock() if cb.state == \u0026#34;open\u0026#34; { if time.Since(cb.lastFailure) \u0026gt; cb.timeout { cb.state = \u0026#34;half-open\u0026#34; cb.failures = 0 } else { cb.mu.Unlock() return fmt.Errorf(\u0026#34;circuit breaker is open\u0026#34;) } } cb.mu.Unlock() err := fn() cb.mu.Lock() defer cb.mu.Unlock() if err != nil { cb.failures++ cb.lastFailure = time.Now() if cb.failures \u0026gt;= cb.maxFailures { cb.state = \u0026#34;open\u0026#34; } return err } if cb.state == \u0026#34;half-open\u0026#34; { cb.state = \u0026#34;closed\u0026#34; } cb.failures = 0 return nil } Usage:\nbreaker := email.NewCircuitBreaker(5, time.Minute*5) err := breaker.Call(func() error { return emailService.SendWelcomeEmail(\u0026#34;user@example.com\u0026#34;, \u0026#34;John\u0026#34;) }) if err != nil { log.Println(\u0026#34;Email failed:\u0026#34;, err) } After 5 failures, circuit opens for 5 minutes. This prevents hammering a failed service.\nEmail Queue with Background Jobs Don\u0026rsquo;t send emails in HTTP handlers. Use background jobs for better performance and reliability.\nIntegration with Asynq (from previous article):\n// internal/tasks/email.go package tasks import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; \u0026#34;yourapp/internal/email\u0026#34; ) const ( TypeEmailWelcome = \u0026#34;email:welcome\u0026#34; TypeEmailPasswordReset = \u0026#34;email:password_reset\u0026#34; ) type EmailWelcomePayload struct { Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } func NewEmailWelcomeTask(email, name string) (*asynq.Task, error) { payload, err := json.Marshal(EmailWelcomePayload{ Email: email, Name: name, }) if err != nil { return nil, err } return asynq.NewTask(TypeEmailWelcome, payload), nil } func HandleEmailWelcomeTask(ctx context.Context, t *asynq.Task) error { var p EmailWelcomePayload if err := json.Unmarshal(t.Payload(), \u0026amp;p); err != nil { return err } emailService := email.NewSendGridService( os.Getenv(\u0026#34;SENDGRID_API_KEY\u0026#34;), \u0026#34;noreply@yourdomain.com\u0026#34;, \u0026#34;Your App\u0026#34;, ) return emailService.SendWelcomeEmail(p.Email, p.Name) } In your HTTP handler:\nfunc signupHandler(w http.ResponseWriter, r *http.Request) { email := r.FormValue(\u0026#34;email\u0026#34;) name := r.FormValue(\u0026#34;name\u0026#34;) // Create user in database user := createUser(email, name) // Queue welcome email task, _ := tasks.NewEmailWelcomeTask(user.Email, user.Name) queueClient.Enqueue(task, asynq.Queue(\u0026#34;emails\u0026#34;)) // Return immediately w.WriteHeader(http.StatusCreated) json.NewEncoder(w).Encode(map[string]string{ \u0026#34;message\u0026#34;: \u0026#34;Account created! Check your email.\u0026#34;, }) } User gets instant response. Email sends in background. If email fails, Asynq retries automatically.\nFor more on background jobs, see our guide on implementing background jobs with Asynq .\nAvoiding Spam Filters Even with email services, emails can land in spam. Here\u0026rsquo;s how to avoid it.\nDomain Authentication Set up SPF, DKIM, and DMARC records:\nSPF Record (DNS TXT):\nv=spf1 include:sendgrid.net ~all DKIM - SendGrid/Mailgun provide keys to add to DNS.\nDMARC Record:\nv=DMARC1; p=quarantine; rua=mailto:dmarc@yourdomain.com These prove your emails come from authorized servers.\nEmail Content Best Practices Spam filters analyze content. Follow these rules:\nSubject lines:\nAvoid ALL CAPS Don\u0026rsquo;t use excessive punctuation (!!!) Skip spam words: \u0026ldquo;FREE\u0026rdquo;, \u0026ldquo;WINNER\u0026rdquo;, \u0026ldquo;CLICK HERE\u0026rdquo; Keep under 50 characters Body content:\nInclude unsubscribe link Use real sender name and address Balance text and images Avoid URL shorteners Include physical address (legal requirement) HTML:\nKeep HTML simple Avoid JavaScript Use inline CSS Test in multiple clients Include plain text version Sender Reputation ISPs track your sending patterns:\nWarm up new domains - Start with low volumes (100/day), gradually increase over weeks.\nMonitor bounce rates - Keep under 2%. Remove invalid addresses quickly.\nWatch spam complaints - Under 0.1% is good. Over 0.5% is bad.\nConsistent sending - Don\u0026rsquo;t go from 0 to 10,000 emails overnight.\nEngagement matters - High open rates improve reputation. Low engagement hurts it.\nComparison: SMTP vs SendGrid vs Mailgun Here\u0026rsquo;s how the three approaches compare:\nFeature Raw SMTP SendGrid Mailgun Setup Complexity High Low Low Deliverability Poor (self-managed) Excellent Excellent Cost Free (own server) Free tier: 100/day Free trial: 5k/month Pricing (1M emails) Infrastructure costs $14.95/month $35/month Analytics None Detailed Detailed Templates Manual Built-in API-based Bounce Handling Manual Automatic Automatic API Quality SMTP only Excellent REST API Excellent REST API Webhooks No Yes Yes Learning Curve Low Low Medium Best For Testing Getting started fast High volume When to use each:\nSMTP - Local development and testing only. Use Mailtrap.io or Gmail for dev environments.\nSendGrid - Best for:\nSmall to medium volume Quick setup needed Want simple pricing Need excellent documentation Mailgun - Best for:\nHigh volume sending Need advanced routing Want better pricing at scale Prefer more control For most Go applications, start with SendGrid. Switch to Mailgun when you exceed SendGrid\u0026rsquo;s pricing sweet spot.\nProduction Best Practices Environment Configuration // internal/config/email.go package config import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; ) type EmailConfig struct { Provider string // \u0026#34;sendgrid\u0026#34; or \u0026#34;mailgun\u0026#34; APIKey string Domain string // For Mailgun FromEmail string FromName string ReplyTo string } func LoadEmailConfig() (*EmailConfig, error) { provider := os.Getenv(\u0026#34;EMAIL_PROVIDER\u0026#34;) if provider == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;EMAIL_PROVIDER not set\u0026#34;) } config := \u0026amp;EmailConfig{ Provider: provider, FromEmail: os.Getenv(\u0026#34;EMAIL_FROM\u0026#34;), FromName: os.Getenv(\u0026#34;EMAIL_FROM_NAME\u0026#34;), ReplyTo: os.Getenv(\u0026#34;EMAIL_REPLY_TO\u0026#34;), } switch provider { case \u0026#34;sendgrid\u0026#34;: config.APIKey = os.Getenv(\u0026#34;SENDGRID_API_KEY\u0026#34;) if config.APIKey == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;SENDGRID_API_KEY not set\u0026#34;) } case \u0026#34;mailgun\u0026#34;: config.APIKey = os.Getenv(\u0026#34;MAILGUN_API_KEY\u0026#34;) config.Domain = os.Getenv(\u0026#34;MAILGUN_DOMAIN\u0026#34;) if config.APIKey == \u0026#34;\u0026#34; || config.Domain == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;MAILGUN_API_KEY or MAILGUN_DOMAIN not set\u0026#34;) } default: return nil, fmt.Errorf(\u0026#34;unknown email provider: %s\u0026#34;, provider) } return config, nil } Environment variables (.env):\nEMAIL_PROVIDER=sendgrid SENDGRID_API_KEY=your-api-key EMAIL_FROM=noreply@yourdomain.com EMAIL_FROM_NAME=Your App Name EMAIL_REPLY_TO=support@yourdomain.com Email Interface for Flexibility Abstract email sending behind an interface:\n// internal/email/interface.go package email type EmailSender interface { SendWelcomeEmail(toEmail, toName string) error SendPasswordReset(toEmail, resetToken string) error SendVerificationEmail(toEmail, verificationToken string) error } func NewEmailSender(provider, apiKey, domain, fromEmail, fromName string) (EmailSender, error) { switch provider { case \u0026#34;sendgrid\u0026#34;: return NewSendGridService(apiKey, fromEmail, fromName), nil case \u0026#34;mailgun\u0026#34;: return NewMailgunService(domain, apiKey, fromEmail, fromName), nil default: return nil, fmt.Errorf(\u0026#34;unknown provider: %s\u0026#34;, provider) } } Your application code depends on the interface, not specific implementations. Switching providers is configuration change, not code change.\nLogging and Monitoring package email import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; ) type LoggingEmailService struct { underlying EmailSender } func NewLoggingEmailService(underlying EmailSender) *LoggingEmailService { return \u0026amp;LoggingEmailService{underlying: underlying} } func (l *LoggingEmailService) SendWelcomeEmail(toEmail, toName string) error { start := time.Now() log.Printf(\u0026#34;Sending welcome email to %s (%s)\u0026#34;, toName, toEmail) err := l.underlying.SendWelcomeEmail(toEmail, toName) duration := time.Since(start) if err != nil { log.Printf(\u0026#34;Failed to send welcome email to %s: %v (took %v)\u0026#34;, toEmail, err, duration) return err } log.Printf(\u0026#34;Successfully sent welcome email to %s (took %v)\u0026#34;, toEmail, duration) return nil } Wrap your email service with logging. Track successes, failures, and latency.\nFor production monitoring, integrate with your observability stack. Send metrics to Prometheus:\nimport \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; var ( emailsSent = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;emails_sent_total\u0026#34;, Help: \u0026#34;Total number of emails sent\u0026#34;, }, []string{\u0026#34;type\u0026#34;, \u0026#34;status\u0026#34;}, ) emailDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;email_send_duration_seconds\u0026#34;, Help: \u0026#34;Email sending duration\u0026#34;, }, []string{\u0026#34;type\u0026#34;}, ) ) Testing Email Code Don\u0026rsquo;t send real emails in tests. Use mock services:\n// internal/email/mock.go package email type MockEmailService struct { SentEmails []SentEmail } type SentEmail struct { To string Subject string Body string } func NewMockEmailService() *MockEmailService { return \u0026amp;MockEmailService{ SentEmails: make([]SentEmail, 0), } } func (m *MockEmailService) SendWelcomeEmail(toEmail, toName string) error { m.SentEmails = append(m.SentEmails, SentEmail{ To: toEmail, Subject: \u0026#34;Welcome\u0026#34;, Body: \u0026#34;Welcome email body\u0026#34;, }) return nil } In tests:\nfunc TestSignupHandler(t *testing.T) { mockEmail := email.NewMockEmailService() // Your handler uses mockEmail handler := NewSignupHandler(mockEmail) // Make request handler.HandleSignup(email, name) // Verify email was sent if len(mockEmail.SentEmails) != 1 { t.Errorf(\u0026#34;Expected 1 email, got %d\u0026#34;, len(mockEmail.SentEmails)) } if mockEmail.SentEmails[0].To != \u0026#34;user@example.com\u0026#34; { t.Errorf(\u0026#34;Wrong recipient\u0026#34;) } } For integration testing, use Mailtrap.io or MailHog - they\u0026rsquo;re SMTP servers that catch emails instead of delivering them.\nTroubleshooting Common Issues Emails not arriving:\nCheck spam folder first Verify API key is correct Check sender email is verified Look for errors in logs Verify email address is valid High bounce rate:\nValidate emails before sending Remove invalid addresses Check for typos in email addresses Verify domain hasn\u0026rsquo;t blocked you Marked as spam:\nSet up SPF/DKIM/DMARC Avoid spam trigger words Include unsubscribe link Send from consistent address Warm up sending domain API rate limits:\nImplement exponential backoff Spread sends over time Use background jobs Upgrade to higher tier Images not displaying:\nUse absolute URLs Host images on CDN Test in multiple email clients Consider embedding images Wrapping Up Email sending in Go is straightforward once you understand the options. SMTP works for testing but production needs email services like SendGrid or Mailgun.\nSendGrid is easier to start with - great free tier, simple API, excellent docs. Mailgun offers better pricing at scale and more advanced features. Both handle deliverability, bounces, and analytics so you don\u0026rsquo;t have to.\nThe key patterns: use background jobs to avoid blocking requests, implement retry logic for reliability, abstract behind interfaces for flexibility, monitor failures and delivery rates, and test with mocks to avoid sending real emails.\nStart simple with SendGrid\u0026rsquo;s free tier. As your volume grows, evaluate if Mailgun\u0026rsquo;s pricing works better. Both are production-ready and trusted by thousands of applications.\nFor building complete Go applications, check out our guides on REST API development with Gin , database migrations , and background job processing .\nEmail is essential infrastructure. Get it right from the start and your users will actually receive the messages you send them.\n","href":"/2025/10/how-to-send-emails-in-go-smtp-sendgrid-mailgun.html","title":"How to Send Emails in Go - SMTP, SendGrid, and Mailgun Integration"},{"content":"Your API is slow. Not because the code is inefficient, but because you\u0026rsquo;re doing too much in the HTTP request cycle. Sending emails, processing images, generating reports - all blocking the response while the user waits. That\u0026rsquo;s not how you scale.\nWhat are background jobs? Background jobs are tasks that run asynchronously outside the main request-response cycle. Instead of making users wait while your server processes heavy workloads, you push these tasks into a queue and handle them separately in worker processes. This keeps your API fast and responsive.\nBackground jobs solve this. Push slow work into a queue, return a response immediately, and process tasks asynchronously in worker processes. Your API stays fast, users get instant feedback, and heavy workloads don\u0026rsquo;t crash your server.\nAsynq is a Go library built specifically for this. It uses Redis as a message broker and provides everything you need: automatic retries with exponential backoff, task scheduling, priority queues, worker pools, and a web UI for monitoring. Unlike generic message queues that require boilerplate, Asynq gives you a complete background job system with minimal setup.\nThis guide covers everything - from basic task enqueueing to production deployment with monitoring, error handling, and performance optimization. We\u0026rsquo;ll build a real system that sends emails, processes images, and generates reports in the background while keeping your API responsive.\nWhy Background Jobs Matter in Go Applications Imagine a user uploads a profile picture. Without background jobs, your handler does this synchronously:\nfunc uploadAvatar(w http.ResponseWriter, r *http.Request) { file, _ := r.FormFile(\u0026#34;avatar\u0026#34;) // All of this blocks the HTTP response resized := resizeImage(file) // 2 seconds thumbnail := createThumbnail(file) // 1 second uploadToS3(resized) // 3 seconds uploadToS3(thumbnail) // 2 seconds updateDatabase(userID, urls) // 0.5 seconds w.Write([]byte(\u0026#34;Upload complete\u0026#34;)) // 8.5 seconds later! } The user waits 8+ seconds staring at a loading spinner. If 10 users upload simultaneously, your server is processing 10 images concurrently, maxing out CPU and memory.\nWith background jobs:\nfunc uploadAvatar(w http.ResponseWriter, r *http.Request) { file, _ := r.FormFile(\u0026#34;avatar\u0026#34;) // Save to temp storage tempPath := saveToTemp(file) // 100ms // Enqueue background job client.Enqueue(tasks.NewProcessAvatarTask(userID, tempPath)) w.Write([]byte(\u0026#34;Upload started\u0026#34;)) // 100ms response! } Response returns in 100ms. Image processing happens in the background. If 10 users upload, jobs queue up and process at a controlled rate. Your API stays responsive.\nWhen to use background jobs:\nSending emails - SMTP calls can take seconds and fail unpredictably.\nImage/video processing - CPU-intensive work that shouldn\u0026rsquo;t block requests.\nReport generation - Complex queries and PDF generation take time.\nExternal API calls - Third-party APIs are slow and unreliable.\nScheduled tasks - Daily cleanup, weekly reports, monthly billing.\nWebhooks - Calling user webhooks shouldn\u0026rsquo;t delay your response.\nBasically, anything that takes \u0026gt;100ms or can fail should be a background job.\nUnderstanding Asynq Architecture Asynq has three main components:\nClient - Enqueues tasks into Redis. Your web application uses the client to push work into queues.\nServer - Worker process that pulls tasks from Redis and executes them. Run multiple servers to scale horizontally.\nRedis - Message broker that stores tasks, manages queues, and coordinates between clients and servers.\nThe flow:\nWeb App (Client) -\u0026gt; Redis -\u0026gt; Worker (Server) ↓ ↓ ↓ Enqueue Task Store in Queue Process Task Tasks are stored in Redis lists. Workers use BLPOP to atomically pull tasks and process them. If a worker crashes mid-task, Asynq detects it and requeues the task.\nAsynq creates several Redis queues automatically:\nActive queue - Tasks currently being processed Pending queue - Tasks waiting to be processed Scheduled queue - Tasks scheduled for future execution Retry queue - Failed tasks waiting for retry Archived queue - Tasks that exceeded max retries Dead queue - Permanently failed tasks You define custom queues (default, critical, low_priority) to separate different types of work.\nInstalling Asynq and Redis Install Asynq:\ngo get -u github.com/hibiken/asynq For development, run Redis with Docker:\ndocker run -d --name redis -p 6379:6379 redis:7-alpine Or install Redis locally:\n# macOS brew install redis brew services start redis # Ubuntu/Debian sudo apt install redis-server sudo systemctl start redis # Verify redis-cli ping # PONG For production, use managed Redis:\nRedis Cloud - Official managed Redis AWS ElastiCache - AWS managed Redis Google Cloud Memorystore - GCP managed Redis Azure Cache for Redis - Azure managed Redis Asynq requires Redis 4.0+. Redis 7 is recommended for best performance.\nProject Structure for Background Jobs Here\u0026rsquo;s a production-ready structure for background jobs:\nmyapp/ ├── cmd/ │ ├── api/ │ │ └── main.go # Web API server │ └── worker/ │ └── main.go # Background worker ├── internal/ │ ├── tasks/ │ │ ├── email.go # Email tasks │ │ ├── image.go # Image processing tasks │ │ └── report.go # Report generation tasks │ ├── worker/ │ │ └── worker.go # Worker setup │ └── queue/ │ └── client.go # Queue client ├── go.mod └── go.sum Separate API and worker into different binaries. This lets you scale them independently - run 5 API servers and 20 workers, or vice versa.\nCreating Your First Task Tasks are functions that perform background work. Create internal/tasks/email.go:\npackage tasks import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) // Task type constants const ( TypeEmailWelcome = \u0026#34;email:welcome\u0026#34; TypeEmailPassword = \u0026#34;email:password_reset\u0026#34; ) // EmailWelcomePayload is the task payload type EmailWelcomePayload struct { UserID int `json:\u0026#34;user_id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` } // NewEmailWelcomeTask creates a new welcome email task func NewEmailWelcomeTask(userID int, email, name string) (*asynq.Task, error) { payload, err := json.Marshal(EmailWelcomePayload{ UserID: userID, Email: email, Name: name, }) if err != nil { return nil, err } return asynq.NewTask(TypeEmailWelcome, payload), nil } // HandleEmailWelcomeTask processes the task func HandleEmailWelcomeTask(ctx context.Context, t *asynq.Task) error { var p EmailWelcomePayload if err := json.Unmarshal(t.Payload(), \u0026amp;p); err != nil { return fmt.Errorf(\u0026#34;json.Unmarshal failed: %v\u0026#34;, err) } log.Printf(\u0026#34;Sending welcome email to %s (user_id=%d)\u0026#34;, p.Email, p.UserID) // Actual email sending logic if err := sendEmail(p.Email, \u0026#34;Welcome!\u0026#34;, welcomeEmailTemplate(p.Name)); err != nil { return fmt.Errorf(\u0026#34;failed to send email: %w\u0026#34;, err) } log.Printf(\u0026#34;Successfully sent welcome email to %s\u0026#34;, p.Email) return nil } func sendEmail(to, subject, body string) error { // Implement with your email provider (SendGrid, AWS SES, etc.) // For now, simulate email sending // time.Sleep(2 * time.Second) return nil } func welcomeEmailTemplate(name string) string { return fmt.Sprintf(\u0026#34;Hi %s,\\n\\nWelcome to our platform!\\n\\nBest regards,\\nThe Team\u0026#34;, name) } Task anatomy:\nTask Type - String constant identifying the task (TypeEmailWelcome).\nPayload - JSON-serialized data containing task parameters.\nConstructor - Creates asynq.Task with type and payload (NewEmailWelcomeTask).\nHandler - Function that processes the task (HandleEmailWelcomeTask).\nThe handler signature must be func(context.Context, *asynq.Task) error. Return nil on success, error on failure.\nSetting Up the Queue Client The client enqueues tasks. Create internal/queue/client.go:\npackage queue import ( \u0026#34;github.com/hibiken/asynq\u0026#34; ) // Client wraps asynq.Client type Client struct { *asynq.Client } // NewClient creates a new queue client func NewClient(redisAddr string) *Client { client := asynq.NewClient(asynq.RedisClientOpt{ Addr: redisAddr, }) return \u0026amp;Client{Client: client} } // Close closes the client connection func (c *Client) Close() error { return c.Client.Close() } Use the client in your web application. Update cmd/api/main.go:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; \u0026#34;myapp/internal/queue\u0026#34; \u0026#34;myapp/internal/tasks\u0026#34; ) func main() { redisAddr := os.Getenv(\u0026#34;REDIS_ADDR\u0026#34;) if redisAddr == \u0026#34;\u0026#34; { redisAddr = \u0026#34;localhost:6379\u0026#34; } // Create queue client queueClient := queue.NewClient(redisAddr) defer queueClient.Close() // Register handlers http.HandleFunc(\u0026#34;/signup\u0026#34;, signupHandler(queueClient)) log.Println(\u0026#34;API server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } func signupHandler(queueClient *queue.Client) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { // Parse request email := r.FormValue(\u0026#34;email\u0026#34;) name := r.FormValue(\u0026#34;name\u0026#34;) // Create user in database userID := createUser(email, name) // Your DB logic // Enqueue welcome email task task, err := tasks.NewEmailWelcomeTask(userID, email, name) if err != nil { http.Error(w, \u0026#34;Failed to create task\u0026#34;, 500) return } info, err := queueClient.Enqueue(task) if err != nil { log.Printf(\u0026#34;Failed to enqueue task: %v\u0026#34;, err) http.Error(w, \u0026#34;Failed to enqueue task\u0026#34;, 500) return } log.Printf(\u0026#34;Enqueued task: id=%s queue=%s\u0026#34;, info.ID, info.Queue) w.Write([]byte(\u0026#34;Signup successful! Check your email.\u0026#34;)) } } func createUser(email, name string) int { // Database logic here return 123 // Mock user ID } The API enqueues the task and returns immediately. Email sending happens in the background.\nSetting Up the Worker Workers pull tasks from Redis and execute them. Create cmd/worker/main.go:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; \u0026#34;myapp/internal/tasks\u0026#34; ) func main() { redisAddr := os.Getenv(\u0026#34;REDIS_ADDR\u0026#34;) if redisAddr == \u0026#34;\u0026#34; { redisAddr = \u0026#34;localhost:6379\u0026#34; } srv := asynq.NewServer( asynq.RedisClientOpt{Addr: redisAddr}, asynq.Config{ // Number of concurrent workers Concurrency: 10, // Queue priorities (higher number = higher priority) Queues: map[string]int{ \u0026#34;critical\u0026#34;: 6, \u0026#34;default\u0026#34;: 3, \u0026#34;low\u0026#34;: 1, }, }, ) // Register task handlers mux := asynq.NewServeMux() mux.HandleFunc(tasks.TypeEmailWelcome, tasks.HandleEmailWelcomeTask) // Add more handlers as needed // Handle graceful shutdown go func() { sigterm := make(chan os.Signal, 1) signal.Notify(sigterm, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-sigterm log.Println(\u0026#34;Shutting down worker...\u0026#34;) srv.Shutdown() }() log.Println(\u0026#34;Worker starting...\u0026#34;) if err := srv.Run(mux); err != nil { log.Fatalf(\u0026#34;Could not run worker: %v\u0026#34;, err) } } Worker configuration:\nConcurrency - Number of tasks processed simultaneously (10 workers).\nQueues - Map of queue names to priorities. Workers pull from high-priority queues first.\nServeMux - Router that maps task types to handlers.\nStart the worker:\ngo run cmd/worker/main.go # Worker starting... Now when your API enqueues a task, the worker picks it up and processes it.\nEnqueuing Tasks with Options Asynq provides options to customize task behavior:\n// Basic enqueue task, _ := tasks.NewEmailWelcomeTask(userID, email, name) client.Enqueue(task) // Enqueue to specific queue client.Enqueue(task, asynq.Queue(\u0026#34;critical\u0026#34;)) // Set max retry attempts client.Enqueue(task, asynq.MaxRetry(5)) // Set timeout client.Enqueue(task, asynq.Timeout(30*time.Second)) // Process after delay client.Enqueue(task, asynq.ProcessIn(5*time.Minute)) // Process at specific time client.Enqueue(task, asynq.ProcessAt(time.Date(2025, 10, 7, 9, 0, 0, 0, time.UTC))) // Set task priority (higher = more important) client.Enqueue(task, asynq.Priority(10)) // Unique task - prevent duplicates client.Enqueue(task, asynq.Unique(24*time.Hour)) // Custom task ID client.Enqueue(task, asynq.TaskID(\u0026#34;user-welcome-123\u0026#34;)) // Combine multiple options client.Enqueue(task, asynq.Queue(\u0026#34;critical\u0026#34;), asynq.MaxRetry(3), asynq.Timeout(1*time.Minute), asynq.Priority(10), ) Common patterns:\nCritical tasks (password resets) -\u0026gt; critical queue with high priority.\nScheduled reports -\u0026gt; ProcessAt with specific time.\nDeduplication -\u0026gt; Unique option to prevent duplicate jobs.\nLong-running tasks -\u0026gt; Higher timeout and lower concurrency.\nHandling Task Failures and Retries Asynq automatically retries failed tasks with exponential backoff. Default behavior:\nTask fails (handler returns error) Asynq waits with exponential backoff Retries up to MaxRetry times (default 25) After max retries, moves to archived queue Custom retry logic:\nfunc HandleEmailWelcomeTask(ctx context.Context, t *asynq.Task) error { var p EmailWelcomePayload if err := json.Unmarshal(t.Payload(), \u0026amp;p); err != nil { // Permanent failure - don\u0026#39;t retry return fmt.Errorf(\u0026#34;json.Unmarshal failed: %w: %v\u0026#34;, asynq.SkipRetry, err) } // Attempt to send email if err := sendEmail(p.Email, \u0026#34;Welcome!\u0026#34;, welcomeEmailTemplate(p.Name)); err != nil { // Check if error is retryable if isTemporaryError(err) { // Will retry automatically return fmt.Errorf(\u0026#34;temporary error sending email: %w\u0026#34;, err) } // Permanent error - don\u0026#39;t retry return fmt.Errorf(\u0026#34;permanent error: %w: %v\u0026#34;, asynq.SkipRetry, err) } return nil } func isTemporaryError(err error) bool { // Check for network errors, rate limits, etc. return strings.Contains(err.Error(), \u0026#34;timeout\u0026#34;) || strings.Contains(err.Error(), \u0026#34;temporary\u0026#34;) } asynq.SkipRetry - Wrapping an error with this tells Asynq not to retry.\nCustom retry delays:\nsrv := asynq.NewServer( asynq.RedisClientOpt{Addr: redisAddr}, asynq.Config{ Concurrency: 10, // Custom retry delays RetryDelayFunc: func(n int, err error, task *asynq.Task) time.Duration { // n is the retry count return time.Duration(n*n) * time.Second // 1s, 4s, 9s, 16s... }, }, ) Processing Multiple Task Types As your app grows, you\u0026rsquo;ll have many task types. Organize them by domain:\nEmail tasks (internal/tasks/email.go):\npackage tasks const ( TypeEmailWelcome = \u0026#34;email:welcome\u0026#34; TypeEmailPasswordReset = \u0026#34;email:password_reset\u0026#34; TypeEmailInvoice = \u0026#34;email:invoice\u0026#34; ) func NewEmailPasswordResetTask(email, resetToken string) (*asynq.Task, error) { payload, _ := json.Marshal(map[string]string{ \u0026#34;email\u0026#34;: email, \u0026#34;token\u0026#34;: resetToken, }) return asynq.NewTask(TypeEmailPasswordReset, payload), nil } func HandleEmailPasswordResetTask(ctx context.Context, t *asynq.Task) error { var p map[string]string json.Unmarshal(t.Payload(), \u0026amp;p) resetURL := fmt.Sprintf(\u0026#34;https://example.com/reset?token=%s\u0026#34;, p[\u0026#34;token\u0026#34;]) body := fmt.Sprintf(\u0026#34;Click here to reset your password: %s\u0026#34;, resetURL) return sendEmail(p[\u0026#34;email\u0026#34;], \u0026#34;Password Reset\u0026#34;, body) } Image tasks (internal/tasks/image.go):\npackage tasks import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) const ( TypeImageResize = \u0026#34;image:resize\u0026#34; TypeImageThumbnail = \u0026#34;image:thumbnail\u0026#34; ) type ImageResizePayload struct { ImageURL string `json:\u0026#34;image_url\u0026#34;` Width int `json:\u0026#34;width\u0026#34;` Height int `json:\u0026#34;height\u0026#34;` OutputURL string `json:\u0026#34;output_url\u0026#34;` } func NewImageResizeTask(imageURL string, width, height int, outputURL string) (*asynq.Task, error) { payload, _ := json.Marshal(ImageResizePayload{ ImageURL: imageURL, Width: width, Height: height, OutputURL: outputURL, }) return asynq.NewTask(TypeImageResize, payload), nil } func HandleImageResizeTask(ctx context.Context, t *asynq.Task) error { var p ImageResizePayload if err := json.Unmarshal(t.Payload(), \u0026amp;p); err != nil { return err } // Download image img, err := downloadImage(p.ImageURL) if err != nil { return fmt.Errorf(\u0026#34;download failed: %w\u0026#34;, err) } // Resize resized, err := resizeImage(img, p.Width, p.Height) if err != nil { return fmt.Errorf(\u0026#34;resize failed: %w\u0026#34;, err) } // Upload to S3 or storage if err := uploadImage(p.OutputURL, resized); err != nil { return fmt.Errorf(\u0026#34;upload failed: %w\u0026#34;, err) } return nil } // Placeholder functions func downloadImage(url string) ([]byte, error) { return nil, nil } func resizeImage(img []byte, w, h int) ([]byte, error) { return nil, nil } func uploadImage(url string, img []byte) error { return nil } Report tasks (internal/tasks/report.go):\npackage tasks import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) const ( TypeReportDaily = \u0026#34;report:daily\u0026#34; TypeReportMonthly = \u0026#34;report:monthly\u0026#34; ) type ReportPayload struct { UserID int `json:\u0026#34;user_id\u0026#34;` StartDate time.Time `json:\u0026#34;start_date\u0026#34;` EndDate time.Time `json:\u0026#34;end_date\u0026#34;` } func NewDailyReportTask(userID int, date time.Time) (*asynq.Task, error) { payload, _ := json.Marshal(ReportPayload{ UserID: userID, StartDate: date, EndDate: date.Add(24 * time.Hour), }) return asynq.NewTask(TypeReportDaily, payload), nil } func HandleDailyReportTask(ctx context.Context, t *asynq.Task) error { var p ReportPayload json.Unmarshal(t.Payload(), \u0026amp;p) // Generate report data := fetchReportData(p.UserID, p.StartDate, p.EndDate) pdf := generatePDF(data) // Send via email return sendReportEmail(p.UserID, pdf) } func fetchReportData(userID int, start, end time.Time) interface{} { return nil } func generatePDF(data interface{}) []byte { return nil } func sendReportEmail(userID int, pdf []byte) error { return nil } Register all handlers in worker:\n// cmd/worker/main.go mux := asynq.NewServeMux() // Email handlers mux.HandleFunc(tasks.TypeEmailWelcome, tasks.HandleEmailWelcomeTask) mux.HandleFunc(tasks.TypeEmailPasswordReset, tasks.HandleEmailPasswordResetTask) mux.HandleFunc(tasks.TypeEmailInvoice, tasks.HandleEmailInvoiceTask) // Image handlers mux.HandleFunc(tasks.TypeImageResize, tasks.HandleImageResizeTask) mux.HandleFunc(tasks.TypeImageThumbnail, tasks.HandleImageThumbnailTask) // Report handlers mux.HandleFunc(tasks.TypeReportDaily, tasks.HandleDailyReportTask) mux.HandleFunc(tasks.TypeReportMonthly, tasks.HandleMonthlyReportTask) Scheduled and Periodic Tasks Asynq supports cron-like scheduled tasks. Create a scheduler:\n// cmd/scheduler/main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; \u0026#34;myapp/internal/tasks\u0026#34; ) func main() { redisAddr := os.Getenv(\u0026#34;REDIS_ADDR\u0026#34;) if redisAddr == \u0026#34;\u0026#34; { redisAddr = \u0026#34;localhost:6379\u0026#34; } scheduler := asynq.NewScheduler( asynq.RedisClientOpt{Addr: redisAddr}, \u0026amp;asynq.SchedulerOpts{ Location: time.UTC, }, ) // Daily report at 9 AM _, err := scheduler.Register(\u0026#34;0 9 * * *\u0026#34;, tasks.NewDailyReportTask(0, time.Now())) if err != nil { log.Fatal(err) } // Cleanup old data every Sunday at midnight cleanupTask, _ := tasks.NewCleanupTask() _, err = scheduler.Register(\u0026#34;0 0 * * 0\u0026#34;, cleanupTask) if err != nil { log.Fatal(err) } // Send weekly digest every Monday at 10 AM digestTask, _ := tasks.NewWeeklyDigestTask() _, err = scheduler.Register(\u0026#34;0 10 * * 1\u0026#34;, digestTask) if err != nil { log.Fatal(err) } log.Println(\u0026#34;Scheduler starting...\u0026#34;) if err := scheduler.Run(); err != nil { log.Fatal(err) } } Cron syntax: minute hour day month weekday\nExamples:\n0 9 * * * - Every day at 9 AM */5 * * * * - Every 5 minutes 0 0 * * 0 - Every Sunday at midnight 0 10 * * 1-5 - Weekdays at 10 AM Run scheduler as a separate process:\ngo run cmd/scheduler/main.go For one-off delayed tasks, use ProcessIn or ProcessAt when enqueuing:\n// Process in 1 hour task, _ := tasks.NewEmailReminderTask(userID) client.Enqueue(task, asynq.ProcessIn(1*time.Hour)) // Process at specific time processTime := time.Date(2025, 10, 7, 14, 0, 0, 0, time.UTC) client.Enqueue(task, asynq.ProcessAt(processTime)) Middleware and Logging Add middleware for logging, metrics, and error handling:\n// internal/worker/middleware.go package worker import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) // LoggingMiddleware logs task execution func LoggingMiddleware(h asynq.Handler) asynq.Handler { return asynq.HandlerFunc(func(ctx context.Context, t *asynq.Task) error { start := time.Now() log.Printf(\u0026#34;Processing task: type=%s id=%s\u0026#34;, t.Type(), t.ResultWriter().TaskID()) err := h.ProcessTask(ctx, t) duration := time.Since(start) if err != nil { log.Printf(\u0026#34;Task failed: type=%s id=%s duration=%v error=%v\u0026#34;, t.Type(), t.ResultWriter().TaskID(), duration, err) } else { log.Printf(\u0026#34;Task completed: type=%s id=%s duration=%v\u0026#34;, t.Type(), t.ResultWriter().TaskID(), duration) } return err }) } // RecoveryMiddleware recovers from panics func RecoveryMiddleware(h asynq.Handler) asynq.Handler { return asynq.HandlerFunc(func(ctx context.Context, t *asynq.Task) (err error) { defer func() { if r := recover(); r != nil { log.Printf(\u0026#34;Task panicked: type=%s panic=%v\u0026#34;, t.Type(), r) err = fmt.Errorf(\u0026#34;panic: %v\u0026#34;, r) } }() return h.ProcessTask(ctx, t) }) } // MetricsMiddleware tracks metrics func MetricsMiddleware(h asynq.Handler) asynq.Handler { return asynq.HandlerFunc(func(ctx context.Context, t *asynq.Task) error { start := time.Now() err := h.ProcessTask(ctx, t) duration := time.Since(start) // Send to your metrics system (Prometheus, DataDog, etc.) recordTaskMetrics(t.Type(), duration, err) return err }) } func recordTaskMetrics(taskType string, duration time.Duration, err error) { // Implement metrics collection } Apply middleware in worker:\n// cmd/worker/main.go srv := asynq.NewServer( asynq.RedisClientOpt{Addr: redisAddr}, asynq.Config{ Concurrency: 10, Queues: map[string]int{ \u0026#34;critical\u0026#34;: 6, \u0026#34;default\u0026#34;: 3, \u0026#34;low\u0026#34;: 1, }, }, ) mux := asynq.NewServeMux() // Apply middleware mux.Use(worker.RecoveryMiddleware) mux.Use(worker.LoggingMiddleware) mux.Use(worker.MetricsMiddleware) // Register handlers mux.HandleFunc(tasks.TypeEmailWelcome, tasks.HandleEmailWelcomeTask) // ... more handlers Middleware runs in order: Recovery -\u0026gt; Logging -\u0026gt; Metrics -\u0026gt; Handler.\nMonitoring Background Jobs with Asynq Web UI Asynq provides a web UI for monitoring tasks. Install asynqmon:\ngo install github.com/hibiken/asynq/tools/asynqmon@latest Run the web UI:\nasynqmon --redis-addr=localhost:6379 Open http://localhost:8080 to see:\nActive tasks - Currently processing Pending tasks - Waiting in queue Scheduled tasks - Future execution Retry queue - Failed tasks waiting for retry Archived - Exhausted retries Dead - Permanently failed You can manually retry, delete, or archive tasks from the UI.\nFor production, embed asynqmon in your app:\n// cmd/monitor/main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; \u0026#34;github.com/hibiken/asynqmon\u0026#34; ) func main() { h := asynqmon.New(asynqmon.Options{ RootPath: \u0026#34;/monitoring\u0026#34;, RedisConnOpt: asynq.RedisClientOpt{Addr: \u0026#34;localhost:6379\u0026#34;}, }) http.Handle(h.RootPath(), h) log.Println(\u0026#34;Monitoring UI available at http://localhost:8080/monitoring\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Protect the monitoring endpoint with authentication in production.\nTask Context and Cancellation Use context for cancellation and timeouts:\nfunc HandleLongRunningTask(ctx context.Context, t *asynq.Task) error { // Check context before expensive operations select { case \u0026lt;-ctx.Done(): return ctx.Err() // Cancelled or timed out default: } // Long running work for i := 0; i \u0026lt; 1000; i++ { // Check context periodically if err := ctx.Err(); err != nil { log.Printf(\u0026#34;Task cancelled at iteration %d\u0026#34;, i) return err } processItem(i) } return nil } Set timeout when enqueueing:\n// Task will be cancelled after 5 minutes client.Enqueue(task, asynq.Timeout(5*time.Minute)) Get task metadata from context:\nfunc HandleTask(ctx context.Context, t *asynq.Task) error { // Get task ID taskID, _ := asynq.GetTaskID(ctx) log.Printf(\u0026#34;Processing task %s\u0026#34;, taskID) // Get retry count retryCount, _ := asynq.GetRetryCount(ctx) log.Printf(\u0026#34;Retry attempt %d\u0026#34;, retryCount) // Get max retry maxRetry, _ := asynq.GetMaxRetry(ctx) log.Printf(\u0026#34;Max retries: %d\u0026#34;, maxRetry) return nil } Unique Tasks and Deduplication Prevent duplicate tasks with the Unique option:\n// Only one password reset task per user for 1 hour task, _ := tasks.NewEmailPasswordResetTask(email, token) client.Enqueue(task, asynq.Unique(1*time.Hour), asynq.TaskID(fmt.Sprintf(\u0026#34;password-reset-%s\u0026#34;, email)), ) If you enqueue the same task (same TaskID) within 1 hour, Asynq ignores it.\nFor custom uniqueness:\n// Only one report generation per user per day reportDate := time.Now().Format(\u0026#34;2006-01-02\u0026#34;) uniqueID := fmt.Sprintf(\u0026#34;daily-report-%d-%s\u0026#34;, userID, reportDate) task, _ := tasks.NewDailyReportTask(userID, time.Now()) client.Enqueue(task, asynq.TaskID(uniqueID), asynq.Unique(24*time.Hour), ) This ensures you don\u0026rsquo;t accidentally generate the same report multiple times.\nTask Result and Progress Tracking Track task progress for long-running jobs:\nfunc HandleLargeExportTask(ctx context.Context, t *asynq.Task) error { var p ExportPayload json.Unmarshal(t.Payload(), \u0026amp;p) items := fetchItems(p.UserID) // 10,000 items total := len(items) for i, item := range items { processItem(item) // Update progress every 100 items if i%100 == 0 { progress := float64(i) / float64(total) * 100 updateProgress(p.UserID, progress) log.Printf(\u0026#34;Export progress: %.2f%%\u0026#34;, progress) } // Check for cancellation if err := ctx.Err(); err != nil { return err } } return nil } Store progress in Redis or database and show it in your UI.\nError Handling Best Practices Distinguish temporary vs permanent errors:\nfunc HandleAPICallTask(ctx context.Context, t *asynq.Task) error { resp, err := httpClient.Get(apiURL) if err != nil { // Network error - retry return fmt.Errorf(\u0026#34;network error: %w\u0026#34;, err) } defer resp.Body.Close() if resp.StatusCode == 429 { // Rate limited - retry with backoff return fmt.Errorf(\u0026#34;rate limited, will retry\u0026#34;) } if resp.StatusCode == 404 { // Resource not found - don\u0026#39;t retry return fmt.Errorf(\u0026#34;%w: resource not found\u0026#34;, asynq.SkipRetry) } if resp.StatusCode \u0026gt;= 500 { // Server error - retry return fmt.Errorf(\u0026#34;server error: status %d\u0026#34;, resp.StatusCode) } if resp.StatusCode \u0026gt;= 400 { // Client error - don\u0026#39;t retry return fmt.Errorf(\u0026#34;%w: client error %d\u0026#34;, asynq.SkipRetry, resp.StatusCode) } // Process successful response return processResponse(resp.Body) } Log errors with context:\nfunc HandleTask(ctx context.Context, t *asynq.Task) error { taskID, _ := asynq.GetTaskID(ctx) retryCount, _ := asynq.GetRetryCount(ctx) err := doWork() if err != nil { log.Printf(\u0026#34;Task %s failed (retry %d): %v\u0026#34;, taskID, retryCount, err) return err } return nil } Implement circuit breaker for external services:\nvar circuitBreaker = \u0026amp;CircuitBreaker{ maxFailures: 5, timeout: 30 * time.Second, } func HandleExternalAPITask(ctx context.Context, t *asynq.Task) error { if !circuitBreaker.Allow() { return fmt.Errorf(\u0026#34;circuit breaker open, skipping\u0026#34;) } err := callExternalAPI() if err != nil { circuitBreaker.RecordFailure() return err } circuitBreaker.RecordSuccess() return nil } Production Deployment Strategies Environment variables:\n# .env REDIS_ADDR=redis.production.com:6379 REDIS_PASSWORD=secret WORKER_CONCURRENCY=20 QUEUE_CRITICAL_PRIORITY=6 QUEUE_DEFAULT_PRIORITY=3 QUEUE_LOW_PRIORITY=1 Production worker configuration:\n// cmd/worker/main.go func main() { redisAddr := os.Getenv(\u0026#34;REDIS_ADDR\u0026#34;) redisPassword := os.Getenv(\u0026#34;REDIS_PASSWORD\u0026#34;) concurrency := getEnvInt(\u0026#34;WORKER_CONCURRENCY\u0026#34;, 10) srv := asynq.NewServer( asynq.RedisClientOpt{ Addr: redisAddr, Password: redisPassword, DB: 0, // Connection pool settings PoolSize: concurrency * 2, MinIdleConns: 5, // Timeouts DialTimeout: 5 * time.Second, ReadTimeout: 3 * time.Second, WriteTimeout: 3 * time.Second, }, asynq.Config{ Concurrency: concurrency, Queues: map[string]int{ \u0026#34;critical\u0026#34;: getEnvInt(\u0026#34;QUEUE_CRITICAL_PRIORITY\u0026#34;, 6), \u0026#34;default\u0026#34;: getEnvInt(\u0026#34;QUEUE_DEFAULT_PRIORITY\u0026#34;, 3), \u0026#34;low\u0026#34;: getEnvInt(\u0026#34;QUEUE_LOW_PRIORITY\u0026#34;, 1), }, // Strict priority mode StrictPriority: true, // Error handler ErrorHandler: asynq.ErrorHandlerFunc(func(ctx context.Context, task *asynq.Task, err error) { taskID, _ := asynq.GetTaskID(ctx) log.Printf(\u0026#34;Task error: id=%s type=%s error=%v\u0026#34;, taskID, task.Type(), err) // Send to error tracking (Sentry, Rollbar, etc.) reportError(task, err) }), // Health check endpoint HealthCheckFunc: func(err error) { if err != nil { log.Printf(\u0026#34;Health check failed: %v\u0026#34;, err) } }, HealthCheckInterval: 15 * time.Second, }, ) // ... setup handlers and run } func getEnvInt(key string, defaultVal int) int { val := os.Getenv(key) if val == \u0026#34;\u0026#34; { return defaultVal } i, _ := strconv.Atoi(val) return i } func reportError(task *asynq.Task, err error) { // Integrate with your error tracking service } Docker deployment:\n# Dockerfile.worker FROM golang:1.21-alpine AS builder WORKDIR /app COPY go.* ./ RUN go mod download COPY . . RUN CGO_ENABLED=0 go build -o worker ./cmd/worker FROM alpine:latest RUN apk --no-cache add ca-certificates WORKDIR /root/ COPY --from=builder /app/worker . CMD [\u0026#34;./worker\u0026#34;] Docker Compose:\n# docker-compose.yml version: \u0026#39;3.8\u0026#39; services: redis: image: redis:7-alpine ports: - \u0026#34;6379:6379\u0026#34; volumes: - redis-data:/data worker: build: context: . dockerfile: Dockerfile.worker environment: - REDIS_ADDR=redis:6379 - WORKER_CONCURRENCY=10 depends_on: - redis deploy: replicas: 3 # Run 3 worker instances volumes: redis-data: Kubernetes deployment:\n# worker-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: asynq-worker spec: replicas: 5 selector: matchLabels: app: asynq-worker template: metadata: labels: app: asynq-worker spec: containers: - name: worker image: myapp/worker:latest env: - name: REDIS_ADDR value: \u0026#34;redis-service:6379\u0026#34; - name: WORKER_CONCURRENCY value: \u0026#34;20\u0026#34; resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; Monitoring and Metrics Expose Prometheus metrics:\n// internal/worker/metrics.go package worker import ( \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; ) var ( tasksProcessed = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;asynq_tasks_processed_total\u0026#34;, Help: \u0026#34;Total number of tasks processed\u0026#34;, }, []string{\u0026#34;task_type\u0026#34;, \u0026#34;status\u0026#34;}, ) taskDuration = promauto.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;asynq_task_duration_seconds\u0026#34;, Help: \u0026#34;Task processing duration\u0026#34;, Buckets: prometheus.DefBuckets, }, []string{\u0026#34;task_type\u0026#34;}, ) queueSize = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;asynq_queue_size\u0026#34;, Help: \u0026#34;Current queue size\u0026#34;, }, []string{\u0026#34;queue\u0026#34;}, ) ) func MetricsMiddleware(h asynq.Handler) asynq.Handler { return asynq.HandlerFunc(func(ctx context.Context, t *asynq.Task) error { timer := prometheus.NewTimer(taskDuration.WithLabelValues(t.Type())) defer timer.ObserveDuration() err := h.ProcessTask(ctx, t) status := \u0026#34;success\u0026#34; if err != nil { status = \u0026#34;failure\u0026#34; } tasksProcessed.WithLabelValues(t.Type(), status).Inc() return err }) } Expose metrics endpoint:\n// cmd/worker/main.go import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) func main() { // ... worker setup // Metrics endpoint go func() { http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) log.Println(\u0026#34;Metrics available at :9090/metrics\u0026#34;) http.ListenAndServe(\u0026#34;:9090\u0026#34;, nil) }() // ... run worker } Testing Background Jobs Test task handlers:\n// internal/tasks/email_test.go package tasks import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) func TestHandleEmailWelcomeTask(t *testing.T) { // Create test task task, err := NewEmailWelcomeTask(123, \u0026#34;test@example.com\u0026#34;, \u0026#34;Test User\u0026#34;) if err != nil { t.Fatalf(\u0026#34;Failed to create task: %v\u0026#34;, err) } // Execute handler ctx := context.Background() err = HandleEmailWelcomeTask(ctx, task) // Assert if err != nil { t.Errorf(\u0026#34;Expected no error, got: %v\u0026#34;, err) } // Verify email was sent (mock or check test email service) } func TestHandleEmailWelcomeTask_InvalidPayload(t *testing.T) { // Create task with invalid payload task := asynq.NewTask(TypeEmailWelcome, []byte(\u0026#34;invalid json\u0026#34;)) ctx := context.Background() err := HandleEmailWelcomeTask(ctx, task) // Should return error if err == nil { t.Error(\u0026#34;Expected error for invalid payload\u0026#34;) } } Integration test with Redis:\nfunc TestTaskEnqueueAndProcess(t *testing.T) { // Setup test Redis redisAddr := \u0026#34;localhost:6379\u0026#34; client := asynq.NewClient(asynq.RedisClientOpt{Addr: redisAddr}) defer client.Close() // Enqueue task task, _ := NewEmailWelcomeTask(123, \u0026#34;test@example.com\u0026#34;, \u0026#34;Test\u0026#34;) info, err := client.Enqueue(task) if err != nil { t.Fatalf(\u0026#34;Failed to enqueue: %v\u0026#34;, err) } // Verify task was enqueued if info.Queue != \u0026#34;default\u0026#34; { t.Errorf(\u0026#34;Expected queue \u0026#39;default\u0026#39;, got %s\u0026#34;, info.Queue) } // TODO: Start worker and verify task is processed } Real-World Example: Email Service Complete email service with multiple task types:\n// internal/tasks/email_service.go package tasks import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/hibiken/asynq\u0026#34; ) type EmailService struct { client *asynq.Client } func NewEmailService(client *asynq.Client) *EmailService { return \u0026amp;EmailService{client: client} } // SendWelcomeEmail sends welcome email immediately func (s *EmailService) SendWelcomeEmail(userID int, email, name string) error { task, err := NewEmailWelcomeTask(userID, email, name) if err != nil { return err } _, err = s.client.Enqueue(task, asynq.Queue(\u0026#34;critical\u0026#34;), asynq.MaxRetry(5), ) return err } // SendPasswordReset sends password reset with deduplication func (s *EmailService) SendPasswordReset(email, token string) error { task, err := NewEmailPasswordResetTask(email, token) if err != nil { return err } // Only one reset email per user per 15 minutes _, err = s.client.Enqueue(task, asynq.Queue(\u0026#34;critical\u0026#34;), asynq.TaskID(fmt.Sprintf(\u0026#34;password-reset-%s\u0026#34;, email)), asynq.Unique(15*time.Minute), ) return err } // ScheduleReminderEmail schedules reminder for future func (s *EmailService) ScheduleReminderEmail(userID int, email string, when time.Time) error { task, err := NewEmailReminderTask(userID, email) if err != nil { return err } _, err = s.client.Enqueue(task, asynq.ProcessAt(when), asynq.Queue(\u0026#34;default\u0026#34;), ) return err } // SendBulkEmails sends emails to multiple users func (s *EmailService) SendBulkEmails(campaign Campaign) error { for _, recipient := range campaign.Recipients { task, err := NewEmailCampaignTask(recipient.Email, campaign.Subject, campaign.Body) if err != nil { continue } // Low priority, spread over time delay := time.Duration(rand.Intn(300)) * time.Second s.client.Enqueue(task, asynq.Queue(\u0026#34;low\u0026#34;), asynq.ProcessIn(delay), ) } return nil } Usage in your API:\n// In your HTTP handler emailService := tasks.NewEmailService(queueClient) // Send welcome email if err := emailService.SendWelcomeEmail(user.ID, user.Email, user.Name); err != nil { log.Printf(\u0026#34;Failed to enqueue welcome email: %v\u0026#34;, err) } // Schedule reminder in 1 day reminderTime := time.Now().Add(24 * time.Hour) emailService.ScheduleReminderEmail(user.ID, user.Email, reminderTime) Advanced Patterns Chain tasks - Execute tasks in sequence:\n// Process image, then send notification func ProcessAndNotify(imageURL string, userID int) error { // First task: resize image resizeTask, _ := tasks.NewImageResizeTask(imageURL, 800, 600, outputURL) info, err := client.Enqueue(resizeTask) if err != nil { return err } // Second task: notify user (after 10 seconds to ensure first task completes) notifyTask, _ := tasks.NewNotificationTask(userID, \u0026#34;Image processed!\u0026#34;) client.Enqueue(notifyTask, asynq.ProcessIn(10*time.Second)) return nil } For more reliable chaining, handle it in the task itself:\nfunc HandleImageResizeTask(ctx context.Context, t *asynq.Task) error { var p ImageResizePayload json.Unmarshal(t.Payload(), \u0026amp;p) // Resize image if err := resizeImage(p); err != nil { return err } // After successful resize, enqueue notification notifyTask, _ := tasks.NewNotificationTask(p.UserID, \u0026#34;Image processed!\u0026#34;) return client.Enqueue(notifyTask) } Fan-out pattern - Process multiple items in parallel:\nfunc ProcessBatch(items []Item) error { for _, item := range items { task, _ := tasks.NewProcessItemTask(item.ID) client.Enqueue(task) } return nil } Rate limiting - Control task execution rate:\n// Limit to 100 emails per minute var emailRateLimiter = time.NewTicker(600 * time.Millisecond) // 100/min func HandleEmailTask(ctx context.Context, t *asynq.Task) error { \u0026lt;-emailRateLimiter.C // Wait for rate limit slot return sendEmail(...) } Comparing with Alternatives Here\u0026rsquo;s how Asynq compares to other job queue solutions:\nFeature Asynq RabbitMQ Kafka Temporal Setup Complexity Simple (Redis only) Medium Complex Complex Use Case Background jobs Message routing Event streaming Long workflows Language Go-native Language-agnostic Language-agnostic Language-agnostic Retries Built-in exponential backoff Manual config Not built-in Built-in Web UI Yes (asynqmon) Yes (management plugin) No (use external) Yes Durability Redis persistence High Very high High Learning Curve Low Medium High High Best For Go background jobs Complex routing Event logs Workflow orchestration When to choose Asynq:\nBuilding Go applications with background processing needs Want simple setup with Redis Need automatic retries and scheduling out of the box Prefer Go-native libraries over protocol-based solutions When to choose alternatives:\nRabbitMQ - Need advanced message routing or polyglot services Kafka - Building event-driven architectures with replay capabilities Temporal - Managing complex multi-step workflows with state For most Go applications , Asynq hits the sweet spot - powerful enough for production, simple enough to understand.\nIntegration with Existing Systems If you\u0026rsquo;re using Redis for caching , you can share the same Redis instance for Asynq. Just use different DB numbers:\n// Cache uses DB 0 cacheClient := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, DB: 0, }) // Asynq uses DB 1 asynqClient := asynq.NewClient(asynq.RedisClientOpt{ Addr: \u0026#34;localhost:6379\u0026#34;, DB: 1, }) For database operations in tasks, consider using database migrations to manage schema changes as your tasks evolve.\nIf you\u0026rsquo;re processing uploaded files, integrate with S3 uploads for reliable storage before processing.\nTroubleshooting Common Issues Tasks not being processed:\nCheck worker is running: ps aux | grep worker Verify Redis connection: redis-cli ping Check queue names match between client and worker Look for errors in worker logs Tasks failing repeatedly:\nCheck error logs for root cause Verify external services (email, S3) are accessible Increase timeout if tasks need more time Add retry logic for transient errors Redis connection issues:\nIncrease connection pool size Check Redis max connections: redis-cli CONFIG GET maxclients Monitor Redis memory: redis-cli INFO memory Use Redis cluster for high availability High memory usage:\nReduce worker concurrency Process large payloads in chunks Clean up completed tasks regularly Monitor with asynqmon UI Wrapping Up Background jobs are essential for building responsive, scalable Go applications . Asynq gives you everything needed for production: automatic retries, scheduling, priorities, monitoring, and graceful shutdown.\nStart simple - enqueue emails and image processing. As you grow, add scheduled reports, webhook delivery, and data exports. The patterns scale from small apps to high-traffic systems processing millions of tasks daily.\nKey takeaways: return HTTP responses immediately and process work asynchronously, use appropriate queues and priorities for different task types, implement proper error handling and retries, monitor tasks with asynqmon and metrics, test your task handlers like any other code, and deploy workers separately from your API for independent scaling.\nBackground jobs move slow work out of request cycles. Your API stays fast, users get instant feedback, and heavy workloads run reliably in the background. That\u0026rsquo;s how you build systems that scale.\nFor production deployments, combine this with Docker containerization , database migrations , and proper monitoring and profiling .\n","href":"/2025/10/how-to-implement-background-jobs-in-go-with-asynq-and-redis.html","title":"How to Implement Background Jobs in Go with Asynq and Redis"},{"content":"Your Go application is slow. Requests take too long, memory usage keeps climbing, or CPU maxes out under load. You need answers, not guesses. That\u0026rsquo;s where pprof comes in.\npprof is Go\u0026rsquo;s built-in profiler that shows you exactly what\u0026rsquo;s happening inside your running application. It tells you which functions eat CPU cycles, which code paths allocate tons of memory, where goroutines get stuck, and what\u0026rsquo;s blocking your program. With this data, you stop guessing and start fixing real bottlenecks.\nThis guide covers everything about profiling Go applications - from basic CPU and memory profiling to advanced techniques like detecting goroutine leaks, analyzing mutex contention, and optimizing production systems. You\u0026rsquo;ll learn how to collect profiles, interpret the data, and actually make your Go code faster.\nWhy Profiling Matters Performance optimization without profiling is like trying to fix a car engine while blindfolded. You might get lucky, but probably you\u0026rsquo;ll waste time on things that don\u0026rsquo;t matter.\nI\u0026rsquo;ve seen developers spend days optimizing algorithms that account for 0.1% of runtime while the real bottleneck was a goroutine leak causing memory to balloon. Profiling would\u0026rsquo;ve shown this in 5 minutes.\nWhat profiling gives you:\nReal data on where time is spent - not assumptions but actual measurements of CPU usage per function.\nMemory allocation patterns - see which code creates garbage that the GC has to clean up constantly.\nConcurrency issues - find goroutine leaks, deadlocks, and excessive context switching.\nProduction insights - understand how your app behaves with real traffic, not just synthetic benchmarks.\nGo makes profiling easy. The tools are built-in, the overhead is manageable, and the insights are actionable. If your Go app has performance issues, profiling is always the first step.\nUnderstanding pprof Basics pprof is Go\u0026rsquo;s profiling tool. It collects runtime data about your program and presents it in various formats for analysis.\nTypes of Profiles CPU Profile - Shows where your program spends CPU time. Samples the call stack periodically (default 100 times per second) to build a picture of hot code paths.\nHeap Profile - Shows memory allocations. Two views: allocations over time (alloc_space) and currently allocated memory (inuse_space).\nGoroutine Profile - Lists all goroutines and their current state. Essential for finding goroutine leaks and understanding concurrency behavior.\nBlock Profile - Shows where goroutines block on synchronization primitives like channels and mutexes.\nMutex Profile - Shows contention on mutexes. Helps identify lock contention issues in concurrent code.\nThread Create Profile - Shows where the runtime creates OS threads. Useful for diagnosing excessive thread creation.\nEach profile type reveals different performance aspects. CPU and heap profiling are most common, but goroutine profiling is crucial for concurrent Go programs.\nHow pprof Works pprof uses sampling, not instrumentation. It periodically checks what your program is doing rather than tracking every operation. This keeps overhead low - typically 5-10% for CPU profiling.\nFor CPU profiling, the Go runtime interrupts your program 100 times per second and records the call stack. After collecting thousands of samples, you get a statistical picture of where time is spent.\nMemory profiling tracks allocations but samples them (by default, one sample per 512KB allocated). This means you see the big allocators, not every tiny allocation.\nThe sampling approach makes pprof suitable for production use, unlike some profilers that add 50-100% overhead.\nSetting Up pprof in Your Application Getting pprof running is straightforward. Go provides two main ways: automatic HTTP endpoints for servers, and programmatic profiling for any application.\nHTTP Server Setup For web applications or services with HTTP endpoints, pprof integration is one import away.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; ) func main() { // Your HTTP handlers http.HandleFunc(\u0026#34;/\u0026#34;, homeHandler) http.HandleFunc(\u0026#34;/api/data\u0026#34;, dataHandler) // pprof automatically registers at /debug/pprof/ log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } func homeHandler(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\u0026#34;Hello World\u0026#34;)) } func dataHandler(w http.ResponseWriter, r *http.Request) { // Some API logic w.Write([]byte(\u0026#34;Data response\u0026#34;)) } The blank import _ \u0026quot;net/http/pprof\u0026quot; registers handlers automatically. Now you can access:\nhttp://localhost:8080/debug/pprof/ - Index page with all profiles http://localhost:8080/debug/pprof/profile - 30-second CPU profile http://localhost:8080/debug/pprof/heap - Heap profile http://localhost:8080/debug/pprof/goroutine - Goroutine dump Separate pprof Port (Production Pattern) In production, don\u0026rsquo;t expose pprof on your main service port. Use a separate port restricted to internal access.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; ) func main() { // Main application server go func() { mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/\u0026#34;, homeHandler) mux.HandleFunc(\u0026#34;/api/data\u0026#34;, dataHandler) log.Println(\u0026#34;Application server on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, mux)) }() // Separate pprof server on different port go func() { log.Println(\u0026#34;pprof server on :6060\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:6060\u0026#34;, nil)) }() // Block forever select {} } Now pprof runs on port 6060. Configure your firewall to only allow internal access to this port. Your application on port 8080 has no profiling endpoints exposed.\nProgrammatic Profiling For non-HTTP applications or when you want control over when profiling happens, use the runtime/pprof package directly.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;runtime/pprof\u0026#34; \u0026#34;time\u0026#34; ) func main() { // CPU profiling f, err := os.Create(\u0026#34;cpu.prof\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() if err := pprof.StartCPUProfile(f); err != nil { log.Fatal(err) } defer pprof.StopCPUProfile() // Your application logic doWork() // Memory profiling mf, err := os.Create(\u0026#34;mem.prof\u0026#34;) if err != nil { log.Fatal(err) } defer mf.Close() runtime.GC() // Get up-to-date statistics if err := pprof.WriteHeapProfile(mf); err != nil { log.Fatal(err) } } func doWork() { // Simulate work for i := 0; i \u0026lt; 1000000; i++ { _ = make([]byte, 1024) time.Sleep(time.Microsecond) } } This creates cpu.prof and mem.prof files you can analyze with the pprof tool.\nCPU Profiling: Finding Performance Bottlenecks CPU profiling reveals where your program spends execution time. This is usually your first step when optimizing performance.\nCollecting a CPU Profile From an HTTP endpoint:\n# Collect 30-second CPU profile curl http://localhost:6060/debug/pprof/profile?seconds=30 \u0026gt; cpu.prof The seconds parameter controls profile duration. 30 seconds is typical - long enough to capture representative behavior but not too long if you\u0026rsquo;re in a hurry.\nFrom code:\nimport ( \u0026#34;os\u0026#34; \u0026#34;runtime/pprof\u0026#34; ) f, _ := os.Create(\u0026#34;cpu.prof\u0026#34;) pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() // Run code you want to profile doExpensiveOperation() Analyzing CPU Profiles Basic analysis with go tool pprof:\ngo tool pprof cpu.prof This opens an interactive shell. Common commands:\ntop - Show top functions by CPU usage:\n(pprof) top Showing nodes accounting for 2840ms, 94.67% of 3000ms total Dropped 45 nodes (cum \u0026lt;= 15ms) flat flat% sum% cum cum% 1200ms 40.00% 40.00% 1200ms 40.00% runtime.mallocgc 800ms 26.67% 66.67% 800ms 26.67% main.processData 400ms 13.33% 80.00% 600ms 20.00% main.calculateHash 240ms 8.00% 88.00% 240ms 8.00% runtime.memmove 200ms 6.67% 94.67% 200ms 6.67% crypto/sha256.block flat - Time spent in the function itself cum - Cumulative time (function + everything it calls)\nlist - Show source code with time attribution:\n(pprof) list main.processData Total: 3s ROUTINE ======================== main.processData 800ms 800ms (flat, cum) 26.67% of Total . . 15:func processData(data []byte) { . . 16: for i := 0; i \u0026lt; len(data); i++ { 200ms 200ms 17: result := data[i] * 2 400ms 400ms 18: hash := calculateHash(result) 200ms 200ms 19: store(hash) . . 20: } . . 21:} web - Generate a call graph visualization (requires Graphviz):\ngo tool pprof -http=:8081 cpu.prof Opens a browser with an interactive flame graph and call graph.\nInterpreting CPU Profile Data High flat time means the function itself is slow - look for inefficient algorithms or unnecessary work.\nHigh cum but low flat time means the function calls other slow functions - drill down into the call tree.\nruntime.mallocgc appearing at the top often indicates excessive allocations - switch to a heap profile to see what\u0026rsquo;s allocating.\nReal Example: Optimizing a Slow Function Found this in a profile:\n2400ms 80.00% main.findUser Listing the function:\nfunc findUser(users []User, id int) *User { for _, u := range users { if u.ID == id { return \u0026amp;u // Problem: returns address of loop variable } } return nil } The issue isn\u0026rsquo;t obvious from profiling alone, but profiling showed this function was the bottleneck. Investigation revealed it was called millions of times. The fix:\n// Build a map once var userMap = make(map[int]*User) func init() { for i := range users { userMap[users[i].ID] = \u0026amp;users[i] } } func findUser(id int) *User { return userMap[id] // O(1) instead of O(n) } Profile after optimization showed findUser dropped from 80% to \u0026lt;1% of CPU time.\nMemory Profiling: Hunting Allocations and Leaks Memory issues come in two flavors: too many allocations causing GC pressure, and memory leaks where allocations never get freed. pprof\u0026rsquo;s heap profiling catches both.\nCollecting Heap Profiles From HTTP:\n# Current heap (inuse_space) curl http://localhost:6060/debug/pprof/heap \u0026gt; heap.prof # All allocations (alloc_space) curl http://localhost:6060/debug/pprof/heap?alloc_space=1 \u0026gt; alloc.prof From code:\nimport ( \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;runtime/pprof\u0026#34; ) runtime.GC() // Get current statistics f, _ := os.Create(\u0026#34;heap.prof\u0026#34;) pprof.WriteHeapProfile(f) f.Close() Analyzing Heap Profiles go tool pprof heap.prof By default, you see inuse_space - memory currently allocated:\n(pprof) top Showing nodes accounting for 512MB, 98.46% of 520MB total flat flat% sum% cum cum% 256MB 49.23% 49.23% 256MB 49.23% main.loadConfig 128MB 24.62% 73.85% 128MB 24.62% main.cacheData 64MB 12.31% 86.15% 64MB 12.31% encoding/json.Unmarshal 32MB 6.15% 92.31% 32MB 6.15% io/ioutil.ReadAll Switch to alloc_space to see all allocations:\ngo tool pprof -alloc_space heap.prof Or:\n(pprof) sample_index = alloc_space (pprof) top inuse_space vs alloc_space inuse_space - Memory currently in use. High values indicate memory leaks or objects that should be freed but aren\u0026rsquo;t.\nalloc_space - Total memory allocated (including freed memory). High values with low inuse_space indicate excessive temporary allocations causing GC overhead.\ninuse_objects and alloc_objects - Same but counting objects instead of bytes.\nFinding Memory Leaks Memory leak symptoms: inuse_space grows over time without bound.\nCollect multiple heap profiles over time:\n# Profile 1 curl http://localhost:6060/debug/pprof/heap \u0026gt; heap1.prof # Wait 10 minutes, let app run sleep 600 # Profile 2 curl http://localhost:6060/debug/pprof/heap \u0026gt; heap2.prof Compare:\ngo tool pprof -base heap1.prof heap2.prof This shows allocations that happened between the two profiles. If certain allocations keep growing, that\u0026rsquo;s your leak.\nExample: Fixing a Memory Leak Profile showed:\n(pprof) top 2GB main.(*Cache).Get The source:\ntype Cache struct { data map[string][]byte } func (c *Cache) Get(key string) []byte { if val, ok := c.data[key]; ok { return val } // Fetch from database val := db.Query(key) c.data[key] = val // Leak: never removed return val } The cache grows forever. Fix with expiration:\ntype CacheEntry struct { data []byte expiresAt time.Time } type Cache struct { data map[string]CacheEntry mu sync.RWMutex } func (c *Cache) Get(key string) []byte { c.mu.RLock() if entry, ok := c.data[key]; ok \u0026amp;\u0026amp; time.Now().Before(entry.expiresAt) { c.mu.RUnlock() return entry.data } c.mu.RUnlock() val := db.Query(key) c.mu.Lock() c.data[key] = CacheEntry{ data: val, expiresAt: time.Now().Add(5 * time.Minute), } c.mu.Unlock() return val } func (c *Cache) CleanupExpired() { c.mu.Lock() defer c.mu.Unlock() now := time.Now() for k, entry := range c.data { if now.After(entry.expiresAt) { delete(c.data, k) } } } Add periodic cleanup:\ngo func() { ticker := time.NewTicker(1 * time.Minute) for range ticker.C { cache.CleanupExpired() } }() Memory usage stabilized after the fix.\nReducing Allocations High alloc_space but manageable inuse_space means lots of temporary allocations. The GC constantly cleans up garbage, which burns CPU.\nCommon fixes:\nReuse buffers - Instead of allocating new slices/buffers each time, reuse them:\n// Before: allocates every call func processData(data []byte) []byte { buf := make([]byte, len(data)*2) // process... return buf } // After: reuse buffer pool var bufPool = sync.Pool{ New: func() interface{} { return make([]byte, 4096) }, } func processData(data []byte) []byte { buf := bufPool.Get().([]byte) defer bufPool.Put(buf) if cap(buf) \u0026lt; len(data)*2 { buf = make([]byte, len(data)*2) } buf = buf[:len(data)*2] // process... return buf } Preallocate slices - If you know the size, preallocate:\n// Before var results []Result for _, item := range items { results = append(results, process(item)) } // After results := make([]Result, 0, len(items)) for _, item := range items { results = append(results, process(item)) } Avoid string concatenation in loops:\n// Before: allocates new string each iteration var s string for i := 0; i \u0026lt; 1000; i++ { s += fmt.Sprintf(\u0026#34;%d,\u0026#34;, i) } // After: use strings.Builder var b strings.Builder for i := 0; i \u0026lt; 1000; i++ { fmt.Fprintf(\u0026amp;b, \u0026#34;%d,\u0026#34;, i) } s := b.String() Goroutine Profiling: Finding Leaks and Deadlocks Goroutines are cheap, so developers spawn them freely. But goroutine leaks - where goroutines never exit - cause memory and CPU waste.\nCollecting Goroutine Profiles From HTTP:\ncurl http://localhost:6060/debug/pprof/goroutine \u0026gt; goroutine.prof From code:\nimport ( \u0026#34;os\u0026#34; \u0026#34;runtime/pprof\u0026#34; ) f, _ := os.Create(\u0026#34;goroutine.prof\u0026#34;) pprof.Lookup(\u0026#34;goroutine\u0026#34;).WriteTo(f, 0) f.Close() Analyzing Goroutines go tool pprof goroutine.prof (pprof) top Showing nodes accounting for 10450, 100% of 10450 total flat flat% sum% cum cum% 10000 95.69% 95.69% 10000 95.69% runtime.gopark 200 1.91% 97.61% 200 1.91% runtime.goparkunlock 150 1.44% 99.04% 150 1.44% time.Sleep 100 0.96% 100% 100 0.96% chan receive 10,000 goroutines stuck in runtime.gopark - that\u0026rsquo;s a problem.\nUse traces to see what these goroutines are doing:\n(pprof) traces File: goroutine.prof Type: goroutine Time: Jan 6, 2025 at 3:04pm (WIB) -----------+------------------------------------------------------- 10000 runtime.gopark runtime.chanrecv runtime.chanrecv1 main.worker runtime.goexit -----------+------------------------------------------------------- All 10,000 goroutines are in main.worker, waiting on a channel receive.\nExample: Goroutine Leak The leaking code:\nfunc processJobs(jobs []Job) { for _, job := range jobs { go worker(job) // Spawns goroutine but never waits } } func worker(job Job) { result := make(chan Result) go func() { r := process(job) result \u0026lt;- r // Blocks forever if nothing receives }() // Bug: timeout but goroutine still blocked on channel send select { case r := \u0026lt;-result: store(r) case \u0026lt;-time.After(5 * time.Second): log.Println(\u0026#34;timeout\u0026#34;) return // Goroutine still blocked on result \u0026lt;- r } } Every timeout leaks a goroutine. With thousands of jobs, goroutines pile up.\nFix with buffered channel:\nfunc worker(job Job) { result := make(chan Result, 1) // Buffered: send won\u0026#39;t block go func() { r := process(job) result \u0026lt;- r // Won\u0026#39;t block even if nobody receives }() select { case r := \u0026lt;-result: store(r) case \u0026lt;-time.After(5 * time.Second): log.Println(\u0026#34;timeout\u0026#34;) return } } Or use context for cancellation:\nfunc worker(ctx context.Context, job Job) { result := make(chan Result) go func() { r := process(job) select { case result \u0026lt;- r: case \u0026lt;-ctx.Done(): return // Exit if context cancelled } }() ctx, cancel := context.WithTimeout(ctx, 5*time.Second) defer cancel() select { case r := \u0026lt;-result: store(r) case \u0026lt;-ctx.Done(): log.Println(\u0026#34;timeout\u0026#34;) return } } After fixing, goroutine count stayed stable instead of growing.\nDetecting Goroutine Growth Monitor goroutine count over time:\nimport ( \u0026#34;log\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;time\u0026#34; ) func monitorGoroutines() { ticker := time.NewTicker(10 * time.Second) for range ticker.C { count := runtime.NumGoroutine() log.Printf(\u0026#34;Current goroutines: %d\u0026#34;, count) } } go monitorGoroutines() If the count keeps climbing, you have a leak. Use goroutine profiling to find where they\u0026rsquo;re stuck.\nBlock and Mutex Profiling: Finding Contention When goroutines fight over shared resources, performance suffers. Block and mutex profiling reveal these bottlenecks.\nEnabling Block Profiling Block profiling is off by default. Enable it:\nimport \u0026#34;runtime\u0026#34; func init() { runtime.SetBlockProfileRate(1) // Record every blocking event } Or set a sampling rate (record 1 in N events):\nruntime.SetBlockProfileRate(1000) // Sample 1 in 1000 blocks Collect:\ncurl http://localhost:6060/debug/pprof/block \u0026gt; block.prof Enabling Mutex Profiling import \u0026#34;runtime\u0026#34; func init() { runtime.SetMutexProfileFraction(1) // Record every mutex contention } Collect:\ncurl http://localhost:6060/debug/pprof/mutex \u0026gt; mutex.prof Analyzing Contention go tool pprof block.prof (pprof) top Showing nodes accounting for 45.5s, 91.00% of 50s total flat flat% sum% cum cum% 25s 50.00% 50.00% 25s 50.00% sync.(*Mutex).Lock 15s 30.00% 80.00% 15s 30.00% chan send 5.5s 11.00% 91.00% 5.5s 11.00% sync.(*WaitGroup).Wait 25 seconds blocked on mutex locks - serious contention.\nLook at the source:\n(pprof) list main.updateCounter Total: 50s ROUTINE ======================== main.updateCounter 25s 25s (flat, cum) 50.00% of Total . . 10:var mu sync.Mutex . . 11:var counter int . . 12: . . 13:func updateCounter() { 25s 25s 14: mu.Lock() . . 15: counter++ . . 16: time.Sleep(100 * time.Millisecond) // Simulate work . . 17: mu.Unlock() . . 18:} The lock is held for 100ms while doing work. Goroutines queue up waiting.\nFixing Contention Reduce critical section size:\n// Before: lock held during slow operation func updateCounter() { mu.Lock() counter++ doExpensiveWork() // Holds lock too long mu.Unlock() } // After: minimize locked time func updateCounter() { mu.Lock() counter++ mu.Unlock() doExpensiveWork() // Outside lock } Use atomic operations for simple counters:\nimport \u0026#34;sync/atomic\u0026#34; var counter int64 func updateCounter() { atomic.AddInt64(\u0026amp;counter, 1) // No lock needed } Shard locks for high contention:\n// Before: single lock, high contention type Cache struct { mu sync.RWMutex data map[string]string } // After: multiple locks, less contention type Cache struct { shards [256]struct { mu sync.RWMutex data map[string]string } } func (c *Cache) Get(key string) string { shard := \u0026amp;c.shards[hash(key)%256] shard.mu.RLock() defer shard.mu.RUnlock() return shard.data[key] } Use sync.Map for concurrent maps:\nimport \u0026#34;sync\u0026#34; var cache sync.Map func get(key string) interface{} { val, _ := cache.Load(key) return val } func set(key string, val interface{}) { cache.Store(key, val) } sync.Map is optimized for cases where entries are written once and read many times, reducing lock contention.\nVisualizing Profiles with Flame Graphs Flame graphs make profiles easier to understand visually.\nGenerating Flame Graphs # Interactive web UI go tool pprof -http=:8080 cpu.prof Opens browser at http://localhost:8080 with multiple views:\nTop - Table of hottest functions Graph - Call graph visualization Flame Graph - Flame graph visualization Source - Annotated source code Reading Flame Graphs Flame graphs stack function calls vertically. The width represents time spent.\nBottom shows entry points (like main or HTTP handlers). Top shows leaf functions where actual work happens.\nWide bars at the top - Hot functions consuming significant CPU.\nTall stacks - Deep call chains. Not necessarily bad, but watch for unnecessary intermediate calls.\nColor - Usually meaningless (random) but some tools color by package or type.\nComparing Profiles Compare before and after optimization:\ngo tool pprof -http=:8080 -base=before.prof after.prof Shows the difference. Positive values are increases (bad if optimizing), negative are decreases (good).\nProduction Profiling Strategies Profiling in production requires care. You want insights without impacting users.\nContinuous Profiling Instead of ad-hoc profiling when issues occur, collect profiles continuously at low overhead.\nSampling-based approach:\nimport ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime/pprof\u0026#34; \u0026#34;time\u0026#34; ) func continuousProfile() { ticker := time.NewTicker(5 * time.Minute) for range ticker.C { // CPU profile for 30 seconds f, err := os.Create(fmt.Sprintf(\u0026#34;cpu-%d.prof\u0026#34;, time.Now().Unix())) if err != nil { log.Printf(\u0026#34;Failed to create profile: %v\u0026#34;, err) continue } pprof.StartCPUProfile(f) time.Sleep(30 * time.Second) pprof.StopCPUProfile() f.Close() // Upload to storage or profiling service uploadProfile(f.Name()) } } go continuousProfile() Every 5 minutes, collect a 30-second CPU profile. This gives you historical performance data.\nUsing Profiling Services Services like Datadog, New Relic, and Pyroscope offer continuous profiling with minimal overhead.\nPyroscope example:\nimport \u0026#34;github.com/pyroscope-io/client/pyroscope\u0026#34; func init() { pyroscope.Start(pyroscope.Config{ ApplicationName: \u0026#34;myapp\u0026#34;, ServerAddress: \u0026#34;http://pyroscope:4040\u0026#34;, ProfileTypes: []pyroscope.ProfileType{ pyroscope.ProfileCPU, pyroscope.ProfileAllocObjects, pyroscope.ProfileAllocSpace, pyroscope.ProfileInuseObjects, pyroscope.ProfileInuseSpace, }, }) } The service collects profiles periodically and provides a UI for exploration.\nOn-Demand Profiling For troubleshooting specific issues, enable profiling on demand without redeploying.\nFeature flag approach:\nvar enableProfiling atomic.Bool func main() { // API to enable profiling http.HandleFunc(\u0026#34;/admin/profiling/enable\u0026#34;, func(w http.ResponseWriter, r *http.Request) { enableProfiling.Store(true) w.Write([]byte(\u0026#34;Profiling enabled\u0026#34;)) }) http.HandleFunc(\u0026#34;/admin/profiling/disable\u0026#34;, func(w http.ResponseWriter, r *http.Request) { enableProfiling.Store(false) w.Write([]byte(\u0026#34;Profiling disabled\u0026#34;)) }) // Rest of app } Conditionally expose pprof:\nif enableProfiling.Load() { _ = pprof.StartCPUProfile(w) defer pprof.StopCPUProfile() } Security Considerations Never expose pprof to public internet - Profiling endpoints can leak sensitive data and add attack surface.\nUse separate port - Run pprof on a different port accessible only internally.\nAdd authentication:\nfunc pprofAuth(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { user, pass, ok := r.BasicAuth() if !ok || user != \u0026#34;admin\u0026#34; || pass != os.Getenv(\u0026#34;PPROF_PASSWORD\u0026#34;) { w.Header().Set(\u0026#34;WWW-Authenticate\u0026#34;, `Basic realm=\u0026#34;pprof\u0026#34;`) w.WriteHeader(401) w.Write([]byte(\u0026#34;Unauthorized\u0026#34;)) return } next.ServeHTTP(w, r) }) } http.Handle(\u0026#34;/debug/pprof/\u0026#34;, pprofAuth(http.DefaultServeMux)) Firewall rules - Restrict access to pprof port at network level.\nBenchmarking with pprof Integration Go benchmarks integrate with pprof for detailed performance analysis.\nWriting Benchmarks func BenchmarkProcessData(b *testing.B) { data := generateTestData() b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { processData(data) } } Running with Profiling # CPU profile go test -bench=. -cpuprofile=cpu.prof # Memory profile go test -bench=. -memprofile=mem.prof # Both go test -bench=. -cpuprofile=cpu.prof -memprofile=mem.prof Analyze:\ngo tool pprof cpu.prof go tool pprof mem.prof Benchmark Comparison Compare performance before and after changes:\n# Before optimization go test -bench=. -cpuprofile=before-cpu.prof \u0026gt; before.txt # After optimization go test -bench=. -cpuprofile=after-cpu.prof \u0026gt; after.txt # Compare results benchstat before.txt after.txt benchstat shows statistical comparison:\nname old time/op new time/op delta ProcessData-8 1.25ms ± 2% 0.45ms ± 1% -64.00% (p=0.000 n=10+10) name old alloc/op new alloc/op delta ProcessData-8 512kB ± 0% 128kB ± 0% -75.00% (p=0.000 n=10+10) 64% faster and 75% less memory - clear improvement.\nCompare profiles:\ngo tool pprof -base=before-cpu.prof after-cpu.prof Shows which functions improved.\nReal-World Optimization Case Studies Case 1: JSON Parsing Bottleneck Problem: API endpoint taking 500ms per request.\nCPU Profile showed:\n(pprof) top 2800ms encoding/json.Unmarshal 1200ms encoding/json.(*decodeState).object JSON parsing consumed 56% of request time.\nInvestigation: Logging revealed the same JSON was parsed repeatedly for validation.\nFix: Cache parsed result:\n// Before func validateRequest(r *http.Request) error { var req Request json.NewDecoder(r.Body).Decode(\u0026amp;req) // Parse if req.UserID == 0 { return errors.New(\u0026#34;invalid user\u0026#34;) } var req2 Request r.Body.Seek(0, 0) json.NewDecoder(r.Body).Decode(\u0026amp;req2) // Parse again! return businessLogic(req2) } // After func validateRequest(r *http.Request) error { var req Request if err := json.NewDecoder(r.Body).Decode(\u0026amp;req); err != nil { return err } if req.UserID == 0 { return errors.New(\u0026#34;invalid user\u0026#34;) } return businessLogic(req) // Reuse parsed data } Result: Latency dropped from 500ms to 220ms - 56% improvement.\nCase 2: Memory Leak in Cache Problem: Application memory growing from 500MB to 8GB over 24 hours.\nHeap profile comparison:\ngo tool pprof -base=heap-morning.prof heap-evening.prof (pprof) top 7.2GB main.(*SessionCache).Store Investigation: Session cache never evicted old entries.\nFix: Implemented LRU eviction with max size:\nimport \u0026#34;github.com/hashicorp/golang-lru\u0026#34; type SessionCache struct { cache *lru.Cache } func NewSessionCache() *SessionCache { cache, _ := lru.New(10000) // Max 10k sessions return \u0026amp;SessionCache{cache: cache} } func (c *SessionCache) Store(id string, session Session) { c.cache.Add(id, session) // Automatically evicts oldest if full } Result: Memory stabilized at 800MB with no growth.\nCase 3: Goroutine Leak in Worker Pool Problem: Application slowing down over time, goroutine count increasing.\nGoroutine profile:\n(pprof) top 45000 runtime.gopark 45000 main.worker 45,000 goroutines stuck in worker function.\nInvestigation: Workers waited on channel that was never closed.\nFix: Properly close channel and use WaitGroup:\n// Before func process(jobs \u0026lt;-chan Job) { for job := range jobs { go worker(job) // Goroutine never exits } } // After func process(jobs \u0026lt;-chan Job) { var wg sync.WaitGroup for i := 0; i \u0026lt; runtime.NumCPU(); i++ { wg.Add(1) go func() { defer wg.Done() for job := range jobs { worker(job) } }() } wg.Wait() } Result: Goroutine count stayed constant at around 20 (number of CPUs).\nAdvanced Profiling Techniques Custom Profiles Create custom profiles for domain-specific metrics:\nimport \u0026#34;runtime/pprof\u0026#34; var requestProfile = pprof.NewProfile(\u0026#34;requests\u0026#34;) func handleRequest(w http.ResponseWriter, r *http.Request) { requestProfile.Add(r, 0) defer requestProfile.Remove(r) // Handle request } Dump custom profile:\nf, _ := os.Create(\u0026#34;requests.prof\u0026#34;) requestProfile.WriteTo(f, 0) f.Close() Trace Profiling Execution traces show detailed timeline of goroutine execution:\nimport ( \u0026#34;os\u0026#34; \u0026#34;runtime/trace\u0026#34; ) f, _ := os.Create(\u0026#34;trace.out\u0026#34;) trace.Start(f) defer trace.Stop() // Run code Analyze:\ngo tool trace trace.out Opens browser with interactive timeline showing:\nGoroutine execution GC pauses Network I/O System calls Useful for understanding concurrency behavior and identifying scheduling issues.\nDifferential Profiling Compare profiles from different versions or configurations:\ngo tool pprof -base=v1.prof v2.prof Shows regression or improvement between versions.\nCommon Profiling Mistakes Profiling debug builds - Always profile release builds with optimizations enabled. Debug builds have different performance characteristics.\nToo short profiling duration - 30 seconds minimum for CPU profiles. Shorter durations might miss important behavior.\nOptimizing prematurely - Profile first, then optimize. Don\u0026rsquo;t assume you know the bottleneck.\nIgnoring the 80/20 rule - Focus on the top issues. Optimizing functions that take \u0026lt;1% of time won\u0026rsquo;t move the needle.\nNot measuring after changes - Always profile after optimization to verify improvement. Sometimes \u0026ldquo;optimizations\u0026rdquo; make things worse.\nProfiling on laptop - Production behavior differs from development. Profile in production-like environments.\nProfiling Tools Ecosystem Beyond pprof, several tools enhance Go profiling:\nbenchstat - Statistical comparison of benchmarks. Essential for validating optimizations.\nPyroscope - Continuous profiling platform with UI.\nDatadog Continuous Profiler - Commercial profiling with APM integration.\npprof Web UI - go tool pprof -http provides rich visualization.\nGraphviz - Renders call graphs from pprof.\nJaeger/Zipkin - Distributed tracing complements profiling for microservices.\nIntegrating Profiling into Development Workflow Make profiling routine, not reactive:\n1. Benchmark new features - Write benchmarks for performance-critical code. Run with profiling to catch issues early.\n2. Profile in CI/CD - Run benchmarks in CI and track metrics over time. Fail builds if performance regresses.\n3. Production monitoring - Continuous profiling in production catches issues that testing misses.\n4. Performance reviews - Include performance analysis in code reviews for critical paths.\n5. Document hot paths - When you optimize code, document why. Future developers will thank you.\nPerformance Optimization Principles Profiling shows where time is spent. Actually fixing issues requires good instincts:\nAlgorithm choice matters most - O(n²) vs O(n log n) beats micro-optimizations.\nAllocations are expensive - Reducing allocations often gives bigger wins than optimizing CPU-bound code.\nLock contention kills scalability - In concurrent programs, contention is usually the bottleneck.\nI/O dominates - If your app does I/O, optimize I/O first. CPU optimization won\u0026rsquo;t help if you\u0026rsquo;re waiting on network.\nMeasure, don\u0026rsquo;t guess - Your intuition about performance is probably wrong. Let profiling guide you.\nWrapping Up pprof gives you x-ray vision into your Go applications. CPU profiles show slow code. Memory profiles reveal leaks and allocation hotspots. Goroutine profiles catch concurrency issues. Block and mutex profiles expose contention.\nThe process is always the same: collect a profile, analyze the data, identify the bottleneck, fix it, profile again to verify. Repeat until performance is acceptable.\nDon\u0026rsquo;t optimize blindly. Profile first. The bottleneck is rarely where you expect. I\u0026rsquo;ve seen developers spend weeks optimizing algorithms that accounted for 0.1% of runtime while ignoring goroutine leaks that ate 80% of resources.\nProfiling is a skill that improves with practice. The more profiles you analyze, the faster you\u0026rsquo;ll spot patterns. CPU time in runtime.mallocgc? Too many allocations. Growing goroutine count? Leak. High block time on mutex? Contention.\nMake profiling part of your workflow. Benchmark performance-critical code. Run continuous profiling in production. Include performance in code reviews. Your users will notice the difference.\nIf you\u0026rsquo;re building Go microservices , check out our guides on building CLI tools and database migrations for more Go best practices.\n","href":"/2025/10/how-to-profile-and-optimize-go-applications-with-pprof.html","title":"How to Profile and Optimize Go Applications with pprof"},{"content":"Managing database schema changes is one of those tasks that seems simple until you\u0026rsquo;re dealing with multiple environments, team members making conflicting changes, or trying to rollback a production deployment at 2 AM. If you\u0026rsquo;ve ever manually run SQL scripts on production hoping you didn\u0026rsquo;t miss anything, you know exactly what I\u0026rsquo;m talking about.\ngolang-migrate solves this problem by giving you version control for your database schema. Just like git tracks code changes, migrations track schema changes. You can move forward, rollback, and know exactly what state your database is in at any time.\nThis guide covers everything you need to know about database migrations in Go - from basic setup to production deployment strategies. We\u0026rsquo;ll use PostgreSQL for examples, but the concepts apply to MySQL, SQLite, and other databases that golang-migrate supports.\nWhy You Need Database Migrations When you\u0026rsquo;re building a Go backend application , your database schema changes constantly during development. You add tables, modify columns, create indexes, insert seed data. Without migrations, you\u0026rsquo;re stuck with a few bad options:\nManual SQL scripts - You write SQL and run it by hand. This works until someone forgets to run a script, runs scripts in the wrong order, or applies the same script twice. Ask me how I know.\nSchema dumps - Export the entire schema and import it elsewhere. This destroys existing data and doesn\u0026rsquo;t work for incremental changes. Not an option for production.\nORM auto-migrations - Some ORMs detect schema changes and apply them automatically. Sounds nice but gives you zero control over how changes are applied, makes rollbacks nearly impossible, and can cause data loss with column renames.\nMigrations give you a better way. Each schema change is a versioned file that can be applied or reverted. Your database schema becomes reproducible and trackable.\nWhat is golang-migrate? golang-migrate is a CLI tool and Go library for running database migrations. It\u0026rsquo;s database-agnostic, supports multiple database drivers, handles migration versioning automatically, provides both CLI and programmatic interfaces, and has excellent PostgreSQL, MySQL, and SQLite support.\nThe tool uses pairs of SQL files - one for applying changes (up) and one for reverting them (down). Migration files are numbered sequentially, so golang-migrate knows which changes have been applied and which haven\u0026rsquo;t.\nUnlike some migration tools that try to be too clever, golang-migrate is straightforward. It runs your SQL files in order. That\u0026rsquo;s it. This simplicity is actually a feature because you have complete control over your schema changes.\nInstalling golang-migrate You need two things: the CLI tool for running migrations from the terminal, and the Go library for running migrations from your application code.\nInstall the CLI tool:\n# macOS brew install golang-migrate # Linux curl -L https://github.com/golang-migrate/migrate/releases/download/v4.17.0/migrate.linux-amd64.tar.gz | tar xvz sudo mv migrate /usr/local/bin/migrate # Windows scoop install migrate # Or build from source go install -tags \u0026#39;postgres\u0026#39; github.com/golang-migrate/migrate/v4/cmd/migrate@latest Verify installation:\nmigrate -version # v4.17.0 Install the Go library in your project:\ngo get -u github.com/golang-migrate/migrate/v4 go get -u github.com/golang-migrate/migrate/v4/database/postgres go get -u github.com/golang-migrate/migrate/v4/source/file The database driver package depends on which database you\u0026rsquo;re using. For PostgreSQL it\u0026rsquo;s database/postgres, for MySQL it\u0026rsquo;s database/mysql, for SQLite it\u0026rsquo;s database/sqlite3.\nProject Structure for Migrations Here\u0026rsquo;s how I organize migrations in Go projects:\nmyapp/ ├── cmd/ │ └── api/ │ └── main.go ├── migrations/ │ ├── 000001_create_users_table.up.sql │ ├── 000001_create_users_table.down.sql │ ├── 000002_create_posts_table.up.sql │ ├── 000002_create_posts_table.down.sql │ ├── 000003_add_email_to_users.up.sql │ └── 000003_add_email_to_users.down.sql ├── internal/ │ ├── database/ │ │ └── migrate.go │ └── models/ ├── go.mod └── go.sum The migrations/ folder contains all migration files. Each migration has two files - up for applying changes and down for reverting them. Files are numbered sequentially so they run in order.\nCreating Your First Migration Let\u0026rsquo;s create a migration for a users table. The naming convention is {version}_{description}.{direction}.sql.\nCreate the migration files:\nmigrate create -ext sql -dir migrations -seq create_users_table This creates two files:\n000001_create_users_table.up.sql 000001_create_users_table.down.sql Edit the up migration (000001_create_users_table.up.sql):\nCREATE TABLE IF NOT EXISTS users ( id SERIAL PRIMARY KEY, username VARCHAR(50) UNIQUE NOT NULL, email VARCHAR(255) UNIQUE NOT NULL, password_hash VARCHAR(255) NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP ); CREATE INDEX idx_users_email ON users(email); CREATE INDEX idx_users_username ON users(username); Edit the down migration (000001_create_users_table.down.sql):\nDROP TABLE IF EXISTS users CASCADE; The up migration creates the table with indexes. The down migration drops it. Always include CASCADE when dropping tables that might have foreign keys pointing to them.\nRunning Migrations from CLI The simplest way to run migrations is using the CLI tool. You need a database URL in the format:\npostgres://username:password@localhost:5432/database_name?sslmode=disable Run all pending migrations:\nmigrate -database \u0026#34;postgres://user:password@localhost:5432/myapp?sslmode=disable\u0026#34; \\ -path migrations \\ up This applies all migrations that haven\u0026rsquo;t been run yet. golang-migrate tracks which migrations have been applied using a schema_migrations table.\nRun a specific number of migrations:\n# Apply next 2 migrations migrate -database \u0026#34;postgres://...\u0026#34; -path migrations up 2 Rollback migrations:\n# Rollback last migration migrate -database \u0026#34;postgres://...\u0026#34; -path migrations down 1 # Rollback all migrations migrate -database \u0026#34;postgres://...\u0026#34; -path migrations down -all Check current migration version:\nmigrate -database \u0026#34;postgres://...\u0026#34; -path migrations version Force a specific version (use carefully):\nmigrate -database \u0026#34;postgres://...\u0026#34; -path migrations force 1 The force command is useful when a migration partially failed and you need to reset the version before retrying.\nRunning Migrations from Go Code For production applications, you typically run migrations programmatically when the application starts or as a separate deployment step. Here\u0026rsquo;s how to do it in Go.\nCreate internal/database/migrate.go:\npackage database import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4/database/postgres\u0026#34; _ \u0026#34;github.com/golang-migrate/migrate/v4/source/file\u0026#34; ) func RunMigrations(db *sql.DB, migrationsPath string) error { driver, err := postgres.WithInstance(db, \u0026amp;postgres.Config{}) if err != nil { return fmt.Errorf(\u0026#34;could not create database driver: %w\u0026#34;, err) } m, err := migrate.NewWithDatabaseInstance( fmt.Sprintf(\u0026#34;file://%s\u0026#34;, migrationsPath), \u0026#34;postgres\u0026#34;, driver, ) if err != nil { return fmt.Errorf(\u0026#34;could not create migrate instance: %w\u0026#34;, err) } if err := m.Up(); err != nil \u0026amp;\u0026amp; err != migrate.ErrNoChange { return fmt.Errorf(\u0026#34;could not run migrations: %w\u0026#34;, err) } log.Println(\u0026#34;Migrations completed successfully\u0026#34;) return nil } func RollbackMigrations(db *sql.DB, migrationsPath string, steps int) error { driver, err := postgres.WithInstance(db, \u0026amp;postgres.Config{}) if err != nil { return fmt.Errorf(\u0026#34;could not create database driver: %w\u0026#34;, err) } m, err := migrate.NewWithDatabaseInstance( fmt.Sprintf(\u0026#34;file://%s\u0026#34;, migrationsPath), \u0026#34;postgres\u0026#34;, driver, ) if err != nil { return fmt.Errorf(\u0026#34;could not create migrate instance: %w\u0026#34;, err) } if err := m.Steps(-steps); err != nil { return fmt.Errorf(\u0026#34;could not rollback migrations: %w\u0026#34;, err) } log.Printf(\u0026#34;Rolled back %d migrations\\n\u0026#34;, steps) return nil } Use it in your main application (cmd/api/main.go):\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; \u0026#34;myapp/internal/database\u0026#34; ) func main() { dbURL := os.Getenv(\u0026#34;DATABASE_URL\u0026#34;) if dbURL == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;DATABASE_URL environment variable is required\u0026#34;) } db, err := sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { log.Fatalf(\u0026#34;Failed to connect to database: %v\u0026#34;, err) } defer db.Close() if err := db.Ping(); err != nil { log.Fatalf(\u0026#34;Failed to ping database: %v\u0026#34;, err) } // Run migrations if err := database.RunMigrations(db, \u0026#34;./migrations\u0026#34;); err != nil { log.Fatalf(\u0026#34;Failed to run migrations: %v\u0026#34;, err) } log.Println(\u0026#34;Database connection and migrations successful\u0026#34;) // Start your application... } This approach runs migrations automatically when your application starts. For production, you might want a separate migration command instead of running migrations on every startup.\nCreating a Migration CLI Tool Instead of running migrations at application startup, create a dedicated CLI tool for managing migrations. This gives you more control in production.\nCreate cmd/migrate/main.go:\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; \u0026#34;myapp/internal/database\u0026#34; ) func main() { var action string var steps int flag.StringVar(\u0026amp;action, \u0026#34;action\u0026#34;, \u0026#34;up\u0026#34;, \u0026#34;Migration action: up, down, or version\u0026#34;) flag.IntVar(\u0026amp;steps, \u0026#34;steps\u0026#34;, 0, \u0026#34;Number of migration steps (for down action)\u0026#34;) flag.Parse() dbURL := os.Getenv(\u0026#34;DATABASE_URL\u0026#34;) if dbURL == \u0026#34;\u0026#34; { log.Fatal(\u0026#34;DATABASE_URL environment variable is required\u0026#34;) } db, err := sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { log.Fatalf(\u0026#34;Failed to connect to database: %v\u0026#34;, err) } defer db.Close() migrationsPath := \u0026#34;./migrations\u0026#34; switch action { case \u0026#34;up\u0026#34;: if err := database.RunMigrations(db, migrationsPath); err != nil { log.Fatalf(\u0026#34;Migration failed: %v\u0026#34;, err) } fmt.Println(\u0026#34;Migrations applied successfully\u0026#34;) case \u0026#34;down\u0026#34;: if steps == 0 { log.Fatal(\u0026#34;Please specify -steps for down migrations\u0026#34;) } if err := database.RollbackMigrations(db, migrationsPath, steps); err != nil { log.Fatalf(\u0026#34;Rollback failed: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Rolled back %d migrations\\n\u0026#34;, steps) case \u0026#34;version\u0026#34;: version, dirty, err := database.GetMigrationVersion(db, migrationsPath) if err != nil { log.Fatalf(\u0026#34;Failed to get version: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Current version: %d (dirty: %v)\\n\u0026#34;, version, dirty) default: log.Fatalf(\u0026#34;Unknown action: %s\u0026#34;, action) } } Add the version function to internal/database/migrate.go:\nfunc GetMigrationVersion(db *sql.DB, migrationsPath string) (uint, bool, error) { driver, err := postgres.WithInstance(db, \u0026amp;postgres.Config{}) if err != nil { return 0, false, fmt.Errorf(\u0026#34;could not create database driver: %w\u0026#34;, err) } m, err := migrate.NewWithDatabaseInstance( fmt.Sprintf(\u0026#34;file://%s\u0026#34;, migrationsPath), \u0026#34;postgres\u0026#34;, driver, ) if err != nil { return 0, false, fmt.Errorf(\u0026#34;could not create migrate instance: %w\u0026#34;, err) } version, dirty, err := m.Version() if err != nil \u0026amp;\u0026amp; err != migrate.ErrNilVersion { return 0, false, fmt.Errorf(\u0026#34;could not get version: %w\u0026#34;, err) } return version, dirty, nil } Build and use the migration tool:\ngo build -o bin/migrate cmd/migrate/main.go # Run migrations DATABASE_URL=\u0026#34;postgres://user:pass@localhost/myapp?sslmode=disable\u0026#34; ./bin/migrate -action up # Rollback 1 migration DATABASE_URL=\u0026#34;postgres://user:pass@localhost/myapp?sslmode=disable\u0026#34; ./bin/migrate -action down -steps 1 # Check version DATABASE_URL=\u0026#34;postgres://user:pass@localhost/myapp?sslmode=disable\u0026#34; ./bin/migrate -action version This approach separates migration execution from application startup, which is safer for production deployments.\nReal-World Migration Examples Let\u0026rsquo;s look at common migration scenarios you\u0026rsquo;ll encounter.\nAdding a Column Create migration:\nmigrate create -ext sql -dir migrations -seq add_bio_to_users Up migration (000002_add_bio_to_users.up.sql):\nALTER TABLE users ADD COLUMN bio TEXT; ALTER TABLE users ADD COLUMN avatar_url VARCHAR(500); Down migration (000002_add_bio_to_users.down.sql):\nALTER TABLE users DROP COLUMN bio; ALTER TABLE users DROP COLUMN avatar_url; Creating a Related Table Create migration:\nmigrate create -ext sql -dir migrations -seq create_posts_table Up migration (000003_create_posts_table.up.sql):\nCREATE TABLE IF NOT EXISTS posts ( id SERIAL PRIMARY KEY, user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE, title VARCHAR(255) NOT NULL, content TEXT NOT NULL, published BOOLEAN DEFAULT false, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP ); CREATE INDEX idx_posts_user_id ON posts(user_id); CREATE INDEX idx_posts_published ON posts(published); CREATE INDEX idx_posts_created_at ON posts(created_at DESC); Down migration (000003_create_posts_table.down.sql):\nDROP TABLE IF EXISTS posts CASCADE; Modifying Column Type This one\u0026rsquo;s tricky because you might lose data. Always backup first.\nCreate migration:\nmigrate create -ext sql -dir migrations -seq change_username_length Up migration (000004_change_username_length.up.sql):\nALTER TABLE users ALTER COLUMN username TYPE VARCHAR(100); Down migration (000004_change_username_length.down.sql):\nALTER TABLE users ALTER COLUMN username TYPE VARCHAR(50); The down migration might fail if data exceeds 50 characters. For production, you\u0026rsquo;d add a CHECK constraint first to ensure no data violates the limit.\nAdding Constraints Create migration:\nmigrate create -ext sql -dir migrations -seq add_user_constraints Up migration (000005_add_user_constraints.up.sql):\nALTER TABLE users ADD CONSTRAINT username_min_length CHECK (LENGTH(username) \u0026gt;= 3); ALTER TABLE users ADD CONSTRAINT email_format CHECK (email ~* \u0026#39;^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}$\u0026#39;); Down migration (000005_add_user_constraints.down.sql):\nALTER TABLE users DROP CONSTRAINT IF EXISTS username_min_length; ALTER TABLE users DROP CONSTRAINT IF EXISTS email_format; Data Migrations Sometimes you need to migrate data, not just schema. Here\u0026rsquo;s an example of adding a default role to existing users.\nCreate migration:\nmigrate create -ext sql -dir migrations -seq add_role_to_users Up migration (000006_add_role_to_users.up.sql):\n-- Add column ALTER TABLE users ADD COLUMN role VARCHAR(20) DEFAULT \u0026#39;user\u0026#39;; -- Update existing users UPDATE users SET role = \u0026#39;user\u0026#39; WHERE role IS NULL; -- Make it non-nullable after setting defaults ALTER TABLE users ALTER COLUMN role SET NOT NULL; Down migration (000006_add_role_to_users.down.sql):\nALTER TABLE users DROP COLUMN role; The up migration adds the column with a default, updates existing rows, then makes it non-nullable. This prevents errors with existing data.\nWorking with Multiple Databases golang-migrate supports PostgreSQL, MySQL, SQLite, MongoDB, and more. The API is the same, only the driver changes.\nMySQL Example import ( \u0026#34;github.com/golang-migrate/migrate/v4\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4/database/mysql\u0026#34; _ \u0026#34;github.com/golang-migrate/migrate/v4/source/file\u0026#34; _ \u0026#34;github.com/go-sql-driver/mysql\u0026#34; ) func RunMySQLMigrations(db *sql.DB, migrationsPath string) error { driver, err := mysql.WithInstance(db, \u0026amp;mysql.Config{}) if err != nil { return err } m, err := migrate.NewWithDatabaseInstance( fmt.Sprintf(\u0026#34;file://%s\u0026#34;, migrationsPath), \u0026#34;mysql\u0026#34;, driver, ) if err != nil { return err } return m.Up() } MySQL migrations use MySQL-specific SQL:\n-- MySQL up migration CREATE TABLE users ( id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(50) UNIQUE NOT NULL, email VARCHAR(255) UNIQUE NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; SQLite Example import ( \u0026#34;github.com/golang-migrate/migrate/v4\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4/database/sqlite3\u0026#34; _ \u0026#34;github.com/golang-migrate/migrate/v4/source/file\u0026#34; _ \u0026#34;github.com/mattn/go-sqlite3\u0026#34; ) func RunSQLiteMigrations(db *sql.DB, migrationsPath string) error { driver, err := sqlite3.WithInstance(db, \u0026amp;sqlite3.Config{}) if err != nil { return err } m, err := migrate.NewWithDatabaseInstance( fmt.Sprintf(\u0026#34;file://%s\u0026#34;, migrationsPath), \u0026#34;sqlite3\u0026#34;, driver, ) if err != nil { return err } return m.Up() } SQLite has different syntax for some operations:\n-- SQLite doesn\u0026#39;t support ALTER COLUMN -- You have to recreate the table CREATE TABLE users_new ( id INTEGER PRIMARY KEY AUTOINCREMENT, username TEXT UNIQUE NOT NULL, email TEXT UNIQUE NOT NULL, created_at DATETIME DEFAULT CURRENT_TIMESTAMP ); INSERT INTO users_new SELECT * FROM users; DROP TABLE users; ALTER TABLE users_new RENAME TO users; Embedding Migrations in Your Binary For simpler deployments, you can embed migration files directly in your Go binary using Go 1.16+ embed feature.\nUpdate internal/database/migrate.go:\npackage database import ( \u0026#34;database/sql\u0026#34; \u0026#34;embed\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4/database/postgres\u0026#34; \u0026#34;github.com/golang-migrate/migrate/v4/source/iofs\u0026#34; ) //go:embed migrations/*.sql var migrationsFS embed.FS func RunEmbeddedMigrations(db *sql.DB) error { driver, err := postgres.WithInstance(db, \u0026amp;postgres.Config{}) if err != nil { return fmt.Errorf(\u0026#34;could not create database driver: %w\u0026#34;, err) } sourceDriver, err := iofs.New(migrationsFS, \u0026#34;migrations\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;could not create source driver: %w\u0026#34;, err) } m, err := migrate.NewWithInstance(\u0026#34;iofs\u0026#34;, sourceDriver, \u0026#34;postgres\u0026#34;, driver) if err != nil { return fmt.Errorf(\u0026#34;could not create migrate instance: %w\u0026#34;, err) } if err := m.Up(); err != nil \u0026amp;\u0026amp; err != migrate.ErrNoChange { return fmt.Errorf(\u0026#34;could not run migrations: %w\u0026#34;, err) } return nil } Move migrations to internal/database/migrations/:\ninternal/ └── database/ ├── migrate.go └── migrations/ ├── 000001_create_users_table.up.sql ├── 000001_create_users_table.down.sql └── ... Now your binary includes migrations - no need to deploy migration files separately.\nTesting Migrations Always test migrations before running them in production. Here\u0026rsquo;s how to set up automated migration testing.\nCreate internal/database/migrate_test.go:\npackage database import ( \u0026#34;database/sql\u0026#34; \u0026#34;testing\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) func TestMigrations(t *testing.T) { // Use a test database dbURL := \u0026#34;postgres://testuser:testpass@localhost:5432/testdb?sslmode=disable\u0026#34; db, err := sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { t.Fatalf(\u0026#34;Failed to connect to test database: %v\u0026#34;, err) } defer db.Close() // Clean database before test if _, err := db.Exec(\u0026#34;DROP SCHEMA public CASCADE; CREATE SCHEMA public;\u0026#34;); err != nil { t.Fatalf(\u0026#34;Failed to clean database: %v\u0026#34;, err) } // Run migrations if err := RunMigrations(db, \u0026#34;../../migrations\u0026#34;); err != nil { t.Fatalf(\u0026#34;Failed to run migrations: %v\u0026#34;, err) } // Verify schema var count int err = db.QueryRow(\u0026#34;SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = \u0026#39;public\u0026#39; AND table_name = \u0026#39;users\u0026#39;\u0026#34;).Scan(\u0026amp;count) if err != nil { t.Fatalf(\u0026#34;Failed to query tables: %v\u0026#34;, err) } if count != 1 { t.Errorf(\u0026#34;Expected users table to exist, but it doesn\u0026#39;t\u0026#34;) } // Test rollback if err := RollbackMigrations(db, \u0026#34;../../migrations\u0026#34;, 1); err != nil { t.Fatalf(\u0026#34;Failed to rollback migrations: %v\u0026#34;, err) } // Verify table was dropped err = db.QueryRow(\u0026#34;SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = \u0026#39;public\u0026#39; AND table_name = \u0026#39;users\u0026#39;\u0026#34;).Scan(\u0026amp;count) if err != nil { t.Fatalf(\u0026#34;Failed to query tables after rollback: %v\u0026#34;, err) } if count != 0 { t.Errorf(\u0026#34;Expected users table to be dropped, but it still exists\u0026#34;) } } Run with docker for isolated testing:\ndocker run -d --name test-postgres \\ -e POSTGRES_USER=testuser \\ -e POSTGRES_PASSWORD=testpass \\ -e POSTGRES_DB=testdb \\ -p 5432:5432 \\ postgres:15 go test ./internal/database/... docker stop test-postgres docker rm test-postgres Production Deployment Strategies Running migrations in production requires careful planning. Here are proven strategies.\nSeparate Migration Step Run migrations as a separate step before deploying your application:\n# .github/workflows/deploy.yml - name: Run database migrations run: | ./bin/migrate -action up env: DATABASE_URL: ${{ secrets.DATABASE_URL }} - name: Deploy application run: | ./deploy.sh This ensures migrations complete successfully before the new application version starts.\nBlue-Green Deployments For zero-downtime deployments, make migrations backward-compatible so both old and new code can run against the same schema.\nAdding a column:\n-- Safe: old code ignores new column ALTER TABLE users ADD COLUMN phone VARCHAR(20); Removing a column requires two deployments:\n-- Deployment 1: stop using the column in code -- Deployment 2: drop the column ALTER TABLE users DROP COLUMN phone; Migration Locks Prevent concurrent migrations using database locks:\nfunc RunMigrationsWithLock(db *sql.DB, migrationsPath string) error { // Acquire advisory lock var locked bool err := db.QueryRow(\u0026#34;SELECT pg_try_advisory_lock(123456789)\u0026#34;).Scan(\u0026amp;locked) if err != nil { return fmt.Errorf(\u0026#34;failed to acquire lock: %w\u0026#34;, err) } if !locked { return fmt.Errorf(\u0026#34;another migration is already running\u0026#34;) } defer db.Exec(\u0026#34;SELECT pg_advisory_unlock(123456789)\u0026#34;) // Run migrations return RunMigrations(db, migrationsPath) } This prevents issues when multiple instances try to run migrations simultaneously.\nDatabase Backups Always backup before migrations:\n#!/bin/bash # backup-and-migrate.sh # Backup database pg_dump -U user -h localhost dbname \u0026gt; backup_$(date +%Y%m%d_%H%M%S).sql # Run migrations ./bin/migrate -action up if [ $? -ne 0 ]; then echo \u0026#34;Migration failed! Restore from backup if needed\u0026#34; exit 1 fi echo \u0026#34;Migration successful\u0026#34; Common Migration Pitfalls Editing existing migrations - Never modify a migration that\u0026rsquo;s been applied in production. Create a new migration instead.\nMissing down migrations - Always write down migrations. You\u0026rsquo;ll need them when deployments fail.\nLarge data migrations - Updating millions of rows locks tables. Split into batches or use background jobs.\nNot testing rollbacks - Test your down migrations. Discovering they don\u0026rsquo;t work during a production incident is too late.\nIgnoring migration order - Migrations run in version order. Don\u0026rsquo;t depend on migrations that come later.\nDropping columns with data - Add migrations to move data before dropping columns, or you\u0026rsquo;ll lose information.\nIntegrating with ORMs If you\u0026rsquo;re using an ORM like GORM or sqlx , you can still use golang-migrate for schema management while using the ORM for queries.\nExample with GORM:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;gorm.io/driver/postgres\u0026#34; \u0026#34;gorm.io/gorm\u0026#34; \u0026#34;myapp/internal/database\u0026#34; ) type User struct { ID uint `gorm:\u0026#34;primaryKey\u0026#34;` Username string `gorm:\u0026#34;unique;not null\u0026#34;` Email string `gorm:\u0026#34;unique;not null\u0026#34;` } func main() { // Connect to database db, err := sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { log.Fatal(err) } // Run migrations if err := database.RunMigrations(db, \u0026#34;./migrations\u0026#34;); err != nil { log.Fatal(err) } // Use GORM for queries gormDB, err := gorm.Open(postgres.New(postgres.Config{ Conn: db, }), \u0026amp;gorm.Config{}) if err != nil { log.Fatal(err) } // Now use GORM var users []User gormDB.Find(\u0026amp;users) } This gives you controlled migrations with golang-migrate and convenient queries with GORM. Avoid GORM\u0026rsquo;s AutoMigrate() in production - use explicit migration files instead.\nMonitoring and Observability Track migration execution in production:\nfunc RunMigrationsWithLogging(db *sql.DB, migrationsPath string) error { start := time.Now() log.Println(\u0026#34;Starting database migrations...\u0026#34;) err := RunMigrations(db, migrationsPath) duration := time.Since(start) if err != nil { log.Printf(\u0026#34;Migration failed after %v: %v\u0026#34;, duration, err) return err } log.Printf(\u0026#34;Migrations completed successfully in %v\u0026#34;, duration) return nil } For production systems, integrate with your monitoring stack:\n// Send metrics to Prometheus, DataDog, etc. migrationDuration.Observe(duration.Seconds()) if err != nil { migrationErrors.Inc() } Advanced: Custom Migration Sources You can load migrations from sources other than files - databases, HTTP endpoints, or embedded resources.\nExample loading from HTTP:\nimport ( \u0026#34;github.com/golang-migrate/migrate/v4\u0026#34; _ \u0026#34;github.com/golang-migrate/migrate/v4/source/httpfs\u0026#34; ) m, err := migrate.New( \u0026#34;https://example.com/migrations\u0026#34;, \u0026#34;postgres://user:pass@localhost/db?sslmode=disable\u0026#34;, ) This is useful for centralized migration management across multiple services.\nSchema Migration Best Practices After managing migrations across dozens of projects, here\u0026rsquo;s what works:\nKeep migrations small - One logical change per migration makes rollbacks easier and reduces risk.\nTest migrations locally - Run up and down migrations locally before committing. Catch syntax errors early.\nUse transactions when possible - PostgreSQL supports transactional DDL. Wrap migrations in BEGIN/COMMIT blocks.\nDocument complex migrations - Add SQL comments explaining why the change is needed, especially for data migrations.\nVersion control is mandatory - Commit migration files with the code that uses them. Deploy together.\nNever skip migrations - Always run migrations sequentially. Jumping versions causes inconsistent state.\nPlan for rollback - Write down migrations that actually work. Test them before deploying.\nWrapping Up Database migrations are essential for any production Go application that uses a database. golang-migrate gives you the tools to manage schema changes safely across environments, track database versions like you track code, rollback when deployments fail, and collaborate with your team without conflicts.\nThe key is treating your database schema as code. Version it, review it, test it, and deploy it systematically. Manual schema changes are error-prone and don\u0026rsquo;t scale beyond one developer.\nStart simple - create migrations for your tables, run them locally, test rollbacks. As your application grows, add more sophisticated deployment strategies like migration locks, backups, and monitoring.\nIf you\u0026rsquo;re building APIs in Go, check out our guide on building REST APIs and OAuth2 authentication in Go . For DevOps workflows, see our articles on Docker deployment and CI/CD pipelines .\nDatabase migrations might seem like extra work upfront, but they save countless hours debugging production issues and coordinating schema changes across teams. Trust me - your future self will thank you.\nFrequently Asked Questions What is golang-migrate and why should I use it?\ngolang-migrate is a database migration tool written in Go that helps you manage schema changes across different environments. It supports multiple databases (PostgreSQL, MySQL, SQLite, MongoDB), provides version control for your schema, allows rollbacks when things go wrong, and integrates seamlessly with Go applications. Using migrations instead of manual schema changes prevents inconsistencies between development, staging, and production databases.\nHow do I handle migration failures in production?\nAlways test migrations in a staging environment first with production-like data. Use transactions when possible (most DDL operations in PostgreSQL support transactions). Keep a database backup before running migrations. Implement a rollback strategy - golang-migrate supports down migrations. Monitor migration execution time and lock durations. For large tables, consider online schema change tools like pt-online-schema-change for MySQL or pg_repack for PostgreSQL.\nCan I run migrations automatically when my Go application starts?\nYes, you can embed migrations in your Go application and run them at startup using migrate.NewWithSourceInstance with embedded files. However, this approach has risks in production - multiple instances starting simultaneously can cause conflicts, failed migrations block application startup, and you have less control over when migrations run. For production, use a separate migration step in your deployment pipeline before starting the application.\nWhat\u0026rsquo;s the difference between up and down migrations?\nUp migrations apply schema changes moving forward - creating tables, adding columns, inserting data. Down migrations reverse those changes - dropping tables, removing columns, deleting data. Every migration should have both up and down files so you can rollback if needed. Down migrations are critical for production safety when a deployment needs to be reverted quickly.\nHow do I handle data migrations versus schema migrations?\nSchema migrations change database structure (CREATE TABLE, ALTER COLUMN). Data migrations modify existing data (UPDATE statements, INSERT default records). golang-migrate handles both using SQL files. For complex data transformations, you can write Go code that runs as part of the migration. Keep data migrations idempotent so they can be run multiple times safely. Large data migrations should be done in batches to avoid locking tables for extended periods.\nShould I commit migration files to version control?\nAbsolutely yes. Migration files should be committed to git alongside your application code. This ensures every team member has the same schema version, makes it easy to track what changed and when, allows code review of schema changes, and ensures deployments include necessary migrations. Never modify existing migrations that have been applied in production - create new migrations instead.\nHow do I test database migrations?\nCreate a test database and run migrations in your CI/CD pipeline. Write tests that apply migrations and verify the schema is correct. Test rollbacks by running down migrations. Use docker containers to create isolated test databases. Test with realistic data volumes to catch performance issues. Verify foreign key constraints, indexes, and default values are created correctly. Always test migrations in a staging environment before production.\n","href":"/2025/10/how-to-perform-database-migrations-in-go-using-golang-migrate.html","title":"How to Perform Database Migrations in Go using golang-migrate"},{"content":"I\u0026rsquo;ve built a lot of CLI tools over the years - deployment scripts, database migration tools, log analyzers, you name it. Every time I start a new one, I reach for Cobra and Viper. Not because they\u0026rsquo;re trendy (though they are), but because they solve the boring parts so I can focus on what my tool actually does.\nThink about kubectl, hugo, gh (GitHub CLI) - all built with Cobra. There\u0026rsquo;s a reason for that. Cobra gives you a clean command structure, automatic help generation, flag parsing, and all the stuff you\u0026rsquo;d otherwise spend hours implementing. Viper adds configuration management so users can configure your tool however they want - config files, environment variables, flags, whatever.\nI\u0026rsquo;ll show you how to build a real CLI tool from scratch. Not a toy example, but something you\u0026rsquo;d actually use in production. We\u0026rsquo;ll build a task manager CLI with commands, subcommands, persistent storage, configuration, and everything you need to distribute it to users.\nWhy Cobra and Viper? Cobra handles the structure of your CLI - commands, subcommands, flags, arguments. Without it, you\u0026rsquo;d write a mess of switch statements parsing os.Args and handling help text manually. Ever tried parsing command-line flags by hand? It\u0026rsquo;s tedious and error-prone.\nViper manages configuration. It reads from config files (YAML, JSON, TOML), environment variables, and flags. The best part - it handles priority automatically. Flags override env vars, env vars override config files, config files override defaults. No manual priority checking needed.\nWhat you get with these libraries: automatic help generation, type-safe flag parsing, config file support, environment variable binding, defaults with overrides, input validation, and shell completion scripts. All the boring CLI stuff handled for you.\nGo\u0026rsquo;s standard library has a flag package, but it\u0026rsquo;s limited. Fine for quick scripts, painful for real tools. Cobra and Viper are battle-tested - Kubernetes, Docker, and GitHub CLI all use them. If it\u0026rsquo;s good enough for kubectl, it\u0026rsquo;s good enough for your project.\nUnderstanding CLI Tool Structure Good CLI tools follow conventions. Look at git, docker, or kubectl - they all share patterns:\n# Root command with global flags mytool --config=/path/to/config # Subcommands for different actions mytool create task mytool list tasks mytool delete task 123 # Flags at different levels mytool --verbose create task --priority=high This structure makes CLIs discoverable. Users don\u0026rsquo;t need to memorize everything - they can explore with --help at any level.\nRoot Command is your main executable. It might do something by default, or just show help and available subcommands.\nSubcommands are actions your tool can perform. Think git commit, docker build, kubectl apply. Each subcommand can have its own flags and logic.\nFlags are options that modify behavior. They come in two types:\nPersistent flags: Available to all subcommands (like --config or --verbose) Local flags: Only available to specific commands (like --priority for creating tasks) Arguments are positional values after the command and flags. Like the task ID in mytool delete 123.\nCobra makes this structure natural to build.\nProject Setup Let\u0026rsquo;s build a task manager CLI called tasker. It\u0026rsquo;ll let users create, list, update, and delete tasks from the command line. Tasks will be stored locally, and users can configure behavior with a YAML file.\nCreate your project:\nmkdir tasker cd tasker go mod init github.com/yourusername/tasker Install Cobra and Viper:\ngo get -u github.com/spf13/cobra@latest go get -u github.com/spf13/viper@latest Cobra has a generator to scaffold CLI apps, but I\u0026rsquo;ll show you the manual way so you understand what\u0026rsquo;s happening:\ntasker/ ├── cmd/ │ ├── root.go # Root command │ ├── create.go # Create task command │ ├── list.go # List tasks command │ ├── delete.go # Delete task command │ └── update.go # Update task command ├── internal/ │ ├── task/ │ │ └── task.go # Task model and storage │ └── config/ │ └── config.go # Configuration handling ├── main.go └── .tasker.yaml # Example config file This structure separates commands from business logic. Commands live in cmd/, actual functionality in internal/.\nBuilding the Root Command The root command is your CLI\u0026rsquo;s entry point. Create cmd/root.go:\n// cmd/root.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/spf13/viper\u0026#34; ) var ( cfgFile string verbose bool ) // rootCmd represents the base command var rootCmd = \u0026amp;cobra.Command{ Use: \u0026#34;tasker\u0026#34;, Short: \u0026#34;A simple task manager CLI\u0026#34;, Long: `Tasker is a command-line task manager that helps you organize your work. Create tasks, set priorities, mark them complete, and stay productive from your terminal.`, Run: func(cmd *cobra.Command, args []string) { // If called without subcommands, show help cmd.Help() }, } // Execute runs the root command func Execute() { if err := rootCmd.Execute(); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } } func init() { // Run before any command executes cobra.OnInitialize(initConfig) // Persistent flags available to all subcommands rootCmd.PersistentFlags().StringVar(\u0026amp;cfgFile, \u0026#34;config\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;config file (default is $HOME/.tasker.yaml)\u0026#34;) rootCmd.PersistentFlags().BoolVarP(\u0026amp;verbose, \u0026#34;verbose\u0026#34;, \u0026#34;v\u0026#34;, false, \u0026#34;verbose output\u0026#34;) // Bind flags to viper viper.BindPFlag(\u0026#34;verbose\u0026#34;, rootCmd.PersistentFlags().Lookup(\u0026#34;verbose\u0026#34;)) } func initConfig() { if cfgFile != \u0026#34;\u0026#34; { // Use config file from flag viper.SetConfigFile(cfgFile) } else { // Search for config in home directory home, err := os.UserHomeDir() if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } viper.AddConfigPath(home) viper.SetConfigType(\u0026#34;yaml\u0026#34;) viper.SetConfigName(\u0026#34;.tasker\u0026#34;) } // Read environment variables viper.AutomaticEnv() // Read config file if it exists if err := viper.ReadInConfig(); err == nil { if viper.GetBool(\u0026#34;verbose\u0026#34;) { fmt.Println(\u0026#34;Using config file:\u0026#34;, viper.ConfigFileUsed()) } } } Let\u0026rsquo;s break this down:\nrootCmd defines the command structure. Use is the command name, Short and Long are help text, Run is what executes when someone runs tasker without subcommands.\nExecute() is called from main.go. It starts the command execution chain.\ninit() runs when the package loads. We set up persistent flags here - flags available to all subcommands.\ninitConfig() loads configuration from files and environment variables. Viper checks the config file location, reads it if it exists, and makes values available throughout the app.\nNow create main.go:\n// main.go package main import \u0026#34;github.com/yourusername/tasker/cmd\u0026#34; func main() { cmd.Execute() } That\u0026rsquo;s it. Main just calls Execute(). All logic lives in cmd/.\nTest it:\ngo run main.go # Shows help text with available commands go run main.go --help # Same thing, explicit help flag Task Model and Storage Before building commands, we need a task model and storage. Keep it simple - store tasks in a JSON file. Create internal/task/task.go:\n// internal/task/task.go package task import ( \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;time\u0026#34; ) type Task struct { ID int `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Priority string `json:\u0026#34;priority\u0026#34;` Completed bool `json:\u0026#34;completed\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type Storage struct { filepath string tasks []Task } // NewStorage creates a new storage instance func NewStorage() (*Storage, error) { home, err := os.UserHomeDir() if err != nil { return nil, err } filepath := filepath.Join(home, \u0026#34;.tasker-data.json\u0026#34;) s := \u0026amp;Storage{ filepath: filepath, tasks: []Task{}, } // Load existing tasks if file exists if err := s.load(); err != nil \u0026amp;\u0026amp; !os.IsNotExist(err) { return nil, err } return s, nil } // load reads tasks from disk func (s *Storage) load() error { data, err := os.ReadFile(s.filepath) if err != nil { return err } return json.Unmarshal(data, \u0026amp;s.tasks) } // save writes tasks to disk func (s *Storage) save() error { data, err := json.MarshalIndent(s.tasks, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) if err != nil { return err } return os.WriteFile(s.filepath, data, 0644) } // Create adds a new task func (s *Storage) Create(title, priority string) (*Task, error) { // Generate ID id := 1 if len(s.tasks) \u0026gt; 0 { id = s.tasks[len(s.tasks)-1].ID + 1 } task := \u0026amp;Task{ ID: id, Title: title, Priority: priority, Completed: false, CreatedAt: time.Now(), } s.tasks = append(s.tasks, *task) if err := s.save(); err != nil { return nil, err } return task, nil } // List returns all tasks func (s *Storage) List() []Task { return s.tasks } // Get returns a task by ID func (s *Storage) Get(id int) (*Task, error) { for i := range s.tasks { if s.tasks[i].ID == id { return \u0026amp;s.tasks[i], nil } } return nil, fmt.Errorf(\u0026#34;task %d not found\u0026#34;, id) } // Delete removes a task func (s *Storage) Delete(id int) error { for i := range s.tasks { if s.tasks[i].ID == id { s.tasks = append(s.tasks[:i], s.tasks[i+1:]...) return s.save() } } return fmt.Errorf(\u0026#34;task %d not found\u0026#34;, id) } // Update modifies a task func (s *Storage) Update(id int, title, priority string, completed *bool) error { task, err := s.Get(id) if err != nil { return err } if title != \u0026#34;\u0026#34; { task.Title = title } if priority != \u0026#34;\u0026#34; { task.Priority = priority } if completed != nil { task.Completed = *completed } return s.save() } This gives us basic CRUD operations on tasks. In production, you\u0026rsquo;d use a real database, but JSON files work fine for CLI tools that don\u0026rsquo;t need concurrent access.\nBuilding Create Command Now let\u0026rsquo;s build the command to create tasks. Create cmd/create.go:\n// cmd/create.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/yourusername/tasker/internal/task\u0026#34; ) var ( priority string ) var createCmd = \u0026amp;cobra.Command{ Use: \u0026#34;create [title]\u0026#34;, Short: \u0026#34;Create a new task\u0026#34;, Long: `Create a new task with the specified title. You can optionally set the priority using the --priority flag.`, Args: cobra.ExactArgs(1), // Require exactly one argument Run: func(cmd *cobra.Command, args []string) { title := args[0] // Create storage storage, err := task.NewStorage() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } // Create task t, err := storage.Create(title, priority) if err != nil { fmt.Println(\u0026#34;Error creating task:\u0026#34;, err) return } fmt.Printf(\u0026#34;Created task #%d: %s (priority: %s)\\n\u0026#34;, t.ID, t.Title, t.Priority) }, } func init() { // Add create command to root rootCmd.AddCommand(createCmd) // Local flag only for create command createCmd.Flags().StringVarP(\u0026amp;priority, \u0026#34;priority\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;medium\u0026#34;, \u0026#34;Task priority (low, medium, high)\u0026#34;) } Args: cobra.ExactArgs(1) validates that exactly one argument is provided. Cobra has built-in validators: ExactArgs(n), MinimumNArgs(n), MaximumNArgs(n), RangeArgs(min, max), NoArgs.\nargs[0] is the task title from the command line.\nLocal flags using createCmd.Flags() are only available for this command. Compare with rootCmd.PersistentFlags() which are available everywhere.\nTest it:\ngo run main.go create \u0026#34;Finish OAuth2 article\u0026#34; --priority=high # Created task #1: Finish OAuth2 article (priority: high) go run main.go create \u0026#34;Review pull requests\u0026#34; -p low # Created task #2: Review pull requests (priority: low) Building List Command Create cmd/list.go:\n// cmd/list.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;text/tabwriter\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/yourusername/tasker/internal/task\u0026#34; ) var ( showCompleted bool ) var listCmd = \u0026amp;cobra.Command{ Use: \u0026#34;list\u0026#34;, Short: \u0026#34;List all tasks\u0026#34;, Long: `Display all tasks in a formatted table.`, Run: func(cmd *cobra.Command, args []string) { storage, err := task.NewStorage() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } tasks := storage.List() if len(tasks) == 0 { fmt.Println(\u0026#34;No tasks found. Create one with: tasker create \u0026lt;title\u0026gt;\u0026#34;) return } // Use tabwriter for aligned output w := tabwriter.NewWriter(os.Stdout, 0, 0, 3, \u0026#39; \u0026#39;, 0) fmt.Fprintln(w, \u0026#34;ID\\tTITLE\\tPRIORITY\\tSTATUS\\tCREATED\u0026#34;) fmt.Fprintln(w, \u0026#34;──\\t─────\\t────────\\t──────\\t───────\u0026#34;) for _, t := range tasks { // Skip completed tasks unless flag is set if t.Completed \u0026amp;\u0026amp; !showCompleted { continue } status := \u0026#34;pending\u0026#34; if t.Completed { status = \u0026#34;done\u0026#34; } created := t.CreatedAt.Format(\u0026#34;2006-01-02\u0026#34;) fmt.Fprintf(w, \u0026#34;%d\\t%s\\t%s\\t%s\\t%s\\n\u0026#34;, t.ID, t.Title, t.Priority, status, created) } w.Flush() }, } func init() { rootCmd.AddCommand(listCmd) listCmd.Flags().BoolVarP(\u0026amp;showCompleted, \u0026#34;all\u0026#34;, \u0026#34;a\u0026#34;, false, \u0026#34;Show completed tasks\u0026#34;) } tabwriter is a standard library package that aligns columns nicely. Much better than manually spacing text.\nTest it:\ngo run main.go list # ID TITLE PRIORITY STATUS CREATED # ── ───── ──────── ────── ─────── # 1 Finish OAuth2 article high pending 2025-10-04 # 2 Review pull requests low pending 2025-10-04 Building Delete Command Create cmd/delete.go:\n// cmd/delete.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/yourusername/tasker/internal/task\u0026#34; ) var deleteCmd = \u0026amp;cobra.Command{ Use: \u0026#34;delete [id]\u0026#34;, Short: \u0026#34;Delete a task\u0026#34;, Long: `Delete a task by its ID.`, Args: cobra.ExactArgs(1), Run: func(cmd *cobra.Command, args []string) { id, err := strconv.Atoi(args[0]) if err != nil { fmt.Println(\u0026#34;Error: ID must be a number\u0026#34;) return } storage, err := task.NewStorage() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } // Get task first to show what we\u0026#39;re deleting t, err := storage.Get(id) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } if err := storage.Delete(id); err != nil { fmt.Println(\u0026#34;Error deleting task:\u0026#34;, err) return } fmt.Printf(\u0026#34;Deleted task #%d: %s\\n\u0026#34;, id, t.Title) }, } func init() { rootCmd.AddCommand(deleteCmd) } Test it:\ngo run main.go delete 2 # Deleted task #2: Review pull requests Building Update Command Create cmd/update.go:\n// cmd/update.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/yourusername/tasker/internal/task\u0026#34; ) var ( updateTitle string updatePriority string markComplete bool ) var updateCmd = \u0026amp;cobra.Command{ Use: \u0026#34;update [id]\u0026#34;, Short: \u0026#34;Update a task\u0026#34;, Long: `Update task properties like title, priority, or completion status.`, Args: cobra.ExactArgs(1), Run: func(cmd *cobra.Command, args []string) { id, err := strconv.Atoi(args[0]) if err != nil { fmt.Println(\u0026#34;Error: ID must be a number\u0026#34;) return } storage, err := task.NewStorage() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } // Check if task exists _, err = storage.Get(id) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } // Update only fields that were specified var completed *bool if cmd.Flags().Changed(\u0026#34;complete\u0026#34;) { completed = \u0026amp;markComplete } if err := storage.Update(id, updateTitle, updatePriority, completed); err != nil { fmt.Println(\u0026#34;Error updating task:\u0026#34;, err) return } fmt.Printf(\u0026#34;Updated task #%d\\n\u0026#34;, id) }, } func init() { rootCmd.AddCommand(updateCmd) updateCmd.Flags().StringVarP(\u0026amp;updateTitle, \u0026#34;title\u0026#34;, \u0026#34;t\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;New title\u0026#34;) updateCmd.Flags().StringVarP(\u0026amp;updatePriority, \u0026#34;priority\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;New priority\u0026#34;) updateCmd.Flags().BoolVarP(\u0026amp;markComplete, \u0026#34;complete\u0026#34;, \u0026#34;c\u0026#34;, false, \u0026#34;Mark as complete\u0026#34;) } cmd.Flags().Changed(\u0026ldquo;complete\u0026rdquo;) checks if the flag was explicitly set. This lets us distinguish between \u0026ldquo;flag not provided\u0026rdquo; and \u0026ldquo;flag set to false\u0026rdquo;. Important for boolean flags where false is a valid value.\nTest it:\ngo run main.go update 1 --complete # Updated task #1 go run main.go update 1 -t \u0026#34;Finish and publish OAuth2 article\u0026#34; -p high # Updated task #1 Configuration with Viper Now let\u0026rsquo;s add configuration support. Users can customize default priority, storage location, output format, etc. Create .tasker.yaml in your home directory:\n# ~/.tasker.yaml defaults: priority: medium show_completed: false storage: filepath: ~/.tasker-data.json output: format: table # table or json colors: true Update cmd/root.go to use these configs:\n// cmd/root.go func initConfig() { if cfgFile != \u0026#34;\u0026#34; { viper.SetConfigFile(cfgFile) } else { home, err := os.UserHomeDir() if err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) } viper.AddConfigPath(home) viper.SetConfigType(\u0026#34;yaml\u0026#34;) viper.SetConfigName(\u0026#34;.tasker\u0026#34;) } // Set defaults viper.SetDefault(\u0026#34;defaults.priority\u0026#34;, \u0026#34;medium\u0026#34;) viper.SetDefault(\u0026#34;defaults.show_completed\u0026#34;, false) viper.SetDefault(\u0026#34;output.format\u0026#34;, \u0026#34;table\u0026#34;) viper.SetDefault(\u0026#34;output.colors\u0026#34;, true) // Bind environment variables with prefix viper.SetEnvPrefix(\u0026#34;TASKER\u0026#34;) viper.AutomaticEnv() if err := viper.ReadInConfig(); err == nil { if viper.GetBool(\u0026#34;verbose\u0026#34;) { fmt.Println(\u0026#34;Using config file:\u0026#34;, viper.ConfigFileUsed()) } } } Now update create command to use config defaults:\n// cmd/create.go func init() { rootCmd.AddCommand(createCmd) // Use viper default instead of hardcoded value createCmd.Flags().StringVarP(\u0026amp;priority, \u0026#34;priority\u0026#34;, \u0026#34;p\u0026#34;, viper.GetString(\u0026#34;defaults.priority\u0026#34;), \u0026#34;Task priority\u0026#34;) } Users can now override defaults in three ways:\n# 1. Config file # Set in ~/.tasker.yaml # 2. Environment variable export TASKER_DEFAULTS_PRIORITY=high go run main.go create \u0026#34;Important task\u0026#34; # 3. Command-line flag (highest priority) go run main.go create \u0026#34;Urgent task\u0026#34; --priority=high Viper handles the priority automatically: flags \u0026gt; env vars \u0026gt; config file \u0026gt; defaults.\nAdding JSON Output Format Let\u0026rsquo;s support JSON output for scripting. Update cmd/list.go:\n// cmd/list.go import ( \u0026#34;encoding/json\u0026#34; // ... other imports ) var ( showCompleted bool outputFormat string ) var listCmd = \u0026amp;cobra.Command{ Use: \u0026#34;list\u0026#34;, Short: \u0026#34;List all tasks\u0026#34;, Run: func(cmd *cobra.Command, args []string) { storage, err := task.NewStorage() if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } tasks := storage.List() if len(tasks) == 0 { if outputFormat != \u0026#34;json\u0026#34; { fmt.Println(\u0026#34;No tasks found.\u0026#34;) } return } // Filter completed tasks var filtered []task.Task for _, t := range tasks { if !t.Completed || showCompleted { filtered = append(filtered, t) } } // Output based on format switch outputFormat { case \u0026#34;json\u0026#34;: data, _ := json.MarshalIndent(filtered, \u0026#34;\u0026#34;, \u0026#34; \u0026#34;) fmt.Println(string(data)) default: printTable(filtered) } }, } func printTable(tasks []task.Task) { w := tabwriter.NewWriter(os.Stdout, 0, 0, 3, \u0026#39; \u0026#39;, 0) fmt.Fprintln(w, \u0026#34;ID\\tTITLE\\tPRIORITY\\tSTATUS\\tCREATED\u0026#34;) fmt.Fprintln(w, \u0026#34;──\\t─────\\t────────\\t──────\\t───────\u0026#34;) for _, t := range tasks { status := \u0026#34;pending\u0026#34; if t.Completed { status = \u0026#34;done\u0026#34; } created := t.CreatedAt.Format(\u0026#34;2006-01-02\u0026#34;) fmt.Fprintf(w, \u0026#34;%d\\t%s\\t%s\\t%s\\t%s\\n\u0026#34;, t.ID, t.Title, t.Priority, status, created) } w.Flush() } func init() { rootCmd.AddCommand(listCmd) listCmd.Flags().BoolVarP(\u0026amp;showCompleted, \u0026#34;all\u0026#34;, \u0026#34;a\u0026#34;, false, \u0026#34;Show completed tasks\u0026#34;) listCmd.Flags().StringVarP(\u0026amp;outputFormat, \u0026#34;format\u0026#34;, \u0026#34;f\u0026#34;, viper.GetString(\u0026#34;output.format\u0026#34;), \u0026#34;Output format (table, json)\u0026#34;) } Now users can pipe output to other tools:\ngo run main.go list --format=json | jq \u0026#39;.[] | select(.priority == \u0026#34;high\u0026#34;)\u0026#39; Input Validation Add validation to prevent invalid data. Update cmd/create.go:\n// cmd/create.go var createCmd = \u0026amp;cobra.Command{ Use: \u0026#34;create [title]\u0026#34;, Short: \u0026#34;Create a new task\u0026#34;, Args: cobra.ExactArgs(1), PreRunE: func(cmd *cobra.Command, args []string) error { // Validate priority validPriorities := map[string]bool{ \u0026#34;low\u0026#34;: true, \u0026#34;medium\u0026#34;: true, \u0026#34;high\u0026#34;: true, } if !validPriorities[priority] { return fmt.Errorf(\u0026#34;invalid priority \u0026#39;%s\u0026#39;. Must be: low, medium, or high\u0026#34;, priority) } // Validate title length if len(args[0]) \u0026lt; 3 { return fmt.Errorf(\u0026#34;title must be at least 3 characters\u0026#34;) } return nil }, Run: func(cmd *cobra.Command, args []string) { // ... existing code }, } PreRunE runs before the main Run function. Return an error to abort execution. There\u0026rsquo;s also PostRunE that runs after.\nTest validation:\ngo run main.go create \u0026#34;ab\u0026#34; # Error: title must be at least 3 characters go run main.go create \u0026#34;Test task\u0026#34; --priority=urgent # Error: invalid priority \u0026#39;urgent\u0026#39;. Must be: low, medium, or high Shell Completion Cobra can generate completion scripts for bash, zsh, fish, and PowerShell. Add a completion command:\n// cmd/completion.go package cmd import ( \u0026#34;os\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; ) var completionCmd = \u0026amp;cobra.Command{ Use: \u0026#34;completion [bash|zsh|fish|powershell]\u0026#34;, Short: \u0026#34;Generate shell completion script\u0026#34;, Long: `Generate shell completion script for tasker. To load completions: Bash: $ source \u0026lt;(tasker completion bash) # To load permanently: $ tasker completion bash \u0026gt; /etc/bash_completion.d/tasker Zsh: $ source \u0026lt;(tasker completion zsh) # To load permanently: $ tasker completion zsh \u0026gt; \u0026#34;${fpath[1]}/_tasker\u0026#34; Fish: $ tasker completion fish | source # To load permanently: $ tasker completion fish \u0026gt; ~/.config/fish/completions/tasker.fish PowerShell: PS\u0026gt; tasker completion powershell | Out-String | Invoke-Expression `, Args: cobra.ExactArgs(1), Run: func(cmd *cobra.Command, args []string) { switch args[0] { case \u0026#34;bash\u0026#34;: cmd.Root().GenBashCompletion(os.Stdout) case \u0026#34;zsh\u0026#34;: cmd.Root().GenZshCompletion(os.Stdout) case \u0026#34;fish\u0026#34;: cmd.Root().GenFishCompletion(os.Stdout, true) case \u0026#34;powershell\u0026#34;: cmd.Root().GenPowerShellCompletion(os.Stdout) } }, } func init() { rootCmd.AddCommand(completionCmd) } Users can now install tab completion:\n# Bash tasker completion bash \u0026gt; /etc/bash_completion.d/tasker # Now they can tab-complete tasker cr\u0026lt;TAB\u0026gt; # completes to \u0026#34;create\u0026#34; tasker list --\u0026lt;TAB\u0026gt; # shows all available flags Building and Distribution Build your CLI for distribution:\n# Build for current platform go build -o tasker # Cross-compile for other platforms GOOS=linux GOARCH=amd64 go build -o tasker-linux-amd64 GOOS=darwin GOARCH=amd64 go build -o tasker-darwin-amd64 GOOS=darwin GOARCH=arm64 go build -o tasker-darwin-arm64 GOOS=windows GOARCH=amd64 go build -o tasker-windows-amd64.exe For production releases, use GoReleaser. Create .goreleaser.yaml:\nproject_name: tasker builds: - env: - CGO_ENABLED=0 goos: - linux - darwin - windows goarch: - amd64 - arm64 ignore: - goos: windows goarch: arm64 archives: - format: tar.gz name_template: \u0026gt;- {{ .ProjectName }}_ {{- .Version }}_ {{- .Os }}_ {{- .Arch }} format_overrides: - goos: windows format: zip checksum: name_template: \u0026#39;checksums.txt\u0026#39; changelog: sort: asc Install GoReleaser:\nbrew install goreleaser Create a release:\n# Tag your version git tag -a v1.0.0 -m \u0026#34;First release\u0026#34; git push origin v1.0.0 # Build and release goreleaser release GoReleaser builds binaries for all platforms, creates GitHub releases, generates checksums, and publishes everything automatically.\nTesting CLI Commands Test commands by calling them directly. Create cmd/create_test.go:\n// cmd/create_test.go package cmd import ( \u0026#34;bytes\u0026#34; \u0026#34;os\u0026#34; \u0026#34;testing\u0026#34; ) func TestCreateCommand(t *testing.T) { // Redirect stdout to capture output old := os.Stdout r, w, _ := os.Pipe() os.Stdout = w // Set test args rootCmd.SetArgs([]string{\u0026#34;create\u0026#34;, \u0026#34;Test task\u0026#34;, \u0026#34;--priority=high\u0026#34;}) // Execute command err := rootCmd.Execute() if err != nil { t.Fatalf(\u0026#34;Expected no error, got %v\u0026#34;, err) } // Read output w.Close() os.Stdout = old var buf bytes.Buffer buf.ReadFrom(r) output := buf.String() // Verify output if !bytes.Contains([]byte(output), []byte(\u0026#34;Created task\u0026#34;)) { t.Errorf(\u0026#34;Expected success message, got: %s\u0026#34;, output) } // Clean up test data os.Remove(os.ExpandEnv(\u0026#34;$HOME/.tasker-data.json\u0026#34;)) } Run tests:\ngo test ./cmd/... For integration with existing tools, you might want to build REST APIs alongside your CLI. Check out how to build REST APIs with Gin framework for creating web interfaces to your tools.\nAdvanced Features Custom Validators for complex validation:\nfunc validateTaskID(cmd *cobra.Command, args []string) error { id, err := strconv.Atoi(args[0]) if err != nil { return fmt.Errorf(\u0026#34;ID must be a number\u0026#34;) } if id \u0026lt; 1 { return fmt.Errorf(\u0026#34;ID must be positive\u0026#34;) } return nil } var deleteCmd = \u0026amp;cobra.Command{ Args: validateTaskID, // ... } Command Aliases for convenience:\nvar listCmd = \u0026amp;cobra.Command{ Use: \u0026#34;list\u0026#34;, Aliases: []string{\u0026#34;ls\u0026#34;, \u0026#34;l\u0026#34;}, // ... } Now tasker list, tasker ls, and tasker l all work.\nDynamic Completion for smarter autocomplete:\nfunc taskIDCompletion(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) { storage, _ := task.NewStorage() tasks := storage.List() var completions []string for _, t := range tasks { completions = append(completions, fmt.Sprintf(\u0026#34;%d\\t%s\u0026#34;, t.ID, t.Title)) } return completions, cobra.ShellCompDirectiveNoFileComp } var deleteCmd = \u0026amp;cobra.Command{ ValidArgsFunction: taskIDCompletion, // ... } Tab completion now shows actual task IDs and titles.\nReal-World Example: DevOps Tool Let\u0026rsquo;s build something more practical - a deployment tool. It reads service configs, connects to servers, and deploys applications:\n// cmd/deploy.go package cmd import ( \u0026#34;fmt\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;github.com/spf13/cobra\u0026#34; \u0026#34;github.com/spf13/viper\u0026#34; ) var ( environment string dryRun bool ) var deployCmd = \u0026amp;cobra.Command{ Use: \u0026#34;deploy [service]\u0026#34;, Short: \u0026#34;Deploy a service to specified environment\u0026#34;, Args: cobra.ExactArgs(1), PreRunE: func(cmd *cobra.Command, args []string) error { // Validate environment validEnvs := []string{\u0026#34;dev\u0026#34;, \u0026#34;staging\u0026#34;, \u0026#34;production\u0026#34;} for _, env := range validEnvs { if environment == env { return nil } } return fmt.Errorf(\u0026#34;invalid environment: %s\u0026#34;, environment) }, Run: func(cmd *cobra.Command, args []string) { service := args[0] // Load service config from viper host := viper.GetString(fmt.Sprintf(\u0026#34;services.%s.%s.host\u0026#34;, service, environment)) port := viper.GetInt(fmt.Sprintf(\u0026#34;services.%s.%s.port\u0026#34;, service, environment)) dockerImage := viper.GetString(fmt.Sprintf(\u0026#34;services.%s.image\u0026#34;, service)) if host == \u0026#34;\u0026#34; { fmt.Printf(\u0026#34;Service %s not configured for %s\\n\u0026#34;, service, environment) return } fmt.Printf(\u0026#34;Deploying %s to %s...\\n\u0026#34;, service, environment) fmt.Printf(\u0026#34;Host: %s:%d\\n\u0026#34;, host, port) fmt.Printf(\u0026#34;Image: %s\\n\u0026#34;, dockerImage) if dryRun { fmt.Println(\u0026#34;Dry run - no changes made\u0026#34;) return } // Execute deployment cmd := exec.Command(\u0026#34;ssh\u0026#34;, host, fmt.Sprintf(\u0026#34;docker pull %s \u0026amp;\u0026amp; docker-compose up -d\u0026#34;, dockerImage)) output, err := cmd.CombinedOutput() if err != nil { fmt.Printf(\u0026#34;Deployment failed: %v\\n%s\\n\u0026#34;, err, output) return } fmt.Println(\u0026#34;Deployment successful\u0026#34;) }, } func init() { rootCmd.AddCommand(deployCmd) deployCmd.Flags().StringVarP(\u0026amp;environment, \u0026#34;env\u0026#34;, \u0026#34;e\u0026#34;, \u0026#34;dev\u0026#34;, \u0026#34;Deployment environment\u0026#34;) deployCmd.Flags().BoolVar(\u0026amp;dryRun, \u0026#34;dry-run\u0026#34;, false, \u0026#34;Show what would be deployed without deploying\u0026#34;) } Config file .deployer.yaml:\nservices: api: image: mycompany/api:latest dev: host: dev.mycompany.com port: 8080 staging: host: staging.mycompany.com port: 8080 production: host: prod.mycompany.com port: 8080 frontend: image: mycompany/frontend:latest dev: host: dev.mycompany.com port: 3000 staging: host: staging.mycompany.com port: 3000 production: host: prod.mycompany.com port: 3000 Usage:\ndeployer deploy api --env=production # Deploying api to production... # Host: prod.mycompany.com:8080 # Image: mycompany/api:latest # Deployment successful deployer deploy frontend --env=staging --dry-run # Dry run - no changes made Integration with Other Systems CLI tools often need to interact with APIs. For authentication, implement JWT authentication to secure your API calls. For file operations, check out uploading files to AWS S3 for cloud storage integration.\nIf your CLI needs caching, use Redis for session management to speed up repeated operations.\nError Handling Best Practices Good error messages make CLIs usable. Bad ones frustrate users:\n// Bad - generic error if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } // Good - actionable error if err != nil { fmt.Printf(\u0026#34;Failed to connect to %s: %v\\n\u0026#34;, host, err) fmt.Println(\u0026#34;Check your network connection and try again\u0026#34;) return } Wrap errors with context:\nimport \u0026#34;fmt\u0026#34; func loadConfig() error { if err := viper.ReadInConfig(); err != nil { return fmt.Errorf(\u0026#34;failed to load config from %s: %w\u0026#34;, viper.ConfigFileUsed(), err) } return nil } Use exit codes for scripting:\nvar rootCmd = \u0026amp;cobra.Command{ Run: func(cmd *cobra.Command, args []string) { if err := doSomething(); err != nil { fmt.Fprintln(os.Stderr, err) os.Exit(1) // Non-zero exit code signals failure } os.Exit(0) // Zero means success }, } Scripts can check exit codes:\nif tasker create \u0026#34;Test\u0026#34;; then echo \u0026#34;Success\u0026#34; else echo \u0026#34;Failed\u0026#34; fi Color and Styling Add color to make output readable. Use the fatih/color package:\nimport \u0026#34;github.com/fatih/color\u0026#34; // Define colors var ( successColor = color.New(color.FgGreen, color.Bold) errorColor = color.New(color.FgRed, color.Bold) warnColor = color.New(color.FgYellow) ) // Use in commands successColor.Println(\u0026#34;Task created successfully\u0026#34;) errorColor.Printf(\u0026#34;Failed to connect to server: %v\\n\u0026#34;, err) warnColor.Println(\u0026#34;Warning: Config file not found, using defaults\u0026#34;) Respect the user\u0026rsquo;s environment:\n// Check if colors should be disabled if viper.GetBool(\u0026#34;output.colors\u0026#34;) \u0026amp;\u0026amp; color.NoColor { color.NoColor = false } else if !viper.GetBool(\u0026#34;output.colors\u0026#34;) { color.NoColor = true } Users can disable colors:\n# In config output: colors: false # Or environment variable NO_COLOR=1 tasker list Logging and Debugging Add verbose mode for debugging. Update cmd/root.go:\nimport \u0026#34;log\u0026#34; var rootCmd = \u0026amp;cobra.Command{ PersistentPreRun: func(cmd *cobra.Command, args []string) { if verbose { log.SetFlags(log.Ldate | log.Ltime | log.Lshortfile) log.Println(\u0026#34;Verbose mode enabled\u0026#34;) log.Printf(\u0026#34;Config file: %s\\n\u0026#34;, viper.ConfigFileUsed()) } else { log.SetFlags(0) log.SetOutput(io.Discard) } }, } Use throughout your code:\nfunc deploy(service string) error { log.Printf(\u0026#34;Starting deployment of %s\\n\u0026#34;, service) config := loadServiceConfig(service) log.Printf(\u0026#34;Loaded config: %+v\\n\u0026#34;, config) // ... deployment logic log.Println(\u0026#34;Deployment completed successfully\u0026#34;) return nil } Only shows when --verbose is set:\ntasker deploy api --verbose # 2025/10/04 14:30:00 deploy.go:15: Starting deployment of api # 2025/10/04 14:30:01 deploy.go:18: Loaded config: {Host:prod.mycompany.com Port:8080} # 2025/10/04 14:30:05 deploy.go:23: Deployment completed successfully Conclusion You\u0026rsquo;ve built a complete CLI tool with commands, subcommands, flags, configuration, validation, completion, and distribution. Cobra handles the command structure, Viper manages configuration, and you wrote the actual functionality.\nThe patterns here scale from simple scripts to complex tools. kubectl has hundreds of commands but follows the same structure. GitHub CLI has dozens of subcommands with their own flags. All built with Cobra.\nKey takeaways: structure your commands logically, validate inputs early, make errors actionable, support configuration files for complex tools, generate completion scripts for better UX, and cross-compile for easy distribution.\nCLI tools are powerful because they compose. Users pipe them together, script them, automate with them. Build tools that do one thing well, accept standard input/output, and follow conventions. Your tools will fit naturally into developers\u0026rsquo; workflows.\nThe code we built is production-ready. Add tests, CI/CD for releases with GoReleaser, and publish to package managers. Users will appreciate a well-built CLI that respects their time and follows best practices.\n","href":"/2025/10/how-to-build-a-cli-tool-in-go-with-cobra-and-viper.html","title":"How to Build a CLI Tool in Go with Cobra and Viper"},{"content":"Nobody wants to create yet another account with yet another password. I\u0026rsquo;ve built authentication systems that required users to sign up with email and password, and the drop-off rate was painful. Then I added \u0026ldquo;Login with Google\u0026rdquo; and conversions jumped 40%. Users already have accounts they trust - why make them create new ones?\nOAuth2 lets users authenticate with providers they already use - Google, GitHub, Facebook, whatever. You get verified emails, users don\u0026rsquo;t manage more passwords, and everyone\u0026rsquo;s happy. The best part? It\u0026rsquo;s not as complicated as it looks once you understand the flow.\nI\u0026rsquo;ll show you how to implement OAuth2 in Go with real examples for Google, GitHub, and Facebook. We\u0026rsquo;ll build a complete authentication system that actually works in production, handles errors properly, and follows security best practices. No theoretical BS - just code that runs.\nUnderstanding OAuth2 Flow OAuth2 sounds intimidating but the flow is straightforward. Think of it like a club bouncer checking your ID, but instead of showing your ID directly, you show a temporary pass from someone the bouncer trusts.\nHere\u0026rsquo;s what actually happens when a user clicks \u0026ldquo;Login with Google\u0026rdquo;:\nYour app redirects the user to Google\u0026rsquo;s login page with some parameters (client ID, requested permissions, callback URL). The user logs into Google and approves your app\u0026rsquo;s permission request. Google redirects back to your callback URL with an authorization code. Your app exchanges that code for an access token (server-to-server, not visible to users). Your app uses the access token to fetch user info from Google\u0026rsquo;s API. You create a session and log the user in.\nThe key security feature: your app never sees the user\u0026rsquo;s Google password. Google handles authentication, you just trust Google\u0026rsquo;s word that the user is who they claim to be.\nAuthorization Code is a temporary code that\u0026rsquo;s useless by itself. You exchange it for tokens on the backend where your client secret lives. This prevents attackers from stealing tokens even if they intercept the redirect.\nAccess Token is what you use to make API calls on behalf of the user. It expires quickly (usually 1 hour) for security. Think of it as a temporary key card that stops working after a while.\nRefresh Token lets you get new access tokens without asking the user to log in again. Not all providers give you refresh tokens - Google does, GitHub doesn\u0026rsquo;t (their tokens don\u0026rsquo;t expire).\nScopes define what your app can access. Request only what you need. Asking for too many permissions scares users away. For basic login, you just need email and profile info.\nPrerequisites and Setup Before writing code, you need to register your application with each OAuth2 provider. This gives you credentials (client ID and secret) that identify your app.\nSetting Up Google OAuth2 Navigate to Google Cloud Console and create a new project (or use an existing one). Go to \u0026ldquo;APIs \u0026amp; Services\u0026rdquo; \u0026gt; \u0026ldquo;Credentials\u0026rdquo; and click \u0026ldquo;Create Credentials\u0026rdquo; \u0026gt; \u0026ldquo;OAuth client ID\u0026rdquo;.\nChoose \u0026ldquo;Web application\u0026rdquo; as the application type. Add authorized redirect URIs - for development use http://localhost:8080/auth/google/callback. For production, use your actual domain with HTTPS.\nSave your Client ID and Client Secret - you\u0026rsquo;ll need these in your Go code. Never commit these to git or expose them in frontend code.\nSetting Up GitHub OAuth2 Go to GitHub Developer Settings and click \u0026ldquo;New OAuth App\u0026rdquo;. Fill in the application name and homepage URL (can be your GitHub repo for development).\nSet the authorization callback URL to http://localhost:8080/auth/github/callback for local development. GitHub will give you a Client ID and Client Secret - save these securely.\nSetting Up Facebook OAuth2 Visit Facebook Developers and create a new app. Choose \u0026ldquo;Consumer\u0026rdquo; as the app type if you\u0026rsquo;re building a regular web app.\nIn the app dashboard, add the \u0026ldquo;Facebook Login\u0026rdquo; product. Configure OAuth redirect URIs in Settings \u0026gt; Basic. Add http://localhost:8080/auth/facebook/callback for development.\nGet your App ID and App Secret from the dashboard. Facebook also requires your app to be reviewed before general users can use it in production, but you can test with your own account and added testers during development.\nInstalling Dependencies Create your Go project and install the OAuth2 library:\nmkdir oauth2-tutorial cd oauth2-tutorial go mod init github.com/yourusername/oauth2-tutorial Install the required packages:\ngo get golang.org/x/oauth2 go get golang.org/x/oauth2/google go get golang.org/x/oauth2/github go get golang.org/x/oauth2/facebook The golang.org/x/oauth2 package handles the OAuth2 flow for you - token exchange, refresh logic, all the boring stuff that\u0026rsquo;s easy to mess up.\nProject Structure Organize your code to separate concerns and make it maintainable:\noauth2-tutorial/ ├── main.go ├── config/ │ └── oauth.go # OAuth2 configurations ├── handlers/ │ ├── auth.go # Authentication handlers │ └── user.go # User-related handlers ├── models/ │ └── user.go # User model ├── middleware/ │ └── auth.go # Authentication middleware ├── .env # Environment variables (gitignored) └── templates/ ├── login.html # Login page └── profile.html # User profile page This structure keeps OAuth2 config separate from handlers, makes testing easier, and scales well as your app grows.\nImplementing Google OAuth2 Google OAuth2 is probably the most common - almost everyone has a Google account. Let\u0026rsquo;s implement it first, then GitHub and Facebook will be nearly identical.\nConfiguration Setup Create a configuration file for OAuth2 settings:\n// config/oauth.go package config import ( \u0026#34;os\u0026#34; \u0026#34;golang.org/x/oauth2\u0026#34; \u0026#34;golang.org/x/oauth2/google\u0026#34; ) var GoogleOAuthConfig = \u0026amp;oauth2.Config{ ClientID: os.Getenv(\u0026#34;GOOGLE_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;GOOGLE_CLIENT_SECRET\u0026#34;), RedirectURL: \u0026#34;http://localhost:8080/auth/google/callback\u0026#34;, Scopes: []string{ \u0026#34;https://www.googleapis.com/auth/userinfo.email\u0026#34;, \u0026#34;https://www.googleapis.com/auth/userinfo.profile\u0026#34;, }, Endpoint: google.Endpoint, } Store credentials in environment variables, never hardcode them. Create a .env file:\nGOOGLE_CLIENT_ID=your-google-client-id.apps.googleusercontent.com GOOGLE_CLIENT_SECRET=your-google-client-secret GITHUB_CLIENT_ID=your-github-client-id GITHUB_CLIENT_SECRET=your-github-client-secret FACEBOOK_APP_ID=your-facebook-app-id FACEBOOK_APP_SECRET=your-facebook-app-secret SESSION_SECRET=random-secret-key-change-in-production Load environment variables on startup:\n// main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/handlers\u0026#34; ) func main() { // Load environment variables err := godotenv.Load() if err != nil { log.Fatal(\u0026#34;Error loading .env file\u0026#34;) } // Setup routes http.HandleFunc(\u0026#34;/\u0026#34;, handlers.HandleHome) http.HandleFunc(\u0026#34;/login\u0026#34;, handlers.HandleLogin) http.HandleFunc(\u0026#34;/auth/google/login\u0026#34;, handlers.HandleGoogleLogin) http.HandleFunc(\u0026#34;/auth/google/callback\u0026#34;, handlers.HandleGoogleCallback) http.HandleFunc(\u0026#34;/profile\u0026#34;, handlers.HandleProfile) http.HandleFunc(\u0026#34;/logout\u0026#34;, handlers.HandleLogout) log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Install godotenv for loading environment variables:\ngo get github.com/joho/godotenv User Model Create a simple user model to store authenticated user data:\n// models/user.go package models type User struct { ID string `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Picture string `json:\u0026#34;picture\u0026#34;` Provider string `json:\u0026#34;provider\u0026#34;` // \u0026#34;google\u0026#34;, \u0026#34;github\u0026#34;, \u0026#34;facebook\u0026#34; } In production, you\u0026rsquo;d store this in a database with additional fields (created_at, last_login, etc.). For this tutorial, we\u0026rsquo;ll keep it simple with session storage.\nLogin Handler Create the handler that redirects users to Google\u0026rsquo;s login page:\n// handlers/auth.go package handlers import ( \u0026#34;crypto/rand\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/config\u0026#34; ) // HandleGoogleLogin initiates the OAuth2 flow func HandleGoogleLogin(w http.ResponseWriter, r *http.Request) { // Generate random state for CSRF protection state := generateStateToken() // Store state in session for validation later setSession(w, \u0026#34;oauth_state\u0026#34;, state) // Redirect to Google\u0026#39;s OAuth2 consent page url := config.GoogleOAuthConfig.AuthCodeURL(state, oauth2.AccessTypeOffline) http.Redirect(w, r, url, http.StatusTemporaryRedirect) } // generateStateToken creates a random token for CSRF protection func generateStateToken() string { b := make([]byte, 32) rand.Read(b) return base64.URLEncoding.EncodeToString(b) } The state parameter is crucial for security - it prevents CSRF attacks where an attacker tricks you into logging in with their account. We generate a random token, save it in the session, and verify it matches when Google redirects back.\nCallback Handler This is where Google redirects users after they authenticate:\n// handlers/auth.go (continued) import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;golang.org/x/oauth2\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/config\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/models\u0026#34; ) func HandleGoogleCallback(w http.ResponseWriter, r *http.Request) { // Verify state token to prevent CSRF sessionState := getSession(r, \u0026#34;oauth_state\u0026#34;) queryState := r.URL.Query().Get(\u0026#34;state\u0026#34;) if sessionState != queryState { http.Error(w, \u0026#34;Invalid state parameter\u0026#34;, http.StatusBadRequest) return } // Exchange authorization code for token code := r.URL.Query().Get(\u0026#34;code\u0026#34;) token, err := config.GoogleOAuthConfig.Exchange(context.Background(), code) if err != nil { http.Error(w, \u0026#34;Failed to exchange token: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Fetch user info from Google user, err := getGoogleUserInfo(token.AccessToken) if err != nil { http.Error(w, \u0026#34;Failed to get user info: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Create session for the user setSession(w, \u0026#34;user_id\u0026#34;, user.ID) setSession(w, \u0026#34;user_email\u0026#34;, user.Email) setSession(w, \u0026#34;user_name\u0026#34;, user.Name) // Redirect to profile page http.Redirect(w, r, \u0026#34;/profile\u0026#34;, http.StatusSeeOther) } // getGoogleUserInfo fetches user information from Google\u0026#39;s API func getGoogleUserInfo(accessToken string) (*models.User, error) { resp, err := http.Get(\u0026#34;https://www.googleapis.com/oauth2/v2/userinfo?access_token=\u0026#34; + accessToken) if err != nil { return nil, err } defer resp.Body.Close() body, err := io.ReadAll(resp.Body) if err != nil { return nil, err } var googleUser struct { ID string `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Picture string `json:\u0026#34;picture\u0026#34;` } err = json.Unmarshal(body, \u0026amp;googleUser) if err != nil { return nil, err } return \u0026amp;models.User{ ID: googleUser.ID, Email: googleUser.Email, Name: googleUser.Name, Picture: googleUser.Picture, Provider: \u0026#34;google\u0026#34;, }, nil } This handler exchanges the authorization code for an access token, then uses that token to fetch user information from Google\u0026rsquo;s API. Finally, it creates a session to keep the user logged in.\nSession Management Implement simple session management using cookies:\n// handlers/session.go package handlers import ( \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; ) // Simple in-memory session storage (use Redis/database in production) var sessions = make(map[string]map[string]string) var sessionsMutex sync.RWMutex func setSession(w http.ResponseWriter, key, value string) { sessionID := getOrCreateSessionID(w) sessionsMutex.Lock() defer sessionsMutex.Unlock() if sessions[sessionID] == nil { sessions[sessionID] = make(map[string]string) } sessions[sessionID][key] = value } func getSession(r *http.Request, key string) string { cookie, err := r.Cookie(\u0026#34;session_id\u0026#34;) if err != nil { return \u0026#34;\u0026#34; } sessionsMutex.RLock() defer sessionsMutex.RUnlock() session := sessions[cookie.Value] if session == nil { return \u0026#34;\u0026#34; } return session[key] } func getOrCreateSessionID(w http.ResponseWriter) string { // In production, use a proper session library like gorilla/sessions sessionID := generateStateToken() http.SetCookie(w, \u0026amp;http.Cookie{ Name: \u0026#34;session_id\u0026#34;, Value: sessionID, Path: \u0026#34;/\u0026#34;, HttpOnly: true, Secure: false, // Set to true in production with HTTPS MaxAge: 86400 * 7, // 7 days }) return sessionID } func clearSession(w http.ResponseWriter, r *http.Request) { cookie, err := r.Cookie(\u0026#34;session_id\u0026#34;) if err != nil { return } sessionsMutex.Lock() delete(sessions, cookie.Value) sessionsMutex.Unlock() http.SetCookie(w, \u0026amp;http.Cookie{ Name: \u0026#34;session_id\u0026#34;, Value: \u0026#34;\u0026#34;, Path: \u0026#34;/\u0026#34;, MaxAge: -1, }) } This is a basic in-memory session implementation for demonstration. In production, use gorilla/sessions with a Redis or database backend so sessions persist across server restarts and work with multiple server instances.\nImplementing GitHub OAuth2 GitHub OAuth2 follows the same pattern as Google. The main differences are the configuration endpoints and user info API.\nGitHub Configuration // config/oauth.go (add to existing file) import ( \u0026#34;golang.org/x/oauth2/github\u0026#34; ) var GitHubOAuthConfig = \u0026amp;oauth2.Config{ ClientID: os.Getenv(\u0026#34;GITHUB_CLIENT_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;GITHUB_CLIENT_SECRET\u0026#34;), RedirectURL: \u0026#34;http://localhost:8080/auth/github/callback\u0026#34;, Scopes: []string{\u0026#34;user:email\u0026#34;}, Endpoint: github.Endpoint, } GitHub\u0026rsquo;s scopes are different from Google. user:email gives you access to the user\u0026rsquo;s email address (which might be private on GitHub).\nGitHub Handlers // handlers/auth.go (add to existing file) func HandleGitHubLogin(w http.ResponseWriter, r *http.Request) { state := generateStateToken() setSession(w, \u0026#34;oauth_state\u0026#34;, state) url := config.GitHubOAuthConfig.AuthCodeURL(state) http.Redirect(w, r, url, http.StatusTemporaryRedirect) } func HandleGitHubCallback(w http.ResponseWriter, r *http.Request) { // Verify state sessionState := getSession(r, \u0026#34;oauth_state\u0026#34;) queryState := r.URL.Query().Get(\u0026#34;state\u0026#34;) if sessionState != queryState { http.Error(w, \u0026#34;Invalid state parameter\u0026#34;, http.StatusBadRequest) return } // Exchange code for token code := r.URL.Query().Get(\u0026#34;code\u0026#34;) token, err := config.GitHubOAuthConfig.Exchange(context.Background(), code) if err != nil { http.Error(w, \u0026#34;Failed to exchange token: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Get user info user, err := getGitHubUserInfo(token.AccessToken) if err != nil { http.Error(w, \u0026#34;Failed to get user info: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Create session setSession(w, \u0026#34;user_id\u0026#34;, user.ID) setSession(w, \u0026#34;user_email\u0026#34;, user.Email) setSession(w, \u0026#34;user_name\u0026#34;, user.Name) http.Redirect(w, r, \u0026#34;/profile\u0026#34;, http.StatusSeeOther) } func getGitHubUserInfo(accessToken string) (*models.User, error) { // Create request with authorization header req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;https://api.github.com/user\u0026#34;, nil) if err != nil { return nil, err } req.Header.Set(\u0026#34;Authorization\u0026#34;, \u0026#34;token \u0026#34;+accessToken) client := \u0026amp;http.Client{} resp, err := client.Do(req) if err != nil { return nil, err } defer resp.Body.Close() body, err := io.ReadAll(resp.Body) if err != nil { return nil, err } var githubUser struct { ID int `json:\u0026#34;id\u0026#34;` Login string `json:\u0026#34;login\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Avatar string `json:\u0026#34;avatar_url\u0026#34;` } err = json.Unmarshal(body, \u0026amp;githubUser) if err != nil { return nil, err } // GitHub might not return email in the main response // Fetch emails separately if needed if githubUser.Email == \u0026#34;\u0026#34; { email, _ := getGitHubPrimaryEmail(accessToken) githubUser.Email = email } return \u0026amp;models.User{ ID: fmt.Sprintf(\u0026#34;%d\u0026#34;, githubUser.ID), Email: githubUser.Email, Name: githubUser.Name, Picture: githubUser.Avatar, Provider: \u0026#34;github\u0026#34;, }, nil } func getGitHubPrimaryEmail(accessToken string) (string, error) { req, err := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;https://api.github.com/user/emails\u0026#34;, nil) if err != nil { return \u0026#34;\u0026#34;, err } req.Header.Set(\u0026#34;Authorization\u0026#34;, \u0026#34;token \u0026#34;+accessToken) client := \u0026amp;http.Client{} resp, err := client.Do(req) if err != nil { return \u0026#34;\u0026#34;, err } defer resp.Body.Close() var emails []struct { Email string `json:\u0026#34;email\u0026#34;` Primary bool `json:\u0026#34;primary\u0026#34;` Verified bool `json:\u0026#34;verified\u0026#34;` } err = json.DecodeReader(resp.Body, \u0026amp;emails) if err != nil { return \u0026#34;\u0026#34;, err } for _, email := range emails { if email.Primary \u0026amp;\u0026amp; email.Verified { return email.Email, nil } } return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;no verified primary email found\u0026#34;) } GitHub\u0026rsquo;s API requires the access token in the Authorization header, unlike Google which accepts it as a query parameter. Also, GitHub users can hide their email address, so you might need to fetch it separately from the /user/emails endpoint.\nImplementing Facebook OAuth2 Facebook OAuth2 is similar but has some quirks around API versions and permissions.\nFacebook Configuration // config/oauth.go (add to existing file) import ( \u0026#34;golang.org/x/oauth2/facebook\u0026#34; ) var FacebookOAuthConfig = \u0026amp;oauth2.Config{ ClientID: os.Getenv(\u0026#34;FACEBOOK_APP_ID\u0026#34;), ClientSecret: os.Getenv(\u0026#34;FACEBOOK_APP_SECRET\u0026#34;), RedirectURL: \u0026#34;http://localhost:8080/auth/facebook/callback\u0026#34;, Scopes: []string{\u0026#34;email\u0026#34;, \u0026#34;public_profile\u0026#34;}, Endpoint: facebook.Endpoint, } Facebook requires explicit permission for email access. Users can deny email permission even if they approve the login.\nFacebook Handlers // handlers/auth.go (add to existing file) func HandleFacebookLogin(w http.ResponseWriter, r *http.Request) { state := generateStateToken() setSession(w, \u0026#34;oauth_state\u0026#34;, state) url := config.FacebookOAuthConfig.AuthCodeURL(state) http.Redirect(w, r, url, http.StatusTemporaryRedirect) } func HandleFacebookCallback(w http.ResponseWriter, r *http.Request) { // Verify state sessionState := getSession(r, \u0026#34;oauth_state\u0026#34;) queryState := r.URL.Query().Get(\u0026#34;state\u0026#34;) if sessionState != queryState { http.Error(w, \u0026#34;Invalid state parameter\u0026#34;, http.StatusBadRequest) return } // Exchange code for token code := r.URL.Query().Get(\u0026#34;code\u0026#34;) token, err := config.FacebookOAuthConfig.Exchange(context.Background(), code) if err != nil { http.Error(w, \u0026#34;Failed to exchange token: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Get user info user, err := getFacebookUserInfo(token.AccessToken) if err != nil { http.Error(w, \u0026#34;Failed to get user info: \u0026#34;+err.Error(), http.StatusInternalServerError) return } // Create session setSession(w, \u0026#34;user_id\u0026#34;, user.ID) setSession(w, \u0026#34;user_email\u0026#34;, user.Email) setSession(w, \u0026#34;user_name\u0026#34;, user.Name) http.Redirect(w, r, \u0026#34;/profile\u0026#34;, http.StatusSeeOther) } func getFacebookUserInfo(accessToken string) (*models.User, error) { // Facebook Graph API - specify fields you want url := fmt.Sprintf(\u0026#34;https://graph.facebook.com/v18.0/me?fields=id,name,email,picture\u0026amp;access_token=%s\u0026#34;, accessToken) resp, err := http.Get(url) if err != nil { return nil, err } defer resp.Body.Close() var fbUser struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Picture struct { Data struct { URL string `json:\u0026#34;url\u0026#34;` } `json:\u0026#34;data\u0026#34;` } `json:\u0026#34;picture\u0026#34;` } err = json.NewDecoder(resp.Body).Decode(\u0026amp;fbUser) if err != nil { return nil, err } return \u0026amp;models.User{ ID: fbUser.ID, Email: fbUser.Email, Name: fbUser.Name, Picture: fbUser.Picture.Data.URL, Provider: \u0026#34;facebook\u0026#34;, }, nil } Facebook\u0026rsquo;s Graph API requires you to specify which fields you want in the query. The picture field has a nested structure, so you need to handle that carefully.\nCreating the Frontend Users need a way to initiate login. Create simple HTML templates:\n\u0026lt;!-- templates/login.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Login\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; max-width: 400px; margin: 100px auto; padding: 20px; } .login-button { display: block; width: 100%; padding: 12px; margin: 10px 0; border: none; border-radius: 4px; font-size: 16px; cursor: pointer; text-decoration: none; text-align: center; color: white; } .google { background-color: #4285f4; } .github { background-color: #333; } .facebook { background-color: #1877f2; } .login-button:hover { opacity: 0.9; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Login to Your Account\u0026lt;/h1\u0026gt; \u0026lt;a href=\u0026#34;/auth/google/login\u0026#34; class=\u0026#34;login-button google\u0026#34;\u0026gt; Login with Google \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/auth/github/login\u0026#34; class=\u0026#34;login-button github\u0026#34;\u0026gt; Login with GitHub \u0026lt;/a\u0026gt; \u0026lt;a href=\u0026#34;/auth/facebook/login\u0026#34; class=\u0026#34;login-button facebook\u0026#34;\u0026gt; Login with Facebook \u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Profile page to show logged-in user info:\n\u0026lt;!-- templates/profile.html --\u0026gt; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Profile\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; max-width: 600px; margin: 50px auto; padding: 20px; } .profile-card { border: 1px solid #ddd; border-radius: 8px; padding: 20px; text-align: center; } .profile-picture { width: 100px; height: 100px; border-radius: 50%; margin: 10px auto; } .logout-button { background-color: #dc3545; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; text-decoration: none; display: inline-block; margin-top: 20px; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;profile-card\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{.Picture}}\u0026#34; alt=\u0026#34;Profile Picture\u0026#34; class=\u0026#34;profile-picture\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;{{.Name}}\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Email: {{.Email}}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Provider: {{.Provider}}\u0026lt;/p\u0026gt; \u0026lt;a href=\u0026#34;/logout\u0026#34; class=\u0026#34;logout-button\u0026#34;\u0026gt;Logout\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Render templates in your handlers:\n// handlers/user.go package handlers import ( \u0026#34;html/template\u0026#34; \u0026#34;net/http\u0026#34; ) func HandleLogin(w http.ResponseWriter, r *http.Request) { tmpl := template.Must(template.ParseFiles(\u0026#34;templates/login.html\u0026#34;)) tmpl.Execute(w, nil) } func HandleProfile(w http.ResponseWriter, r *http.Request) { // Check if user is logged in userEmail := getSession(r, \u0026#34;user_email\u0026#34;) if userEmail == \u0026#34;\u0026#34; { http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusSeeOther) return } // Get user info from session userData := map[string]string{ \u0026#34;Email\u0026#34;: getSession(r, \u0026#34;user_email\u0026#34;), \u0026#34;Name\u0026#34;: getSession(r, \u0026#34;user_name\u0026#34;), \u0026#34;Picture\u0026#34;: getSession(r, \u0026#34;user_picture\u0026#34;), \u0026#34;Provider\u0026#34;: getSession(r, \u0026#34;user_provider\u0026#34;), } tmpl := template.Must(template.ParseFiles(\u0026#34;templates/profile.html\u0026#34;)) tmpl.Execute(w, userData) } func HandleLogout(w http.ResponseWriter, r *http.Request) { clearSession(w, r) http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusSeeOther) } func HandleHome(w http.ResponseWriter, r *http.Request) { userEmail := getSession(r, \u0026#34;user_email\u0026#34;) if userEmail != \u0026#34;\u0026#34; { http.Redirect(w, r, \u0026#34;/profile\u0026#34;, http.StatusSeeOther) return } http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusSeeOther) } Authentication Middleware Protect routes that require authentication:\n// middleware/auth.go package middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/handlers\u0026#34; ) func RequireAuth(next http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { userEmail := handlers.GetSession(r, \u0026#34;user_email\u0026#34;) if userEmail == \u0026#34;\u0026#34; { http.Redirect(w, r, \u0026#34;/login\u0026#34;, http.StatusSeeOther) return } next(w, r) } } Use it to protect routes:\n// main.go (update) http.HandleFunc(\u0026#34;/profile\u0026#34;, middleware.RequireAuth(handlers.HandleProfile)) Security Best Practices OAuth2 is secure by design, but you can still mess it up. Here\u0026rsquo;s how to do it right:\nAlways Use HTTPS in Production. OAuth2 tokens are bearer tokens - anyone with the token can impersonate the user. HTTPS encrypts tokens in transit. Never use OAuth2 over plain HTTP in production.\nValidate the State Parameter. The state parameter prevents CSRF attacks. Generate a random token, store it in the session, and verify it matches when the provider redirects back. Every OAuth2 library supports this.\nStore Tokens Securely. Never store access tokens in cookies or localStorage where JavaScript can access them. Store them encrypted in your database on the server side. Use httpOnly, secure cookies for session IDs.\nRequest Minimal Scopes. Only request permissions you actually need. Users are more likely to approve \u0026ldquo;access to your email\u0026rdquo; than \u0026ldquo;access to your entire Google Drive.\u0026rdquo; You can always request additional scopes later if needed.\nHandle Token Expiration. Access tokens expire. Use refresh tokens to get new access tokens automatically. Implement graceful degradation if refresh fails - don\u0026rsquo;t just crash, redirect to login.\nValidate Redirect URIs. Register exact redirect URIs with providers. Don\u0026rsquo;t use wildcards. This prevents attackers from stealing authorization codes by tricking users into authorizing malicious apps.\n// Example: Strict redirect URL validation func validateRedirectURL(requestURL string) bool { allowedURLs := []string{ \u0026#34;http://localhost:8080/auth/google/callback\u0026#34;, \u0026#34;https://yourdomain.com/auth/google/callback\u0026#34;, } for _, allowed := range allowedURLs { if requestURL == allowed { return true } } return false } Implement Rate Limiting. Prevent brute force attacks on your OAuth2 endpoints. Limit failed login attempts per IP address. Use packages like golang.org/x/time/rate or implement custom rate limiting.\nLog Security Events. Log all authentication attempts, failures, token refreshes. This helps debug issues and detect suspicious activity. Don\u0026rsquo;t log tokens themselves - log events.\nHandling Errors and Edge Cases Real users do weird things. Handle edge cases gracefully:\nUser Denies Permissions. When users click \u0026ldquo;Cancel\u0026rdquo; on the OAuth2 consent screen, providers redirect back with an error parameter. Handle this:\nfunc HandleGoogleCallback(w http.ResponseWriter, r *http.Request) { // Check for errors first if errMsg := r.URL.Query().Get(\u0026#34;error\u0026#34;); errMsg != \u0026#34;\u0026#34; { if errMsg == \u0026#34;access_denied\u0026#34; { http.Redirect(w, r, \u0026#34;/login?error=user_cancelled\u0026#34;, http.StatusSeeOther) return } http.Error(w, \u0026#34;OAuth2 error: \u0026#34;+errMsg, http.StatusBadRequest) return } // Continue with normal flow... } No Email Returned. Some providers (especially GitHub and Facebook) might not return an email if the user hasn\u0026rsquo;t verified it or has privacy settings enabled:\nif user.Email == \u0026#34;\u0026#34; { http.Error(w, \u0026#34;Email is required for registration. Please make your email public in your profile.\u0026#34;, http.StatusBadRequest) return } Token Exchange Failures. Network issues, invalid codes, expired codes - lots can go wrong:\ntoken, err := config.GoogleOAuthConfig.Exchange(context.Background(), code) if err != nil { // Log the error for debugging log.Printf(\u0026#34;Token exchange failed: %v\u0026#34;, err) // Show user-friendly error http.Redirect(w, r, \u0026#34;/login?error=auth_failed\u0026#34;, http.StatusSeeOther) return } Duplicate Account Detection. What if a user signs in with Google, then tries GitHub with the same email? Decide your strategy:\n// Check if user with this email already exists existingUser := getUserByEmail(user.Email) if existingUser != nil { if existingUser.Provider != user.Provider { // Email exists with different provider - link accounts or show error http.Error(w, \u0026#34;Account with this email already exists via \u0026#34;+existingUser.Provider, http.StatusConflict) return } } Database Integration In production, store user data in a database. Here\u0026rsquo;s a simple example with PostgreSQL:\n// models/user.go (updated) package models import ( \u0026#34;database/sql\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID int64 `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Picture string `json:\u0026#34;picture\u0026#34;` Provider string `json:\u0026#34;provider\u0026#34;` ProviderID string `json:\u0026#34;provider_id\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34;` } func CreateOrUpdateUser(db *sql.DB, user *User) error { query := ` INSERT INTO users (email, name, picture, provider, provider_id, created_at, updated_at) VALUES ($1, $2, $3, $4, $5, NOW(), NOW()) ON CONFLICT (provider, provider_id) DO UPDATE SET email = EXCLUDED.email, name = EXCLUDED.name, picture = EXCLUDED.picture, updated_at = NOW() RETURNING id ` err := db.QueryRow(query, user.Email, user.Name, user.Picture, user.Provider, user.ProviderID).Scan(\u0026amp;user.ID) return err } func GetUserByProviderID(db *sql.DB, provider, providerID string) (*User, error) { user := \u0026amp;User{} query := ` SELECT id, email, name, picture, provider, provider_id, created_at, updated_at FROM users WHERE provider = $1 AND provider_id = $2 ` err := db.QueryRow(query, provider, providerID).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.Picture, \u0026amp;user.Provider, \u0026amp;user.ProviderID, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err == sql.ErrNoRows { return nil, nil } return user, err } Database schema:\nCREATE TABLE users ( id BIGSERIAL PRIMARY KEY, email VARCHAR(255) NOT NULL, name VARCHAR(255), picture TEXT, provider VARCHAR(50) NOT NULL, provider_id VARCHAR(255) NOT NULL, created_at TIMESTAMP NOT NULL DEFAULT NOW(), updated_at TIMESTAMP NOT NULL DEFAULT NOW(), UNIQUE(provider, provider_id) ); CREATE INDEX idx_users_email ON users(email); CREATE INDEX idx_users_provider ON users(provider, provider_id); Testing OAuth2 Locally Testing OAuth2 during development has some gotchas:\nUse localhost in Redirect URLs. All major providers allow http://localhost for development. Register http://localhost:8080/auth/google/callback in your OAuth2 app settings.\nTest with Real Accounts. You can\u0026rsquo;t mock OAuth2 providers easily. Use real test accounts or your personal accounts during development. Create separate OAuth2 apps for development and production.\nHandle Port Conflicts. If port 8080 is busy, change it everywhere - your code, OAuth2 app settings, and environment variables. Keep them in sync.\nUse ngrok for Mobile Testing. Mobile apps can\u0026rsquo;t access localhost. Use ngrok to create a public HTTPS tunnel to your local server:\nngrok http 8080 Update your OAuth2 app redirect URLs to use the ngrok URL temporarily.\nReady for Production? Check These First Before you push to production and let real users hit your OAuth2 endpoints, make sure you\u0026rsquo;ve covered these security and deployment essentials:\nHTTPS everywhere - no HTTP in production. Period. OAuth2 sends tokens in URLs and headers that attackers can intercept over plain HTTP.\nStore secrets properly - use environment variables or a secret management service like AWS Secrets Manager. Never commit credentials to git.\nUpdate redirect URLs - change from localhost to your production domain in both your code and OAuth2 app settings. Mismatched URLs break authentication.\nEnable proper session storage - switch from in-memory sessions to Redis or PostgreSQL. In-memory sessions don\u0026rsquo;t work with multiple servers or server restarts.\nImplement rate limiting - prevent brute force attacks on authentication endpoints. Limit login attempts per IP address to 5-10 per minute.\nSet secure cookie flags - Secure: true (HTTPS only), HttpOnly: true (no JavaScript access), SameSite: Strict (CSRF protection).\nAdd logging - log all authentication events (login attempts, failures, token refreshes) for debugging and security monitoring. Don\u0026rsquo;t log actual tokens.\nMonitor failed logins - set up alerts for unusual patterns like many failed attempts from one IP or sudden spikes in authentication errors.\nCSRF protection - validate state parameters on every callback. This is already implemented in our examples but verify it\u0026rsquo;s working.\nError handling - show user-friendly error messages instead of stack traces. \u0026ldquo;Login failed, please try again\u0026rdquo; instead of \u0026ldquo;token exchange error: invalid grant\u0026rdquo;.\nTest account linking - decide what happens if users sign in with different providers using the same email. Link accounts or show clear error messages.\nVerify emails from providers - check that emails are verified by the provider. Unverified emails can be spoofed by attackers.\nToken expiration - set reasonable session timeouts (7 days is common). Shorter for sensitive apps, longer for convenience.\nToken refresh logic - implement automatic token refresh using refresh tokens. Users shouldn\u0026rsquo;t need to log in every hour.\nLogout functionality - clear sessions properly and invalidate tokens. Don\u0026rsquo;t just delete cookies - clean up server-side session storage too.\nCross-browser testing - test on Chrome, Firefox, Safari, and mobile browsers. OAuth2 redirects can behave differently across browsers.\nComplete Production Example Here\u0026rsquo;s a full production-ready example putting everything together:\n// main.go (production version) package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; \u0026#34;github.com/gorilla/sessions\u0026#34; \u0026#34;github.com/joho/godotenv\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/config\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/handlers\u0026#34; \u0026#34;github.com/yourusername/oauth2-tutorial/middleware\u0026#34; ) var ( db *sql.DB store *sessions.CookieStore ) func main() { // Load environment variables if err := godotenv.Load(); err != nil { log.Println(\u0026#34;No .env file found\u0026#34;) } // Initialize database var err error db, err = sql.Open(\u0026#34;postgres\u0026#34;, os.Getenv(\u0026#34;DATABASE_URL\u0026#34;)) if err != nil { log.Fatal(\u0026#34;Failed to connect to database:\u0026#34;, err) } defer db.Close() // Initialize session store store = sessions.NewCookieStore([]byte(os.Getenv(\u0026#34;SESSION_SECRET\u0026#34;))) store.Options = \u0026amp;sessions.Options{ Path: \u0026#34;/\u0026#34;, MaxAge: 86400 * 7, // 7 days HttpOnly: true, Secure: true, // HTTPS only SameSite: http.SameSiteStrictMode, } // Setup handlers with dependencies h := handlers.New(db, store) // Public routes http.HandleFunc(\u0026#34;/\u0026#34;, h.HandleHome) http.HandleFunc(\u0026#34;/login\u0026#34;, h.HandleLogin) // OAuth2 routes http.HandleFunc(\u0026#34;/auth/google/login\u0026#34;, h.HandleGoogleLogin) http.HandleFunc(\u0026#34;/auth/google/callback\u0026#34;, h.HandleGoogleCallback) http.HandleFunc(\u0026#34;/auth/github/login\u0026#34;, h.HandleGitHubLogin) http.HandleFunc(\u0026#34;/auth/github/callback\u0026#34;, h.HandleGitHubCallback) http.HandleFunc(\u0026#34;/auth/facebook/login\u0026#34;, h.HandleFacebookLogin) http.HandleFunc(\u0026#34;/auth/facebook/callback\u0026#34;, h.HandleFacebookCallback) // Protected routes http.HandleFunc(\u0026#34;/profile\u0026#34;, middleware.RequireAuth(h.HandleProfile)) http.HandleFunc(\u0026#34;/logout\u0026#34;, middleware.RequireAuth(h.HandleLogout)) // Static files fs := http.FileServer(http.Dir(\u0026#34;./static\u0026#34;)) http.Handle(\u0026#34;/static/\u0026#34;, http.StripPrefix(\u0026#34;/static/\u0026#34;, fs)) port := os.Getenv(\u0026#34;PORT\u0026#34;) if port == \u0026#34;\u0026#34; { port = \u0026#34;8080\u0026#34; } log.Printf(\u0026#34;Server starting on port %s\u0026#34;, port) log.Fatal(http.ListenAndServe(\u0026#34;:\u0026#34;+port, nil)) } This production setup includes database connection, proper session management with Gorilla sessions, secure cookies, and environment-based configuration.\nCommon Issues and Troubleshooting \u0026ldquo;redirect_uri_mismatch\u0026rdquo; Error. The redirect URL in your code doesn\u0026rsquo;t exactly match what you registered with the provider. Check for trailing slashes, HTTP vs HTTPS, www vs non-www. They must match exactly.\n\u0026ldquo;invalid_client\u0026rdquo; Error. Your client ID or secret is wrong. Double-check your .env file. Make sure you\u0026rsquo;re using the right credentials for the right environment (development vs production).\nEmail is Empty or Null. Provider didn\u0026rsquo;t return email. Check your scope requests include email permission. For GitHub, explicitly request user:email scope. For Facebook, request email permission.\nState Parameter Mismatch. Your session expired between initiating login and the callback. Increase session timeout or regenerate state if the session is new.\nTokens Expire Too Fast. Access tokens are meant to be short-lived. Use refresh tokens to get new access tokens. Don\u0026rsquo;t try to extend access token lifetime - that defeats the security model.\nNext Steps You\u0026rsquo;ve got OAuth2 working - what\u0026rsquo;s next? For complete authentication systems, add JWT tokens for API authentication to secure your backend endpoints. Implement rate limiting to prevent abuse of login endpoints.\nConsider building a complete REST API with Gin to handle your business logic, and use Redis for session management instead of in-memory storage when you scale horizontally.\nConclusion OAuth2 looks complicated at first, but it\u0026rsquo;s really just a series of redirects and API calls. Users click login, get redirected to the provider, approve your app, get redirected back with a code, you exchange the code for a token, fetch user info, create a session. That\u0026rsquo;s it.\nThe code patterns are identical across providers - Google, GitHub, Facebook all work the same way. Once you implement one, adding more is copy-paste with minor adjustments to endpoints and scopes.\nSecurity comes down to a few key practices: always use HTTPS in production, validate state parameters, store tokens securely, request minimal scopes, and handle errors gracefully. Get these right and you\u0026rsquo;ve got a solid authentication system.\nThe examples in this guide are production-ready. I\u0026rsquo;ve used these exact patterns in apps serving millions of users. Add proper database integration, session management with Redis or PostgreSQL, and monitoring, and you\u0026rsquo;re good to scale.\nRemember, OAuth2 isn\u0026rsquo;t just about convenience - it\u0026rsquo;s about security. You don\u0026rsquo;t store passwords, users don\u0026rsquo;t create weak passwords, and you leverage providers\u0026rsquo; security infrastructure. Everyone wins.\n","href":"/2025/10/how-to-implement-oauth2-in-go-google-github-facebook-login.html","title":"How to Implement OAuth2 in Go Google GitHub and Facebook Login"},{"content":"If you\u0026rsquo;ve built any real application, you know file storage becomes a problem fast. User avatars, document uploads, video files - they pile up quickly, and you need somewhere reliable to put them. That\u0026rsquo;s where AWS S3 comes in. It\u0026rsquo;s like having unlimited storage that you only pay for what you use, and it integrates beautifully with Go.\nAWS recently rewrote their entire Go SDK with v2, and honestly, it\u0026rsquo;s a massive improvement. Cleaner APIs, better error handling, proper context support - everything you\u0026rsquo;d want in a modern Go library. If you\u0026rsquo;re starting fresh or thinking about upgrading from v1, this guide has you covered.\nI\u0026rsquo;ll walk you through building a complete S3 upload system - from basic file uploads to handling massive files with multipart uploads, plus presigned URLs so users can upload directly without hitting your server. We\u0026rsquo;ll cover all the production stuff too: error handling, retries, security, and the gotchas I learned the hard way.\nUnderstanding AWS S3 and SDK v2 Think of S3 as Amazon\u0026rsquo;s version of Dropbox for your applications. Instead of traditional folders and files, S3 uses buckets (containers) and objects (your files plus their metadata). You create a bucket once, then throw as many files into it as you want. Each file gets a unique key - basically its path within the bucket.\nThe v2 SDK is a total rewrite from the ground up. AWS learned from v1\u0026rsquo;s mistakes and built something that actually feels like modern Go. Proper module support, errors that make sense, context everywhere it should be, and APIs that don\u0026rsquo;t make you scratch your head.\nHere\u0026rsquo;s what you need to know before we start coding:\nBuckets are like folders, but globally unique across all of AWS. Pick a good name because you can\u0026rsquo;t change it later. Each bucket lives in a specific AWS region - choose one close to your users for better performance.\nObjects are your actual files. S3 stores the file content, lets you attach custom metadata (think tags or additional info), and identifies each object by its key. Keys look like file paths: users/avatars/user123.jpg.\nRegions matter for speed and compliance. Store data in us-east-1 if your users are in New York, eu-west-1 for London. Wrong region choice means slower uploads and higher latency.\nAccess Control is how you decide who can do what. Use IAM roles for your app, bucket policies for broad rules, and presigned URLs when you want to give temporary access to specific files.\nPrerequisites and Setup Before writing code, you need an AWS account and proper credentials. If you don\u0026rsquo;t have an AWS account yet, create one at aws.amazon.com. The free tier includes 5GB of S3 storage which is plenty for development and testing.\nCreating IAM Credentials For security, never use your root AWS account credentials in applications. Instead, create an IAM user with specific S3 permissions:\nNavigate to IAM in the AWS Console Create a new user with programmatic access Attach the AmazonS3FullAccess policy (or create a custom policy with specific permissions) Save the Access Key ID and Secret Access Key For production, use more restrictive policies that grant only the permissions your application actually needs.\nConfiguring AWS Credentials The AWS SDK for Go v2 supports multiple credential sources. The recommended approach for development is using the AWS credentials file:\nCreate ~/.aws/credentials:\n[default] aws_access_key_id = YOUR_ACCESS_KEY_ID aws_secret_access_key = YOUR_SECRET_ACCESS_KEY Create ~/.aws/config:\n[default] region = us-east-1 For production environments, use IAM roles attached to EC2 instances, ECS tasks, or Lambda functions instead of hardcoded credentials. The SDK automatically discovers and uses these roles.\nInstalling Dependencies Create a new Go project and install the required AWS SDK v2 packages:\nmkdir s3-upload-tutorial cd s3-upload-tutorial go mod init github.com/yourusername/s3-upload-tutorial Install the AWS SDK v2 packages:\ngo get github.com/aws/aws-sdk-go-v2/config go get github.com/aws/aws-sdk-go-v2/service/s3 go get github.com/aws/aws-sdk-go-v2/feature/s3/manager go get github.com/aws/aws-sdk-go-v2/aws The SDK v2 is modular, so you only import the packages you actually need. This reduces binary size and dependency bloat compared to v1.\nCreating an S3 Client The first step in any S3 operation is creating a properly configured client. The AWS SDK v2 uses a configuration-first approach where you load configuration once and use it to create service clients.\nCreate the basic S3 client:\n// main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func main() { // Load AWS configuration cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithRegion(\u0026#34;us-east-1\u0026#34;), ) if err != nil { log.Fatalf(\u0026#34;unable to load SDK config, %v\u0026#34;, err) } // Create S3 client client := s3.NewFromConfig(cfg) fmt.Println(\u0026#34;S3 client created successfully\u0026#34;) } The LoadDefaultConfig function automatically discovers credentials from the environment, credentials file, IAM roles, or other sources. You can override the region or provide custom configuration options as needed.\nFor production applications, you typically want more control over timeouts and retries:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws/retry\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func createS3Client(ctx context.Context, region string) (*s3.Client, error) { cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(region), config.WithRetryMaxAttempts(3), config.WithRetryMode(aws.RetryModeAdaptive), ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to load config: %w\u0026#34;, err) } // Create S3 client with custom options client := s3.NewFromConfig(cfg, func(o *s3.Options) { o.UsePathStyle = false // Use virtual-hosted-style URLs }) return client, nil } func main() { ctx := context.Background() client, err := createS3Client(ctx, \u0026#34;us-east-1\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to create S3 client: %v\u0026#34;, err) } fmt.Println(\u0026#34;S3 client created successfully\u0026#34;) _ = client // Use the client for operations } This production-ready configuration includes retry logic and proper error wrapping. The adaptive retry mode automatically adjusts retry behavior based on error types and server responses.\nSimple File Upload Time to actually upload something. We\u0026rsquo;ll start simple - grabbing a file from your disk and pushing it to S3. This approach works perfectly for files under 5GB, which covers most use cases.\nBasic Upload Implementation package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func uploadFile(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string) error { // Open the file file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file %s: %w\u0026#34;, filePath, err) } defer file.Close() // Upload the file to S3 _, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, }) if err != nil { return fmt.Errorf(\u0026#34;failed to upload file: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Successfully uploaded %s to %s/%s\\n\u0026#34;, filePath, bucketName, objectKey) return nil } func main() { ctx := context.Background() // Load config and create client cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\u0026#34;us-east-1\u0026#34;)) if err != nil { log.Fatalf(\u0026#34;unable to load SDK config: %v\u0026#34;, err) } client := s3.NewFromConfig(cfg) // Upload a file err = uploadFile(ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;uploads/example.txt\u0026#34;, \u0026#34;./example.txt\u0026#34;) if err != nil { log.Fatalf(\u0026#34;upload failed: %v\u0026#34;, err) } } This basic implementation uploads a file in a single PutObject operation. The SDK handles reading the file content and streaming it to S3.\nAdding Metadata and Content Type In production, you\u0026rsquo;ll want to set appropriate metadata and content types for your objects:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;mime\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3/types\u0026#34; ) func uploadFileWithMetadata(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string, metadata map[string]string) error { file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() // Detect content type based on file extension contentType := mime.TypeByExtension(filepath.Ext(filePath)) if contentType == \u0026#34;\u0026#34; { contentType = \u0026#34;application/octet-stream\u0026#34; } // Get file info for content length fileInfo, err := file.Stat() if err != nil { return fmt.Errorf(\u0026#34;failed to stat file: %w\u0026#34;, err) } _, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, ContentType: aws.String(contentType), ContentLength: aws.Int64(fileInfo.Size()), Metadata: metadata, ACL: types.ObjectCannedACLPrivate, }) if err != nil { return fmt.Errorf(\u0026#34;failed to upload: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Uploaded %s (%d bytes, type: %s)\\n\u0026#34;, objectKey, fileInfo.Size(), contentType) return nil } func main() { // Usage example ctx := context.Background() // Assume client is already created var client *s3.Client // Created as shown earlier metadata := map[string]string{ \u0026#34;uploaded-by\u0026#34;: \u0026#34;tutorial-app\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, } err := uploadFileWithMetadata(ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;documents/report.pdf\u0026#34;, \u0026#34;./report.pdf\u0026#34;, metadata) if err != nil { fmt.Printf(\u0026#34;Upload failed: %v\\n\u0026#34;, err) } } Setting the correct ContentType matters more than you think. Browsers use this to decide whether to display an image inline or force a download. Get it wrong and your PDFs might try to render as text. Custom metadata is your chance to attach extra info - user IDs, upload timestamps, whatever your app needs to track.\nUpload from Memory (Byte Slice) Sometimes you generate content in memory rather than reading from disk. For example, image processing, PDF generation, or API responses:\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func uploadFromMemory(ctx context.Context, client *s3.Client, bucketName, objectKey string, data []byte, contentType string) error { _, err := client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: bytes.NewReader(data), ContentType: aws.String(contentType), }) if err != nil { return fmt.Errorf(\u0026#34;failed to upload: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Uploaded %d bytes to %s/%s\\n\u0026#34;, len(data), bucketName, objectKey) return nil } func main() { ctx := context.Background() // Example: Upload generated JSON data jsonData := []byte(`{\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Hello from S3\u0026#34;}`) var client *s3.Client // Created as shown earlier err := uploadFromMemory(ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;api/response.json\u0026#34;, jsonData, \u0026#34;application/json\u0026#34;) if err != nil { fmt.Printf(\u0026#34;Upload failed: %v\\n\u0026#34;, err) } } The bytes.NewReader creates an io.Reader from a byte slice, which is exactly what the S3 API expects.\nMultipart Upload for Large Files Got a big video file or database backup? Regular uploads will time out and make you sad. Multipart upload is your answer - it chops your file into chunks (5MB to 5GB each), uploads them simultaneously, and S3 stitches them back together. Way faster and more reliable than trying to upload a 2GB file in one go.\nThe SDK v2\u0026rsquo;s manager.Uploader does all the heavy lifting automatically:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/feature/s3/manager\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func uploadLargeFile(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string) error { // Open the file file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() // Create an uploader with custom options uploader := manager.NewUploader(client, func(u *manager.Uploader) { u.PartSize = 10 * 1024 * 1024 // 10MB per part u.Concurrency = 5 // Upload 5 parts concurrently }) // Upload the file result, err := uploader.Upload(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, }) if err != nil { return fmt.Errorf(\u0026#34;failed to upload large file: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Successfully uploaded to %s\\n\u0026#34;, result.Location) return nil } func main() { ctx := context.Background() cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(\u0026#34;us-east-1\u0026#34;)) if err != nil { panic(err) } client := s3.NewFromConfig(cfg) // Upload a large file err = uploadLargeFile(ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;videos/large-video.mp4\u0026#34;, \u0026#34;./large-video.mp4\u0026#34;) if err != nil { fmt.Printf(\u0026#34;Upload failed: %v\\n\u0026#34;, err) } } The manager.Uploader automatically determines when to use multipart upload based on file size. You can customize the part size and concurrency to optimize for your network conditions and file sizes.\nProgress Tracking for Large Uploads Nobody likes staring at a blank screen wondering if their upload is actually working. Let\u0026rsquo;s add a progress bar:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/feature/s3/manager\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) type ProgressReader struct { file *os.File totalBytes int64 readBytes *atomic.Int64 onProgress func(uploaded, total int64) } func (pr *ProgressReader) Read(p []byte) (int, error) { n, err := pr.file.Read(p) if n \u0026gt; 0 { uploaded := pr.readBytes.Add(int64(n)) if pr.onProgress != nil { pr.onProgress(uploaded, pr.totalBytes) } } return n, err } func uploadWithProgress(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string) error { file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() // Get file size fileInfo, err := file.Stat() if err != nil { return fmt.Errorf(\u0026#34;failed to stat file: %w\u0026#34;, err) } totalBytes := fileInfo.Size() // Create progress reader var readBytes atomic.Int64 progressReader := \u0026amp;ProgressReader{ file: file, totalBytes: totalBytes, readBytes: \u0026amp;readBytes, onProgress: func(uploaded, total int64) { percentage := float64(uploaded) / float64(total) * 100 fmt.Printf(\u0026#34;\\rProgress: %.2f%% (%d/%d bytes)\u0026#34;, percentage, uploaded, total) }, } // Create uploader uploader := manager.NewUploader(client, func(u *manager.Uploader) { u.PartSize = 10 * 1024 * 1024 u.Concurrency = 5 }) // Upload with progress tracking _, err = uploader.Upload(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: progressReader, }) if err != nil { return fmt.Errorf(\u0026#34;upload failed: %w\u0026#34;, err) } fmt.Println(\u0026#34;\\nUpload completed successfully!\u0026#34;) return nil } The progress tracking wraps the file reader and reports bytes uploaded. This is perfect for building upload progress bars in web applications or CLI tools.\nPresigned URLs for Direct Uploads Want users to upload straight to S3 without hitting your server? Presigned URLs are magic. Your server creates a temporary URL with upload permissions built in, hands it to the client, and the client uploads directly to S3. Your server never sees the file - less bandwidth, less processing, happier servers.\nGenerating Presigned Upload URLs package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func generatePresignedUploadURL(ctx context.Context, client *s3.Client, bucketName, objectKey string, duration time.Duration) (string, error) { // Create presign client presignClient := s3.NewPresignClient(client) // Generate presigned PUT request request, err := presignClient.PresignPutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), }, func(opts *s3.PresignOptions) { opts.Expires = duration }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to presign request: %w\u0026#34;, err) } return request.URL, nil } func main() { ctx := context.Background() var client *s3.Client // Created as shown earlier // Generate URL valid for 15 minutes url, err := generatePresignedUploadURL(ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;user-uploads/photo.jpg\u0026#34;, 15*time.Minute) if err != nil { fmt.Printf(\u0026#34;Failed to generate URL: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;Upload your file to: %s\\n\u0026#34;, url) fmt.Println(\u0026#34;Use PUT method with the file content in the request body\u0026#34;) } The presigned URL contains authentication information in the query parameters, allowing unauthenticated requests to upload for a limited time.\nClient-Side Upload with Presigned URL Here\u0026rsquo;s how a client would use the presigned URL to upload a file:\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func uploadToPresignedURL(presignedURL, filePath string) error { // Read file file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() // Read file content fileContent, err := io.ReadAll(file) if err != nil { return fmt.Errorf(\u0026#34;failed to read file: %w\u0026#34;, err) } // Create PUT request req, err := http.NewRequest(http.MethodPut, presignedURL, bytes.NewReader(fileContent)) if err != nil { return fmt.Errorf(\u0026#34;failed to create request: %w\u0026#34;, err) } // Set content type (important!) req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/jpeg\u0026#34;) // Execute request client := \u0026amp;http.Client{} resp, err := client.Do(req) if err != nil { return fmt.Errorf(\u0026#34;failed to upload: %w\u0026#34;, err) } defer resp.Body.Close() if resp.StatusCode != http.StatusOK { body, _ := io.ReadAll(resp.Body) return fmt.Errorf(\u0026#34;upload failed with status %d: %s\u0026#34;, resp.StatusCode, string(body)) } fmt.Println(\u0026#34;File uploaded successfully via presigned URL\u0026#34;) return nil } func main() { presignedURL := \u0026#34;https://my-bucket.s3.amazonaws.com/...\u0026#34; // From server err := uploadToPresignedURL(presignedURL, \u0026#34;./photo.jpg\u0026#34;) if err != nil { fmt.Printf(\u0026#34;Upload failed: %v\\n\u0026#34;, err) } } This approach is perfect for web applications where users upload files directly from their browsers, mobile apps, or any HTTP client.\nPresigned URLs with Custom Metadata You can also enforce specific metadata and content type in presigned URLs:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) func generatePresignedUploadWithMetadata(ctx context.Context, client *s3.Client, bucketName, objectKey, contentType string, metadata map[string]string) (string, error) { presignClient := s3.NewPresignClient(client) request, err := presignClient.PresignPutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), ContentType: aws.String(contentType), Metadata: metadata, }, func(opts *s3.PresignOptions) { opts.Expires = 15 * time.Minute }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to presign: %w\u0026#34;, err) } return request.URL, nil } func main() { ctx := context.Background() var client *s3.Client // Created as shown earlier metadata := map[string]string{ \u0026#34;user-id\u0026#34;: \u0026#34;12345\u0026#34;, \u0026#34;upload-date\u0026#34;: time.Now().Format(time.RFC3339), } url, err := generatePresignedUploadWithMetadata( ctx, client, \u0026#34;my-bucket\u0026#34;, \u0026#34;user-uploads/document.pdf\u0026#34;, \u0026#34;application/pdf\u0026#34;, metadata, ) if err != nil { fmt.Printf(\u0026#34;Failed: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;Presigned URL: %s\\n\u0026#34;, url) } When using presigned URLs with specific headers, the client must include those exact headers in the upload request, or the request will fail. This provides an additional layer of validation.\nProduction Best Practices Development code that works on your laptop will crash in production. Trust me, I\u0026rsquo;ve been there. Here\u0026rsquo;s what you need to actually ship this to real users.\nError Handling and Retries The SDK v2 includes automatic retry logic, but you should handle specific error cases:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3/types\u0026#34; ) func uploadWithErrorHandling(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string) error { file, err := os.Open(filePath) if err != nil { return fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() _, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, }) if err != nil { // Check for specific error types var noBucket *types.NoSuchBucket if errors.As(err, \u0026amp;noBucket) { return fmt.Errorf(\u0026#34;bucket %s does not exist: %w\u0026#34;, bucketName, err) } var notFound *types.NotFound if errors.As(err, \u0026amp;notFound) { return fmt.Errorf(\u0026#34;resource not found: %w\u0026#34;, err) } // Generic error return fmt.Errorf(\u0026#34;upload failed: %w\u0026#34;, err) } return nil } The SDK v2 uses typed errors that you can check with errors.As(). This allows you to handle different error scenarios appropriately.\nConcurrent Upload Limits When uploading multiple files concurrently, control the number of simultaneous operations to avoid overwhelming your network or hitting AWS rate limits:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) type UploadJob struct { FilePath string ObjectKey string } type UploadResult struct { ObjectKey string Error error } func uploadConcurrent(ctx context.Context, client *s3.Client, bucketName string, jobs []UploadJob, maxConcurrency int) []UploadResult { jobChan := make(chan UploadJob, len(jobs)) resultChan := make(chan UploadResult, len(jobs)) // Worker pool var wg sync.WaitGroup for i := 0; i \u0026lt; maxConcurrency; i++ { wg.Add(1) go func() { defer wg.Done() for job := range jobChan { err := uploadSingleFile(ctx, client, bucketName, job.ObjectKey, job.FilePath) resultChan \u0026lt;- UploadResult{ ObjectKey: job.ObjectKey, Error: err, } } }() } // Send jobs for _, job := range jobs { jobChan \u0026lt;- job } close(jobChan) // Wait for completion go func() { wg.Wait() close(resultChan) }() // Collect results var results []UploadResult for result := range resultChan { results = append(results, result) } return results } func uploadSingleFile(ctx context.Context, client *s3.Client, bucketName, objectKey, filePath string) error { file, err := os.Open(filePath) if err != nil { return err } defer file.Close() _, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, }) return err } func main() { ctx := context.Background() var client *s3.Client // Created as shown earlier // Prepare upload jobs jobs := []UploadJob{ {FilePath: \u0026#34;./file1.txt\u0026#34;, ObjectKey: \u0026#34;uploads/file1.txt\u0026#34;}, {FilePath: \u0026#34;./file2.txt\u0026#34;, ObjectKey: \u0026#34;uploads/file2.txt\u0026#34;}, {FilePath: \u0026#34;./file3.txt\u0026#34;, ObjectKey: \u0026#34;uploads/file3.txt\u0026#34;}, } // Upload with max 3 concurrent uploads results := uploadConcurrent(ctx, client, \u0026#34;my-bucket\u0026#34;, jobs, 3) // Check results for _, result := range results { if result.Error != nil { fmt.Printf(\u0026#34;Failed to upload %s: %v\\n\u0026#34;, result.ObjectKey, result.Error) } else { fmt.Printf(\u0026#34;Successfully uploaded %s\\n\u0026#34;, result.ObjectKey) } } } This worker pool pattern prevents resource exhaustion while maximizing throughput. Adjust maxConcurrency based on your network bandwidth and AWS account limits.\nSecurity Considerations Security should be a priority from day one:\n1. Use Server-Side Encryption:\n_, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, ServerSideEncryption: types.ServerSideEncryptionAes256, }) 2. Validate File Types:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) var allowedMimeTypes = map[string]bool{ \u0026#34;image/jpeg\u0026#34;: true, \u0026#34;image/png\u0026#34;: true, \u0026#34;image/gif\u0026#34;: true, \u0026#34;application/pdf\u0026#34;: true, } func validateFileType(filePath string) error { file, err := os.Open(filePath) if err != nil { return err } defer file.Close() // Read first 512 bytes for MIME detection buffer := make([]byte, 512) _, err = file.Read(buffer) if err != nil { return err } contentType := http.DetectContentType(buffer) if !allowedMimeTypes[contentType] { return fmt.Errorf(\u0026#34;file type %s not allowed\u0026#34;, contentType) } return nil } 3. Set Appropriate ACLs:\nNever use public-read ACLs unless absolutely necessary. Use bucket policies and IAM roles for access control:\n_, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, ACL: types.ObjectCannedACLPrivate, // Default: private }) 4. Implement File Size Limits:\nfunc checkFileSize(filePath string, maxSize int64) error { info, err := os.Stat(filePath) if err != nil { return err } if info.Size() \u0026gt; maxSize { return fmt.Errorf(\u0026#34;file size %d exceeds limit %d\u0026#34;, info.Size(), maxSize) } return nil } Cost Optimization S3 costs can add up quickly if you\u0026rsquo;re not careful:\n1. Use Lifecycle Policies to automatically transition old files to cheaper storage classes or delete them:\nYou configure this in the AWS Console or via CloudFormation, but your application should design with this in mind.\n2. Set Proper Storage Classes:\n_, err = client.PutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(bucketName), Key: aws.String(objectKey), Body: file, StorageClass: types.StorageClassIntelligentTiering, // Auto-optimize costs }) 3. Use Multipart Upload for Large Files to avoid re-uploading the entire file on failure.\nComplete Production Example Let\u0026rsquo;s put everything together into a production-ready upload service:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; \u0026#34;mime\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/feature/s3/manager\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3/types\u0026#34; ) // S3Uploader handles file uploads to S3 type S3Uploader struct { client *s3.Client uploader *manager.Uploader bucketName string region string } // Config holds uploader configuration type Config struct { BucketName string Region string MaxFileSize int64 AllowedTypes map[string]bool PartSize int64 Concurrency int } // NewS3Uploader creates a new uploader instance func NewS3Uploader(ctx context.Context, cfg Config) (*S3Uploader, error) { awsCfg, err := config.LoadDefaultConfig(ctx, config.WithRegion(cfg.Region), ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to load AWS config: %w\u0026#34;, err) } client := s3.NewFromConfig(awsCfg) uploader := manager.NewUploader(client, func(u *manager.Uploader) { if cfg.PartSize \u0026gt; 0 { u.PartSize = cfg.PartSize } if cfg.Concurrency \u0026gt; 0 { u.Concurrency = cfg.Concurrency } }) return \u0026amp;S3Uploader{ client: client, uploader: uploader, bucketName: cfg.BucketName, region: cfg.Region, }, nil } // UploadFile uploads a file with validation and metadata func (u *S3Uploader) UploadFile(ctx context.Context, filePath, objectKey string, metadata map[string]string) (string, error) { // Validate file exists info, err := os.Stat(filePath) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;file not found: %w\u0026#34;, err) } // Open file file, err := os.Open(filePath) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to open file: %w\u0026#34;, err) } defer file.Close() // Detect content type contentType := mime.TypeByExtension(filepath.Ext(filePath)) if contentType == \u0026#34;\u0026#34; { contentType = \u0026#34;application/octet-stream\u0026#34; } // Upload result, err := u.uploader.Upload(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(u.bucketName), Key: aws.String(objectKey), Body: file, ContentType: aws.String(contentType), Metadata: metadata, ServerSideEncryption: types.ServerSideEncryptionAes256, }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;upload failed: %w\u0026#34;, err) } fmt.Printf(\u0026#34;Uploaded %s (%d bytes) to %s\\n\u0026#34;, objectKey, info.Size(), result.Location) return result.Location, nil } // GeneratePresignedURL creates a presigned upload URL func (u *S3Uploader) GeneratePresignedURL(ctx context.Context, objectKey, contentType string, duration time.Duration) (string, error) { presignClient := s3.NewPresignClient(u.client) request, err := presignClient.PresignPutObject(ctx, \u0026amp;s3.PutObjectInput{ Bucket: aws.String(u.bucketName), Key: aws.String(objectKey), ContentType: aws.String(contentType), }, func(opts *s3.PresignOptions) { opts.Expires = duration }) if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to presign: %w\u0026#34;, err) } return request.URL, nil } // ValidateFile checks file type and size func ValidateFile(filePath string, maxSize int64, allowedTypes map[string]bool) error { info, err := os.Stat(filePath) if err != nil { return fmt.Errorf(\u0026#34;file not found: %w\u0026#34;, err) } // Check size if info.Size() \u0026gt; maxSize { return fmt.Errorf(\u0026#34;file size %d exceeds limit %d\u0026#34;, info.Size(), maxSize) } // Check type file, err := os.Open(filePath) if err != nil { return err } defer file.Close() buffer := make([]byte, 512) _, err = file.Read(buffer) if err != nil \u0026amp;\u0026amp; err != io.EOF { return err } contentType := http.DetectContentType(buffer) if !allowedTypes[contentType] { return fmt.Errorf(\u0026#34;file type %s not allowed\u0026#34;, contentType) } return nil } func main() { ctx := context.Background() // Configure uploader config := Config{ BucketName: \u0026#34;my-production-bucket\u0026#34;, Region: \u0026#34;us-east-1\u0026#34;, MaxFileSize: 100 * 1024 * 1024, // 100MB AllowedTypes: map[string]bool{ \u0026#34;image/jpeg\u0026#34;: true, \u0026#34;image/png\u0026#34;: true, \u0026#34;application/pdf\u0026#34;: true, }, PartSize: 10 * 1024 * 1024, // 10MB Concurrency: 5, } // Create uploader uploader, err := NewS3Uploader(ctx, config) if err != nil { panic(err) } // Example 1: Direct upload filePath := \u0026#34;./document.pdf\u0026#34; err = ValidateFile(filePath, config.MaxFileSize, config.AllowedTypes) if err != nil { fmt.Printf(\u0026#34;Validation failed: %v\\n\u0026#34;, err) return } metadata := map[string]string{ \u0026#34;uploaded-at\u0026#34;: time.Now().Format(time.RFC3339), \u0026#34;user-id\u0026#34;: \u0026#34;user123\u0026#34;, } location, err := uploader.UploadFile(ctx, filePath, \u0026#34;documents/report.pdf\u0026#34;, metadata) if err != nil { fmt.Printf(\u0026#34;Upload failed: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;File available at: %s\\n\u0026#34;, location) // Example 2: Generate presigned URL presignedURL, err := uploader.GeneratePresignedURL(ctx, \u0026#34;uploads/photo.jpg\u0026#34;, \u0026#34;image/jpeg\u0026#34;, 15*time.Minute) if err != nil { fmt.Printf(\u0026#34;Failed to generate presigned URL: %v\\n\u0026#34;, err) return } fmt.Printf(\u0026#34;Upload to: %s\\n\u0026#34;, presignedURL) } This production-ready implementation includes:\nProper configuration management File validation (type and size) Automatic content type detection Server-side encryption Custom metadata Error handling Both direct upload and presigned URL support Testing Your Implementation Testing S3 operations can be tricky since they interact with external services. Here\u0026rsquo;s how to approach testing:\nUnit Testing with Mocks For unit tests, mock the S3 client interface:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/s3\u0026#34; ) // Mock S3 client for testing type MockS3Client struct { PutObjectFunc func(ctx context.Context, params *s3.PutObjectInput, optFns ...func(*s3.Options)) (*s3.PutObjectOutput, error) } func (m *MockS3Client) PutObject(ctx context.Context, params *s3.PutObjectInput, optFns ...func(*s3.Options)) (*s3.PutObjectOutput, error) { return m.PutObjectFunc(ctx, params, optFns...) } func TestUpload(t *testing.T) { // Test implementation here // Use the mock client to verify behavior } Integration Testing For integration tests, use a test bucket or LocalStack (local AWS simulator):\n# Install LocalStack pip install localstack # Start LocalStack with S3 localstack start # Set AWS endpoint for testing export AWS_ENDPOINT_URL=http://localhost:4566 Then configure your S3 client to use the local endpoint:\nclient := s3.NewFromConfig(cfg, func(o *s3.Options) { o.BaseEndpoint = aws.String(\u0026#34;http://localhost:4566\u0026#34;) o.UsePathStyle = true }) Common Issues and Troubleshooting Access Denied Errors Symptom: Upload fails with \u0026ldquo;Access Denied\u0026rdquo; error\nSolutions:\nVerify IAM permissions include s3:PutObject for the bucket Check bucket policy doesn\u0026rsquo;t deny uploads Ensure credentials are correctly configured Verify the bucket exists and you have access Slow Upload Performance Symptom: Uploads take longer than expected\nSolutions:\nUse multipart upload for files over 100MB Increase concurrency in uploader configuration Check network bandwidth and latency to AWS region Consider using Transfer Acceleration for global uploads Invalid Content Type Symptom: Files don\u0026rsquo;t display correctly when accessed via browser\nSolution: Set correct ContentType in PutObjectInput:\nContentType: aws.String(\u0026#34;image/jpeg\u0026#34;), // Set appropriate type Presigned URL Failures Symptom: Presigned URL uploads fail with signature errors\nSolutions:\nEnsure system clock is synchronized (signature includes timestamp) Client must use exact headers specified in presigned request URL must be used before expiration time Check for URL encoding issues Next Steps and Advanced Topics Now that you understand S3 uploads with Go SDK v2, consider exploring these advanced topics:\nObject Lifecycle Management: Configure automatic transitions to cheaper storage classes or deletion after specific periods. This can significantly reduce storage costs for temporary or archival data.\nCross-Region Replication: Automatically replicate objects across AWS regions for disaster recovery or reduced latency for global users.\nEvent Notifications: Trigger Lambda functions or send SNS notifications when objects are uploaded. Perfect for implementing image processing pipelines, virus scanning, or automatic backup systems.\nS3 Select: Query data directly in S3 without downloading entire objects. Great for large CSV or JSON files where you only need specific records.\nVersioning: Enable S3 versioning to keep multiple versions of objects, protecting against accidental deletions or overwrites.\nFor building complete backend systems with Go, check out how to build REST APIs with Gin framework and implementing JWT authentication to secure your upload endpoints. You might also want to implement rate limiting to prevent upload abuse, and use Redis for caching metadata or tracking upload progress across multiple servers.\nConclusion You\u0026rsquo;ve got everything you need now to handle file uploads in Go with S3. The SDK v2 makes it surprisingly pleasant to work with, and once you get the basics down, scaling to millions of files is just a matter of tuning some knobs.\nQuick recap of what matters:\nStart with simple PutObject calls for files under 5GB - no need to overcomplicate things. When you hit larger files or want faster uploads, switch to manager.Uploader and let it handle multipart uploads automatically.\nPresigned URLs are your best friend for user uploads. Generate them server-side, send to clients, and watch uploads happen without touching your infrastructure. Just remember to set reasonable expiration times.\nSecurity isn\u0026rsquo;t optional. Validate file types and sizes, encrypt everything with server-side encryption, keep ACLs private by default, and never hardcode AWS credentials in your code.\nThe code examples here aren\u0026rsquo;t toys - they\u0026rsquo;re production patterns I\u0026rsquo;ve used in real applications handling millions of files. Adapt them to your needs, add proper error handling, set up monitoring, and you\u0026rsquo;re good to go.\nS3 isn\u0026rsquo;t just storage - it\u0026rsquo;s infrastructure that scales from hobby projects to enterprise systems without you changing code. Combined with Go\u0026rsquo;s speed and concurrency, you\u0026rsquo;ve got a solid foundation for building file-heavy applications that actually work at scale.\n","href":"/2025/10/how-to-upload-files-to-aws-s3-in-go-with-sdk-v2.html","title":"How to Upload Files to AWS S3 in Go Complete Guide with SDK v2"},{"content":"Modern applications demand speed and scalability that traditional databases struggle to provide alone. Users expect instant responses, APIs must handle thousands of concurrent requests, and systems need to scale horizontally without performance degradation. Redis addresses these challenges by providing blazing-fast in-memory data storage that complements your existing database infrastructure.\nThis comprehensive guide demonstrates how to integrate Redis with Go applications for caching and session management. You\u0026rsquo;ll learn to set up the go-redis client, implement various caching patterns, manage user sessions across distributed servers, optimize connection pooling, handle cache invalidation, and follow production best practices that ensure reliability and performance at scale.\nUnderstanding Redis and Its Use Cases Redis operates as an in-memory data structure store, keeping all data in RAM for microsecond-level access times. Unlike traditional databases that read from disk, Redis eliminates I/O bottlenecks by serving data directly from memory. This architecture makes it perfect for scenarios where speed matters more than persistence guarantees.\nThe most common use case involves caching database query results. When your application repeatedly queries the same data, storing results in Redis reduces database load and improves response times dramatically. A query that takes 100 milliseconds from PostgreSQL might return in 2 milliseconds from Redis.\nSession management represents another critical use case. Web applications need to maintain user state across requests and server instances. Redis provides a centralized session store that all servers can access, enabling stateless application design while maintaining user context. Sessions can expire automatically, reducing the maintenance burden.\nReal-time features like leaderboards, rate limiting, and message queues leverage Redis data structures like sorted sets, counters, and lists. These specialized structures provide atomic operations that would require complex SQL queries, making Redis the natural choice for these patterns.\nInstalling and Configuring Redis Start by installing Redis on your development machine. For production, you\u0026rsquo;ll use managed services or properly configured Redis instances, but local installation helps during development.\n# macOS brew install redis brew services start redis # Ubuntu/Debian sudo apt update sudo apt install redis-server sudo systemctl start redis # Verify installation redis-cli ping # Should return: PONG Install the go-redis client library in your Go project. This official client provides comprehensive Redis functionality with excellent performance characteristics.\ngo get github.com/redis/go-redis/v9 Create a basic configuration file to manage Redis connection settings across environments.\n// internal/config/redis.go package config import ( \u0026#34;time\u0026#34; ) type RedisConfig struct { Host string Port string Password string DB int PoolSize int MinIdleConns int DialTimeout time.Duration ReadTimeout time.Duration WriteTimeout time.Duration } func LoadRedisConfig() *RedisConfig { return \u0026amp;RedisConfig{ Host: getEnv(\u0026#34;REDIS_HOST\u0026#34;, \u0026#34;localhost\u0026#34;), Port: getEnv(\u0026#34;REDIS_PORT\u0026#34;, \u0026#34;6379\u0026#34;), Password: getEnv(\u0026#34;REDIS_PASSWORD\u0026#34;, \u0026#34;\u0026#34;), DB: getEnvAsInt(\u0026#34;REDIS_DB\u0026#34;, 0), PoolSize: getEnvAsInt(\u0026#34;REDIS_POOL_SIZE\u0026#34;, 10), MinIdleConns: getEnvAsInt(\u0026#34;REDIS_MIN_IDLE\u0026#34;, 5), DialTimeout: 5 * time.Second, ReadTimeout: 3 * time.Second, WriteTimeout: 3 * time.Second, } } func getEnv(key, defaultValue string) string { if value := os.Getenv(key); value != \u0026#34;\u0026#34; { return value } return defaultValue } func getEnvAsInt(key string, defaultValue int) int { if value := os.Getenv(key); value != \u0026#34;\u0026#34; { if intValue, err := strconv.Atoi(value); err == nil { return intValue } } return defaultValue } Setting Up Redis Client Connection Initialize the Redis client with proper connection pooling and timeout configurations. Connection pooling reuses TCP connections across requests, dramatically improving performance compared to creating new connections for each operation.\n// pkg/redis/client.go package redis import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type Client struct { rdb *redis.Client } func NewClient(config *config.RedisConfig) (*Client, error) { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: fmt.Sprintf(\u0026#34;%s:%s\u0026#34;, config.Host, config.Port), Password: config.Password, DB: config.DB, PoolSize: config.PoolSize, MinIdleConns: config.MinIdleConns, DialTimeout: config.DialTimeout, ReadTimeout: config.ReadTimeout, WriteTimeout: config.WriteTimeout, }) ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() if err := rdb.Ping(ctx).Err(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to connect to Redis: %w\u0026#34;, err) } return \u0026amp;Client{rdb: rdb}, nil } func (c *Client) Close() error { return c.rdb.Close() } func (c *Client) GetClient() *redis.Client { return c.rdb } func (c *Client) HealthCheck(ctx context.Context) error { return c.rdb.Ping(ctx).Err() } The client initializes with connection pooling enabled by default. The pool size determines maximum concurrent connections, while minimum idle connections ensure ready connections for incoming requests without connection establishment overhead.\nImplementing Basic Caching Operations Create a cache service that wraps Redis operations with a clean interface. This abstraction makes it easy to swap implementations or add features like compression or encryption later.\n// internal/cache/service.go package cache import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type Service struct { client *redis.Client } func NewService(client *redis.Client) *Service { return \u0026amp;Service{client: client} } func (s *Service) Set(ctx context.Context, key string, value interface{}, expiration time.Duration) error { data, err := json.Marshal(value) if err != nil { return fmt.Errorf(\u0026#34;failed to marshal value: %w\u0026#34;, err) } return s.client.Set(ctx, key, data, expiration).Err() } func (s *Service) Get(ctx context.Context, key string, dest interface{}) error { data, err := s.client.Get(ctx, key).Bytes() if err != nil { if err == redis.Nil { return ErrCacheMiss } return fmt.Errorf(\u0026#34;failed to get value: %w\u0026#34;, err) } if err := json.Unmarshal(data, dest); err != nil { return fmt.Errorf(\u0026#34;failed to unmarshal value: %w\u0026#34;, err) } return nil } func (s *Service) Delete(ctx context.Context, keys ...string) error { return s.client.Del(ctx, keys...).Err() } func (s *Service) Exists(ctx context.Context, key string) (bool, error) { count, err := s.client.Exists(ctx, key).Result() if err != nil { return false, err } return count \u0026gt; 0, nil } func (s *Service) SetNX(ctx context.Context, key string, value interface{}, expiration time.Duration) (bool, error) { data, err := json.Marshal(value) if err != nil { return false, fmt.Errorf(\u0026#34;failed to marshal value: %w\u0026#34;, err) } return s.client.SetNX(ctx, key, data, expiration).Result() } var ErrCacheMiss = fmt.Errorf(\u0026#34;cache miss\u0026#34;) The service provides type-safe caching with automatic JSON serialization. The SetNX operation sets a value only if the key doesn\u0026rsquo;t exist, useful for implementing distributed locks or preventing race conditions.\nImplementing Cache-Aside Pattern The cache-aside pattern, also known as lazy loading, checks the cache before querying the database. This pattern gives you control over cache population and works well for read-heavy workloads.\n// internal/repository/user.go package repository import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type User struct { ID int `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type UserRepository struct { db *sql.DB cache *cache.Service } func NewUserRepository(db *sql.DB, cache *cache.Service) *UserRepository { return \u0026amp;UserRepository{ db: db, cache: cache, } } func (r *UserRepository) GetByID(ctx context.Context, id int) (*User, error) { cacheKey := fmt.Sprintf(\u0026#34;user:%d\u0026#34;, id) var user User err := r.cache.Get(ctx, cacheKey, \u0026amp;user) if err == nil { return \u0026amp;user, nil } if err != cache.ErrCacheMiss { return nil, fmt.Errorf(\u0026#34;cache error: %w\u0026#34;, err) } query := \u0026#34;SELECT id, email, name, created_at FROM users WHERE id = $1\u0026#34; err = r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.CreatedAt, ) if err != nil { return nil, fmt.Errorf(\u0026#34;database error: %w\u0026#34;, err) } if err := r.cache.Set(ctx, cacheKey, \u0026amp;user, 15*time.Minute); err != nil { return \u0026amp;user, nil } return \u0026amp;user, nil } func (r *UserRepository) Update(ctx context.Context, user *User) error { query := \u0026#34;UPDATE users SET email = $1, name = $2 WHERE id = $3\u0026#34; _, err := r.db.ExecContext(ctx, query, user.Email, user.Name, user.ID) if err != nil { return fmt.Errorf(\u0026#34;database error: %w\u0026#34;, err) } cacheKey := fmt.Sprintf(\u0026#34;user:%d\u0026#34;, user.ID) if err := r.cache.Delete(ctx, cacheKey); err != nil { return nil } return nil } This implementation checks Redis first, returning cached data if available. On cache miss, it queries the database and stores the result in Redis with a 15-minute expiration. Updates invalidate the cache to maintain consistency.\nImplementing Write-Through Cache Pattern Write-through caching updates both cache and database simultaneously, ensuring cache consistency at the cost of write latency. This pattern suits scenarios where stale cache data causes problems.\nfunc (r *UserRepository) Create(ctx context.Context, user *User) error { query := \u0026#34;INSERT INTO users (email, name) VALUES ($1, $2) RETURNING id, created_at\u0026#34; err := r.db.QueryRowContext(ctx, query, user.Email, user.Name).Scan( \u0026amp;user.ID, \u0026amp;user.CreatedAt, ) if err != nil { return fmt.Errorf(\u0026#34;database error: %w\u0026#34;, err) } cacheKey := fmt.Sprintf(\u0026#34;user:%d\u0026#34;, user.ID) if err := r.cache.Set(ctx, cacheKey, user, 15*time.Minute); err != nil { return nil } return nil } The write-through pattern maintains cache consistency but increases write latency since every write operation touches both systems. Choose this pattern when cache consistency matters more than write performance.\nManaging Sessions with Redis Sessions store user state across requests in web applications. Redis provides fast, distributed session storage that scales across multiple application servers.\n// internal/session/manager.go package session import ( \u0026#34;context\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type Manager struct { client *redis.Client expiration time.Duration } type Session struct { ID string `json:\u0026#34;id\u0026#34;` UserID int `json:\u0026#34;user_id\u0026#34;` Data map[string]interface{} `json:\u0026#34;data\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` ExpiresAt time.Time `json:\u0026#34;expires_at\u0026#34;` } func NewManager(client *redis.Client, expiration time.Duration) *Manager { return \u0026amp;Manager{ client: client, expiration: expiration, } } func (m *Manager) Create(ctx context.Context, userID int) (*Session, error) { sessionID, err := generateSessionID() if err != nil { return nil, fmt.Errorf(\u0026#34;failed to generate session ID: %w\u0026#34;, err) } session := \u0026amp;Session{ ID: sessionID, UserID: userID, Data: make(map[string]interface{}), CreatedAt: time.Now(), ExpiresAt: time.Now().Add(m.expiration), } key := fmt.Sprintf(\u0026#34;session:%s\u0026#34;, sessionID) if err := m.client.HSet(ctx, key, session).Err(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to create session: %w\u0026#34;, err) } if err := m.client.Expire(ctx, key, m.expiration).Err(); err != nil { return nil, fmt.Errorf(\u0026#34;failed to set expiration: %w\u0026#34;, err) } return session, nil } func (m *Manager) Get(ctx context.Context, sessionID string) (*Session, error) { key := fmt.Sprintf(\u0026#34;session:%s\u0026#34;, sessionID) var session Session if err := m.client.HGetAll(ctx, key).Scan(\u0026amp;session); err != nil { if err == redis.Nil { return nil, ErrSessionNotFound } return nil, fmt.Errorf(\u0026#34;failed to get session: %w\u0026#34;, err) } return \u0026amp;session, nil } func (m *Manager) Update(ctx context.Context, session *Session) error { key := fmt.Sprintf(\u0026#34;session:%s\u0026#34;, session.ID) if err := m.client.HSet(ctx, key, session).Err(); err != nil { return fmt.Errorf(\u0026#34;failed to update session: %w\u0026#34;, err) } if err := m.client.Expire(ctx, key, m.expiration).Err(); err != nil { return fmt.Errorf(\u0026#34;failed to refresh expiration: %w\u0026#34;, err) } return nil } func (m *Manager) Destroy(ctx context.Context, sessionID string) error { key := fmt.Sprintf(\u0026#34;session:%s\u0026#34;, sessionID) return m.client.Del(ctx, key).Err() } func generateSessionID() (string, error) { b := make([]byte, 32) if _, err := rand.Read(b); err != nil { return \u0026#34;\u0026#34;, err } return base64.URLEncoding.EncodeToString(b), nil } var ErrSessionNotFound = fmt.Errorf(\u0026#34;session not found\u0026#34;) The session manager uses Redis hashes to store session data, supporting automatic expiration and efficient updates. Sessions expire automatically after the configured duration, eliminating the need for manual cleanup.\nBuilding Session Middleware Create middleware that automatically loads and saves sessions for each request, providing transparent session access to your handlers.\n// internal/middleware/session.go package middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) const SessionCookieName = \u0026#34;session_id\u0026#34; func SessionMiddleware(manager *session.Manager) gin.HandlerFunc { return func(c *gin.Context) { sessionID, err := c.Cookie(SessionCookieName) if err != nil || sessionID == \u0026#34;\u0026#34; { c.Next() return } sess, err := manager.Get(c.Request.Context(), sessionID) if err != nil { c.Next() return } c.Set(\u0026#34;session\u0026#34;, sess) c.Next() updatedSession, exists := c.Get(\u0026#34;session\u0026#34;) if !exists { return } if sess, ok := updatedSession.(*session.Session); ok { manager.Update(c.Request.Context(), sess) } } } func RequireSession() gin.HandlerFunc { return func(c *gin.Context) { _, exists := c.Get(\u0026#34;session\u0026#34;) if !exists { c.JSON(http.StatusUnauthorized, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;authentication required\u0026#34;, }) c.Abort() return } c.Next() } } The middleware loads sessions from cookies, makes them available to handlers through the context, and automatically saves changes after request processing. This pattern integrates seamlessly with authentication systems like those in our JWT authentication guide .\nImplementing Cache Invalidation Strategies Cache invalidation ensures data consistency when underlying data changes. Different strategies suit different scenarios based on consistency requirements and traffic patterns.\n// internal/cache/invalidation.go package cache import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type InvalidationService struct { cache *Service } func NewInvalidationService(cache *Service) *InvalidationService { return \u0026amp;InvalidationService{cache: cache} } func (s *InvalidationService) InvalidateUser(ctx context.Context, userID int) error { patterns := []string{ fmt.Sprintf(\u0026#34;user:%d\u0026#34;, userID), fmt.Sprintf(\u0026#34;user:%d:*\u0026#34;, userID), } for _, pattern := range patterns { if err := s.cache.DeletePattern(ctx, pattern); err != nil { return err } } return nil } func (s *InvalidationService) InvalidateUserPosts(ctx context.Context, userID int) error { return s.cache.Delete(ctx, fmt.Sprintf(\u0026#34;user:%d:posts\u0026#34;, userID)) } func (s *InvalidationService) InvalidateWithTTL(ctx context.Context, key string, ttl time.Duration) error { exists, err := s.cache.Exists(ctx, key) if err != nil { return err } if exists { return s.cache.client.Expire(ctx, key, ttl).Err() } return nil } Time-based expiration provides the simplest invalidation strategy. Set appropriate TTL values based on how frequently data changes and how stale data affects your application. Event-based invalidation deletes cache entries when the underlying data changes, maintaining stronger consistency.\nAdvanced Caching Patterns Implement cache warming to pre-populate frequently accessed data before it\u0026rsquo;s requested. This eliminates cache misses for predictable access patterns.\nfunc (s *Service) WarmCache(ctx context.Context) error { popularUsers := []int{1, 2, 3, 4, 5} for _, userID := range popularUsers { user, err := s.userRepo.GetFromDB(ctx, userID) if err != nil { continue } cacheKey := fmt.Sprintf(\u0026#34;user:%d\u0026#34;, userID) s.cache.Set(ctx, cacheKey, user, 1*time.Hour) } return nil } Use cache stampede prevention when many requests simultaneously trigger cache population for the same key. This prevents overwhelming your database during cache misses.\nfunc (r *UserRepository) GetWithStampedeProtection(ctx context.Context, id int) (*User, error) { cacheKey := fmt.Sprintf(\u0026#34;user:%d\u0026#34;, id) lockKey := fmt.Sprintf(\u0026#34;lock:user:%d\u0026#34;, id) var user User err := r.cache.Get(ctx, cacheKey, \u0026amp;user) if err == nil { return \u0026amp;user, nil } acquired, err := r.cache.SetNX(ctx, lockKey, true, 10*time.Second) if err != nil { return nil, err } if !acquired { time.Sleep(100 * time.Millisecond) return r.GetByID(ctx, id) } defer r.cache.Delete(ctx, lockKey) query := \u0026#34;SELECT id, email, name, created_at FROM users WHERE id = $1\u0026#34; err = r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Email, \u0026amp;user.Name, \u0026amp;user.CreatedAt, ) if err != nil { return nil, err } r.cache.Set(ctx, cacheKey, \u0026amp;user, 15*time.Minute) return \u0026amp;user, nil } Monitoring Redis Performance Track cache hit rates, connection pool usage, and operation latencies to identify performance issues and optimize cache configuration.\n// internal/metrics/redis.go package metrics import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type RedisMetrics struct { Hits int64 Misses int64 TotalRequests int64 } func (m *RedisMetrics) RecordHit() { atomic.AddInt64(\u0026amp;m.Hits, 1) atomic.AddInt64(\u0026amp;m.TotalRequests, 1) } func (m *RedisMetrics) RecordMiss() { atomic.AddInt64(\u0026amp;m.Misses, 1) atomic.AddInt64(\u0026amp;m.TotalRequests, 1) } func (m *RedisMetrics) HitRate() float64 { total := atomic.LoadInt64(\u0026amp;m.TotalRequests) if total == 0 { return 0 } hits := atomic.LoadInt64(\u0026amp;m.Hits) return float64(hits) / float64(total) * 100 } func MonitorRedisStats(ctx context.Context, client *redis.Client) { ticker := time.NewTicker(30 * time.Second) defer ticker.Stop() for range ticker.C { stats := client.PoolStats() log.Printf(\u0026#34;Redis Pool Stats - Hits: %d, Misses: %d, Timeouts: %d, TotalConns: %d, IdleConns: %d\u0026#34;, stats.Hits, stats.Misses, stats.Timeouts, stats.TotalConns, stats.IdleConns) } } Monitor these metrics in production to ensure your caching strategy provides the expected performance improvements. High miss rates might indicate incorrect TTL values or ineffective cache warming.\nProduction Best Practices Configure appropriate pool sizes based on your workload. The default of 10 connections per CPU works well for most applications, but high-throughput systems might need larger pools.\nAlways set timeouts for dial, read, and write operations to prevent hanging connections from degrading performance. Five seconds for dial timeout and three seconds for read/write operations provide good defaults.\nImplement retry logic with exponential backoff for transient errors. Network issues, Redis restarts, or high load can cause temporary failures that succeed on retry.\nfunc (s *Service) GetWithRetry(ctx context.Context, key string, dest interface{}, maxRetries int) error { var err error for i := 0; i \u0026lt; maxRetries; i++ { err = s.Get(ctx, key, dest) if err == nil { return nil } if err == ErrCacheMiss { return err } backoff := time.Duration(i*i) * 100 * time.Millisecond time.Sleep(backoff) } return err } Use Redis Sentinel or Cluster for high availability in production. Sentinel provides automatic failover for master-replica setups, while Cluster enables horizontal scaling across multiple Redis instances.\nMonitor memory usage and configure appropriate eviction policies. The allkeys-lru policy evicts least recently used keys when memory limits are reached, suitable for cache workloads.\nIntegrating with Rate Limiting Combine Redis caching with rate limiting from our rate limiting guide to protect cached endpoints while maintaining high performance.\nfunc CachedRateLimitMiddleware(cache *cache.Service, limiter *ratelimit.RedisStore) gin.HandlerFunc { return func(c *gin.Context) { ip := c.ClientIP() allowed, err := limiter.Allow(c.Request.Context(), ip) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;rate limit check failed\u0026#34;}) c.Abort() return } if !allowed { c.JSON(http.StatusTooManyRequests, gin.H{\u0026#34;error\u0026#34;: \u0026#34;rate limit exceeded\u0026#34;}) c.Abort() return } c.Next() } } This integration ensures rate limiting state persists across server instances while cached data reduces database load, creating a robust and performant API.\nConclusion Redis transforms application performance by providing sub-millisecond data access through intelligent caching and efficient session management. The patterns and implementations covered in this guide enable you to build high-performance Go applications that scale horizontally while maintaining excellent user experience.\nThe cache-aside and write-through patterns offer flexibility in balancing consistency and performance based on your specific requirements. Session management with Redis enables stateless application design while maintaining user context across distributed servers. Connection pooling and proper configuration ensure Redis operates efficiently even under high load.\nRemember that caching introduces complexity through data staleness and invalidation challenges. Monitor cache hit rates, adjust TTL values based on data change patterns, and implement proper invalidation strategies to maintain data consistency. When combined with authentication and rate limiting , Redis caching creates production-ready APIs that handle massive scale while delivering exceptional performance.\nAs your application grows, consider advanced Redis features like pub/sub for real-time updates, sorted sets for leaderboards, and geospatial indexes for location-based features. The foundation built here supports these advanced use cases, making Redis a versatile tool that grows with your application needs.\n","href":"/2025/10/how-to-use-redis-with-go-caching-session-management.html","title":"How to Use Redis with Go - Caching and Session Management Tutorial"},{"content":"APIs power modern applications by exposing functionality to clients, but unrestricted access creates vulnerabilities. A single misbehaving client can overwhelm your server, degrading performance for all users. Malicious actors can exploit unprotected endpoints to scrape data, attempt credential stuffing, or launch denial of service attacks. Rate limiting provides the first line of defense against these threats.\nThis comprehensive guide demonstrates how to implement rate limiting in Go applications. You\u0026rsquo;ll learn multiple algorithms including token bucket and sliding window approaches, build middleware for automatic request throttling, implement per-IP and per-user limiting strategies, integrate Redis for distributed systems, and follow production best practices for protecting your APIs effectively.\nUnderstanding Rate Limiting Fundamentals Rate limiting restricts the number of requests a client can make within a defined time period. When implemented correctly, it prevents abuse while allowing legitimate traffic to flow smoothly. The mechanism tracks request counts for each client and rejects requests that exceed configured thresholds.\nDifferent scenarios require different rate limiting strategies. Public APIs might allow 100 requests per minute per IP address to prevent scraping. Authenticated endpoints could permit higher limits based on user subscription tiers. Critical operations like password reset might enforce stricter limits of 3 attempts per hour per account to prevent brute force attacks.\nThe key challenge lies in efficiently tracking request counts across potentially millions of clients while maintaining low latency. Your rate limiting implementation must add minimal overhead to request processing while accurately enforcing limits even under high load conditions.\nRate Limiting Algorithms Explained The token bucket algorithm represents the most popular approach for rate limiting in Go applications. Imagine a bucket that holds tokens, where each request consumes one token. The bucket refills at a constant rate and has a maximum capacity. When a request arrives, the system checks for available tokens. If tokens exist, the request proceeds and consumes a token. Otherwise, the request is rejected.\nThis algorithm naturally handles burst traffic since the bucket can accumulate tokens up to its maximum capacity. A client that stays idle builds up tokens, allowing a brief burst of requests when needed. The refill rate determines the sustained request rate, while bucket capacity controls the maximum burst size.\nThe sliding window algorithm provides stricter control by examining the actual request count within a rolling time window. For each request, the system counts how many requests occurred in the past N seconds. This approach prevents gaming the system by timing requests across fixed window boundaries, ensuring more accurate rate limiting at the cost of slightly higher computational overhead.\nFixed window counters offer the simplest implementation. They count requests in discrete time windows, resetting the count when each window expires. While easy to implement and efficient, this approach has a flaw where clients can make double the allowed requests by timing them at window boundaries.\nImplementing Basic In-Memory Rate Limiting Start with a simple in-memory rate limiter using Go\u0026rsquo;s standard library. This approach works well for single-server deployments and helps understand core concepts before adding complexity.\n// internal/ratelimit/memory.go package ratelimit import ( \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type client struct { limiter *Limiter lastSeen time.Time } type MemoryStore struct { clients map[string]*client mu sync.RWMutex rate int burst int } func NewMemoryStore(requestsPerSecond, burst int) *MemoryStore { store := \u0026amp;MemoryStore{ clients: make(map[string]*client), rate: requestsPerSecond, burst: burst, } go store.cleanupVisitors() return store } func (s *MemoryStore) GetLimiter(key string) *Limiter { s.mu.Lock() defer s.mu.Unlock() client, exists := s.clients[key] if !exists { limiter := NewLimiter(s.rate, s.burst) s.clients[key] = \u0026amp;client{ limiter: limiter, lastSeen: time.Now(), } return limiter } client.lastSeen = time.Now() return client.limiter } func (s *MemoryStore) cleanupVisitors() { ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for range ticker.C { s.mu.Lock() for key, client := range s.clients { if time.Since(client.lastSeen) \u0026gt; 3*time.Minute { delete(s.clients, key) } } s.mu.Unlock() } } The memory store maintains a map of client limiters, indexed by client identifier. A background goroutine periodically removes entries for clients that haven\u0026rsquo;t made requests recently, preventing unbounded memory growth.\nBuilding the Token Bucket Limiter Implement the core token bucket algorithm that controls request rates while allowing controlled bursts.\n// internal/ratelimit/limiter.go package ratelimit import ( \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) type Limiter struct { tokens float64 maxTokens float64 refillRate float64 lastRefillTime time.Time mu sync.Mutex } func NewLimiter(requestsPerSecond, burst int) *Limiter { return \u0026amp;Limiter{ tokens: float64(burst), maxTokens: float64(burst), refillRate: float64(requestsPerSecond), lastRefillTime: time.Now(), } } func (l *Limiter) Allow() bool { l.mu.Lock() defer l.mu.Unlock() now := time.Now() elapsed := now.Sub(l.lastRefillTime).Seconds() l.tokens += elapsed * l.refillRate if l.tokens \u0026gt; l.maxTokens { l.tokens = l.maxTokens } l.lastRefillTime = now if l.tokens \u0026gt;= 1.0 { l.tokens -= 1.0 return true } return false } func (l *Limiter) Tokens() float64 { l.mu.Lock() defer l.mu.Unlock() return l.tokens } The limiter calculates tokens available based on elapsed time since the last refill. Each request consumes one token if available. The implementation uses mutex locks to ensure thread safety when multiple goroutines access the same limiter.\nCreating Rate Limiting Middleware Build middleware that automatically applies rate limiting to your HTTP handlers without cluttering your business logic.\n// internal/middleware/ratelimit.go package middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;ratelimit-example/internal/ratelimit\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func RateLimitMiddleware(store *ratelimit.MemoryStore) gin.HandlerFunc { return func(c *gin.Context) { ip := c.ClientIP() limiter := store.GetLimiter(ip) if !limiter.Allow() { c.Header(\u0026#34;X-RateLimit-Limit\u0026#34;, \u0026#34;100\u0026#34;) c.Header(\u0026#34;X-RateLimit-Remaining\u0026#34;, \u0026#34;0\u0026#34;) c.Header(\u0026#34;Retry-After\u0026#34;, \u0026#34;60\u0026#34;) c.JSON(http.StatusTooManyRequests, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;rate limit exceeded\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;too many requests, please try again later\u0026#34;, }) c.Abort() return } remaining := int(limiter.Tokens()) c.Header(\u0026#34;X-RateLimit-Limit\u0026#34;, \u0026#34;100\u0026#34;) c.Header(\u0026#34;X-RateLimit-Remaining\u0026#34;, string(rune(remaining))) c.Next() } } The middleware extracts the client IP address, retrieves the corresponding limiter, checks if the request is allowed, sets appropriate headers, and either allows the request to proceed or returns a 429 status code. Standard rate limit headers inform clients about their quota and remaining requests.\nUsing Go\u0026rsquo;s Standard Rate Limiter Go provides a production-ready rate limiter in the golang.org/x/time/rate package that implements the token bucket algorithm with additional features.\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;golang.org/x/time/rate\u0026#34; ) type visitor struct { limiter *rate.Limiter lastSeen time.Time } type RateLimiter struct { visitors map[string]*visitor mu sync.RWMutex rate rate.Limit burst int } func NewRateLimiter(r rate.Limit, b int) *RateLimiter { rl := \u0026amp;RateLimiter{ visitors: make(map[string]*visitor), rate: r, burst: b, } go rl.cleanupVisitors() return rl } func (rl *RateLimiter) getLimiter(ip string) *rate.Limiter { rl.mu.Lock() defer rl.mu.Unlock() v, exists := rl.visitors[ip] if !exists { limiter := rate.NewLimiter(rl.rate, rl.burst) rl.visitors[ip] = \u0026amp;visitor{limiter, time.Now()} return limiter } v.lastSeen = time.Now() return v.limiter } func (rl *RateLimiter) cleanupVisitors() { ticker := time.NewTicker(1 * time.Minute) defer ticker.Stop() for range ticker.C { rl.mu.Lock() for ip, v := range rl.visitors { if time.Since(v.lastSeen) \u0026gt; 3*time.Minute { delete(rl.visitors, ip) } } rl.mu.Unlock() } } func (rl *RateLimiter) Middleware() gin.HandlerFunc { return func(c *gin.Context) { limiter := rl.getLimiter(c.ClientIP()) if !limiter.Allow() { c.JSON(http.StatusTooManyRequests, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;rate limit exceeded\u0026#34;, }) c.Abort() return } c.Next() } } The standard library limiter provides methods like Allow for immediate decisions, Wait for blocking until tokens are available, and Reserve for advanced scheduling. This implementation handles most common use cases efficiently.\nImplementing Redis-Based Rate Limiting Distributed systems require shared state across multiple server instances. Redis provides a centralized store for rate limiting data that works across all servers.\n// internal/ratelimit/redis.go package ratelimit import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type RedisStore struct { client *redis.Client maxRequest int window time.Duration } func NewRedisStore(client *redis.Client, maxRequest int, window time.Duration) *RedisStore { return \u0026amp;RedisStore{ client: client, maxRequest: maxRequest, window: window, } } func (r *RedisStore) Allow(ctx context.Context, key string) (bool, error) { rateKey := fmt.Sprintf(\u0026#34;rate_limit:%s\u0026#34;, key) pipe := r.client.Pipeline() incr := pipe.Incr(ctx, rateKey) pipe.Expire(ctx, rateKey, r.window) _, err := pipe.Exec(ctx) if err != nil { return false, err } count := incr.Val() return count \u0026lt;= int64(r.maxRequest), nil } func (r *RedisStore) Remaining(ctx context.Context, key string) (int, error) { rateKey := fmt.Sprintf(\u0026#34;rate_limit:%s\u0026#34;, key) count, err := r.client.Get(ctx, rateKey).Int() if err == redis.Nil { return r.maxRequest, nil } if err != nil { return 0, err } remaining := r.maxRequest - count if remaining \u0026lt; 0 { return 0, nil } return remaining, nil } func (r *RedisStore) Reset(ctx context.Context, key string) error { rateKey := fmt.Sprintf(\u0026#34;rate_limit:%s\u0026#34;, key) return r.client.Del(ctx, rateKey).Err() } The Redis implementation uses atomic increment operations and expiration to track request counts. This approach ensures consistency even when multiple servers process requests simultaneously for the same client.\nImplementing Sliding Window Rate Limiting The sliding window algorithm provides more accurate rate limiting by considering the exact timing of requests within a rolling time window.\n// internal/ratelimit/sliding_window.go package ratelimit import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; ) type SlidingWindowLimiter struct { client *redis.Client maxRequest int window time.Duration } func NewSlidingWindowLimiter(client *redis.Client, maxRequest int, window time.Duration) *SlidingWindowLimiter { return \u0026amp;SlidingWindowLimiter{ client: client, maxRequest: maxRequest, window: window, } } func (s *SlidingWindowLimiter) Allow(ctx context.Context, key string) (bool, error) { now := time.Now() windowStart := now.Add(-s.window) rateKey := fmt.Sprintf(\u0026#34;sliding_window:%s\u0026#34;, key) pipe := s.client.Pipeline() pipe.ZRemRangeByScore(ctx, rateKey, \u0026#34;0\u0026#34;, fmt.Sprintf(\u0026#34;%d\u0026#34;, windowStart.UnixNano())) count := pipe.ZCard(ctx, rateKey) pipe.ZAdd(ctx, rateKey, redis.Z{ Score: float64(now.UnixNano()), Member: fmt.Sprintf(\u0026#34;%d\u0026#34;, now.UnixNano()), }) pipe.Expire(ctx, rateKey, s.window) _, err := pipe.Exec(ctx) if err != nil { return false, err } return count.Val() \u0026lt; int64(s.maxRequest), nil } This implementation uses Redis sorted sets to store request timestamps. It removes old requests outside the window, counts remaining requests, adds the current request, and determines if the limit is exceeded. The sorted set automatically maintains chronological order.\nBuilding Complete Rate Limiting Middleware Create production-ready middleware with comprehensive features including header management, error handling, and configurable limits.\n// cmd/server/main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; \u0026#34;github.com/redis/go-redis/v9\u0026#34; \u0026#34;golang.org/x/time/rate\u0026#34; ) func setupRateLimiting(router *gin.Engine) { limiter := NewRateLimiter(rate.Limit(2), 5) router.Use(limiter.Middleware()) router.GET(\u0026#34;/api/public\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Public endpoint with rate limiting\u0026#34;, }) }) redisClient := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, }) apiGroup := router.Group(\u0026#34;/api/v1\u0026#34;) apiGroup.Use(RedisRateLimitMiddleware(redisClient, 100, time.Minute)) { apiGroup.GET(\u0026#34;/users\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Users endpoint with Redis rate limiting\u0026#34;, }) }) apiGroup.POST(\u0026#34;/orders\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;message\u0026#34;: \u0026#34;Orders endpoint\u0026#34;, }) }) } } func main() { router := gin.Default() setupRateLimiting(router) log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := router.Run(\u0026#34;:8080\u0026#34;); err != nil { log.Fatal(\u0026#34;Failed to start server:\u0026#34;, err) } } The server demonstrates both in-memory and Redis-based rate limiting applied to different route groups. This allows fine-tuned control over rate limits based on endpoint sensitivity and expected traffic patterns.\nAdvanced Rate Limiting Strategies Implement tiered rate limiting based on user authentication status or subscription level. Free users might get 100 requests per hour while premium users receive 1000 requests per hour.\nfunc TieredRateLimitMiddleware(store *ratelimit.MemoryStore) gin.HandlerFunc { return func(c *gin.Context) { var key string var limiter *ratelimit.Limiter userID, exists := c.Get(\u0026#34;user_id\u0026#34;) if exists { userTier, _ := c.Get(\u0026#34;user_tier\u0026#34;) key = fmt.Sprintf(\u0026#34;user:%v\u0026#34;, userID) if userTier == \u0026#34;premium\u0026#34; { limiter = store.GetLimiterWithRate(key, 1000, 100) } else { limiter = store.GetLimiterWithRate(key, 100, 10) } } else { key = c.ClientIP() limiter = store.GetLimiter(key) } if !limiter.Allow() { c.JSON(http.StatusTooManyRequests, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;rate limit exceeded for your tier\u0026#34;, }) c.Abort() return } c.Next() } } Different endpoints might require different rate limits based on their computational cost or sensitivity. Authentication endpoints need stricter limits to prevent brute force attacks, while read operations can handle higher request volumes.\nMonitoring and Logging Rate Limits Track rate limiting metrics to understand API usage patterns and adjust limits appropriately. Log rate limit violations to identify potential abuse or misconfigured clients.\ntype RateLimitMetrics struct { TotalRequests int64 AllowedRequests int64 BlockedRequests int64 UniqueClients int64 } func MetricsMiddleware(metrics *RateLimitMetrics) gin.HandlerFunc { return func(c *gin.Context) { atomic.AddInt64(\u0026amp;metrics.TotalRequests, 1) c.Next() if c.Writer.Status() == http.StatusTooManyRequests { atomic.AddInt64(\u0026amp;metrics.BlockedRequests, 1) log.Printf(\u0026#34;Rate limit exceeded for IP: %s, Path: %s\u0026#34;, c.ClientIP(), c.Request.URL.Path) } else { atomic.AddInt64(\u0026amp;metrics.AllowedRequests, 1) } } } Expose metrics through a monitoring endpoint that operations teams can track. This helps identify when to adjust rate limits or investigate unusual traffic patterns.\nTesting Rate Limiting Implementation Write tests to verify rate limiting behavior under various conditions including normal traffic, burst requests, and sustained high load.\nfunc TestRateLimiter(t *testing.T) { limiter := NewLimiter(2, 5) for i := 0; i \u0026lt; 5; i++ { if !limiter.Allow() { t.Errorf(\u0026#34;Request %d should be allowed (burst)\u0026#34;, i) } } if limiter.Allow() { t.Error(\u0026#34;Request should be blocked after burst\u0026#34;) } time.Sleep(time.Second) if !limiter.Allow() { t.Error(\u0026#34;Request should be allowed after refill\u0026#34;) } } func TestRateLimitMiddleware(t *testing.T) { router := gin.New() store := NewMemoryStore(2, 5) router.Use(RateLimitMiddleware(store)) router.GET(\u0026#34;/test\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;}) }) for i := 0; i \u0026lt; 5; i++ { w := httptest.NewRecorder() req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;/test\u0026#34;, nil) router.ServeHTTP(w, req) if w.Code != http.StatusOK { t.Errorf(\u0026#34;Request %d should succeed\u0026#34;, i) } } w := httptest.NewRecorder() req, _ := http.NewRequest(\u0026#34;GET\u0026#34;, \u0026#34;/test\u0026#34;, nil) router.ServeHTTP(w, req) if w.Code != http.StatusTooManyRequests { t.Error(\u0026#34;Request should be rate limited\u0026#34;) } } Production Best Practices Configure rate limits based on actual traffic analysis rather than arbitrary numbers. Monitor your API usage patterns over time and adjust limits to balance protection against legitimate usage needs.\nImplement graceful degradation when rate limiting infrastructure fails. Rather than blocking all requests if Redis becomes unavailable, fall back to permissive mode while logging errors. This maintains availability during infrastructure issues.\nUse circuit breakers in combination with rate limiting to protect downstream services. If a service becomes slow or unresponsive, the circuit breaker prevents cascading failures while rate limiting prevents overwhelming the struggling service.\nConsider geographic distribution when implementing rate limits. Users in different regions might have different usage patterns or infrastructure limitations that justify different rate limiting configurations.\nDocument rate limits clearly in your API documentation. Clients need to know the limits to implement proper retry logic and backoff strategies. Include rate limit information in response headers to help clients track their usage.\nHandling Edge Cases Account for clock skew in distributed systems when implementing time-based rate limiting. Use monotonic clocks where available and add tolerance for small time differences between servers.\nHandle IP address spoofing by combining IP-based rate limiting with other identifiers like API keys or user sessions. Relying solely on IP addresses leaves you vulnerable to distributed attacks.\nConsider shared IP addresses from corporate networks or mobile carriers where many users share the same IP. Implement authenticated rate limiting for logged-in users to provide fair limits without penalizing legitimate users behind shared IPs.\nPlan for rate limit resets and communicate them clearly to clients. Clients should know when their quota resets to implement efficient retry strategies rather than repeatedly hitting rate-limited endpoints.\nIntegrating with Authentication Systems Combine rate limiting with JWT authentication from our JWT authentication guide to protect authenticated endpoints while providing better limits for verified users.\nfunc AuthenticatedRateLimitMiddleware(jwtService *auth.JWTService, store *ratelimit.MemoryStore) gin.HandlerFunc { return func(c *gin.Context) { var key string var limiter *ratelimit.Limiter token := extractToken(c) if token != \u0026#34;\u0026#34; { claims, err := jwtService.ValidateToken(token) if err == nil { key = fmt.Sprintf(\u0026#34;user:%d\u0026#34;, claims.UserID) limiter = store.GetLimiterWithRate(key, 1000, 100) } } if limiter == nil { key = c.ClientIP() limiter = store.GetLimiter(key) } if !limiter.Allow() { c.JSON(http.StatusTooManyRequests, gin.H{ \u0026#34;error\u0026#34;: \u0026#34;rate limit exceeded\u0026#34;, }) c.Abort() return } c.Next() } } This approach provides generous limits for authenticated users while maintaining strict limits for anonymous access. It incentivizes registration while protecting against abuse from unauthenticated sources.\nConclusion Rate limiting forms an essential security layer for any production API. The implementation strategies covered in this guide protect your services from abuse while maintaining excellent performance for legitimate users. Token bucket algorithms provide flexible rate limiting with burst support, sliding window approaches offer precise control, and Redis integration enables consistent limits across distributed systems.\nThe middleware patterns demonstrated here integrate seamlessly with authentication systems like those covered in our JWT authentication tutorial , creating comprehensive API protection. By following these implementation patterns and best practices, you build resilient APIs that handle both normal traffic and attack scenarios gracefully.\nRemember that rate limiting represents just one component of API security. Combine it with proper authentication, input validation, and error handling to create truly secure systems. Monitor your rate limiting metrics continuously and adjust limits based on real-world usage patterns to maintain the balance between security and usability.\nAs your application scales, consider implementing more sophisticated strategies like adaptive rate limiting that adjusts thresholds based on current system load, or distributed rate limiting with consensus algorithms for extremely high-scale deployments. The foundational techniques presented here provide a solid base for these advanced implementations.\n","href":"/2025/10/how-to-implement-rate-limiting-in-go-protect-api-from-abuse.html","title":"How to Implement Rate Limiting in Go - Protect Your API from Abuse"},{"content":"Authentication sits at the foundation of any secure application. Whether you\u0026rsquo;re building a REST API, microservice, or full-stack web application, you need a reliable way to verify user identity and protect sensitive endpoints. JWT (JSON Web Token) has become the de facto standard for stateless authentication in modern applications, and Go provides excellent tools for implementing it correctly.\nThis guide walks through implementing JWT authentication in Go from the ground up. You\u0026rsquo;ll learn how to generate tokens, validate them, handle refresh tokens, create authentication middleware, and follow security best practices that work in production environments. By the end, you\u0026rsquo;ll have a complete authentication system ready to integrate into your Go applications.\nUnderstanding JWT Authentication JWT authentication works by issuing cryptographically signed tokens to authenticated users. Each token contains claims about the user (like their ID, email, or permissions) encoded in JSON format. The server signs these claims using a secret key, creating a tamper-proof token that clients include with subsequent requests.\nWhen a client makes an authenticated request, they send the JWT in the Authorization header. The server validates the signature, checks the expiration time, and extracts the user information from the claims. This stateless approach means the server doesn\u0026rsquo;t need to store session data, making it ideal for distributed systems and microservices.\nThe process involves three main components: the header (algorithm and token type), the payload (claims about the user), and the signature (cryptographic hash verifying authenticity). The server combines these components with base64 encoding and separates them with dots, creating the familiar JWT format you see in authentication headers.\nSetting Up the Project Start by creating a new Go project and installing the necessary dependencies. You\u0026rsquo;ll need the golang-jwt library for token operations and a router like Gin for handling HTTP requests.\nmkdir jwt-auth-example cd jwt-auth-example go mod init jwt-auth-example go get github.com/golang-jwt/jwt/v5 go get github.com/gin-gonic/gin Create the basic project structure to organize your code logically. Separate concerns into different packages for handlers, middleware, models, and utilities.\nmkdir -p cmd/server mkdir -p internal/{handlers,middleware,models,auth} mkdir -p pkg/utils Set up environment variables for sensitive configuration like JWT secrets. Create a .env file in your project root to store these values securely.\nJWT_SECRET=your-secret-key-change-this-in-production JWT_EXPIRATION=15m REFRESH_TOKEN_EXPIRATION=168h Creating User Models and Database Setup Define the user model that represents authenticated users in your system. Include fields for storing user credentials and metadata that you\u0026rsquo;ll need for token generation.\n// internal/models/user.go package models import ( \u0026#34;time\u0026#34; ) type User struct { ID uint `json:\u0026#34;id\u0026#34; gorm:\u0026#34;primaryKey\u0026#34;` Email string `json:\u0026#34;email\u0026#34; gorm:\u0026#34;unique;not null\u0026#34;` Password string `json:\u0026#34;-\u0026#34; gorm:\u0026#34;not null\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34;` } type LoginRequest struct { Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required,min=6\u0026#34;` } type RegisterRequest struct { Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required,min=6\u0026#34;` Name string `json:\u0026#34;name\u0026#34; binding:\u0026#34;required\u0026#34;` } type TokenResponse struct { AccessToken string `json:\u0026#34;access_token\u0026#34;` RefreshToken string `json:\u0026#34;refresh_token\u0026#34;` ExpiresIn int64 `json:\u0026#34;expires_in\u0026#34;` } For this tutorial, we\u0026rsquo;ll use an in-memory user store to keep things simple. In production, you\u0026rsquo;d connect to a real database using GORM or sqlx as shown in our PostgreSQL connection guide .\n// internal/models/store.go package models import ( \u0026#34;errors\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; ) var ( ErrUserNotFound = errors.New(\u0026#34;user not found\u0026#34;) ErrInvalidCredentials = errors.New(\u0026#34;invalid credentials\u0026#34;) ErrUserExists = errors.New(\u0026#34;user already exists\u0026#34;) ) type UserStore struct { users map[string]*User mu sync.RWMutex nextID uint } func NewUserStore() *UserStore { return \u0026amp;UserStore{ users: make(map[string]*User), nextID: 1, } } func (s *UserStore) CreateUser(email, password, name string) (*User, error) { s.mu.Lock() defer s.mu.Unlock() if _, exists := s.users[email]; exists { return nil, ErrUserExists } hashedPassword, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost) if err != nil { return nil, err } user := \u0026amp;User{ ID: s.nextID, Email: email, Password: string(hashedPassword), Name: name, } s.users[email] = user s.nextID++ return user, nil } func (s *UserStore) GetUserByEmail(email string) (*User, error) { s.mu.RLock() defer s.mu.RUnlock() user, exists := s.users[email] if !exists { return nil, ErrUserNotFound } return user, nil } func (s *UserStore) ValidateCredentials(email, password string) (*User, error) { user, err := s.GetUserByEmail(email) if err != nil { return nil, ErrInvalidCredentials } err = bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(password)) if err != nil { return nil, ErrInvalidCredentials } return user, nil } Implementing JWT Token Generation Create a service that handles all JWT operations including token generation, validation, and claims extraction. This centralizes your authentication logic and makes it easier to maintain.\n// internal/auth/jwt.go package auth import ( \u0026#34;errors\u0026#34; \u0026#34;time\u0026#34; \u0026#34;jwt-auth-example/internal/models\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v5\u0026#34; ) var ( ErrInvalidToken = errors.New(\u0026#34;invalid token\u0026#34;) ErrExpiredToken = errors.New(\u0026#34;token has expired\u0026#34;) ) type JWTClaims struct { UserID uint `json:\u0026#34;user_id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` jwt.RegisteredClaims } type JWTService struct { secretKey []byte accessExpiry time.Duration refreshExpiry time.Duration } func NewJWTService(secret string, accessExpiry, refreshExpiry time.Duration) *JWTService { return \u0026amp;JWTService{ secretKey: []byte(secret), accessExpiry: accessExpiry, refreshExpiry: refreshExpiry, } } func (s *JWTService) GenerateAccessToken(user *models.User) (string, error) { claims := JWTClaims{ UserID: user.ID, Email: user.Email, RegisteredClaims: jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(s.accessExpiry)), IssuedAt: jwt.NewNumericDate(time.Now()), NotBefore: jwt.NewNumericDate(time.Now()), Issuer: \u0026#34;jwt-auth-example\u0026#34;, Subject: string(rune(user.ID)), }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) return token.SignedString(s.secretKey) } func (s *JWTService) GenerateRefreshToken(user *models.User) (string, error) { claims := JWTClaims{ UserID: user.ID, Email: user.Email, RegisteredClaims: jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(time.Now().Add(s.refreshExpiry)), IssuedAt: jwt.NewNumericDate(time.Now()), NotBefore: jwt.NewNumericDate(time.Now()), Issuer: \u0026#34;jwt-auth-example\u0026#34;, }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) return token.SignedString(s.secretKey) } func (s *JWTService) ValidateToken(tokenString string) (*JWTClaims, error) { token, err := jwt.ParseWithClaims(tokenString, \u0026amp;JWTClaims{}, func(token *jwt.Token) (interface{}, error) { if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok { return nil, ErrInvalidToken } return s.secretKey, nil }) if err != nil { if errors.Is(err, jwt.ErrTokenExpired) { return nil, ErrExpiredToken } return nil, ErrInvalidToken } claims, ok := token.Claims.(*JWTClaims) if !ok || !token.Valid { return nil, ErrInvalidToken } return claims, nil } func (s *JWTService) GenerateTokenPair(user *models.User) (*models.TokenResponse, error) { accessToken, err := s.GenerateAccessToken(user) if err != nil { return nil, err } refreshToken, err := s.GenerateRefreshToken(user) if err != nil { return nil, err } return \u0026amp;models.TokenResponse{ AccessToken: accessToken, RefreshToken: refreshToken, ExpiresIn: int64(s.accessExpiry.Seconds()), }, nil } The token generation process creates a JWT with specific claims about the user. The access token expires quickly (typically 15 minutes) to limit the window of opportunity if compromised. The refresh token lasts much longer but serves only to obtain new access tokens.\nBuilding Authentication Handlers Create HTTP handlers for registration, login, and token refresh endpoints. These handlers validate input, interact with the user store, and return appropriate responses.\n// internal/handlers/auth.go package handlers import ( \u0026#34;net/http\u0026#34; \u0026#34;jwt-auth-example/internal/auth\u0026#34; \u0026#34;jwt-auth-example/internal/models\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) type AuthHandler struct { userStore *models.UserStore jwtService *auth.JWTService } func NewAuthHandler(userStore *models.UserStore, jwtService *auth.JWTService) *AuthHandler { return \u0026amp;AuthHandler{ userStore: userStore, jwtService: jwtService, } } func (h *AuthHandler) Register(c *gin.Context) { var req models.RegisterRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } user, err := h.userStore.CreateUser(req.Email, req.Password, req.Name) if err != nil { if err == models.ErrUserExists { c.JSON(http.StatusConflict, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user already exists\u0026#34;}) return } c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to create user\u0026#34;}) return } tokens, err := h.jwtService.GenerateTokenPair(user) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to generate tokens\u0026#34;}) return } c.JSON(http.StatusCreated, gin.H{ \u0026#34;user\u0026#34;: user, \u0026#34;tokens\u0026#34;: tokens, }) } func (h *AuthHandler) Login(c *gin.Context) { var req models.LoginRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } user, err := h.userStore.ValidateCredentials(req.Email, req.Password) if err != nil { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid credentials\u0026#34;}) return } tokens, err := h.jwtService.GenerateTokenPair(user) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to generate tokens\u0026#34;}) return } c.JSON(http.StatusOK, gin.H{ \u0026#34;user\u0026#34;: user, \u0026#34;tokens\u0026#34;: tokens, }) } func (h *AuthHandler) RefreshToken(c *gin.Context) { var req struct { RefreshToken string `json:\u0026#34;refresh_token\u0026#34; binding:\u0026#34;required\u0026#34;` } if err := c.ShouldBindJSON(\u0026amp;req); err != nil { c.JSON(http.StatusBadRequest, gin.H{\u0026#34;error\u0026#34;: err.Error()}) return } claims, err := h.jwtService.ValidateToken(req.RefreshToken) if err != nil { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid refresh token\u0026#34;}) return } user, err := h.userStore.GetUserByEmail(claims.Email) if err != nil { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;user not found\u0026#34;}) return } tokens, err := h.jwtService.GenerateTokenPair(user) if err != nil { c.JSON(http.StatusInternalServerError, gin.H{\u0026#34;error\u0026#34;: \u0026#34;failed to generate tokens\u0026#34;}) return } c.JSON(http.StatusOK, tokens) } Creating Authentication Middleware Middleware intercepts requests to protected endpoints and validates the JWT before allowing the request to proceed. This keeps authentication logic separate from your business logic.\n// internal/middleware/auth.go package middleware import ( \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;jwt-auth-example/internal/auth\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func AuthMiddleware(jwtService *auth.JWTService) gin.HandlerFunc { return func(c *gin.Context) { authHeader := c.GetHeader(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;authorization header required\u0026#34;}) c.Abort() return } parts := strings.SplitN(authHeader, \u0026#34; \u0026#34;, 2) if len(parts) != 2 || parts[0] != \u0026#34;Bearer\u0026#34; { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: \u0026#34;invalid authorization header format\u0026#34;}) c.Abort() return } claims, err := jwtService.ValidateToken(parts[1]) if err != nil { c.JSON(http.StatusUnauthorized, gin.H{\u0026#34;error\u0026#34;: err.Error()}) c.Abort() return } c.Set(\u0026#34;user_id\u0026#34;, claims.UserID) c.Set(\u0026#34;email\u0026#34;, claims.Email) c.Next() } } The middleware extracts the token from the Authorization header, validates it using the JWT service, and stores user information in the context for downstream handlers to access. Similar middleware patterns appear in our Gin framework tutorial .\nSetting Up the Server and Routes Bring everything together by creating the main server that wires up routes, handlers, and middleware.\n// cmd/server/main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;jwt-auth-example/internal/auth\u0026#34; \u0026#34;jwt-auth-example/internal/handlers\u0026#34; \u0026#34;jwt-auth-example/internal/middleware\u0026#34; \u0026#34;jwt-auth-example/internal/models\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { router := gin.Default() userStore := models.NewUserStore() jwtService := auth.NewJWTService( \u0026#34;your-secret-key-change-this-in-production\u0026#34;, 15*time.Minute, 7*24*time.Hour, ) authHandler := handlers.NewAuthHandler(userStore, jwtService) router.POST(\u0026#34;/api/auth/register\u0026#34;, authHandler.Register) router.POST(\u0026#34;/api/auth/login\u0026#34;, authHandler.Login) router.POST(\u0026#34;/api/auth/refresh\u0026#34;, authHandler.RefreshToken) protected := router.Group(\u0026#34;/api\u0026#34;) protected.Use(middleware.AuthMiddleware(jwtService)) { protected.GET(\u0026#34;/profile\u0026#34;, func(c *gin.Context) { userID := c.GetUint(\u0026#34;user_id\u0026#34;) email := c.GetString(\u0026#34;email\u0026#34;) c.JSON(200, gin.H{ \u0026#34;user_id\u0026#34;: userID, \u0026#34;email\u0026#34;: email, \u0026#34;message\u0026#34;: \u0026#34;This is a protected endpoint\u0026#34;, }) }) } log.Println(\u0026#34;Server starting on :8080\u0026#34;) if err := router.Run(\u0026#34;:8080\u0026#34;); err != nil { log.Fatal(\u0026#34;Failed to start server:\u0026#34;, err) } } Testing the Authentication Flow Test your authentication system by making requests to each endpoint. Start with registration to create a user account.\ncurl -X POST http://localhost:8080/api/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;securepass123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }\u0026#39; The server responds with user information and a token pair. Save the access token for subsequent requests.\n{ \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }, \u0026#34;tokens\u0026#34;: { \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\u0026#34;, \u0026#34;refresh_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...\u0026#34;, \u0026#34;expires_in\u0026#34;: 900 } } Test the protected endpoint using the access token in the Authorization header.\ncurl -X GET http://localhost:8080/api/profile \\ -H \u0026#34;Authorization: Bearer YOUR_ACCESS_TOKEN\u0026#34; When the access token expires, use the refresh token to obtain a new token pair without requiring the user to log in again.\ncurl -X POST http://localhost:8080/api/auth/refresh \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;refresh_token\u0026#34;: \u0026#34;YOUR_REFRESH_TOKEN\u0026#34; }\u0026#39; Security Best Practices Production authentication systems require additional security measures beyond basic token generation and validation. Always use environment variables for JWT secrets and never commit them to version control.\nChoose signing algorithms carefully. HMAC-SHA256 (HS256) works well for single-server applications, but consider RSA (RS256) for distributed systems where multiple services need to verify tokens without sharing secrets.\nSet appropriate token expiration times based on your security requirements. Access tokens should expire quickly (15 minutes is standard) while refresh tokens can last days or weeks. Balance security against user experience to avoid excessive re-authentication.\nStore tokens in HttpOnly cookies when building web applications. This prevents JavaScript access and protects against XSS attacks. For mobile apps or single-page applications, implement secure storage mechanisms appropriate to the platform.\nImplement token revocation for logout and security events. While JWTs are stateless, you can maintain a blocklist of revoked tokens in Redis with expiration times matching the token lifetime. Check this blocklist in your authentication middleware.\nValidate all token claims on the server side. Check the issuer, expiration time, not-before time, and any custom claims. Never trust client-provided data without verification.\nUse HTTPS exclusively in production. JWT tokens transmitted over unencrypted connections can be intercepted and stolen. Configure your server to reject HTTP requests and redirect to HTTPS.\nHandling Common Challenges Token refresh timing requires careful consideration. Implement automatic token refresh in your client applications before tokens expire to avoid interrupting user sessions. Monitor token expiration times and refresh proactively.\nConcurrent requests during token refresh can cause race conditions. Implement request queuing in your client to ensure only one refresh request proceeds at a time. Subsequent requests should wait for the new token before retrying.\nMultiple device support requires tracking refresh tokens per device. Store refresh tokens with device identifiers to allow users to log out individual devices without affecting others. This provides better security and user control.\nDatabase queries for user information on every request can impact performance. Consider caching user data in memory with appropriate invalidation strategies, or include necessary user information directly in JWT claims while keeping tokens reasonably sized.\nIntegrating with Production Systems Real applications require database persistence for user accounts and token metadata. Replace the in-memory user store with a database connection as demonstrated in our guide on connecting to PostgreSQL .\nAdd rate limiting to authentication endpoints to prevent brute force attacks. Implement progressive delays after failed login attempts and consider account lockouts after repeated failures.\nLog authentication events for security monitoring and debugging. Track successful logins, failed attempts, token refreshes, and unusual patterns that might indicate security issues.\nImplement role-based access control by including user roles or permissions in JWT claims. Validate these permissions in your middleware or handlers to restrict access to sensitive functionality.\nConsider adding email verification and password reset flows to create a complete authentication system. These features enhance security and provide better user experience.\nConclusion JWT authentication provides a robust, scalable solution for securing Go applications. The stateless nature of JWTs eliminates server-side session storage while maintaining security through cryptographic signatures. By following the implementation patterns and security practices covered in this guide, you can build production-ready authentication systems that protect user data and scale with your application.\nThe token-based approach integrates seamlessly with microservices architectures and works across different clients including web applications, mobile apps, and API consumers. Proper implementation of token generation, validation, refresh mechanisms, and security measures creates authentication systems that balance security requirements with user experience.\nRemember that authentication forms just one part of application security. Combine JWT authentication with other security measures like input validation, error handling , secure database queries, and regular security audits to create truly secure applications. Stay updated on security best practices and adjust your implementation as threats and technologies evolve.\n","href":"/2025/09/how-to-implement-jwt-authentication-in-go-secure-rest-api.html","title":"How to Implement JWT Authentication in Go - Secure REST API Tutorial"},{"content":"In the early days of web development, finding services was simple. Your database lived at localhost:5432, your cache at localhost:6379, and everything was predictable. But when you move to microservices, suddenly you have dozens of services spinning up and down across multiple servers, and nobody knows where anything is anymore.\nThis is where service discovery becomes your lifeline. Instead of hardcoding addresses and hoping for the best, you get a dynamic phone book that keeps track of who\u0026rsquo;s available, where they live, and whether they\u0026rsquo;re actually working. After building several distributed systems in Go, I can tell you that getting service discovery right is often the difference between a system that scales gracefully and one that becomes an operational nightmare.\nToday, we\u0026rsquo;ll implement service discovery using two of the most popular tools in the ecosystem: Consul and etcd. Both have their strengths, and understanding how to work with each will make you a more effective distributed systems engineer. We\u0026rsquo;ll build real implementations that handle service registration, health checking, and automatic failover.\nUnderstanding Service Discovery Fundamentals Before diving into implementation, let\u0026rsquo;s understand what service discovery solves. In a traditional monolithic application, components communicate through direct method calls or well-known local endpoints. When you break that monolith into microservices, those components become separate processes that need to find and communicate with each other over the network.\nThe challenge isn\u0026rsquo;t just finding services - it\u0026rsquo;s finding healthy instances. Services crash, get overloaded, or become temporarily unavailable. A good service discovery system automatically removes unhealthy instances from rotation and adds them back when they recover.\nService discovery operates on two main patterns: client-side and server-side discovery. In client-side discovery, the service consumer queries the service registry directly and chooses which instance to call. In server-side discovery, clients make requests to a load balancer that queries the service registry and forwards requests to healthy instances.\nBoth Consul and etcd excel at different aspects of this problem. Consul provides built-in health checking, DNS integration, and a robust HTTP API. etcd offers strong consistency guarantees, efficient watching mechanisms, and excellent performance under high load. Understanding when to use each is crucial for building resilient systems.\nSetting Up the Development Environment Let\u0026rsquo;s start by setting up both Consul and etcd locally, then build our Go services that can work with either system. This approach gives you flexibility to choose the right tool for your specific requirements.\nFirst, install Consul and etcd. On macOS with Homebrew:\nbrew install consul brew install etcd For other operating systems, download the binaries from their respective websites. Create our project structure:\nmkdir service-discovery-go cd service-discovery-go go mod init service-discovery-go Set up the project structure:\nservice-discovery-go/ ├── main.go ├── internal/ │ ├── discovery/ │ │ ├── consul.go │ │ ├── etcd.go │ │ └── registry.go │ ├── service/ │ │ └── userservice.go │ └── config/ │ └── config.go ├── cmd/ │ ├── user-service/ │ │ └── main.go │ └── api-gateway/ │ └── main.go └── configs/ └── config.yaml Install the required dependencies:\ngo get github.com/hashicorp/consul/api go get go.etcd.io/etcd/clientv3/v3 go get github.com/gorilla/mux go get gopkg.in/yaml.v2 go get github.com/google/uuid Building the Service Registry Interface Let\u0026rsquo;s start with a common interface that can work with both Consul and etcd. This abstraction allows us to switch between implementations without changing our service code:\n// internal/discovery/registry.go package discovery import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; ) type ServiceInstance struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Address string `json:\u0026#34;address\u0026#34;` Port int `json:\u0026#34;port\u0026#34;` Tags []string `json:\u0026#34;tags\u0026#34;` Metadata map[string]string `json:\u0026#34;metadata\u0026#34;` Health string `json:\u0026#34;health\u0026#34;` } type ServiceRegistry interface { Register(ctx context.Context, instance *ServiceInstance) error Deregister(ctx context.Context, instanceID string) error Discover(ctx context.Context, serviceName string) ([]*ServiceInstance, error) Watch(ctx context.Context, serviceName string) (\u0026lt;-chan []*ServiceInstance, error) HealthCheck(ctx context.Context, instanceID string) error Close() error } type RegistryConfig struct { Type string `yaml:\u0026#34;type\u0026#34;` // \u0026#34;consul\u0026#34; or \u0026#34;etcd\u0026#34; Address string `yaml:\u0026#34;address\u0026#34;` Username string `yaml:\u0026#34;username\u0026#34;` Password string `yaml:\u0026#34;password\u0026#34;` Timeout time.Duration `yaml:\u0026#34;timeout\u0026#34;` } Implementing Consul Service Discovery Consul\u0026rsquo;s strength lies in its built-in health checking and DNS integration. Here\u0026rsquo;s our Consul implementation:\n// internal/discovery/consul.go package discovery import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/hashicorp/consul/api\u0026#34; ) type ConsulRegistry struct { client *api.Client config *RegistryConfig } func NewConsulRegistry(config *RegistryConfig) (*ConsulRegistry, error) { consulConfig := api.DefaultConfig() consulConfig.Address = config.Address if config.Username != \u0026#34;\u0026#34; { consulConfig.HttpAuth = \u0026amp;api.HttpBasicAuth{ Username: config.Username, Password: config.Password, } } client, err := api.NewClient(consulConfig) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create consul client: %w\u0026#34;, err) } return \u0026amp;ConsulRegistry{ client: client, config: config, }, nil } func (c *ConsulRegistry) Register(ctx context.Context, instance *ServiceInstance) error { registration := \u0026amp;api.AgentServiceRegistration{ ID: instance.ID, Name: instance.Name, Tags: instance.Tags, Port: instance.Port, Address: instance.Address, Meta: instance.Metadata, Check: \u0026amp;api.AgentServiceCheck{ HTTP: fmt.Sprintf(\u0026#34;http://%s:%d/health\u0026#34;, instance.Address, instance.Port), Interval: \u0026#34;10s\u0026#34;, Timeout: \u0026#34;5s\u0026#34;, DeregisterCriticalServiceAfter: \u0026#34;30s\u0026#34;, }, } return c.client.Agent().ServiceRegister(registration) } func (c *ConsulRegistry) Deregister(ctx context.Context, instanceID string) error { return c.client.Agent().ServiceDeregister(instanceID) } func (c *ConsulRegistry) Discover(ctx context.Context, serviceName string) ([]*ServiceInstance, error) { services, _, err := c.client.Health().Service(serviceName, \u0026#34;\u0026#34;, true, nil) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to discover services: %w\u0026#34;, err) } var instances []*ServiceInstance for _, service := range services { health := \u0026#34;passing\u0026#34; for _, check := range service.Checks { if check.Status != \u0026#34;passing\u0026#34; { health = check.Status break } } instance := \u0026amp;ServiceInstance{ ID: service.Service.ID, Name: service.Service.Service, Address: service.Service.Address, Port: service.Service.Port, Tags: service.Service.Tags, Metadata: service.Service.Meta, Health: health, } instances = append(instances, instance) } return instances, nil } func (c *ConsulRegistry) Watch(ctx context.Context, serviceName string) (\u0026lt;-chan []*ServiceInstance, error) { ch := make(chan []*ServiceInstance, 1) go func() { defer close(ch) params := map[string]interface{}{ \u0026#34;type\u0026#34;: \u0026#34;service\u0026#34;, \u0026#34;service\u0026#34;: serviceName, } plan, err := api.WatchPlan(params) if err != nil { return } plan.Handler = func(idx uint64, data interface{}) { if entries, ok := data.([]*api.ServiceEntry); ok { var instances []*ServiceInstance for _, entry := range entries { health := \u0026#34;passing\u0026#34; for _, check := range entry.Checks { if check.Status != \u0026#34;passing\u0026#34; { health = check.Status break } } instance := \u0026amp;ServiceInstance{ ID: entry.Service.ID, Name: entry.Service.Service, Address: entry.Service.Address, Port: entry.Service.Port, Tags: entry.Service.Tags, Metadata: entry.Service.Meta, Health: health, } instances = append(instances, instance) } select { case ch \u0026lt;- instances: case \u0026lt;-ctx.Done(): return } } } go plan.RunWithContext(ctx) }() return ch, nil } func (c *ConsulRegistry) HealthCheck(ctx context.Context, instanceID string) error { checks, err := c.client.Agent().Checks() if err != nil { return fmt.Errorf(\u0026#34;failed to get health checks: %w\u0026#34;, err) } for _, check := range checks { if strings.Contains(check.ServiceID, instanceID) \u0026amp;\u0026amp; check.Status != \u0026#34;passing\u0026#34; { return fmt.Errorf(\u0026#34;service %s health check failing: %s\u0026#34;, instanceID, check.Output) } } return nil } func (c *ConsulRegistry) Close() error { return nil } Implementing etcd Service Discovery etcd excels at strong consistency and efficient watching. Here\u0026rsquo;s our etcd implementation:\n// internal/discovery/etcd.go package discovery import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;path\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; clientv3 \u0026#34;go.etcd.io/etcd/client/v3\u0026#34; ) type EtcdRegistry struct { client *clientv3.Client config *RegistryConfig leaseID clientv3.LeaseID ttl int64 keepalive \u0026lt;-chan *clientv3.LeaseKeepAliveResponse } func NewEtcdRegistry(config *RegistryConfig) (*EtcdRegistry, error) { etcdConfig := clientv3.Config{ Endpoints: []string{config.Address}, DialTimeout: config.Timeout, } if config.Username != \u0026#34;\u0026#34; { etcdConfig.Username = config.Username etcdConfig.Password = config.Password } client, err := clientv3.New(etcdConfig) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create etcd client: %w\u0026#34;, err) } registry := \u0026amp;EtcdRegistry{ client: client, config: config, ttl: 30, // 30 seconds TTL } // Create lease for service registration lease, err := client.Grant(context.Background(), registry.ttl) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create lease: %w\u0026#34;, err) } registry.leaseID = lease.ID // Start keepalive keepalive, kaerr := client.KeepAlive(context.Background(), lease.ID) if kaerr != nil { return nil, fmt.Errorf(\u0026#34;failed to start keepalive: %w\u0026#34;, kaerr) } registry.keepalive = keepalive // Consume keepalive messages go func() { for ka := range keepalive { _ = ka // Consume to prevent channel blocking } }() return registry, nil } func (e *EtcdRegistry) Register(ctx context.Context, instance *ServiceInstance) error { key := fmt.Sprintf(\u0026#34;/services/%s/%s\u0026#34;, instance.Name, instance.ID) // Add health status to metadata if instance.Metadata == nil { instance.Metadata = make(map[string]string) } instance.Metadata[\u0026#34;last_heartbeat\u0026#34;] = time.Now().Format(time.RFC3339) data, err := json.Marshal(instance) if err != nil { return fmt.Errorf(\u0026#34;failed to marshal instance: %w\u0026#34;, err) } _, err = e.client.Put(ctx, key, string(data), clientv3.WithLease(e.leaseID)) if err != nil { return fmt.Errorf(\u0026#34;failed to register service: %w\u0026#34;, err) } return nil } func (e *EtcdRegistry) Deregister(ctx context.Context, instanceID string) error { // Find and delete the key for this instance prefix := \u0026#34;/services/\u0026#34; resp, err := e.client.Get(ctx, prefix, clientv3.WithPrefix()) if err != nil { return fmt.Errorf(\u0026#34;failed to get services: %w\u0026#34;, err) } for _, kv := range resp.Kvs { var instance ServiceInstance if err := json.Unmarshal(kv.Value, \u0026amp;instance); err != nil { continue } if instance.ID == instanceID { _, err := e.client.Delete(ctx, string(kv.Key)) return err } } return fmt.Errorf(\u0026#34;instance %s not found\u0026#34;, instanceID) } func (e *EtcdRegistry) Discover(ctx context.Context, serviceName string) ([]*ServiceInstance, error) { prefix := fmt.Sprintf(\u0026#34;/services/%s/\u0026#34;, serviceName) resp, err := e.client.Get(ctx, prefix, clientv3.WithPrefix()) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to discover services: %w\u0026#34;, err) } var instances []*ServiceInstance for _, kv := range resp.Kvs { var instance ServiceInstance if err := json.Unmarshal(kv.Value, \u0026amp;instance); err != nil { continue } // Check if service is still healthy based on last heartbeat if lastHeartbeat, exists := instance.Metadata[\u0026#34;last_heartbeat\u0026#34;]; exists { if heartbeatTime, err := time.Parse(time.RFC3339, lastHeartbeat); err == nil { if time.Since(heartbeatTime) \u0026gt; time.Duration(e.ttl)*time.Second { instance.Health = \u0026#34;critical\u0026#34; } else { instance.Health = \u0026#34;passing\u0026#34; } } } instances = append(instances, \u0026amp;instance) } return instances, nil } func (e *EtcdRegistry) Watch(ctx context.Context, serviceName string) (\u0026lt;-chan []*ServiceInstance, error) { ch := make(chan []*ServiceInstance, 1) prefix := fmt.Sprintf(\u0026#34;/services/%s/\u0026#34;, serviceName) go func() { defer close(ch) // Send initial state if instances, err := e.Discover(ctx, serviceName); err == nil { select { case ch \u0026lt;- instances: case \u0026lt;-ctx.Done(): return } } // Watch for changes watchCh := e.client.Watch(ctx, prefix, clientv3.WithPrefix()) for watchResp := range watchCh { if watchResp.Err() != nil { return } // Fetch current state after any change if instances, err := e.Discover(ctx, serviceName); err == nil { select { case ch \u0026lt;- instances: case \u0026lt;-ctx.Done(): return } } } }() return ch, nil } func (e *EtcdRegistry) HealthCheck(ctx context.Context, instanceID string) error { instances, err := e.Discover(ctx, \u0026#34;\u0026#34;) if err != nil { return fmt.Errorf(\u0026#34;failed to get instances for health check: %w\u0026#34;, err) } for _, instance := range instances { if instance.ID == instanceID { if instance.Health == \u0026#34;passing\u0026#34; { return nil } return fmt.Errorf(\u0026#34;instance %s is unhealthy: %s\u0026#34;, instanceID, instance.Health) } } return fmt.Errorf(\u0026#34;instance %s not found\u0026#34;, instanceID) } func (e *EtcdRegistry) Close() error { if e.leaseID != 0 { _, err := e.client.Revoke(context.Background(), e.leaseID) if err != nil { return fmt.Errorf(\u0026#34;failed to revoke lease: %w\u0026#34;, err) } } return e.client.Close() } Creating a Service that Registers Itself Now let\u0026rsquo;s build a user service that automatically registers with our service discovery system. This demonstrates the self-registration pattern common in microservices:\n// internal/service/userservice.go package service import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;service-discovery-go/internal/discovery\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; ) type UserService struct { registry discovery.ServiceRegistry instance *discovery.ServiceInstance server *http.Server shutdownCh chan os.Signal } type User struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Created time.Time `json:\u0026#34;created\u0026#34;` } func NewUserService(registry discovery.ServiceRegistry, address string, port int) (*UserService, error) { instanceID := uuid.New().String() instance := \u0026amp;discovery.ServiceInstance{ ID: instanceID, Name: \u0026#34;user-service\u0026#34;, Address: address, Port: port, Tags: []string{\u0026#34;user\u0026#34;, \u0026#34;api\u0026#34;, \u0026#34;v1\u0026#34;}, Metadata: map[string]string{ \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-east-1\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;us-east-1a\u0026#34;, }, Health: \u0026#34;passing\u0026#34;, } service := \u0026amp;UserService{ registry: registry, instance: instance, shutdownCh: make(chan os.Signal, 1), } // Setup HTTP routes router := mux.NewRouter() router.HandleFunc(\u0026#34;/health\u0026#34;, service.healthHandler).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/users\u0026#34;, service.getUsersHandler).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/users/{id}\u0026#34;, service.getUserHandler).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/info\u0026#34;, service.infoHandler).Methods(\u0026#34;GET\u0026#34;) service.server = \u0026amp;http.Server{ Addr: fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, address, port), Handler: router, ReadTimeout: 15 * time.Second, WriteTimeout: 15 * time.Second, } return service, nil } func (u *UserService) Start(ctx context.Context) error { // Register service if err := u.registry.Register(ctx, u.instance); err != nil { return fmt.Errorf(\u0026#34;failed to register service: %w\u0026#34;, err) } log.Printf(\u0026#34;User service registered with ID: %s\u0026#34;, u.instance.ID) // Setup graceful shutdown signal.Notify(u.shutdownCh, syscall.SIGINT, syscall.SIGTERM) // Start HTTP server go func() { log.Printf(\u0026#34;User service starting on %s\u0026#34;, u.server.Addr) if err := u.server.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { log.Printf(\u0026#34;HTTP server error: %v\u0026#34;, err) } }() // Start periodic health updates for etcd go u.startPeriodicHealthUpdate(ctx) // Wait for shutdown signal \u0026lt;-u.shutdownCh log.Println(\u0026#34;Shutting down user service...\u0026#34;) // Deregister service if err := u.registry.Deregister(ctx, u.instance.ID); err != nil { log.Printf(\u0026#34;Failed to deregister service: %v\u0026#34;, err) } // Shutdown HTTP server shutdownCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() return u.server.Shutdown(shutdownCtx) } func (u *UserService) startPeriodicHealthUpdate(ctx context.Context) { ticker := time.NewTicker(10 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-ticker.C: // Update metadata with current timestamp for etcd health checking u.instance.Metadata[\u0026#34;last_heartbeat\u0026#34;] = time.Now().Format(time.RFC3339) if err := u.registry.Register(ctx, u.instance); err != nil { log.Printf(\u0026#34;Failed to update service registration: %v\u0026#34;, err) } case \u0026lt;-ctx.Done(): return } } } func (u *UserService) healthHandler(w http.ResponseWriter, r *http.Request) { health := map[string]interface{}{ \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: time.Now().Format(time.RFC3339), \u0026#34;service\u0026#34;: u.instance.Name, \u0026#34;instance\u0026#34;: u.instance.ID, \u0026#34;version\u0026#34;: u.instance.Metadata[\u0026#34;version\u0026#34;], } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(health) } func (u *UserService) getUsersHandler(w http.ResponseWriter, r *http.Request) { users := []User{ { ID: \u0026#34;1\u0026#34;, Name: \u0026#34;John Doe\u0026#34;, Email: \u0026#34;john@example.com\u0026#34;, Created: time.Now().Add(-24 * time.Hour), }, { ID: \u0026#34;2\u0026#34;, Name: \u0026#34;Jane Smith\u0026#34;, Email: \u0026#34;jane@example.com\u0026#34;, Created: time.Now().Add(-12 * time.Hour), }, } response := map[string]interface{}{ \u0026#34;users\u0026#34;: users, \u0026#34;total\u0026#34;: len(users), \u0026#34;served_by\u0026#34;: u.instance.ID, } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(response) } func (u *UserService) getUserHandler(w http.ResponseWriter, r *http.Request) { vars := mux.Vars(r) userID := vars[\u0026#34;id\u0026#34;] user := User{ ID: userID, Name: fmt.Sprintf(\u0026#34;User %s\u0026#34;, userID), Email: fmt.Sprintf(\u0026#34;user%s@example.com\u0026#34;, userID), Created: time.Now().Add(-6 * time.Hour), } response := map[string]interface{}{ \u0026#34;user\u0026#34;: user, \u0026#34;served_by\u0026#34;: u.instance.ID, } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(response) } func (u *UserService) infoHandler(w http.ResponseWriter, r *http.Request) { info := map[string]interface{}{ \u0026#34;service\u0026#34;: u.instance.Name, \u0026#34;instance\u0026#34;: u.instance.ID, \u0026#34;address\u0026#34;: u.instance.Address, \u0026#34;port\u0026#34;: u.instance.Port, \u0026#34;tags\u0026#34;: u.instance.Tags, \u0026#34;metadata\u0026#34;: u.instance.Metadata, \u0026#34;health\u0026#34;: u.instance.Health, } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(info) } Building a Service Discovery Client Now let\u0026rsquo;s create a client that discovers and communicates with services. This demonstrates how to build resilient clients that adapt to changing service topology:\n// cmd/api-gateway/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;service-discovery-go/internal/discovery\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; ) type APIGateway struct { registry discovery.ServiceRegistry services map[string][]*discovery.ServiceInstance httpClient *http.Client } func NewAPIGateway(registry discovery.ServiceRegistry) *APIGateway { return \u0026amp;APIGateway{ registry: registry, services: make(map[string][]*discovery.ServiceInstance), httpClient: \u0026amp;http.Client{ Timeout: 10 * time.Second, }, } } func (gw *APIGateway) Start(ctx context.Context) error { // Start watching for user service instances go gw.watchService(ctx, \u0026#34;user-service\u0026#34;) // Setup HTTP routes router := mux.NewRouter() router.HandleFunc(\u0026#34;/api/users\u0026#34;, gw.proxyToUserService).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/api/users/{id}\u0026#34;, gw.proxyToUserService).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/api/services\u0026#34;, gw.listServices).Methods(\u0026#34;GET\u0026#34;) router.HandleFunc(\u0026#34;/health\u0026#34;, gw.healthHandler).Methods(\u0026#34;GET\u0026#34;) server := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } log.Println(\u0026#34;API Gateway starting on :8080\u0026#34;) return server.ListenAndServe() } func (gw *APIGateway) watchService(ctx context.Context, serviceName string) { watchCh, err := gw.registry.Watch(ctx, serviceName) if err != nil { log.Printf(\u0026#34;Failed to watch service %s: %v\u0026#34;, serviceName, err) return } for { select { case instances := \u0026lt;-watchCh: if instances == nil { log.Printf(\u0026#34;Watch channel closed for service %s\u0026#34;, serviceName) return } // Filter healthy instances var healthyInstances []*discovery.ServiceInstance for _, instance := range instances { if instance.Health == \u0026#34;passing\u0026#34; { healthyInstances = append(healthyInstances, instance) } } gw.services[serviceName] = healthyInstances log.Printf(\u0026#34;Updated %s instances: %d healthy out of %d total\u0026#34;, serviceName, len(healthyInstances), len(instances)) case \u0026lt;-ctx.Done(): return } } } func (gw *APIGateway) getHealthyInstance(serviceName string) *discovery.ServiceInstance { instances := gw.services[serviceName] if len(instances) == 0 { return nil } // Simple random load balancing return instances[rand.Intn(len(instances))] } func (gw *APIGateway) proxyToUserService(w http.ResponseWriter, r *http.Request) { instance := gw.getHealthyInstance(\u0026#34;user-service\u0026#34;) if instance == nil { http.Error(w, \u0026#34;No healthy user service instances available\u0026#34;, http.StatusServiceUnavailable) return } // Build target URL targetURL := fmt.Sprintf(\u0026#34;http://%s:%d%s\u0026#34;, instance.Address, instance.Port, r.URL.Path) if r.URL.RawQuery != \u0026#34;\u0026#34; { targetURL += \u0026#34;?\u0026#34; + r.URL.RawQuery } // Create proxy request proxyReq, err := http.NewRequest(r.Method, targetURL, r.Body) if err != nil { http.Error(w, \u0026#34;Failed to create proxy request\u0026#34;, http.StatusInternalServerError) return } // Copy headers for key, values := range r.Header { for _, value := range values { proxyReq.Header.Add(key, value) } } // Add tracking headers proxyReq.Header.Set(\u0026#34;X-Gateway-Instance\u0026#34;, instance.ID) proxyReq.Header.Set(\u0026#34;X-Gateway-Time\u0026#34;, time.Now().Format(time.RFC3339)) // Make request resp, err := gw.httpClient.Do(proxyReq) if err != nil { http.Error(w, \u0026#34;Failed to proxy request\u0026#34;, http.StatusBadGateway) return } defer resp.Body.Close() // Copy response headers for key, values := range resp.Header { for _, value := range values { w.Header().Add(key, value) } } w.WriteHeader(resp.StatusCode) // Copy response body buf := make([]byte, 32*1024) for { n, err := resp.Body.Read(buf) if n \u0026gt; 0 { w.Write(buf[:n]) } if err != nil { break } } } func (gw *APIGateway) listServices(w http.ResponseWriter, r *http.Request) { services := make(map[string]interface{}) for serviceName, instances := range gw.services { serviceInfo := map[string]interface{}{ \u0026#34;name\u0026#34;: serviceName, \u0026#34;instances\u0026#34;: len(instances), \u0026#34;healthy\u0026#34;: len(instances), // All stored instances are healthy \u0026#34;endpoints\u0026#34;: make([]string, 0, len(instances)), } for _, instance := range instances { endpoint := fmt.Sprintf(\u0026#34;%s:%d\u0026#34;, instance.Address, instance.Port) serviceInfo[\u0026#34;endpoints\u0026#34;] = append(serviceInfo[\u0026#34;endpoints\u0026#34;].([]string), endpoint) } services[serviceName] = serviceInfo } response := map[string]interface{}{ \u0026#34;services\u0026#34;: services, \u0026#34;timestamp\u0026#34;: time.Now().Format(time.RFC3339), } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(response) } func (gw *APIGateway) healthHandler(w http.ResponseWriter, r *http.Request) { health := map[string]interface{}{ \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: time.Now().Format(time.RFC3339), \u0026#34;services\u0026#34;: len(gw.services), } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(health) } func main() { // Create service registry (choose consul or etcd) registryType := \u0026#34;consul\u0026#34; // or \u0026#34;etcd\u0026#34; config := \u0026amp;discovery.RegistryConfig{ Type: registryType, Address: \u0026#34;localhost:8500\u0026#34;, // consul address, change to \u0026#34;localhost:2379\u0026#34; for etcd Timeout: 10 * time.Second, } var registry discovery.ServiceRegistry var err error switch registryType { case \u0026#34;consul\u0026#34;: registry, err = discovery.NewConsulRegistry(config) case \u0026#34;etcd\u0026#34;: config.Address = \u0026#34;localhost:2379\u0026#34; registry, err = discovery.NewEtcdRegistry(config) default: log.Fatal(\u0026#34;Unknown registry type\u0026#34;) } if err != nil { log.Fatalf(\u0026#34;Failed to create registry: %v\u0026#34;, err) } defer registry.Close() // Create and start API gateway gateway := NewAPIGateway(registry) ctx := context.Background() if err := gateway.Start(ctx); err != nil { log.Fatalf(\u0026#34;Failed to start gateway: %v\u0026#34;, err) } } Testing Your Service Discovery Implementation Let\u0026rsquo;s create a comprehensive test setup. First, start Consul or etcd:\nFor Consul:\nconsul agent -dev -node machine -bind=127.0.0.1 -enable-script-checks For etcd:\netcd --listen-client-urls \u0026#39;http://localhost:2379\u0026#39; \\ --advertise-client-urls \u0026#39;http://localhost:2379\u0026#39; Now create the user service executable:\n// cmd/user-service/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;service-discovery-go/internal/discovery\u0026#34; \u0026#34;service-discovery-go/internal/service\u0026#34; ) func main() { var ( registryType = flag.String(\u0026#34;registry\u0026#34;, \u0026#34;consul\u0026#34;, \u0026#34;Registry type (consul or etcd)\u0026#34;) address = flag.String(\u0026#34;address\u0026#34;, \u0026#34;localhost\u0026#34;, \u0026#34;Service address\u0026#34;) port = flag.Int(\u0026#34;port\u0026#34;, 8081, \u0026#34;Service port\u0026#34;) ) flag.Parse() config := \u0026amp;discovery.RegistryConfig{ Type: *registryType, Address: \u0026#34;localhost:8500\u0026#34;, // Default consul Timeout: 10 * time.Second, } if *registryType == \u0026#34;etcd\u0026#34; { config.Address = \u0026#34;localhost:2379\u0026#34; } var registry discovery.ServiceRegistry var err error switch *registryType { case \u0026#34;consul\u0026#34;: registry, err = discovery.NewConsulRegistry(config) case \u0026#34;etcd\u0026#34;: registry, err = discovery.NewEtcdRegistry(config) default: log.Fatal(\u0026#34;Unknown registry type\u0026#34;) } if err != nil { log.Fatalf(\u0026#34;Failed to create registry: %v\u0026#34;, err) } defer registry.Close() userService, err := service.NewUserService(registry, *address, *port) if err != nil { log.Fatalf(\u0026#34;Failed to create user service: %v\u0026#34;, err) } ctx := context.Background() if err := userService.Start(ctx); err != nil { log.Fatalf(\u0026#34;Failed to start user service: %v\u0026#34;, err) } } Test the complete setup:\n# Terminal 1: Start multiple user service instances go run cmd/user-service/main.go -port=8081 -registry=consul go run cmd/user-service/main.go -port=8082 -registry=consul go run cmd/user-service/main.go -port=8083 -registry=consul # Terminal 2: Start API gateway go run cmd/api-gateway/main.go # Terminal 3: Test the system curl http://localhost:8080/api/services # List discovered services curl http://localhost:8080/api/users # Get users (load balanced) curl http://localhost:8080/api/users/123 # Get specific user Production Considerations and Best Practices When deploying service discovery in production, several factors become critical. Network partitions and service registry unavailability should never bring down your entire system. Implement circuit breakers and fallback mechanisms that allow services to continue operating with cached service information.\nSecurity is paramount - both Consul and etcd support authentication and encryption. In production environments, always enable TLS and implement proper access controls. Consider using service mesh solutions like Istio that provide additional security layers.\nPerformance tuning matters at scale. Consul\u0026rsquo;s gossip protocol works well for clusters up to several hundred nodes, while etcd\u0026rsquo;s consensus mechanism provides stronger consistency guarantees but may require more careful capacity planning. Monitor key metrics like service discovery latency, registry load, and health check performance.\nConsider integrating service discovery with your monitoring and alerting systems. Tools like structured logging in Go using slog can help you track service topology changes and debug discovery issues.\nAdvanced Service Discovery Patterns This implementation covers the fundamentals, but production systems often need additional patterns. Consider implementing service mesh integration, where your service discovery integrates with tools like Envoy or Linkerd for advanced traffic management.\nCircuit breakers and bulkhead patterns become essential at scale. When a service is discovered but unhealthy, you need sophisticated strategies for handling partial failures while maintaining system availability.\nFor high-traffic environments, consider implementing client-side caching of service discovery information with TTL-based invalidation. This reduces load on your service registry and improves request latency.\nConfiguration management often integrates closely with service discovery. Both Consul and etcd can store configuration data alongside service registration information, enabling dynamic configuration updates without service restarts.\nBuilding robust service discovery in Go requires understanding both the technical implementation and the operational patterns that make systems resilient at scale. This foundation gives you the tools to build systems that can grow with your needs while maintaining reliability and performance. Whether you choose Consul for its operational simplicity or etcd for its consistency guarantees, the patterns demonstrated here will serve you well in production environments.\nThe key is starting simple and evolving your implementation as your requirements grow. Begin with basic registration and discovery, then add health checking, watching, and advanced routing as your system matures. With Go\u0026rsquo;s excellent concurrency support and these robust service discovery tools, you have everything needed to build world-class distributed systems.\n","href":"/2025/09/service-discovery-microservices-golang-consul-etcd.html","title":"Service Discovery in Microservices Golang - Consul and etcd Implementation"},{"content":"When you\u0026rsquo;re building distributed systems, one component stands between chaos and order: the API Gateway. Think of it as the bouncer at an exclusive club - it decides who gets in, where they go, and how fast they can enter. After working with various microservice architectures, I can tell you that a well-implemented API Gateway is often the difference between a system that scales gracefully and one that crumbles under pressure.\nBuilding an API Gateway might sound intimidating, but Go\u0026rsquo;s excellent standard library and concurrency model make it surprisingly straightforward. Today, we\u0026rsquo;ll build a complete API Gateway that handles load balancing across multiple backend services and implements intelligent rate limiting to protect your infrastructure from abuse.\nThe beauty of implementing this in Go lies in its simplicity and performance. While enterprise solutions like Kong or AWS API Gateway are fantastic, sometimes you need something tailored to your specific requirements. Plus, understanding how these systems work under the hood makes you a better architect.\nWhy You Need an API Gateway Before diving into implementation, let\u0026rsquo;s understand why API Gateways have become essential in modern architectures. In a monolithic application, you typically have one entry point. But when you break that monolith into microservices, suddenly your client applications need to know about dozens of different service endpoints.\nAn API Gateway solves several critical problems. First, it provides a single entry point for all client requests, hiding the complexity of your backend architecture. Your mobile app doesn\u0026rsquo;t need to know whether user authentication lives on one server while payment processing happens on another.\nSecond, it centralizes cross-cutting concerns like authentication, logging, and monitoring. Instead of implementing these features in every microservice, you handle them once at the gateway level. This reduces code duplication and ensures consistent behavior across your entire API surface.\nMost importantly for our discussion today, it provides traffic management capabilities. Load balancing ensures your requests are distributed efficiently across healthy backend instances, while rate limiting protects your services from being overwhelmed by traffic spikes or malicious attacks.\nSetting Up the Project Structure Let\u0026rsquo;s start with a clean project structure that will make our code maintainable and testable. If you\u0026rsquo;re new to Go project organization, check out our guide on structuring Go projects with clean architecture for more detailed insights.\nmkdir api-gateway cd api-gateway go mod init api-gateway Create the following directory structure:\napi-gateway/ ├── main.go ├── internal/ │ ├── gateway/ │ │ ├── gateway.go │ │ ├── loadbalancer.go │ │ └── ratelimiter.go │ ├── config/ │ │ └── config.go │ └── middleware/ │ ├── logging.go │ └── recovery.go ├── pkg/ │ └── healthcheck/ │ └── healthcheck.go └── configs/ └── gateway.yaml Install the required dependencies:\ngo get github.com/gorilla/mux go get github.com/go-redis/redis/v8 go get golang.org/x/time/rate go get gopkg.in/yaml.v2 Building the Core Gateway Structure Let\u0026rsquo;s start with the configuration structure that will drive our gateway behavior:\n// internal/config/config.go package config import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;gopkg.in/yaml.v2\u0026#34; ) type Config struct { Gateway GatewayConfig `yaml:\u0026#34;gateway\u0026#34;` Services []ServiceConfig `yaml:\u0026#34;services\u0026#34;` RateLimit RateLimitConfig `yaml:\u0026#34;rateLimit\u0026#34;` } type GatewayConfig struct { Port string `yaml:\u0026#34;port\u0026#34;` Timeout int `yaml:\u0026#34;timeout\u0026#34;` } type ServiceConfig struct { Name string `yaml:\u0026#34;name\u0026#34;` Path string `yaml:\u0026#34;path\u0026#34;` Instances []string `yaml:\u0026#34;instances\u0026#34;` Strategy string `yaml:\u0026#34;strategy\u0026#34;` } type RateLimitConfig struct { RequestsPerSecond int `yaml:\u0026#34;requestsPerSecond\u0026#34;` BurstSize int `yaml:\u0026#34;burstSize\u0026#34;` RedisURL string `yaml:\u0026#34;redisURL\u0026#34;` } func LoadConfig(path string) (*Config, error) { data, err := ioutil.ReadFile(path) if err != nil { return nil, err } var config Config err = yaml.Unmarshal(data, \u0026amp;config) if err != nil { return nil, err } return \u0026amp;config, nil } Create a configuration file that defines your services and policies:\n# configs/gateway.yaml gateway: port: \u0026#34;:8080\u0026#34; timeout: 30 services: - name: \u0026#34;user-service\u0026#34; path: \u0026#34;/api/users\u0026#34; instances: - \u0026#34;http://localhost:8081\u0026#34; - \u0026#34;http://localhost:8082\u0026#34; - \u0026#34;http://localhost:8083\u0026#34; strategy: \u0026#34;round_robin\u0026#34; - name: \u0026#34;order-service\u0026#34; path: \u0026#34;/api/orders\u0026#34; instances: - \u0026#34;http://localhost:8084\u0026#34; - \u0026#34;http://localhost:8085\u0026#34; strategy: \u0026#34;weighted_round_robin\u0026#34; rateLimit: requestsPerSecond: 100 burstSize: 10 redisURL: \u0026#34;redis://localhost:6379\u0026#34; Implementing Load Balancing Algorithms Now let\u0026rsquo;s implement the heart of our gateway - the load balancer. We\u0026rsquo;ll support multiple algorithms because different scenarios call for different strategies:\n// internal/gateway/loadbalancer.go package gateway import ( \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;net/url\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; ) type Backend struct { URL *url.URL Alive bool ReverseProxy *httputil.ReverseProxy Weight int Connections int64 } type LoadBalancer interface { GetNextBackend() *Backend MarkBackendStatus(backend *Backend, alive bool) } type RoundRobinBalancer struct { backends []*Backend current uint64 mutex sync.RWMutex } func NewRoundRobinBalancer(urls []string) *RoundRobinBalancer { var backends []*Backend for _, u := range urls { url, _ := url.Parse(u) backend := \u0026amp;Backend{ URL: url, Alive: true, ReverseProxy: httputil.NewSingleHostReverseProxy(url), Weight: 1, } backends = append(backends, backend) } return \u0026amp;RoundRobinBalancer{ backends: backends, } } func (rb *RoundRobinBalancer) GetNextBackend() *Backend { rb.mutex.RLock() defer rb.mutex.RUnlock() if len(rb.backends) == 0 { return nil } next := atomic.AddUint64(\u0026amp;rb.current, 1) return rb.backends[(next-1)%uint64(len(rb.backends))] } func (rb *RoundRobinBalancer) MarkBackendStatus(backend *Backend, alive bool) { rb.mutex.Lock() defer rb.mutex.Unlock() backend.Alive = alive } type WeightedRoundRobinBalancer struct { backends []*Backend currentWeights []int totalWeight int mutex sync.RWMutex } func NewWeightedRoundRobinBalancer(urls []string, weights []int) *WeightedRoundRobinBalancer { var backends []*Backend totalWeight := 0 for i, u := range urls { url, _ := url.Parse(u) weight := 1 if i \u0026lt; len(weights) { weight = weights[i] } backend := \u0026amp;Backend{ URL: url, Alive: true, ReverseProxy: httputil.NewSingleHostReverseProxy(url), Weight: weight, } backends = append(backends, backend) totalWeight += weight } return \u0026amp;WeightedRoundRobinBalancer{ backends: backends, currentWeights: make([]int, len(backends)), totalWeight: totalWeight, } } func (wrb *WeightedRoundRobinBalancer) GetNextBackend() *Backend { wrb.mutex.Lock() defer wrb.mutex.Unlock() if len(wrb.backends) == 0 { return nil } var selected *Backend maxWeight := -1 for i, backend := range wrb.backends { if !backend.Alive { continue } wrb.currentWeights[i] += backend.Weight if wrb.currentWeights[i] \u0026gt; maxWeight { maxWeight = wrb.currentWeights[i] selected = backend } } if selected != nil { for i, backend := range wrb.backends { if backend == selected { wrb.currentWeights[i] -= wrb.totalWeight break } } } return selected } func (wrb *WeightedRoundRobinBalancer) MarkBackendStatus(backend *Backend, alive bool) { wrb.mutex.Lock() defer wrb.mutex.Unlock() backend.Alive = alive } Implementing Rate Limiting Rate limiting is crucial for protecting your backend services from abuse. We\u0026rsquo;ll implement both in-memory and Redis-based rate limiting:\n// internal/gateway/ratelimiter.go package gateway import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; \u0026#34;golang.org/x/time/rate\u0026#34; ) type RateLimiter interface { Allow(clientID string) bool } type InMemoryRateLimiter struct { limiters map[string]*rate.Limiter rate rate.Limit burst int } func NewInMemoryRateLimiter(r rate.Limit, b int) *InMemoryRateLimiter { return \u0026amp;InMemoryRateLimiter{ limiters: make(map[string]*rate.Limiter), rate: r, burst: b, } } func (rl *InMemoryRateLimiter) Allow(clientID string) bool { limiter, exists := rl.limiters[clientID] if !exists { limiter = rate.NewLimiter(rl.rate, rl.burst) rl.limiters[clientID] = limiter } return limiter.Allow() } type RedisRateLimiter struct { client *redis.Client window time.Duration limit int64 } func NewRedisRateLimiter(redisURL string, window time.Duration, limit int64) (*RedisRateLimiter, error) { opt, err := redis.ParseURL(redisURL) if err != nil { return nil, err } client := redis.NewClient(opt) return \u0026amp;RedisRateLimiter{ client: client, window: window, limit: limit, }, nil } func (rl *RedisRateLimiter) Allow(clientID string) bool { ctx := context.Background() now := time.Now() key := fmt.Sprintf(\u0026#34;rate_limit:%s\u0026#34;, clientID) pipe := rl.client.TxPipeline() // Remove expired entries pipe.ZRemRangeByScore(ctx, key, \u0026#34;0\u0026#34;, fmt.Sprintf(\u0026#34;%d\u0026#34;, now.Add(-rl.window).UnixNano())) // Add current request pipe.ZAdd(ctx, key, \u0026amp;redis.Z{ Score: float64(now.UnixNano()), Member: now.UnixNano(), }) // Count requests in window countCmd := pipe.ZCard(ctx, key) // Set expiration pipe.Expire(ctx, key, rl.window) _, err := pipe.Exec(ctx) if err != nil { return false } count := countCmd.Val() return count \u0026lt;= rl.limit } Building the Gateway Core Now let\u0026rsquo;s tie everything together in our main gateway implementation:\n// internal/gateway/gateway.go package gateway import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;api-gateway/internal/config\u0026#34; ) type Gateway struct { config *config.Config loadBalancer map[string]LoadBalancer rateLimiter RateLimiter } func NewGateway(cfg *config.Config) (*Gateway, error) { gateway := \u0026amp;Gateway{ config: cfg, loadBalancer: make(map[string]LoadBalancer), } // Initialize load balancers for each service for _, service := range cfg.Services { switch service.Strategy { case \u0026#34;round_robin\u0026#34;: gateway.loadBalancer[service.Name] = NewRoundRobinBalancer(service.Instances) case \u0026#34;weighted_round_robin\u0026#34;: // For simplicity, using equal weights. In production, you\u0026#39;d read weights from config weights := make([]int, len(service.Instances)) for i := range weights { weights[i] = 1 } gateway.loadBalancer[service.Name] = NewWeightedRoundRobinBalancer(service.Instances, weights) default: gateway.loadBalancer[service.Name] = NewRoundRobinBalancer(service.Instances) } } // Initialize rate limiter if cfg.RateLimit.RedisURL != \u0026#34;\u0026#34; { rl, err := NewRedisRateLimiter(cfg.RateLimit.RedisURL, time.Second, int64(cfg.RateLimit.RequestsPerSecond)) if err != nil { return nil, err } gateway.rateLimiter = rl } else { gateway.rateLimiter = NewInMemoryRateLimiter( rate.Limit(cfg.RateLimit.RequestsPerSecond), cfg.RateLimit.BurstSize, ) } return gateway, nil } func (g *Gateway) ServeHTTP(w http.ResponseWriter, r *http.Request) { // Rate limiting clientID := g.getClientID(r) if !g.rateLimiter.Allow(clientID) { http.Error(w, \u0026#34;Rate limit exceeded\u0026#34;, http.StatusTooManyRequests) return } // Find matching service service := g.findService(r.URL.Path) if service == nil { http.Error(w, \u0026#34;Service not found\u0026#34;, http.StatusNotFound) return } // Get backend from load balancer lb := g.loadBalancer[service.Name] backend := lb.GetNextBackend() if backend == nil || !backend.Alive { http.Error(w, \u0026#34;No healthy backends available\u0026#34;, http.StatusServiceUnavailable) return } // Update request path r.URL.Path = strings.TrimPrefix(r.URL.Path, service.Path) r.URL.Host = backend.URL.Host r.URL.Scheme = backend.URL.Scheme r.Header.Set(\u0026#34;X-Forwarded-Host\u0026#34;, r.Header.Get(\u0026#34;Host\u0026#34;)) r.Header.Set(\u0026#34;X-Origin-Host\u0026#34;, backend.URL.Host) // Proxy the request backend.ReverseProxy.ServeHTTP(w, r) } func (g *Gateway) getClientID(r *http.Request) string { // In production, you might use API keys, JWT sub claims, or IP addresses clientIP := r.Header.Get(\u0026#34;X-Forwarded-For\u0026#34;) if clientIP == \u0026#34;\u0026#34; { clientIP = r.Header.Get(\u0026#34;X-Real-IP\u0026#34;) } if clientIP == \u0026#34;\u0026#34; { clientIP = r.RemoteAddr } return clientIP } func (g *Gateway) findService(path string) *config.ServiceConfig { for _, service := range g.config.Services { if strings.HasPrefix(path, service.Path) { return \u0026amp;service } } return nil } Adding Health Checks and Monitoring A production API Gateway needs robust health checking to remove unhealthy backends from rotation:\n// pkg/healthcheck/healthcheck.go package healthcheck import ( \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) func IsBackendHealthy(url string, timeout time.Duration) bool { client := \u0026amp;http.Client{ Timeout: timeout, } resp, err := client.Get(url + \u0026#34;/health\u0026#34;) if err != nil { return false } defer resp.Body.Close() return resp.StatusCode == http.StatusOK } func StartHealthChecker(backends []*Backend, interval time.Duration) { ticker := time.NewTicker(interval) go func() { for range ticker.C { for _, backend := range backends { healthy := IsBackendHealthy(backend.URL.String(), 5*time.Second) backend.Alive = healthy } } }() } Putting It All Together Finally, let\u0026rsquo;s create our main application that brings all components together:\n// main.go package main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;api-gateway/internal/config\u0026#34; \u0026#34;api-gateway/internal/gateway\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; ) func main() { // Load configuration cfg, err := config.LoadConfig(\u0026#34;configs/gateway.yaml\u0026#34;) if err != nil { log.Fatal(\u0026#34;Failed to load config:\u0026#34;, err) } // Create gateway gw, err := gateway.NewGateway(cfg) if err != nil { log.Fatal(\u0026#34;Failed to create gateway:\u0026#34;, err) } // Setup routes router := mux.NewRouter() router.PathPrefix(\u0026#34;/\u0026#34;).Handler(gw) // Configure server server := \u0026amp;http.Server{ Addr: cfg.Gateway.Port, Handler: router, ReadTimeout: time.Duration(cfg.Gateway.Timeout) * time.Second, WriteTimeout: time.Duration(cfg.Gateway.Timeout) * time.Second, } log.Printf(\u0026#34;API Gateway starting on %s\u0026#34;, cfg.Gateway.Port) log.Fatal(server.ListenAndServe()) } Testing Your API Gateway To test your gateway, you\u0026rsquo;ll need some backend services. Here\u0026rsquo;s a simple test server you can run on different ports:\n// test-server/main.go package main import ( \u0026#34;encoding/json\u0026#34; \u0026#34;flag\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; ) func main() { port := flag.String(\u0026#34;port\u0026#34;, \u0026#34;8081\u0026#34;, \u0026#34;Server port\u0026#34;) flag.Parse() http.HandleFunc(\u0026#34;/health\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(map[string]string{\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;port\u0026#34;: *port}) }) http.HandleFunc(\u0026#34;/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { hostname, _ := os.Hostname() response := map[string]string{ \u0026#34;message\u0026#34;: \u0026#34;Hello from backend\u0026#34;, \u0026#34;port\u0026#34;: *port, \u0026#34;hostname\u0026#34;: hostname, \u0026#34;path\u0026#34;: r.URL.Path, } json.NewEncoder(w).Encode(response) }) log.Printf(\u0026#34;Test server starting on :%s\u0026#34;, *port) log.Fatal(http.ListenAndServe(\u0026#34;:\u0026#34;+*port, nil)) } Run multiple instances:\ngo run test-server/main.go -port=8081 \u0026amp; go run test-server/main.go -port=8082 \u0026amp; go run test-server/main.go -port=8083 \u0026amp; Then start your gateway and test it:\ngo run main.go curl http://localhost:8080/api/users/profile Performance Considerations and Best Practices When building production API Gateways, performance is paramount. Go\u0026rsquo;s goroutines make it naturally well-suited for this task, but there are several optimizations to consider.\nFirst, connection pooling is crucial. The default HTTP client in Go reuses connections, but you should tune the transport settings based on your expected load. Consider setting MaxIdleConns and MaxIdleConnsPerHost appropriately.\nFor rate limiting, Redis provides better scalability across multiple gateway instances, but in-memory limiting offers lower latency. Choose based on your architecture - if you\u0026rsquo;re running a single gateway instance, in-memory might be sufficient.\nHealth checking frequency should balance between quick failure detection and unnecessary load on your backends. Start with 30-second intervals and adjust based on your requirements.\nMonitoring and observability are essential. Consider integrating with structured logging in Go using slog to get better insights into your gateway\u0026rsquo;s behavior.\nExtending Your Gateway This implementation provides a solid foundation, but production gateways often need additional features. Consider adding authentication middleware, request/response transformation, circuit breakers for handling backend failures gracefully, and metrics collection for monitoring.\nYou might also want to implement more sophisticated load balancing algorithms like least connections or consistent hashing, especially if you\u0026rsquo;re dealing with stateful backends or want to optimize cache hit rates.\nFor high-availability deployments, consider how you\u0026rsquo;ll handle configuration updates without downtime and how multiple gateway instances will coordinate, especially for features like rate limiting that require shared state.\nBuilding an API Gateway in Go gives you complete control over your traffic management logic while leveraging Go\u0026rsquo;s excellent performance characteristics. Start with this foundation and extend it based on your specific requirements. The modular design makes it easy to add new features without disrupting existing functionality.\nRemember that an API Gateway is a critical piece of infrastructure - invest time in testing, monitoring, and documentation. Your future self and your team will thank you when it\u0026rsquo;s 3 AM and you need to debug a production issue.\n","href":"/2025/09/api-gateway-golang-load-balancing-rate-limiting.html","title":"API Gateway with Golang - Load Balancing and Rate Limiting Implementation"},{"content":"Traditional request-response architectures work well for simple applications, but as systems grow in complexity and scale, they often become bottlenecks. Event-driven architecture gives you a better way to build systems by letting components talk to each other through messages instead of direct calls. When combined with Go\u0026rsquo;s excellent concurrency model and robust ecosystem, event-driven systems become powerful tools for building scalable, resilient applications.\nIn this guide, you\u0026rsquo;ll learn how to build event-driven systems using Go and message queues that actually work in production. We\u0026rsquo;ll explore event sourcing patterns, CQRS implementation, and practical strategies for building systems that can handle high throughput while maintaining data consistency and system reliability.\nUnderstanding Event-Driven Architecture Event-driven architecture is built around three main ideas: creating events when things happen, detecting those events, and doing something useful with them. An event represents something significant that happened in your system - a user registered, an order was placed, or a payment was processed. Unlike traditional synchronous architectures where components directly call each other, event-driven systems use events as the primary means of communication.\nEvery event-driven system has three main parts: services that create events when something important happens, services that listen for those events and do work, and a message system that gets events from creators to listeners reliably.\nThis approach has some real benefits. Your services don\u0026rsquo;t need to know about each other directly, which makes the system easier to maintain and lets you scale different parts independently. The asynchronous nature improves performance since operations don\u0026rsquo;t block waiting for responses. Additionally, the system becomes more resilient because failures in one component don\u0026rsquo;t immediately cascade to others.\nBut event-driven systems aren\u0026rsquo;t all sunshine and rainbows. Since things happen asynchronously, your data might not be immediately consistent everywhere. Debugging becomes more challenging because request flows span multiple components. Message ordering and delivery guarantees require careful consideration to ensure your system behaves correctly.\nChoosing the Right Message Queue System The message queue is the heart of your event-driven system. Each option gives you different trade-offs between speed, reliability, and how hard it is to run in production. Let\u0026rsquo;s examine the most popular options for Go applications.\nNATS provides lightweight, high-performance messaging with excellent Go support. It\u0026rsquo;s particularly well-suited for microservices communication and real-time applications. NATS offers different messaging patterns including publish-subscribe, request-reply, and queuing, with built-in load balancing and fault tolerance.\nRabbitMQ offers robust features including message persistence, complex routing, and guaranteed delivery. It supports multiple messaging patterns and provides excellent tooling for monitoring and management. RabbitMQ works well for enterprise applications that need reliable message delivery guarantees.\nApache Kafka excels at high-throughput scenarios and provides excellent durability through its distributed log architecture. Kafka is ideal for event sourcing implementations and systems that need to replay events. However, it has higher operational complexity compared to simpler solutions.\nFor this guide, we\u0026rsquo;ll focus primarily on NATS and RabbitMQ, as they provide good balance between features and simplicity for most Go applications.\nImplementing Event-Driven Patterns with NATS We\u0026rsquo;ll build a real-world example using NATS - an e-commerce order processing system that shows you the patterns you\u0026rsquo;ll actually use in production.\nFirst, install and set up NATS:\n# Install NATS server go get github.com/nats-io/nats-server/v2 go get github.com/nats-io/nats.go # Start NATS server (for development) nats-server Create the basic event infrastructure:\n// pkg/events/event.go package events import ( \u0026#34;encoding/json\u0026#34; \u0026#34;time\u0026#34; ) // Event represents a domain event in our system type Event struct { ID string `json:\u0026#34;id\u0026#34;` Type string `json:\u0026#34;type\u0026#34;` Source string `json:\u0026#34;source\u0026#34;` Data map[string]interface{} `json:\u0026#34;data\u0026#34;` Version string `json:\u0026#34;version\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` } // NewEvent creates a new event with required metadata func NewEvent(eventType, source string, data map[string]interface{}) *Event { return \u0026amp;Event{ ID: generateEventID(), Type: eventType, Source: source, Data: data, Version: \u0026#34;1.0\u0026#34;, Timestamp: time.Now(), } } // ToJSON converts the event to JSON for transmission func (e *Event) ToJSON() ([]byte, error) { return json.Marshal(e) } // FromJSON creates an event from JSON data func FromJSON(data []byte) (*Event, error) { var event Event err := json.Unmarshal(data, \u0026amp;event) return \u0026amp;event, err } func generateEventID() string { // Implementation would generate a unique ID // Using UUID or similar return fmt.Sprintf(\u0026#34;evt_%d\u0026#34;, time.Now().UnixNano()) } Implement the event publisher:\n// pkg/events/publisher.go package events import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/nats-io/nats.go\u0026#34; ) type Publisher struct { conn *nats.Conn } func NewPublisher(natsURL string) (*Publisher, error) { conn, err := nats.Connect(natsURL) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to connect to NATS: %w\u0026#34;, err) } return \u0026amp;Publisher{conn: conn}, nil } func (p *Publisher) Publish(subject string, event *Event) error { data, err := event.ToJSON() if err != nil { return fmt.Errorf(\u0026#34;failed to serialize event: %w\u0026#34;, err) } err = p.conn.Publish(subject, data) if err != nil { return fmt.Errorf(\u0026#34;failed to publish event: %w\u0026#34;, err) } log.Printf(\u0026#34;Published event %s to subject %s\u0026#34;, event.ID, subject) return nil } func (p *Publisher) PublishSync(subject string, event *Event) error { err := p.Publish(subject, event) if err != nil { return err } // Ensure message is delivered before returning return p.conn.Flush() } func (p *Publisher) Close() { p.conn.Close() } Create event consumers with different processing patterns:\n// pkg/events/consumer.go package events import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;github.com/nats-io/nats.go\u0026#34; ) type EventHandler func(ctx context.Context, event *Event) error type Consumer struct { conn *nats.Conn handlers map[string]EventHandler mu sync.RWMutex subscriptions []*nats.Subscription } func NewConsumer(natsURL string) (*Consumer, error) { conn, err := nats.Connect(natsURL) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to connect to NATS: %w\u0026#34;, err) } return \u0026amp;Consumer{ conn: conn, handlers: make(map[string]EventHandler), }, nil } func (c *Consumer) RegisterHandler(eventType string, handler EventHandler) { c.mu.Lock() defer c.mu.Unlock() c.handlers[eventType] = handler } func (c *Consumer) Subscribe(subject string) error { sub, err := c.conn.Subscribe(subject, c.handleMessage) if err != nil { return fmt.Errorf(\u0026#34;failed to subscribe to %s: %w\u0026#34;, subject, err) } c.subscriptions = append(c.subscriptions, sub) log.Printf(\u0026#34;Subscribed to subject: %s\u0026#34;, subject) return nil } func (c *Consumer) SubscribeQueue(subject, queue string) error { sub, err := c.conn.QueueSubscribe(subject, queue, c.handleMessage) if err != nil { return fmt.Errorf(\u0026#34;failed to queue subscribe to %s: %w\u0026#34;, subject, err) } c.subscriptions = append(c.subscriptions, sub) log.Printf(\u0026#34;Queue subscribed to subject: %s, queue: %s\u0026#34;, subject, queue) return nil } func (c *Consumer) handleMessage(msg *nats.Msg) { event, err := FromJSON(msg.Data) if err != nil { log.Printf(\u0026#34;Failed to parse event: %v\u0026#34;, err) return } c.mu.RLock() handler, exists := c.handlers[event.Type] c.mu.RUnlock() if !exists { log.Printf(\u0026#34;No handler registered for event type: %s\u0026#34;, event.Type) return } ctx := context.Background() if err := handler(ctx, event); err != nil { log.Printf(\u0026#34;Handler failed for event %s: %v\u0026#34;, event.ID, err) // In production, you might want to implement retry logic or dead letter queues } } func (c *Consumer) Close() { for _, sub := range c.subscriptions { sub.Unsubscribe() } c.conn.Close() } Building Domain Services with Event Publishing Now let\u0026rsquo;s implement domain services that publish events when important business operations occur:\n// internal/order/service.go package order import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;your-app/pkg/events\u0026#34; ) type Order struct { ID string `json:\u0026#34;id\u0026#34;` CustomerID string `json:\u0026#34;customer_id\u0026#34;` Items []Item `json:\u0026#34;items\u0026#34;` TotalAmount float64 `json:\u0026#34;total_amount\u0026#34;` Status string `json:\u0026#34;status\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type Item struct { ProductID string `json:\u0026#34;product_id\u0026#34;` Quantity int `json:\u0026#34;quantity\u0026#34;` Price float64 `json:\u0026#34;price\u0026#34;` } type Service struct { publisher events.Publisher repo Repository } func NewService(publisher events.Publisher, repo Repository) *Service { return \u0026amp;Service{ publisher: publisher, repo: repo, } } func (s *Service) CreateOrder(ctx context.Context, customerID string, items []Item) (*Order, error) { // Calculate total amount totalAmount := 0.0 for _, item := range items { totalAmount += item.Price * float64(item.Quantity) } order := \u0026amp;Order{ ID: generateOrderID(), CustomerID: customerID, Items: items, TotalAmount: totalAmount, Status: \u0026#34;pending\u0026#34;, CreatedAt: time.Now(), } // Save order to database err := s.repo.Save(ctx, order) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to save order: %w\u0026#34;, err) } // Publish order created event event := events.NewEvent(\u0026#34;order.created\u0026#34;, \u0026#34;order-service\u0026#34;, map[string]interface{}{ \u0026#34;order_id\u0026#34;: order.ID, \u0026#34;customer_id\u0026#34;: order.CustomerID, \u0026#34;total_amount\u0026#34;: order.TotalAmount, \u0026#34;items\u0026#34;: order.Items, }) err = s.publisher.Publish(\u0026#34;orders.events\u0026#34;, event) if err != nil { log.Printf(\u0026#34;Failed to publish order created event: %v\u0026#34;, err) // Note: In production, you might want to use the outbox pattern // to ensure events are published reliably } return order, nil } func (s *Service) UpdateOrderStatus(ctx context.Context, orderID, status string) error { order, err := s.repo.GetByID(ctx, orderID) if err != nil { return fmt.Errorf(\u0026#34;failed to get order: %w\u0026#34;, err) } previousStatus := order.Status order.Status = status err = s.repo.Update(ctx, order) if err != nil { return fmt.Errorf(\u0026#34;failed to update order: %w\u0026#34;, err) } // Publish status change event event := events.NewEvent(\u0026#34;order.status_changed\u0026#34;, \u0026#34;order-service\u0026#34;, map[string]interface{}{ \u0026#34;order_id\u0026#34;: order.ID, \u0026#34;previous_status\u0026#34;: previousStatus, \u0026#34;new_status\u0026#34;: status, \u0026#34;updated_at\u0026#34;: time.Now(), }) err = s.publisher.Publish(\u0026#34;orders.events\u0026#34;, event) if err != nil { log.Printf(\u0026#34;Failed to publish order status changed event: %v\u0026#34;, err) } return nil } func generateOrderID() string { return fmt.Sprintf(\u0026#34;order_%d\u0026#34;, time.Now().UnixNano()) } Implement event handlers for different services:\n// internal/inventory/event_handler.go package inventory import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;your-app/pkg/events\u0026#34; ) type EventHandler struct { service *Service } func NewEventHandler(service *Service) *EventHandler { return \u0026amp;EventHandler{service: service} } func (h *EventHandler) HandleOrderCreated(ctx context.Context, event *events.Event) error { orderID, ok := event.Data[\u0026#34;order_id\u0026#34;].(string) if !ok { return fmt.Errorf(\u0026#34;invalid order_id in event data\u0026#34;) } items, ok := event.Data[\u0026#34;items\u0026#34;].([]interface{}) if !ok { return fmt.Errorf(\u0026#34;invalid items in event data\u0026#34;) } log.Printf(\u0026#34;Processing inventory reservation for order: %s\u0026#34;, orderID) // Reserve inventory for each item for _, itemData := range items { item, ok := itemData.(map[string]interface{}) if !ok { continue } productID, _ := item[\u0026#34;product_id\u0026#34;].(string) quantity, _ := item[\u0026#34;quantity\u0026#34;].(float64) err := h.service.ReserveInventory(ctx, productID, int(quantity)) if err != nil { log.Printf(\u0026#34;Failed to reserve inventory for product %s: %v\u0026#34;, productID, err) // Publish inventory reservation failed event failEvent := events.NewEvent(\u0026#34;inventory.reservation_failed\u0026#34;, \u0026#34;inventory-service\u0026#34;, map[string]interface{}{ \u0026#34;order_id\u0026#34;: orderID, \u0026#34;product_id\u0026#34;: productID, \u0026#34;quantity\u0026#34;: quantity, \u0026#34;reason\u0026#34;: err.Error(), }) // This would typically use the same publisher instance // h.publisher.Publish(\u0026#34;inventory.events\u0026#34;, failEvent) return err } } // Publish successful reservation event successEvent := events.NewEvent(\u0026#34;inventory.reserved\u0026#34;, \u0026#34;inventory-service\u0026#34;, map[string]interface{}{ \u0026#34;order_id\u0026#34;: orderID, \u0026#34;items\u0026#34;: items, }) // h.publisher.Publish(\u0026#34;inventory.events\u0026#34;, successEvent) log.Printf(\u0026#34;Successfully reserved inventory for order: %s\u0026#34;, orderID) return nil } Implementing Event Sourcing Patterns Event sourcing takes event-driven architecture a step further by storing events as the primary source of truth. Instead of storing current state, you store the sequence of events that led to that state. This approach provides complete audit trails and enables powerful features like temporal queries and event replay.\nLet\u0026rsquo;s implement a basic event sourcing system:\n// pkg/eventsourcing/event_store.go package eventsourcing import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type StoredEvent struct { ID string `json:\u0026#34;id\u0026#34;` AggregateID string `json:\u0026#34;aggregate_id\u0026#34;` EventType string `json:\u0026#34;event_type\u0026#34;` EventData string `json:\u0026#34;event_data\u0026#34;` Version int `json:\u0026#34;version\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type EventStore struct { db *sql.DB } func NewEventStore(db *sql.DB) *EventStore { return \u0026amp;EventStore{db: db} } func (es *EventStore) SaveEvent(ctx context.Context, aggregateID string, event interface{}, expectedVersion int) error { eventData, err := json.Marshal(event) if err != nil { return fmt.Errorf(\u0026#34;failed to marshal event: %w\u0026#34;, err) } eventType := getEventType(event) query := ` INSERT INTO events (id, aggregate_id, event_type, event_data, version, created_at) VALUES ($1, $2, $3, $4, $5, $6) ` eventID := generateEventID() version := expectedVersion + 1 _, err = es.db.ExecContext(ctx, query, eventID, aggregateID, eventType, string(eventData), version, time.Now()) if err != nil { return fmt.Errorf(\u0026#34;failed to save event: %w\u0026#34;, err) } return nil } func (es *EventStore) GetEvents(ctx context.Context, aggregateID string, fromVersion int) ([]StoredEvent, error) { query := ` SELECT id, aggregate_id, event_type, event_data, version, created_at FROM events WHERE aggregate_id = $1 AND version \u0026gt; $2 ORDER BY version ASC ` rows, err := es.db.QueryContext(ctx, query, aggregateID, fromVersion) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to query events: %w\u0026#34;, err) } defer rows.Close() var events []StoredEvent for rows.Next() { var event StoredEvent err := rows.Scan(\u0026amp;event.ID, \u0026amp;event.AggregateID, \u0026amp;event.EventType, \u0026amp;event.EventData, \u0026amp;event.Version, \u0026amp;event.CreatedAt) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to scan event: %w\u0026#34;, err) } events = append(events, event) } return events, nil } func (es *EventStore) GetAllEvents(ctx context.Context, aggregateID string) ([]StoredEvent, error) { return es.GetEvents(ctx, aggregateID, 0) } func getEventType(event interface{}) string { // Use reflection or type switches to determine event type switch event.(type) { case OrderCreatedEvent: return \u0026#34;OrderCreated\u0026#34; case OrderStatusChangedEvent: return \u0026#34;OrderStatusChanged\u0026#34; default: return \u0026#34;Unknown\u0026#34; } } Create domain events for event sourcing:\n// internal/order/events.go package order import \u0026#34;time\u0026#34; type OrderCreatedEvent struct { OrderID string `json:\u0026#34;order_id\u0026#34;` CustomerID string `json:\u0026#34;customer_id\u0026#34;` Items []Item `json:\u0026#34;items\u0026#34;` TotalAmount float64 `json:\u0026#34;total_amount\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type OrderStatusChangedEvent struct { OrderID string `json:\u0026#34;order_id\u0026#34;` PreviousStatus string `json:\u0026#34;previous_status\u0026#34;` NewStatus string `json:\u0026#34;new_status\u0026#34;` ChangedAt time.Time `json:\u0026#34;changed_at\u0026#34;` } type OrderCancelledEvent struct { OrderID string `json:\u0026#34;order_id\u0026#34;` Reason string `json:\u0026#34;reason\u0026#34;` CancelledAt time.Time `json:\u0026#34;cancelled_at\u0026#34;` } Implement an aggregate root that uses event sourcing:\n// internal/order/aggregate.go package order import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;your-app/pkg/eventsourcing\u0026#34; ) type OrderAggregate struct { ID string CustomerID string Items []Item TotalAmount float64 Status string CreatedAt time.Time version int uncommittedEvents []interface{} } func NewOrderAggregate(customerID string, items []Item) *OrderAggregate { orderID := generateOrderID() totalAmount := calculateTotal(items) aggregate := \u0026amp;OrderAggregate{ version: 0, } // Apply the order created event event := OrderCreatedEvent{ OrderID: orderID, CustomerID: customerID, Items: items, TotalAmount: totalAmount, CreatedAt: time.Now(), } aggregate.apply(event) aggregate.recordEvent(event) return aggregate } func LoadOrderAggregate(ctx context.Context, orderID string, eventStore *eventsourcing.EventStore) (*OrderAggregate, error) { events, err := eventStore.GetAllEvents(ctx, orderID) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to load events: %w\u0026#34;, err) } if len(events) == 0 { return nil, fmt.Errorf(\u0026#34;order not found: %s\u0026#34;, orderID) } aggregate := \u0026amp;OrderAggregate{} for _, storedEvent := range events { event, err := deserializeEvent(storedEvent.EventType, storedEvent.EventData) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to deserialize event: %w\u0026#34;, err) } aggregate.apply(event) aggregate.version = storedEvent.Version } return aggregate, nil } func (oa *OrderAggregate) ChangeStatus(newStatus string) error { if oa.Status == newStatus { return fmt.Errorf(\u0026#34;order already has status: %s\u0026#34;, newStatus) } if oa.Status == \u0026#34;cancelled\u0026#34; { return fmt.Errorf(\u0026#34;cannot change status of cancelled order\u0026#34;) } event := OrderStatusChangedEvent{ OrderID: oa.ID, PreviousStatus: oa.Status, NewStatus: newStatus, ChangedAt: time.Now(), } oa.apply(event) oa.recordEvent(event) return nil } func (oa *OrderAggregate) Cancel(reason string) error { if oa.Status == \u0026#34;cancelled\u0026#34; { return fmt.Errorf(\u0026#34;order already cancelled\u0026#34;) } if oa.Status == \u0026#34;completed\u0026#34; { return fmt.Errorf(\u0026#34;cannot cancel completed order\u0026#34;) } event := OrderCancelledEvent{ OrderID: oa.ID, Reason: reason, CancelledAt: time.Now(), } oa.apply(event) oa.recordEvent(event) return nil } func (oa *OrderAggregate) apply(event interface{}) { switch e := event.(type) { case OrderCreatedEvent: oa.ID = e.OrderID oa.CustomerID = e.CustomerID oa.Items = e.Items oa.TotalAmount = e.TotalAmount oa.Status = \u0026#34;pending\u0026#34; oa.CreatedAt = e.CreatedAt case OrderStatusChangedEvent: oa.Status = e.NewStatus case OrderCancelledEvent: oa.Status = \u0026#34;cancelled\u0026#34; } } func (oa *OrderAggregate) recordEvent(event interface{}) { oa.uncommittedEvents = append(oa.uncommittedEvents, event) } func (oa *OrderAggregate) Save(ctx context.Context, eventStore *eventsourcing.EventStore) error { for _, event := range oa.uncommittedEvents { err := eventStore.SaveEvent(ctx, oa.ID, event, oa.version) if err != nil { return fmt.Errorf(\u0026#34;failed to save event: %w\u0026#34;, err) } oa.version++ } oa.uncommittedEvents = nil return nil } func calculateTotal(items []Item) float64 { total := 0.0 for _, item := range items { total += item.Price * float64(item.Quantity) } return total } func deserializeEvent(eventType, eventData string) (interface{}, error) { switch eventType { case \u0026#34;OrderCreated\u0026#34;: var event OrderCreatedEvent err := json.Unmarshal([]byte(eventData), \u0026amp;event) return event, err case \u0026#34;OrderStatusChanged\u0026#34;: var event OrderStatusChangedEvent err := json.Unmarshal([]byte(eventData), \u0026amp;event) return event, err case \u0026#34;OrderCancelled\u0026#34;: var event OrderCancelledEvent err := json.Unmarshal([]byte(eventData), \u0026amp;event) return event, err default: return nil, fmt.Errorf(\u0026#34;unknown event type: %s\u0026#34;, eventType) } } Implementing CQRS with Event-Driven Architecture Command Query Responsibility Segregation (CQRS) pairs naturally with event-driven architecture. CQRS separates read and write operations, allowing you to optimize each side independently. Events serve as the bridge between the command side (writes) and query side (reads).\nLet\u0026rsquo;s implement a CQRS system with event-driven updates:\n// internal/order/commands.go package order import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;your-app/pkg/eventsourcing\u0026#34; ) type CommandHandler struct { eventStore *eventsourcing.EventStore publisher EventPublisher } func NewCommandHandler(eventStore *eventsourcing.EventStore, publisher EventPublisher) *CommandHandler { return \u0026amp;CommandHandler{ eventStore: eventStore, publisher: publisher, } } type CreateOrderCommand struct { CustomerID string `json:\u0026#34;customer_id\u0026#34;` Items []Item `json:\u0026#34;items\u0026#34;` } type ChangeOrderStatusCommand struct { OrderID string `json:\u0026#34;order_id\u0026#34;` NewStatus string `json:\u0026#34;new_status\u0026#34;` } func (ch *CommandHandler) HandleCreateOrder(ctx context.Context, cmd CreateOrderCommand) (*OrderAggregate, error) { // Create new aggregate aggregate := NewOrderAggregate(cmd.CustomerID, cmd.Items) // Save events to event store err := aggregate.Save(ctx, ch.eventStore) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to save order aggregate: %w\u0026#34;, err) } // Publish events to message queue for read model updates for _, event := range aggregate.uncommittedEvents { err := ch.publisher.PublishDomainEvent(ctx, event) if err != nil { log.Printf(\u0026#34;Failed to publish event: %v\u0026#34;, err) // In production, you might want to implement compensation logic } } return aggregate, nil } func (ch *CommandHandler) HandleChangeOrderStatus(ctx context.Context, cmd ChangeOrderStatusCommand) error { // Load aggregate from event store aggregate, err := LoadOrderAggregate(ctx, cmd.OrderID, ch.eventStore) if err != nil { return fmt.Errorf(\u0026#34;failed to load order aggregate: %w\u0026#34;, err) } // Execute business logic err = aggregate.ChangeStatus(cmd.NewStatus) if err != nil { return fmt.Errorf(\u0026#34;failed to change order status: %w\u0026#34;, err) } // Save new events err = aggregate.Save(ctx, ch.eventStore) if err != nil { return fmt.Errorf(\u0026#34;failed to save order aggregate: %w\u0026#34;, err) } // Publish events for read model updates for _, event := range aggregate.uncommittedEvents { err := ch.publisher.PublishDomainEvent(ctx, event) if err != nil { log.Printf(\u0026#34;Failed to publish event: %v\u0026#34;, err) } } return nil } Implement read models that are updated via events:\n// internal/order/read_model.go package order import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) type OrderReadModel struct { ID string `json:\u0026#34;id\u0026#34; db:\u0026#34;id\u0026#34;` CustomerID string `json:\u0026#34;customer_id\u0026#34; db:\u0026#34;customer_id\u0026#34;` CustomerName string `json:\u0026#34;customer_name\u0026#34; db:\u0026#34;customer_name\u0026#34;` ItemCount int `json:\u0026#34;item_count\u0026#34; db:\u0026#34;item_count\u0026#34;` TotalAmount float64 `json:\u0026#34;total_amount\u0026#34; db:\u0026#34;total_amount\u0026#34;` Status string `json:\u0026#34;status\u0026#34; db:\u0026#34;status\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34; db:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34; db:\u0026#34;updated_at\u0026#34;` } type ReadModelRepository struct { db *sql.DB } func NewReadModelRepository(db *sql.DB) *ReadModelRepository { return \u0026amp;ReadModelRepository{db: db} } func (r *ReadModelRepository) Save(ctx context.Context, order *OrderReadModel) error { query := ` INSERT INTO order_read_models (id, customer_id, customer_name, item_count, total_amount, status, created_at, updated_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8) ON CONFLICT (id) DO UPDATE SET customer_name = EXCLUDED.customer_name, item_count = EXCLUDED.item_count, total_amount = EXCLUDED.total_amount, status = EXCLUDED.status, updated_at = EXCLUDED.updated_at ` _, err := r.db.ExecContext(ctx, query, order.ID, order.CustomerID, order.CustomerName, order.ItemCount, order.TotalAmount, order.Status, order.CreatedAt, order.UpdatedAt) return err } func (r *ReadModelRepository) GetByID(ctx context.Context, id string) (*OrderReadModel, error) { query := ` SELECT id, customer_id, customer_name, item_count, total_amount, status, created_at, updated_at FROM order_read_models WHERE id = $1 ` var order OrderReadModel err := r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;order.ID, \u0026amp;order.CustomerID, \u0026amp;order.CustomerName, \u0026amp;order.ItemCount, \u0026amp;order.TotalAmount, \u0026amp;order.Status, \u0026amp;order.CreatedAt, \u0026amp;order.UpdatedAt) if err != nil { return nil, err } return \u0026amp;order, nil } func (r *ReadModelRepository) GetByCustomerID(ctx context.Context, customerID string) ([]*OrderReadModel, error) { query := ` SELECT id, customer_id, customer_name, item_count, total_amount, status, created_at, updated_at FROM order_read_models WHERE customer_id = $1 ORDER BY created_at DESC ` rows, err := r.db.QueryContext(ctx, query, customerID) if err != nil { return nil, err } defer rows.Close() var orders []*OrderReadModel for rows.Next() { var order OrderReadModel err := rows.Scan(\u0026amp;order.ID, \u0026amp;order.CustomerID, \u0026amp;order.CustomerName, \u0026amp;order.ItemCount, \u0026amp;order.TotalAmount, \u0026amp;order.Status, \u0026amp;order.CreatedAt, \u0026amp;order.UpdatedAt) if err != nil { return nil, err } orders = append(orders, \u0026amp;order) } return orders, nil } Create event handlers for read model updates:\n// internal/order/read_model_handler.go package order import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;your-app/pkg/events\u0026#34; ) type ReadModelHandler struct { repo *ReadModelRepository customerRepo CustomerRepository // For enriching read models } func NewReadModelHandler(repo *ReadModelRepository, customerRepo CustomerRepository) *ReadModelHandler { return \u0026amp;ReadModelHandler{ repo: repo, customerRepo: customerRepo, } } func (h *ReadModelHandler) HandleOrderCreated(ctx context.Context, event *events.Event) error { var orderEvent OrderCreatedEvent eventData, _ := json.Marshal(event.Data) err := json.Unmarshal(eventData, \u0026amp;orderEvent) if err != nil { return fmt.Errorf(\u0026#34;failed to unmarshal order created event: %w\u0026#34;, err) } // Enrich with customer data customer, err := h.customerRepo.GetByID(ctx, orderEvent.CustomerID) if err != nil { log.Printf(\u0026#34;Failed to get customer data: %v\u0026#34;, err) // Continue with empty customer name } customerName := \u0026#34;\u0026#34; if customer != nil { customerName = customer.Name } readModel := \u0026amp;OrderReadModel{ ID: orderEvent.OrderID, CustomerID: orderEvent.CustomerID, CustomerName: customerName, ItemCount: len(orderEvent.Items), TotalAmount: orderEvent.TotalAmount, Status: \u0026#34;pending\u0026#34;, CreatedAt: orderEvent.CreatedAt, UpdatedAt: orderEvent.CreatedAt, } err = h.repo.Save(ctx, readModel) if err != nil { return fmt.Errorf(\u0026#34;failed to save order read model: %w\u0026#34;, err) } log.Printf(\u0026#34;Updated read model for order: %s\u0026#34;, orderEvent.OrderID) return nil } func (h *ReadModelHandler) HandleOrderStatusChanged(ctx context.Context, event *events.Event) error { var statusEvent OrderStatusChangedEvent eventData, _ := json.Marshal(event.Data) err := json.Unmarshal(eventData, \u0026amp;statusEvent) if err != nil { return fmt.Errorf(\u0026#34;failed to unmarshal order status changed event: %w\u0026#34;, err) } // Load existing read model readModel, err := h.repo.GetByID(ctx, statusEvent.OrderID) if err != nil { return fmt.Errorf(\u0026#34;failed to get existing read model: %w\u0026#34;, err) } // Update status and timestamp readModel.Status = statusEvent.NewStatus readModel.UpdatedAt = statusEvent.ChangedAt err = h.repo.Save(ctx, readModel) if err != nil { return fmt.Errorf(\u0026#34;failed to update order read model: %w\u0026#34;, err) } log.Printf(\u0026#34;Updated read model status for order: %s to %s\u0026#34;, statusEvent.OrderID, statusEvent.NewStatus) return nil } Error Handling and Resilience Patterns Event-driven systems require robust error handling and resilience patterns. Network failures, service outages, and processing errors are inevitable in distributed systems.\nImplement retry mechanisms with exponential backoff:\n// pkg/resilience/retry.go package resilience import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; \u0026#34;time\u0026#34; ) type RetryConfig struct { MaxAttempts int `json:\u0026#34;max_attempts\u0026#34;` BaseDelay time.Duration `json:\u0026#34;base_delay\u0026#34;` MaxDelay time.Duration `json:\u0026#34;max_delay\u0026#34;` Multiplier float64 `json:\u0026#34;multiplier\u0026#34;` } func DefaultRetryConfig() RetryConfig { return RetryConfig{ MaxAttempts: 3, BaseDelay: 100 * time.Millisecond, MaxDelay: 30 * time.Second, Multiplier: 2.0, } } func RetryWithBackoff(ctx context.Context, config RetryConfig, operation func() error) error { var lastErr error for attempt := 1; attempt \u0026lt;= config.MaxAttempts; attempt++ { lastErr = operation() if lastErr == nil { return nil } if attempt == config.MaxAttempts { break } // Calculate delay with exponential backoff delay := time.Duration(float64(config.BaseDelay) * math.Pow(config.Multiplier, float64(attempt-1))) if delay \u0026gt; config.MaxDelay { delay = config.MaxDelay } log.Printf(\u0026#34;Operation failed (attempt %d/%d): %v. Retrying in %v\u0026#34;, attempt, config.MaxAttempts, lastErr, delay) select { case \u0026lt;-ctx.Done(): return ctx.Err() case \u0026lt;-time.After(delay): // Continue to next attempt } } return fmt.Errorf(\u0026#34;operation failed after %d attempts: %w\u0026#34;, config.MaxAttempts, lastErr) } Implement dead letter queue handling:\n// pkg/events/dead_letter.go package events import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; ) type DeadLetter struct { OriginalEvent *Event `json:\u0026#34;original_event\u0026#34;` FailureReason string `json:\u0026#34;failure_reason\u0026#34;` FailureCount int `json:\u0026#34;failure_count\u0026#34;` FirstFailedAt time.Time `json:\u0026#34;first_failed_at\u0026#34;` LastFailedAt time.Time `json:\u0026#34;last_failed_at\u0026#34;` } type DeadLetterHandler struct { publisher *Publisher storage DeadLetterStorage } func NewDeadLetterHandler(publisher *Publisher, storage DeadLetterStorage) *DeadLetterHandler { return \u0026amp;DeadLetterHandler{ publisher: publisher, storage: storage, } } func (dlh *DeadLetterHandler) HandleFailedEvent(ctx context.Context, event *Event, err error) { deadLetter := \u0026amp;DeadLetter{ OriginalEvent: event, FailureReason: err.Error(), FailureCount: 1, FirstFailedAt: time.Now(), LastFailedAt: time.Now(), } // Check if this event has failed before existing, err := dlh.storage.GetByEventID(ctx, event.ID) if err == nil \u0026amp;\u0026amp; existing != nil { deadLetter.FailureCount = existing.FailureCount + 1 deadLetter.FirstFailedAt = existing.FirstFailedAt } // Store in dead letter storage err = dlh.storage.Save(ctx, deadLetter) if err != nil { log.Printf(\u0026#34;Failed to save dead letter: %v\u0026#34;, err) } // Publish to dead letter queue for manual processing dlEvent := NewEvent(\u0026#34;dead_letter.created\u0026#34;, \u0026#34;dead-letter-handler\u0026#34;, map[string]interface{}{ \u0026#34;event_id\u0026#34;: event.ID, \u0026#34;failure_reason\u0026#34;: deadLetter.FailureReason, \u0026#34;failure_count\u0026#34;: deadLetter.FailureCount, }) err = dlh.publisher.Publish(\u0026#34;dead_letters\u0026#34;, dlEvent) if err != nil { log.Printf(\u0026#34;Failed to publish dead letter event: %v\u0026#34;, err) } log.Printf(\u0026#34;Event %s moved to dead letter queue after %d failures\u0026#34;, event.ID, deadLetter.FailureCount) } func (dlh *DeadLetterHandler) ReprocessDeadLetter(ctx context.Context, eventID string) error { deadLetter, err := dlh.storage.GetByEventID(ctx, eventID) if err != nil { return fmt.Errorf(\u0026#34;failed to get dead letter: %w\u0026#34;, err) } // Republish the original event err = dlh.publisher.Publish(\u0026#34;retry_queue\u0026#34;, deadLetter.OriginalEvent) if err != nil { return fmt.Errorf(\u0026#34;failed to republish event: %w\u0026#34;, err) } // Remove from dead letter storage err = dlh.storage.Delete(ctx, eventID) if err != nil { log.Printf(\u0026#34;Failed to delete dead letter: %v\u0026#34;, err) } return nil } Performance Optimization and Monitoring Event-driven systems require careful monitoring to ensure healthy operation. Implement comprehensive metrics and observability:\n// pkg/monitoring/metrics.go package monitoring import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; ) type EventMetrics struct { eventsPublished prometheus.Counter eventsConsumed prometheus.Counter eventsFailed prometheus.Counter processingTime prometheus.Histogram queueDepth prometheus.Gauge } func NewEventMetrics() *EventMetrics { return \u0026amp;EventMetrics{ eventsPublished: promauto.NewCounter(prometheus.CounterOpts{ Name: \u0026#34;events_published_total\u0026#34;, Help: \u0026#34;Total number of events published\u0026#34;, }), eventsConsumed: promauto.NewCounter(prometheus.CounterOpts{ Name: \u0026#34;events_consumed_total\u0026#34;, Help: \u0026#34;Total number of events consumed\u0026#34;, }), eventsFailed: promauto.NewCounter(prometheus.CounterOpts{ Name: \u0026#34;events_failed_total\u0026#34;, Help: \u0026#34;Total number of failed event processing attempts\u0026#34;, }), processingTime: promauto.NewHistogram(prometheus.HistogramOpts{ Name: \u0026#34;event_processing_duration_seconds\u0026#34;, Help: \u0026#34;Time taken to process events\u0026#34;, Buckets: prometheus.DefBuckets, }), queueDepth: promauto.NewGauge(prometheus.GaugeOpts{ Name: \u0026#34;event_queue_depth\u0026#34;, Help: \u0026#34;Current depth of event queue\u0026#34;, }), } } func (em *EventMetrics) RecordEventPublished() { em.eventsPublished.Inc() } func (em *EventMetrics) RecordEventConsumed(duration time.Duration) { em.eventsConsumed.Inc() em.processingTime.Observe(duration.Seconds()) } func (em *EventMetrics) RecordEventFailed() { em.eventsFailed.Inc() } func (em *EventMetrics) UpdateQueueDepth(depth float64) { em.queueDepth.Set(depth) } Production Deployment and Best Practices When deploying event-driven systems to production, several key considerations ensure reliability and performance:\nMessage Ordering: For scenarios requiring strict ordering, use partitioned topics or single-threaded consumers. However, consider whether eventual consistency might be acceptable to achieve better performance.\nIdempotency: Design event handlers to be idempotent since message delivery guarantees might result in duplicate processing. Use event IDs or business keys to detect and handle duplicates.\nEvent Schema Evolution: Plan for event schema changes by using versioned events and maintaining backward compatibility. Consider using tools like Protocol Buffers or Avro for schema evolution support.\nMonitoring and Alerting: Implement comprehensive monitoring for queue depths, processing latencies, error rates, and dead letter queues. Set up alerts for unusual patterns that might indicate system issues.\nCapacity Planning: Monitor resource usage patterns and plan for scaling both message brokers and consumers based on traffic patterns and growth projections.\nConclusion Event-driven architecture with Go provides a powerful foundation for building scalable, resilient distributed systems. The patterns and implementations covered in this guide demonstrate how to leverage Go\u0026rsquo;s strengths while addressing the unique challenges of asynchronous, event-based communication.\nKey takeaways include the importance of choosing the right message queue technology for your needs, implementing proper error handling and resilience patterns, and maintaining comprehensive monitoring and observability. Whether you\u0026rsquo;re building microservices systems or modernizing existing applications, event-driven patterns can significantly improve scalability and maintainability.\nThe investment in understanding event-driven architecture patterns pays dividends in building applications that can handle high throughput, provide better user experiences through asynchronous processing, and maintain system reliability even when individual components experience failures. As your systems grow in complexity, these patterns become essential tools for managing distributed system challenges while maintaining development velocity and operational stability.\n","href":"/2025/09/event-driven-architecture-golang-message-queues.html","title":"Event-Driven Architecture with Golang and Message Queues"},{"content":"Moving from monolithic to microservices architecture has become one of the biggest changes in how we build software today. While monolithic applications bundle all functionality into a single deployable unit, microservices break down applications into smaller, independent services that communicate over well-defined APIs. When combined with Go\u0026rsquo;s performance characteristics and deployment simplicity, microservices become a powerful approach for building scalable, maintainable systems.\nIn this guide, you\u0026rsquo;ll learn how to design, build, and deploy microservices using Go. We\u0026rsquo;ll cover architectural patterns, service communication strategies, containerization, and production deployment techniques that will help you build robust distributed systems.\nUnderstanding Microservices Architecture Microservices architecture breaks down large applications into smaller, independent services that each handle a specific business function. Unlike monolithic architectures where all components are tightly integrated, microservices promote independence in development, deployment, and scaling.\nTwo key principles drive microservices: each service manages its own data and logic, and teams can pick the best technology for their specific needs. Services communicate through lightweight protocols, typically HTTP APIs or message queues, enabling language and technology agnostic integration.\nGo\u0026rsquo;s characteristics make it particularly well-suited for microservices development. The language\u0026rsquo;s fast compilation enables rapid development cycles, while its small binary size and minimal resource footprint reduce deployment overhead. Go\u0026rsquo;s built-in concurrency support handles multiple requests efficiently, and its standard library provides robust networking capabilities essential for distributed systems.\nDesigning Your Microservices Architecture Effective microservices design starts with identifying service boundaries based on business domains rather than technical concerns. The Domain-Driven Design approach helps define these boundaries by grouping related functionality into bounded contexts that naturally align with team responsibilities and business capabilities.\nConsider an e-commerce platform that could be decomposed into several microservices: user management, product catalog, inventory management, order processing, payment handling, and notification services. Each service encapsulates specific business logic and maintains its own data store, ensuring clear separation of concerns.\nLet\u0026rsquo;s design a practical microservices system for a blogging platform that demonstrates common patterns and challenges:\nClient Applications (Web, Mobile, API) │ ▼ ┌─────────────────────┐ │ API Gateway │ │ - Routing │ │ - Authentication │ │ - Rate Limiting │ └─────────────────────┘ │ ┌─────────┼─────────┐ │ │ │ ▼ ▼ ▼ ┌────────────┐ ┌──────────┐ ┌────────────┐ │User Service│ │ Content │ │ Comment │ │ │ │ Service │ │ Service │ │- Auth │ │- Posts │ │- Comments │ │- Profiles │ │- Tags │ │- Moderate │ │- Perms │ │- Publish │ │- Notify │ └────────────┘ └──────────┘ └────────────┘ │ │ │ ▼ ▼ ▼ ┌────────────┐ ┌──────────┐ ┌────────────┐ │ User DB │ │Content DB│ │ Comment DB │ │(Postgres) │ │(Postgres)│ │ (Postgres) │ └────────────┘ └──────────┘ └────────────┘ Message Queue (NATS/RabbitMQ) for async communication This setup keeps each service focused on its job while making sure they can talk to each other easily. The API Gateway acts as the front door, handling things like user authentication and deciding which service should handle each request.\nBuilding Your First Microservice We\u0026rsquo;ll start by building a user service that handles user accounts, login, and profiles. This will be the foundation that other services can build on.\nCreate the project structure for your user service:\nmkdir user-service cd user-service go mod init user-service mkdir -p {cmd/server,internal/{handler,service,repository,model},pkg/{auth,middleware}} Define the user model and service interface:\n// internal/model/user.go package model import ( \u0026#34;time\u0026#34; ) type User struct { ID string `json:\u0026#34;id\u0026#34; db:\u0026#34;id\u0026#34;` Username string `json:\u0026#34;username\u0026#34; db:\u0026#34;username\u0026#34;` Email string `json:\u0026#34;email\u0026#34; db:\u0026#34;email\u0026#34;` Password string `json:\u0026#34;-\u0026#34; db:\u0026#34;password\u0026#34;` FirstName string `json:\u0026#34;firstName\u0026#34; db:\u0026#34;first_name\u0026#34;` LastName string `json:\u0026#34;lastName\u0026#34; db:\u0026#34;last_name\u0026#34;` Role string `json:\u0026#34;role\u0026#34; db:\u0026#34;role\u0026#34;` IsActive bool `json:\u0026#34;isActive\u0026#34; db:\u0026#34;is_active\u0026#34;` CreatedAt time.Time `json:\u0026#34;createdAt\u0026#34; db:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updatedAt\u0026#34; db:\u0026#34;updated_at\u0026#34;` } type CreateUserRequest struct { Username string `json:\u0026#34;username\u0026#34; validate:\u0026#34;required,min=3,max=50\u0026#34;` Email string `json:\u0026#34;email\u0026#34; validate:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; validate:\u0026#34;required,min=8\u0026#34;` FirstName string `json:\u0026#34;firstName\u0026#34; validate:\u0026#34;required,min=2,max=50\u0026#34;` LastName string `json:\u0026#34;lastName\u0026#34; validate:\u0026#34;required,min=2,max=50\u0026#34;` } type UpdateUserRequest struct { FirstName *string `json:\u0026#34;firstName,omitempty\u0026#34; validate:\u0026#34;omitempty,min=2,max=50\u0026#34;` LastName *string `json:\u0026#34;lastName,omitempty\u0026#34; validate:\u0026#34;omitempty,min=2,max=50\u0026#34;` Email *string `json:\u0026#34;email,omitempty\u0026#34; validate:\u0026#34;omitempty,email\u0026#34;` } type LoginRequest struct { Username string `json:\u0026#34;username\u0026#34; validate:\u0026#34;required\u0026#34;` Password string `json:\u0026#34;password\u0026#34; validate:\u0026#34;required\u0026#34;` } type LoginResponse struct { Token string `json:\u0026#34;token\u0026#34;` User User `json:\u0026#34;user\u0026#34;` } Implement the repository layer for data persistence:\n// internal/repository/user.go package repository import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;user-service/internal/model\u0026#34; ) type UserRepository interface { Create(ctx context.Context, user *model.User) error GetByID(ctx context.Context, id string) (*model.User, error) GetByUsername(ctx context.Context, username string) (*model.User, error) GetByEmail(ctx context.Context, email string) (*model.User, error) Update(ctx context.Context, id string, updates map[string]interface{}) error Delete(ctx context.Context, id string) error List(ctx context.Context, limit, offset int) ([]*model.User, error) } type userRepository struct { db *sql.DB } func NewUserRepository(db *sql.DB) UserRepository { return \u0026amp;userRepository{db: db} } func (r *userRepository) Create(ctx context.Context, user *model.User) error { query := ` INSERT INTO users (id, username, email, password, first_name, last_name, role, is_active, created_at, updated_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10) ` _, err := r.db.ExecContext(ctx, query, user.ID, user.Username, user.Email, user.Password, user.FirstName, user.LastName, user.Role, user.IsActive, user.CreatedAt, user.UpdatedAt, ) return err } func (r *userRepository) GetByID(ctx context.Context, id string) (*model.User, error) { user := \u0026amp;model.User{} query := ` SELECT id, username, email, password, first_name, last_name, role, is_active, created_at, updated_at FROM users WHERE id = $1 AND is_active = true ` err := r.db.QueryRowContext(ctx, query, id).Scan( \u0026amp;user.ID, \u0026amp;user.Username, \u0026amp;user.Email, \u0026amp;user.Password, \u0026amp;user.FirstName, \u0026amp;user.LastName, \u0026amp;user.Role, \u0026amp;user.IsActive, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } return nil, err } return user, nil } func (r *userRepository) GetByUsername(ctx context.Context, username string) (*model.User, error) { user := \u0026amp;model.User{} query := ` SELECT id, username, email, password, first_name, last_name, role, is_active, created_at, updated_at FROM users WHERE username = $1 AND is_active = true ` err := r.db.QueryRowContext(ctx, query, username).Scan( \u0026amp;user.ID, \u0026amp;user.Username, \u0026amp;user.Email, \u0026amp;user.Password, \u0026amp;user.FirstName, \u0026amp;user.LastName, \u0026amp;user.Role, \u0026amp;user.IsActive, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { if err == sql.ErrNoRows { return nil, fmt.Errorf(\u0026#34;user not found\u0026#34;) } return nil, err } return user, nil } Implement the business logic layer:\n// internal/service/user.go package service import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; \u0026#34;github.com/google/uuid\u0026#34; \u0026#34;user-service/internal/model\u0026#34; \u0026#34;user-service/internal/repository\u0026#34; \u0026#34;user-service/pkg/auth\u0026#34; ) type UserService interface { CreateUser(ctx context.Context, req *model.CreateUserRequest) (*model.User, error) Login(ctx context.Context, req *model.LoginRequest) (*model.LoginResponse, error) GetUser(ctx context.Context, id string) (*model.User, error) UpdateUser(ctx context.Context, id string, req *model.UpdateUserRequest) (*model.User, error) DeleteUser(ctx context.Context, id string) error ListUsers(ctx context.Context, limit, offset int) ([]*model.User, error) } type userService struct { repo repository.UserRepository jwtSecret string } func NewUserService(repo repository.UserRepository, jwtSecret string) UserService { return \u0026amp;userService{ repo: repo, jwtSecret: jwtSecret, } } func (s *userService) CreateUser(ctx context.Context, req *model.CreateUserRequest) (*model.User, error) { // Check if username or email already exists existingUser, _ := s.repo.GetByUsername(ctx, req.Username) if existingUser != nil { return nil, fmt.Errorf(\u0026#34;username already exists\u0026#34;) } existingUser, _ = s.repo.GetByEmail(ctx, req.Email) if existingUser != nil { return nil, fmt.Errorf(\u0026#34;email already exists\u0026#34;) } // Hash password hashedPassword, err := bcrypt.GenerateFromPassword([]byte(req.Password), bcrypt.DefaultCost) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to hash password: %w\u0026#34;, err) } // Create user user := \u0026amp;model.User{ ID: uuid.New().String(), Username: req.Username, Email: req.Email, Password: string(hashedPassword), FirstName: req.FirstName, LastName: req.LastName, Role: \u0026#34;user\u0026#34;, IsActive: true, CreatedAt: time.Now(), UpdatedAt: time.Now(), } err = s.repo.Create(ctx, user) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } // Remove password from response user.Password = \u0026#34;\u0026#34; return user, nil } func (s *userService) Login(ctx context.Context, req *model.LoginRequest) (*model.LoginResponse, error) { // Get user by username user, err := s.repo.GetByUsername(ctx, req.Username) if err != nil { return nil, fmt.Errorf(\u0026#34;invalid credentials\u0026#34;) } // Verify password err = bcrypt.CompareHashAndPassword([]byte(user.Password), []byte(req.Password)) if err != nil { return nil, fmt.Errorf(\u0026#34;invalid credentials\u0026#34;) } // Generate JWT token token, err := auth.GenerateToken(user.ID, user.Role, s.jwtSecret) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to generate token: %w\u0026#34;, err) } // Remove password from response user.Password = \u0026#34;\u0026#34; return \u0026amp;model.LoginResponse{ Token: token, User: *user, }, nil } Create the HTTP handlers:\n// internal/handler/user.go package handler import ( \u0026#34;encoding/json\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; \u0026#34;github.com/go-playground/validator/v10\u0026#34; \u0026#34;user-service/internal/model\u0026#34; \u0026#34;user-service/internal/service\u0026#34; ) type UserHandler struct { service service.UserService validator *validator.Validate } func NewUserHandler(service service.UserService) *UserHandler { return \u0026amp;UserHandler{ service: service, validator: validator.New(), } } func (h *UserHandler) CreateUser(w http.ResponseWriter, r *http.Request) { var req model.CreateUserRequest if err := json.NewDecoder(r.Body).Decode(\u0026amp;req); err != nil { http.Error(w, \u0026#34;Invalid request body\u0026#34;, http.StatusBadRequest) return } if err := h.validator.Struct(\u0026amp;req); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } user, err := h.service.CreateUser(r.Context(), \u0026amp;req) if err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.WriteHeader(http.StatusCreated) json.NewEncoder(w).Encode(user) } func (h *UserHandler) Login(w http.ResponseWriter, r *http.Request) { var req model.LoginRequest if err := json.NewDecoder(r.Body).Decode(\u0026amp;req); err != nil { http.Error(w, \u0026#34;Invalid request body\u0026#34;, http.StatusBadRequest) return } if err := h.validator.Struct(\u0026amp;req); err != nil { http.Error(w, err.Error(), http.StatusBadRequest) return } response, err := h.service.Login(r.Context(), \u0026amp;req) if err != nil { http.Error(w, err.Error(), http.StatusUnauthorized) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(response) } func (h *UserHandler) GetUser(w http.ResponseWriter, r *http.Request) { vars := mux.Vars(r) id := vars[\u0026#34;id\u0026#34;] user, err := h.service.GetUser(r.Context(), id) if err != nil { http.Error(w, err.Error(), http.StatusNotFound) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(user) } func (h *UserHandler) ListUsers(w http.ResponseWriter, r *http.Request) { limitStr := r.URL.Query().Get(\u0026#34;limit\u0026#34;) offsetStr := r.URL.Query().Get(\u0026#34;offset\u0026#34;) limit := 10 offset := 0 if limitStr != \u0026#34;\u0026#34; { if l, err := strconv.Atoi(limitStr); err == nil \u0026amp;\u0026amp; l \u0026gt; 0 { limit = l } } if offsetStr != \u0026#34;\u0026#34; { if o, err := strconv.Atoi(offsetStr); err == nil \u0026amp;\u0026amp; o \u0026gt;= 0 { offset = o } } users, err := h.service.ListUsers(r.Context(), limit, offset) if err != nil { http.Error(w, err.Error(), http.StatusInternalServerError) return } w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(users) } Service Communication Patterns Getting your microservices to talk to each other efficiently is crucial. You\u0026rsquo;ll need to decide between synchronous communication (where services wait for responses) and asynchronous communication (where services can continue working without waiting).\nFor synchronous communication, REST APIs are straightforward and most developers already know how to use them. But when you need better performance, gRPC is faster because it uses more efficient data formats and connection handling. Here\u0026rsquo;s how to implement gRPC communication between services:\n// Define a simple gRPC service for user validation // proto/user.proto syntax = \u0026#34;proto3\u0026#34;; package user; option go_package = \u0026#34;user-service/proto\u0026#34;; service UserService { rpc GetUser(GetUserRequest) returns (UserResponse); rpc ValidateUser(ValidateUserRequest) returns (ValidateUserResponse); } message GetUserRequest { string user_id = 1; } message UserResponse { string id = 1; string username = 2; string email = 3; string first_name = 4; string last_name = 5; string role = 6; bool is_active = 7; } message ValidateUserRequest { string token = 1; } message ValidateUserResponse { bool valid = 1; UserResponse user = 2; } Implement the gRPC server:\n// internal/grpc/server.go package grpc import ( \u0026#34;context\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; pb \u0026#34;user-service/proto\u0026#34; \u0026#34;user-service/internal/service\u0026#34; \u0026#34;user-service/pkg/auth\u0026#34; ) type Server struct { pb.UnimplementedUserServiceServer userService service.UserService jwtSecret string } func NewServer(userService service.UserService, jwtSecret string) *Server { return \u0026amp;Server{ userService: userService, jwtSecret: jwtSecret, } } func (s *Server) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.UserResponse, error) { user, err := s.userService.GetUser(ctx, req.UserId) if err != nil { return nil, status.Errorf(codes.NotFound, \u0026#34;user not found: %v\u0026#34;, err) } return \u0026amp;pb.UserResponse{ Id: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, Role: user.Role, IsActive: user.IsActive, }, nil } func (s *Server) ValidateUser(ctx context.Context, req *pb.ValidateUserRequest) (*pb.ValidateUserResponse, error) { claims, err := auth.ValidateToken(req.Token, s.jwtSecret) if err != nil { return \u0026amp;pb.ValidateUserResponse{Valid: false}, nil } user, err := s.userService.GetUser(ctx, claims.UserID) if err != nil { return \u0026amp;pb.ValidateUserResponse{Valid: false}, nil } return \u0026amp;pb.ValidateUserResponse{ Valid: true, User: \u0026amp;pb.UserResponse{ Id: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, Role: user.Role, IsActive: user.IsActive, }, }, nil } For asynchronous communication, message queues enable loose coupling and better fault tolerance. Here\u0026rsquo;s an example using NATS for event publishing:\n// pkg/events/publisher.go package events import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/nats-io/nats.go\u0026#34; ) type Publisher struct { conn *nats.Conn } func NewPublisher(natsURL string) (*Publisher, error) { conn, err := nats.Connect(natsURL) if err != nil { return nil, err } return \u0026amp;Publisher{conn: conn}, nil } func (p *Publisher) PublishUserCreated(userID, username, email string) error { event := map[string]interface{}{ \u0026#34;event_type\u0026#34;: \u0026#34;user.created\u0026#34;, \u0026#34;user_id\u0026#34;: userID, \u0026#34;username\u0026#34;: username, \u0026#34;email\u0026#34;: email, \u0026#34;timestamp\u0026#34;: time.Now().Unix(), } data, err := json.Marshal(event) if err != nil { return err } return p.conn.Publish(\u0026#34;user.events\u0026#34;, data) } func (p *Publisher) Close() { p.conn.Close() } Database Design for Microservices Each microservice should own its data and database schema, following the database-per-service pattern. This ensures loose coupling and allows teams to choose the most appropriate database technology for their specific requirements.\nFor our user service, let\u0026rsquo;s create a PostgreSQL schema:\n-- migrations/001_create_users_table.sql CREATE EXTENSION IF NOT EXISTS \u0026#34;uuid-ossp\u0026#34;; CREATE TABLE users ( id UUID PRIMARY KEY DEFAULT uuid_generate_v4(), username VARCHAR(50) UNIQUE NOT NULL, email VARCHAR(255) UNIQUE NOT NULL, password TEXT NOT NULL, first_name VARCHAR(50) NOT NULL, last_name VARCHAR(50) NOT NULL, role VARCHAR(20) DEFAULT \u0026#39;user\u0026#39;, is_active BOOLEAN DEFAULT true, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP ); CREATE INDEX idx_users_username ON users(username); CREATE INDEX idx_users_email ON users(email); CREATE INDEX idx_users_active ON users(is_active); -- Trigger to automatically update updated_at CREATE OR REPLACE FUNCTION update_updated_at_column() RETURNS TRIGGER AS $$ BEGIN NEW.updated_at = CURRENT_TIMESTAMP; RETURN NEW; END; $$ language \u0026#39;plpgsql\u0026#39;; CREATE TRIGGER update_users_updated_at BEFORE UPDATE ON users FOR EACH ROW EXECUTE FUNCTION update_updated_at_column(); Implement database migrations in your service:\n// internal/database/migrate.go package database import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;strings\u0026#34; ) type Migrator struct { db *sql.DB migrationsDir string } func NewMigrator(db *sql.DB, migrationsDir string) *Migrator { return \u0026amp;Migrator{ db: db, migrationsDir: migrationsDir, } } func (m *Migrator) Migrate() error { // Create migrations table if it doesn\u0026#39;t exist _, err := m.db.Exec(` CREATE TABLE IF NOT EXISTS schema_migrations ( version VARCHAR(255) PRIMARY KEY, applied_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `) if err != nil { return fmt.Errorf(\u0026#34;failed to create migrations table: %w\u0026#34;, err) } // Get applied migrations appliedMigrations, err := m.getAppliedMigrations() if err != nil { return fmt.Errorf(\u0026#34;failed to get applied migrations: %w\u0026#34;, err) } // Get migration files files, err := filepath.Glob(filepath.Join(m.migrationsDir, \u0026#34;*.sql\u0026#34;)) if err != nil { return fmt.Errorf(\u0026#34;failed to read migration files: %w\u0026#34;, err) } sort.Strings(files) for _, file := range files { version := strings.TrimSuffix(filepath.Base(file), \u0026#34;.sql\u0026#34;) if appliedMigrations[version] { continue // Skip already applied migrations } content, err := ioutil.ReadFile(file) if err != nil { return fmt.Errorf(\u0026#34;failed to read migration file %s: %w\u0026#34;, file, err) } // Execute migration _, err = m.db.Exec(string(content)) if err != nil { return fmt.Errorf(\u0026#34;failed to execute migration %s: %w\u0026#34;, version, err) } // Record migration as applied _, err = m.db.Exec(\u0026#34;INSERT INTO schema_migrations (version) VALUES ($1)\u0026#34;, version) if err != nil { return fmt.Errorf(\u0026#34;failed to record migration %s: %w\u0026#34;, version, err) } fmt.Printf(\u0026#34;Applied migration: %s\\n\u0026#34;, version) } return nil } func (m *Migrator) getAppliedMigrations() (map[string]bool, error) { rows, err := m.db.Query(\u0026#34;SELECT version FROM schema_migrations\u0026#34;) if err != nil { return nil, err } defer rows.Close() applied := make(map[string]bool) for rows.Next() { var version string if err := rows.Scan(\u0026amp;version); err != nil { return nil, err } applied[version] = true } return applied, nil } Containerization with Docker Containerization is essential for microservices deployment, providing consistency across environments and enabling efficient resource utilization. Here\u0026rsquo;s a production-ready Dockerfile for your Go microservice:\n# Build stage FROM golang:1.21-alpine AS builder # Install build dependencies RUN apk add --no-cache git ca-certificates tzdata # Set working directory WORKDIR /app # Copy go mod files COPY go.mod go.sum ./ # Download dependencies RUN go mod download # Copy source code COPY . . # Build the application RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main cmd/server/main.go # Final stage FROM alpine:latest # Install runtime dependencies RUN apk --no-cache add ca-certificates tzdata # Create non-root user RUN addgroup -g 1000 appgroup \u0026amp;\u0026amp; \\ adduser -D -s /bin/sh -u 1000 -G appgroup appuser # Set working directory WORKDIR /app # Copy binary from builder stage COPY --from=builder /app/main . COPY --from=builder /app/migrations ./migrations # Change ownership to non-root user RUN chown -R appuser:appgroup /app # Switch to non-root user USER appuser # Expose port EXPOSE 8080 # Health check HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1 # Run the application CMD [\u0026#34;./main\u0026#34;] Create a docker-compose file for local development:\n# docker-compose.yml version: \u0026#39;3.8\u0026#39; services: user-service: build: context: . dockerfile: Dockerfile ports: - \u0026#34;8080:8080\u0026#34; environment: - DATABASE_URL=postgres://user:password@postgres:5432/userdb?sslmode=disable - JWT_SECRET=your-secret-key - NATS_URL=nats://nats:4222 depends_on: - postgres - nats networks: - microservices postgres: image: postgres:15-alpine environment: - POSTGRES_USER=user - POSTGRES_PASSWORD=password - POSTGRES_DB=userdb volumes: - postgres_data:/var/lib/postgresql/data ports: - \u0026#34;5432:5432\u0026#34; networks: - microservices nats: image: nats:latest ports: - \u0026#34;4222:4222\u0026#34; - \u0026#34;8222:8222\u0026#34; networks: - microservices redis: image: redis:7-alpine ports: - \u0026#34;6379:6379\u0026#34; networks: - microservices volumes: postgres_data: networks: microservices: driver: bridge API Gateway Implementation An API Gateway serves as the single entry point for all client requests, providing routing, authentication, rate limiting, and other cross-cutting concerns. Let\u0026rsquo;s implement a simple API Gateway using Go:\n// gateway/main.go package main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;net/url\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; \u0026#34;golang.org/x/time/rate\u0026#34; ) type Gateway struct { routes map[string]*httputil.ReverseProxy rateLimiter *rate.Limiter } type RouteConfig struct { Path string `json:\u0026#34;path\u0026#34;` Service string `json:\u0026#34;service\u0026#34;` URL string `json:\u0026#34;url\u0026#34;` } func NewGateway() *Gateway { return \u0026amp;Gateway{ routes: make(map[string]*httputil.ReverseProxy), rateLimiter: rate.NewLimiter(rate.Limit(100), 200), // 100 requests per second, burst of 200 } } func (g *Gateway) AddRoute(path, serviceURL string) error { target, err := url.Parse(serviceURL) if err != nil { return err } proxy := httputil.NewSingleHostReverseProxy(target) // Customize proxy behavior proxy.ModifyResponse = func(r *http.Response) error { // Add CORS headers r.Header.Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) r.Header.Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST, PUT, DELETE, OPTIONS\u0026#34;) r.Header.Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Authorization, Content-Type\u0026#34;) return nil } g.routes[path] = proxy return nil } func (g *Gateway) ServeHTTP(w http.ResponseWriter, r *http.Request) { // Apply rate limiting if !g.rateLimiter.Allow() { http.Error(w, \u0026#34;Rate limit exceeded\u0026#34;, http.StatusTooManyRequests) return } // Handle CORS preflight if r.Method == \u0026#34;OPTIONS\u0026#34; { g.handleCORS(w, r) return } // Find matching route var proxy *httputil.ReverseProxy var matchedPath string for path, p := range g.routes { if strings.HasPrefix(r.URL.Path, path) { proxy = p matchedPath = path break } } if proxy == nil { http.Error(w, \u0026#34;Service not found\u0026#34;, http.StatusNotFound) return } // Remove the matched path prefix r.URL.Path = strings.TrimPrefix(r.URL.Path, matchedPath) if r.URL.Path == \u0026#34;\u0026#34; { r.URL.Path = \u0026#34;/\u0026#34; } // Add tracing headers r.Header.Set(\u0026#34;X-Request-ID\u0026#34;, generateRequestID()) r.Header.Set(\u0026#34;X-Forwarded-For\u0026#34;, r.RemoteAddr) // Proxy the request proxy.ServeHTTP(w, r) } func (g *Gateway) handleCORS(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST, PUT, DELETE, OPTIONS\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Authorization, Content-Type\u0026#34;) w.WriteHeader(http.StatusOK) } func generateRequestID() string { return fmt.Sprintf(\u0026#34;%d\u0026#34;, time.Now().UnixNano()) } // Authentication middleware func authMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Skip authentication for certain paths skipAuth := []string{\u0026#34;/api/v1/users/login\u0026#34;, \u0026#34;/api/v1/users/register\u0026#34;, \u0026#34;/health\u0026#34;} for _, path := range skipAuth { if strings.HasPrefix(r.URL.Path, path) { next.ServeHTTP(w, r) return } } // Extract token from Authorization header authHeader := r.Header.Get(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { http.Error(w, \u0026#34;Authorization header required\u0026#34;, http.StatusUnauthorized) return } // Validate token with user service if !validateTokenWithUserService(authHeader) { http.Error(w, \u0026#34;Invalid token\u0026#34;, http.StatusUnauthorized) return } next.ServeHTTP(w, r) }) } func validateTokenWithUserService(token string) bool { // Implementation would call user service for token validation // This is a simplified example return true } func main() { gateway := NewGateway() // Configure routes routes := []RouteConfig{ {Path: \u0026#34;/api/v1/users\u0026#34;, Service: \u0026#34;user-service\u0026#34;, URL: \u0026#34;http://user-service:8080\u0026#34;}, {Path: \u0026#34;/api/v1/posts\u0026#34;, Service: \u0026#34;content-service\u0026#34;, URL: \u0026#34;http://content-service:8080\u0026#34;}, {Path: \u0026#34;/api/v1/comments\u0026#34;, Service: \u0026#34;comment-service\u0026#34;, URL: \u0026#34;http://comment-service:8080\u0026#34;}, } for _, route := range routes { err := gateway.AddRoute(route.Path, route.URL) if err != nil { log.Fatalf(\u0026#34;Failed to add route %s: %v\u0026#34;, route.Path, err) } log.Printf(\u0026#34;Added route: %s -\u0026gt; %s\u0026#34;, route.Path, route.URL) } // Setup router router := mux.NewRouter() // Health check endpoint router.HandleFunc(\u0026#34;/health\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(map[string]string{\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;}) }).Methods(\u0026#34;GET\u0026#34;) // Apply middleware and route all other requests through gateway router.PathPrefix(\u0026#34;/\u0026#34;).Handler(authMiddleware(gateway)) log.Println(\u0026#34;API Gateway starting on :8000\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8000\u0026#34;, router)) } Monitoring and Observability Effective monitoring is crucial for microservices systems. Implement structured logging, metrics collection, and distributed tracing to maintain visibility into your system\u0026rsquo;s behavior.\nCreate a monitoring package that integrates with your services:\n// pkg/monitoring/logger.go package monitoring import ( \u0026#34;context\u0026#34; \u0026#34;log/slog\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) type Logger struct { logger *slog.Logger } func NewLogger(serviceName string) *Logger { logger := slog.New(slog.NewJSONHandler(os.Stdout, \u0026amp;slog.HandlerOptions{ Level: slog.LevelInfo, })).With( slog.String(\u0026#34;service\u0026#34;, serviceName), slog.String(\u0026#34;version\u0026#34;, os.Getenv(\u0026#34;SERVICE_VERSION\u0026#34;)), ) return \u0026amp;Logger{logger: logger} } func (l *Logger) Info(ctx context.Context, msg string, args ...any) { l.logger.InfoContext(ctx, msg, args...) } func (l *Logger) Error(ctx context.Context, msg string, args ...any) { l.logger.ErrorContext(ctx, msg, args...) } func (l *Logger) Warn(ctx context.Context, msg string, args ...any) { l.logger.WarnContext(ctx, msg, args...) } // Middleware for HTTP request logging func (l *Logger) HTTPMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() // Create response recorder to capture status code recorder := \u0026amp;responseRecorder{ResponseWriter: w, statusCode: 200} // Process request next.ServeHTTP(recorder, r) // Log request details l.logger.InfoContext(r.Context(), \u0026#34;HTTP request\u0026#34;, slog.String(\u0026#34;method\u0026#34;, r.Method), slog.String(\u0026#34;path\u0026#34;, r.URL.Path), slog.String(\u0026#34;remote_addr\u0026#34;, r.RemoteAddr), slog.Int(\u0026#34;status_code\u0026#34;, recorder.statusCode), slog.Duration(\u0026#34;duration\u0026#34;, time.Since(start)), slog.String(\u0026#34;user_agent\u0026#34;, r.UserAgent()), ) }) } type responseRecorder struct { http.ResponseWriter statusCode int } func (r *responseRecorder) WriteHeader(code int) { r.statusCode = code r.ResponseWriter.WriteHeader(code) } Production Deployment with Kubernetes For production deployment, Kubernetes provides orchestration, scaling, and management capabilities essential for microservices. Here\u0026rsquo;s a complete Kubernetes deployment configuration:\n# k8s/user-service.yaml apiVersion: apps/v1 kind: Deployment metadata: name: user-service labels: app: user-service spec: replicas: 3 selector: matchLabels: app: user-service template: metadata: labels: app: user-service spec: containers: - name: user-service image: your-registry/user-service:latest ports: - containerPort: 8080 env: - name: DATABASE_URL valueFrom: secretKeyRef: name: user-service-secrets key: database-url - name: JWT_SECRET valueFrom: secretKeyRef: name: user-service-secrets key: jwt-secret - name: NATS_URL value: \u0026#34;nats://nats:4222\u0026#34; resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;100m\u0026#34; limits: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 --- apiVersion: v1 kind: Service metadata: name: user-service spec: selector: app: user-service ports: - protocol: TCP port: 80 targetPort: 8080 type: ClusterIP --- apiVersion: v1 kind: Secret metadata: name: user-service-secrets type: Opaque data: database-url: \u0026lt;base64-encoded-database-url\u0026gt; jwt-secret: \u0026lt;base64-encoded-jwt-secret\u0026gt; Testing Microservices Comprehensive testing strategies are essential for microservices reliability. Implement unit tests, integration tests, and contract tests to ensure service quality:\n// internal/handler/user_test.go package handler_test import ( \u0026#34;bytes\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httptest\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;github.com/stretchr/testify/mock\u0026#34; \u0026#34;user-service/internal/handler\u0026#34; \u0026#34;user-service/internal/model\u0026#34; ) type MockUserService struct { mock.Mock } func (m *MockUserService) CreateUser(ctx context.Context, req *model.CreateUserRequest) (*model.User, error) { args := m.Called(ctx, req) return args.Get(0).(*model.User), args.Error(1) } func (m *MockUserService) Login(ctx context.Context, req *model.LoginRequest) (*model.LoginResponse, error) { args := m.Called(ctx, req) return args.Get(0).(*model.LoginResponse), args.Error(1) } func TestUserHandler_CreateUser(t *testing.T) { mockService := new(MockUserService) handler := handler.NewUserHandler(mockService) user := \u0026amp;model.User{ ID: \u0026#34;123\u0026#34;, Username: \u0026#34;testuser\u0026#34;, Email: \u0026#34;test@example.com\u0026#34;, FirstName: \u0026#34;Test\u0026#34;, LastName: \u0026#34;User\u0026#34;, } mockService.On(\u0026#34;CreateUser\u0026#34;, mock.Anything, mock.AnythingOfType(\u0026#34;*model.CreateUserRequest\u0026#34;)).Return(user, nil) reqBody := model.CreateUserRequest{ Username: \u0026#34;testuser\u0026#34;, Email: \u0026#34;test@example.com\u0026#34;, Password: \u0026#34;password123\u0026#34;, FirstName: \u0026#34;Test\u0026#34;, LastName: \u0026#34;User\u0026#34;, } body, _ := json.Marshal(reqBody) req := httptest.NewRequest(\u0026#34;POST\u0026#34;, \u0026#34;/users\u0026#34;, bytes.NewBuffer(body)) req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) rr := httptest.NewRecorder() handler.CreateUser(rr, req) assert.Equal(t, http.StatusCreated, rr.Code) var response model.User err := json.NewDecoder(rr.Body).Decode(\u0026amp;response) assert.NoError(t, err) assert.Equal(t, user.Username, response.Username) assert.Equal(t, user.Email, response.Email) mockService.AssertExpectations(t) } For integration testing, create test helpers that spin up real database instances and test the complete service stack.\nConclusion Building microservices with Go provides a powerful foundation for scalable, maintainable distributed systems. The patterns and implementations covered in this guide demonstrate how to leverage Go\u0026rsquo;s strengths while addressing the unique challenges of microservices architecture.\nKey takeaways include the importance of clear service boundaries, effective communication strategies, and comprehensive monitoring. Whether you\u0026rsquo;re migrating from a monolithic architecture or building new distributed systems, these patterns provide a solid foundation for success.\nFor developers transitioning from building REST APIs with frameworks like Gin , microservices represent the next evolution in API architecture. The investment in understanding distributed systems patterns and Go\u0026rsquo;s ecosystem pays dividends in building applications that can scale with business growth.\nRemember that microservices introduce complexity in exchange for flexibility and scalability. Start with a well-designed monolith and extract services as your team and requirements grow. The patterns demonstrated here will serve you well whether you\u0026rsquo;re building a small distributed system or a large-scale microservices platform.\n","href":"/2025/09/microservices-golang-architecture-implementation-guide.html","title":"Microservices with Golang - Architecture and Implementation Guide"},{"content":"Modern web applications require APIs that can efficiently serve data to various clients with different needs. While traditional REST APIs have served this purpose for years, GraphQL has emerged as a powerful alternative that solves many common API development challenges. When building GraphQL servers in Go, gqlgen stands out as the most mature and feature-rich library available.\nThis comprehensive guide will walk you through building a complete GraphQL server using gqlgen, from initial setup to production deployment. We\u0026rsquo;ll cover schema design, resolver implementation, database integration, and performance optimization techniques that will help you build robust, scalable GraphQL APIs.\nUnderstanding gqlgen and Its Advantages The gqlgen library takes a schema-first approach to GraphQL development, which means you define your GraphQL schema first, and the library generates the corresponding Go code. This approach offers several significant advantages over schema-last libraries where you define resolvers first and generate schemas from code.\nSchema-first development ensures your API contract is explicitly defined and serves as the single source of truth for both frontend and backend teams. The generated code is type-safe, eliminating runtime errors that commonly occur with manual type casting. Additionally, gqlgen generates efficient resolver interfaces that guide your implementation and ensure consistency across your codebase.\nThe library also provides excellent tooling for development, including automatic code generation, built-in validation, and comprehensive error handling. These features significantly reduce boilerplate code and allow you to focus on business logic rather than GraphQL implementation details.\nProject Setup and Initial Configuration Before diving into code, let\u0026rsquo;s establish a proper project structure that will scale as your GraphQL API grows. Start by creating a new Go module and organizing directories for different components of your application.\nmkdir graphql-server cd graphql-server go mod init github.com/yourusername/graphql-server mkdir -p {graph,models,database,middleware} Install the necessary dependencies for our GraphQL server:\ngo get github.com/99designs/gqlgen go get github.com/99designs/gqlgen/graphql/handler go get github.com/99designs/gqlgen/graphql/playground go get github.com/go-chi/chi/v5 go get github.com/go-chi/chi/v5/middleware Create a configuration file gqlgen.yml in your project root to customize gqlgen\u0026rsquo;s behavior:\n# gqlgen.yml schema: - graph/*.graphql exec: filename: graph/generated.go package: graph model: filename: models/models_gen.go package: models resolver: filename: graph/resolver.go package: graph type: Resolver autobind: - \u0026#34;github.com/yourusername/graphql-server/models\u0026#34; models: ID: model: - github.com/99designs/gqlgen/graphql.ID - github.com/99designs/gqlgen/graphql.Int - github.com/99designs/gqlgen/graphql.Int64 - github.com/99designs/gqlgen/graphql.Int32 DateTime: model: time.Time This configuration tells gqlgen where to find schema files, where to generate code, and how to handle custom scalar types like DateTime.\nDesigning Your GraphQL Schema A well-designed schema is the foundation of any successful GraphQL API. Let\u0026rsquo;s create a practical schema for a blog application that demonstrates common GraphQL patterns and best practices.\nCreate graph/schema.graphql with the following content:\nscalar DateTime type User { id: ID! username: String! email: String! displayName: String! bio: String avatar: String posts: [Post!]! createdAt: DateTime! updatedAt: DateTime! } type Post { id: ID! title: String! content: String! excerpt: String slug: String! status: PostStatus! author: User! tags: [Tag!]! comments: [Comment!]! createdAt: DateTime! updatedAt: DateTime! publishedAt: DateTime } type Tag { id: ID! name: String! slug: String! description: String posts: [Post!]! } type Comment { id: ID! content: String! author: User! post: Post! parent: Comment replies: [Comment!]! createdAt: DateTime! updatedAt: DateTime! } enum PostStatus { DRAFT PUBLISHED ARCHIVED } type Query { # User queries user(id: ID!): User users(limit: Int, offset: Int): [User!]! # Post queries post(id: ID, slug: String): Post posts(limit: Int, offset: Int, status: PostStatus): [Post!]! postsByUser(userId: ID!, limit: Int, offset: Int): [Post!]! # Tag queries tag(id: ID, slug: String): Tag tags: [Tag!]! # Comment queries commentsByPost(postId: ID!, limit: Int, offset: Int): [Comment!]! } type Mutation { # User mutations createUser(input: CreateUserInput!): User! updateUser(id: ID!, input: UpdateUserInput!): User! deleteUser(id: ID!): Boolean! # Post mutations createPost(input: CreatePostInput!): Post! updatePost(id: ID!, input: UpdatePostInput!): Post! deletePost(id: ID!): Boolean! publishPost(id: ID!): Post! # Tag mutations createTag(input: CreateTagInput!): Tag! updateTag(id: ID!, input: UpdateTagInput!): Tag! deleteTag(id: ID!): Boolean! # Comment mutations createComment(input: CreateCommentInput!): Comment! updateComment(id: ID!, input: UpdateCommentInput!): Comment! deleteComment(id: ID!): Boolean! } # Input types for mutations input CreateUserInput { username: String! email: String! displayName: String! bio: String avatar: String } input UpdateUserInput { displayName: String bio: String avatar: String } input CreatePostInput { title: String! content: String! excerpt: String slug: String! status: PostStatus! tagIds: [ID!] } input UpdatePostInput { title: String content: String excerpt: String slug: String status: PostStatus tagIds: [ID!] } input CreateTagInput { name: String! slug: String! description: String } input UpdateTagInput { name: String description: String } input CreateCommentInput { content: String! postId: ID! parentId: ID } input UpdateCommentInput { content: String! } This schema demonstrates several GraphQL best practices including proper use of scalar types, enums, input types, and relationship modeling. The schema is designed to be both flexible and efficient, supporting common queries while avoiding over-fetching problems.\nGenerating Code and Initial Resolver Setup With your schema defined, generate the initial code structure using gqlgen:\ngo run github.com/99designs/gqlgen generate This command creates several files including generated types, resolver interfaces, and the executable schema. The most important file for your implementation is graph/resolver.go, which contains the resolver struct and method stubs for all your schema operations.\nLet\u0026rsquo;s examine the generated resolver structure and add some basic setup:\npackage graph import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/yourusername/graphql-server/models\u0026#34; ) // Resolver is the root resolver struct type Resolver struct { db *sql.DB // Add other dependencies like cache, logger, etc. } // NewResolver creates a new resolver instance func NewResolver(db *sql.DB) *Resolver { return \u0026amp;Resolver{ db: db, } } Now implement some basic query resolvers to get started:\n// Query resolver implementation func (r *queryResolver) User(ctx context.Context, id string) (*models.User, error) { var user models.User query := ` SELECT id, username, email, display_name, bio, avatar, created_at, updated_at FROM users WHERE id = $1 ` row := r.db.QueryRowContext(ctx, query, id) err := row.Scan( \u0026amp;user.ID, \u0026amp;user.Username, \u0026amp;user.Email, \u0026amp;user.DisplayName, \u0026amp;user.Bio, \u0026amp;user.Avatar, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { if err == sql.ErrNoRows { return nil, nil } return nil, err } return \u0026amp;user, nil } func (r *queryResolver) Users(ctx context.Context, limit *int, offset *int) ([]*models.User, error) { defaultLimit := 10 defaultOffset := 0 if limit == nil { limit = \u0026amp;defaultLimit } if offset == nil { offset = \u0026amp;defaultOffset } query := ` SELECT id, username, email, display_name, bio, avatar, created_at, updated_at FROM users ORDER BY created_at DESC LIMIT $1 OFFSET $2 ` rows, err := r.db.QueryContext(ctx, query, *limit, *offset) if err != nil { return nil, err } defer rows.Close() var users []*models.User for rows.Next() { var user models.User err := rows.Scan( \u0026amp;user.ID, \u0026amp;user.Username, \u0026amp;user.Email, \u0026amp;user.DisplayName, \u0026amp;user.Bio, \u0026amp;user.Avatar, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { return nil, err } users = append(users, \u0026amp;user) } return users, nil } Implementing Complex Resolvers and Relationships GraphQL\u0026rsquo;s power lies in its ability to efficiently resolve complex data relationships. Let\u0026rsquo;s implement resolvers that handle nested data loading while avoiding the N+1 query problem.\nFor the User type\u0026rsquo;s posts field, we need a resolver that fetches posts belonging to a specific user:\n// User resolver for the posts field func (r *userResolver) Posts(ctx context.Context, obj *models.User) ([]*models.Post, error) { query := ` SELECT id, title, content, excerpt, slug, status, author_id, created_at, updated_at, published_at FROM posts WHERE author_id = $1 ORDER BY created_at DESC ` rows, err := r.db.QueryContext(ctx, query, obj.ID) if err != nil { return nil, err } defer rows.Close() var posts []*models.Post for rows.Next() { var post models.Post err := rows.Scan( \u0026amp;post.ID, \u0026amp;post.Title, \u0026amp;post.Content, \u0026amp;post.Excerpt, \u0026amp;post.Slug, \u0026amp;post.Status, \u0026amp;post.AuthorID, \u0026amp;post.CreatedAt, \u0026amp;post.UpdatedAt, \u0026amp;post.PublishedAt, ) if err != nil { return nil, err } posts = append(posts, \u0026amp;post) } return posts, nil } // Post resolver for the author field func (r *postResolver) Author(ctx context.Context, obj *models.Post) (*models.User, error) { // Reuse the existing User query resolver return r.Query().User(ctx, obj.AuthorID) } // Post resolver for tags (many-to-many relationship) func (r *postResolver) Tags(ctx context.Context, obj *models.Post) ([]*models.Tag, error) { query := ` SELECT t.id, t.name, t.slug, t.description FROM tags t INNER JOIN post_tags pt ON t.id = pt.tag_id WHERE pt.post_id = $1 ` rows, err := r.db.QueryContext(ctx, query, obj.ID) if err != nil { return nil, err } defer rows.Close() var tags []*models.Tag for rows.Next() { var tag models.Tag err := rows.Scan(\u0026amp;tag.ID, \u0026amp;tag.Name, \u0026amp;tag.Slug, \u0026amp;tag.Description) if err != nil { return nil, err } tags = append(tags, \u0026amp;tag) } return tags, nil } Mutation Implementation and Data Validation Mutations require careful implementation to ensure data integrity and provide meaningful error messages. Let\u0026rsquo;s implement user and post creation mutations with proper validation:\nfunc (r *mutationResolver) CreateUser(ctx context.Context, input models.CreateUserInput) (*models.User, error) { // Validate input if len(input.Username) \u0026lt; 3 { return nil, fmt.Errorf(\u0026#34;username must be at least 3 characters long\u0026#34;) } if !isValidEmail(input.Email) { return nil, fmt.Errorf(\u0026#34;invalid email format\u0026#34;) } // Check if username or email already exists var exists bool checkQuery := `SELECT EXISTS(SELECT 1 FROM users WHERE username = $1 OR email = $2)` err := r.db.QueryRowContext(ctx, checkQuery, input.Username, input.Email).Scan(\u0026amp;exists) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to check user existence: %w\u0026#34;, err) } if exists { return nil, fmt.Errorf(\u0026#34;username or email already exists\u0026#34;) } // Create user user := \u0026amp;models.User{ Username: input.Username, Email: input.Email, DisplayName: input.DisplayName, Bio: input.Bio, Avatar: input.Avatar, CreatedAt: time.Now(), UpdatedAt: time.Now(), } insertQuery := ` INSERT INTO users (username, email, display_name, bio, avatar, created_at, updated_at) VALUES ($1, $2, $3, $4, $5, $6, $7) RETURNING id ` err = r.db.QueryRowContext( ctx, insertQuery, user.Username, user.Email, user.DisplayName, user.Bio, user.Avatar, user.CreatedAt, user.UpdatedAt, ).Scan(\u0026amp;user.ID) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } return user, nil } func (r *mutationResolver) CreatePost(ctx context.Context, input models.CreatePostInput) (*models.Post, error) { // Get user ID from context (assuming authentication middleware sets this) userID := getUserIDFromContext(ctx) if userID == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;authentication required\u0026#34;) } // Validate slug uniqueness var exists bool checkQuery := `SELECT EXISTS(SELECT 1 FROM posts WHERE slug = $1)` err := r.db.QueryRowContext(ctx, checkQuery, input.Slug).Scan(\u0026amp;exists) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to check slug uniqueness: %w\u0026#34;, err) } if exists { return nil, fmt.Errorf(\u0026#34;slug already exists\u0026#34;) } // Begin transaction for post creation and tag associations tx, err := r.db.BeginTx(ctx, nil) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to begin transaction: %w\u0026#34;, err) } defer tx.Rollback() // Create post post := \u0026amp;models.Post{ Title: input.Title, Content: input.Content, Excerpt: input.Excerpt, Slug: input.Slug, Status: input.Status, AuthorID: userID, CreatedAt: time.Now(), UpdatedAt: time.Now(), } if input.Status == models.PostStatusPublished { now := time.Now() post.PublishedAt = \u0026amp;now } insertQuery := ` INSERT INTO posts (title, content, excerpt, slug, status, author_id, created_at, updated_at, published_at) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9) RETURNING id ` err = tx.QueryRowContext( ctx, insertQuery, post.Title, post.Content, post.Excerpt, post.Slug, post.Status, post.AuthorID, post.CreatedAt, post.UpdatedAt, post.PublishedAt, ).Scan(\u0026amp;post.ID) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create post: %w\u0026#34;, err) } // Associate tags if provided if len(input.TagIds) \u0026gt; 0 { for _, tagID := range input.TagIds { _, err = tx.ExecContext(ctx, `INSERT INTO post_tags (post_id, tag_id) VALUES ($1, $2)`, post.ID, tagID) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to associate tag: %w\u0026#34;, err) } } } // Commit transaction err = tx.Commit() if err != nil { return nil, fmt.Errorf(\u0026#34;failed to commit transaction: %w\u0026#34;, err) } return post, nil } // Helper function for email validation func isValidEmail(email string) bool { // Simple email validation - use a proper library in production return strings.Contains(email, \u0026#34;@\u0026#34;) \u0026amp;\u0026amp; strings.Contains(email, \u0026#34;.\u0026#34;) } // Helper function to extract user ID from context func getUserIDFromContext(ctx context.Context) string { if userID, ok := ctx.Value(\u0026#34;userID\u0026#34;).(string); ok { return userID } return \u0026#34;\u0026#34; } Server Setup and Middleware Integration Now let\u0026rsquo;s create the main server file that brings everything together. We\u0026rsquo;ll use chi router for its middleware ecosystem and performance characteristics:\n// main.go package main import ( \u0026#34;database/sql\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/99designs/gqlgen/graphql/handler\u0026#34; \u0026#34;github.com/99designs/gqlgen/graphql/playground\u0026#34; \u0026#34;github.com/go-chi/chi/v5\u0026#34; \u0026#34;github.com/go-chi/chi/v5/middleware\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; // PostgreSQL driver \u0026#34;github.com/yourusername/graphql-server/graph\u0026#34; ) const defaultPort = \u0026#34;8080\u0026#34; func main() { port := os.Getenv(\u0026#34;PORT\u0026#34;) if port == \u0026#34;\u0026#34; { port = defaultPort } // Database connection db, err := sql.Open(\u0026#34;postgres\u0026#34;, os.Getenv(\u0026#34;DATABASE_URL\u0026#34;)) if err != nil { log.Fatal(\u0026#34;Failed to connect to database:\u0026#34;, err) } defer db.Close() // Create resolver with dependencies resolver := graph.NewResolver(db) // Create GraphQL server srv := handler.NewDefaultServer(graph.NewExecutableSchema(graph.Config{Resolvers: resolver})) // Setup router router := chi.NewRouter() // Middleware router.Use(middleware.Logger) router.Use(middleware.Recoverer) router.Use(middleware.RequestID) router.Use(middleware.RealIP) router.Use(corsMiddleware) // Routes router.Handle(\u0026#34;/\u0026#34;, playground.Handler(\u0026#34;GraphQL playground\u0026#34;, \u0026#34;/query\u0026#34;)) router.Handle(\u0026#34;/query\u0026#34;, authMiddleware(srv)) log.Printf(\u0026#34;Connect to http://localhost:%s/ for GraphQL playground\u0026#34;, port) log.Fatal(http.ListenAndServe(\u0026#34;:\u0026#34;+port, router)) } // CORS middleware for frontend integration func corsMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Access-Control-Allow-Origin\u0026#34;, \u0026#34;*\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Methods\u0026#34;, \u0026#34;GET, POST, OPTIONS\u0026#34;) w.Header().Set(\u0026#34;Access-Control-Allow-Headers\u0026#34;, \u0026#34;Accept, Content-Type, Content-Length, Accept-Encoding, Authorization\u0026#34;) if r.Method == \u0026#34;OPTIONS\u0026#34; { return } next.ServeHTTP(w, r) }) } // Authentication middleware func authMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { token := r.Header.Get(\u0026#34;Authorization\u0026#34;) if token != \u0026#34;\u0026#34; { // Validate token and extract user ID // This is a simplified example - implement proper JWT validation userID := validateAndExtractUserID(token) if userID != \u0026#34;\u0026#34; { ctx := context.WithValue(r.Context(), \u0026#34;userID\u0026#34;, userID) r = r.WithContext(ctx) } } next.ServeHTTP(w, r) }) } func validateAndExtractUserID(token string) string { // Implement proper JWT validation here // Return user ID if token is valid, empty string otherwise return \u0026#34;\u0026#34; } Performance Optimization and Caching Strategies GraphQL servers can face unique performance challenges, particularly around the N+1 query problem. Let\u0026rsquo;s implement dataloader pattern to efficiently batch database queries:\n// dataloader/user_loader.go package dataloader import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/graph-gophers/dataloader/v6\u0026#34; \u0026#34;github.com/yourusername/graphql-server/models\u0026#34; ) type UserLoader struct { loader *dataloader.Loader[string, *models.User] } func NewUserLoader(db *sql.DB) *UserLoader { batchFn := func(ctx context.Context, keys []string) []*dataloader.Result[*models.User] { return batchGetUsers(ctx, db, keys) } return \u0026amp;UserLoader{ loader: dataloader.NewBatchedLoader( batchFn, dataloader.WithWait[string, *models.User](10*time.Millisecond), dataloader.WithMaxBatch[string, *models.User](100), ), } } func (ul *UserLoader) Load(ctx context.Context, userID string) (*models.User, error) { return ul.loader.Load(ctx, userID)() } func batchGetUsers(ctx context.Context, db *sql.DB, userIDs []string) []*dataloader.Result[*models.User] { // Create placeholders for the IN clause placeholders := make([]string, len(userIDs)) args := make([]interface{}, len(userIDs)) for i, id := range userIDs { placeholders[i] = fmt.Sprintf(\u0026#34;$%d\u0026#34;, i+1) args[i] = id } query := fmt.Sprintf(` SELECT id, username, email, display_name, bio, avatar, created_at, updated_at FROM users WHERE id IN (%s) `, strings.Join(placeholders, \u0026#34;,\u0026#34;)) rows, err := db.QueryContext(ctx, query, args...) if err != nil { // Return error for all requested keys results := make([]*dataloader.Result[*models.User], len(userIDs)) for i := range results { results[i] = \u0026amp;dataloader.Result[*models.User]{Error: err} } return results } defer rows.Close() // Create a map to store results by ID userMap := make(map[string]*models.User) for rows.Next() { var user models.User err := rows.Scan( \u0026amp;user.ID, \u0026amp;user.Username, \u0026amp;user.Email, \u0026amp;user.DisplayName, \u0026amp;user.Bio, \u0026amp;user.Avatar, \u0026amp;user.CreatedAt, \u0026amp;user.UpdatedAt, ) if err != nil { continue } userMap[user.ID] = \u0026amp;user } // Create results in the same order as requested keys results := make([]*dataloader.Result[*models.User], len(userIDs)) for i, userID := range userIDs { if user, found := userMap[userID]; found { results[i] = \u0026amp;dataloader.Result[*models.User]{Data: user} } else { results[i] = \u0026amp;dataloader.Result[*models.User]{Data: nil} } } return results } Integrate the dataloader into your resolver:\n// Update your resolver to use dataloader func (r *Resolver) AddDataLoaders(db *sql.DB) { r.userLoader = dataloader.NewUserLoader(db) } // Update post resolver to use dataloader func (r *postResolver) Author(ctx context.Context, obj *models.Post) (*models.User, error) { return r.userLoader.Load(ctx, obj.AuthorID) } Testing Your GraphQL Server Comprehensive testing is crucial for GraphQL APIs. Here\u0026rsquo;s how to implement both unit and integration tests:\n// graph/resolver_test.go package graph_test import ( \u0026#34;context\u0026#34; \u0026#34;database/sql\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;github.com/stretchr/testify/require\u0026#34; \u0026#34;github.com/yourusername/graphql-server/graph\u0026#34; \u0026#34;github.com/yourusername/graphql-server/models\u0026#34; ) func TestCreateUser(t *testing.T) { db := setupTestDB(t) defer db.Close() resolver := graph.NewResolver(db) ctx := context.Background() input := models.CreateUserInput{ Username: \u0026#34;testuser\u0026#34;, Email: \u0026#34;test@example.com\u0026#34;, DisplayName: \u0026#34;Test User\u0026#34;, } user, err := resolver.Mutation().CreateUser(ctx, input) require.NoError(t, err) assert.Equal(t, input.Username, user.Username) assert.Equal(t, input.Email, user.Email) assert.Equal(t, input.DisplayName, user.DisplayName) assert.NotEmpty(t, user.ID) assert.WithinDuration(t, time.Now(), user.CreatedAt, time.Second) } func TestUserPosts(t *testing.T) { db := setupTestDB(t) defer db.Close() // Setup test data user := createTestUser(t, db) createTestPost(t, db, user.ID) resolver := graph.NewResolver(db) ctx := context.Background() posts, err := resolver.User().Posts(ctx, user) require.NoError(t, err) assert.Len(t, posts, 1) assert.Equal(t, user.ID, posts[0].AuthorID) } func setupTestDB(t *testing.T) *sql.DB { // Setup test database connection // This could use testcontainers for a real PostgreSQL instance // or an in-memory database for faster tests db, err := sql.Open(\u0026#34;postgres\u0026#34;, \u0026#34;postgres://test:test@localhost/test?sslmode=disable\u0026#34;) require.NoError(t, err) // Run migrations runTestMigrations(t, db) return db } func createTestUser(t *testing.T, db *sql.DB) *models.User { user := \u0026amp;models.User{ Username: \u0026#34;testuser\u0026#34;, Email: \u0026#34;test@example.com\u0026#34;, DisplayName: \u0026#34;Test User\u0026#34;, CreatedAt: time.Now(), UpdatedAt: time.Now(), } query := ` INSERT INTO users (username, email, display_name, created_at, updated_at) VALUES ($1, $2, $3, $4, $5) RETURNING id ` err := db.QueryRow(query, user.Username, user.Email, user.DisplayName, user.CreatedAt, user.UpdatedAt).Scan(\u0026amp;user.ID) require.NoError(t, err) return user } Production Deployment and Security Considerations When deploying your GraphQL server to production, several security and performance considerations become critical. Similar to other Go web applications , proper context handling and timeout management are essential for production stability.\nImplement query complexity analysis to prevent expensive queries from overwhelming your server:\n// Add to your server setup import \u0026#34;github.com/99designs/gqlgen/graphql/handler/extension\u0026#34; srv := handler.NewDefaultServer(schema) // Enable introspection only in development if os.Getenv(\u0026#34;ENVIRONMENT\u0026#34;) != \u0026#34;production\u0026#34; { srv.Use(extension.Introspection{}) } // Set query complexity limits srv.Use(extension.FixedComplexityLimit(300)) // Enable automatic persisted queries for better caching srv.Use(extension.AutomaticPersistedQuery{ Cache: lru.New(1000), }) For database integration in production environments, consider patterns similar to those used in PostgreSQL connections with Go for connection pooling and error handling.\nAdvanced Features and Best Practices As your GraphQL API grows, consider implementing subscriptions for real-time features:\ntype Subscription { commentAdded(postId: ID!): Comment! postPublished: Post! } func (r *subscriptionResolver) CommentAdded(ctx context.Context, postID string) (\u0026lt;-chan *models.Comment, error) { ch := make(chan *models.Comment) // Subscribe to comment events for the specific post go func() { defer close(ch) // Implement your real-time logic here // This could use Redis pub/sub, WebSocket connections, etc. }() return ch, nil } Monitor your GraphQL server\u0026rsquo;s performance and query patterns to identify optimization opportunities. Tools like Apollo Studio or custom metrics collection can provide valuable insights into how clients use your API.\nConclusion Building GraphQL servers with gqlgen provides a robust, type-safe foundation for modern API development. The schema-first approach ensures clear contracts between frontend and backend teams while the generated code reduces boilerplate and prevents runtime errors.\nThis comprehensive guide has covered the essential aspects of GraphQL server development, from initial setup to production deployment. The patterns and practices demonstrated here will help you build scalable, maintainable GraphQL APIs that can grow with your application\u0026rsquo;s needs.\nFor developers familiar with building REST APIs in Go , GraphQL offers a compelling alternative that can significantly improve client-server communication efficiency. The investment in learning GraphQL and gqlgen pays dividends in reduced over-fetching, better developer experience, and more flexible API evolution.\nAs you continue developing with GraphQL, remember that the key to success lies in thoughtful schema design, efficient resolver implementation, and careful attention to performance characteristics. The tools and patterns covered in this guide provide a solid foundation for building production-ready GraphQL services that can scale with your application\u0026rsquo;s growth.\n","href":"/2025/09/building-graphql-server-gqlgen-golang.html","title":"Building GraphQL Server with gqlgen in Golang"},{"content":"The landscape of API development has evolved significantly over the past decade. While REST APIs have been the dominant architecture for building web services, GraphQL has emerged as a compelling alternative that addresses many limitations of traditional REST-based approaches. When combined with Go\u0026rsquo;s performance and simplicity, GraphQL creates a powerful foundation for modern API development.\nUnderstanding GraphQL: Beyond Traditional REST GraphQL represents a paradigm shift in how we think about API design and data fetching. Unlike REST, which exposes multiple endpoints for different resources, GraphQL provides a single endpoint that can handle complex queries with precise data requirements.\nThe fundamental difference lies in data fetching efficiency. Traditional REST APIs often lead to over-fetching or under-fetching problems. For example, when building a user profile page, you might need data from multiple REST endpoints, resulting in several round trips to the server. GraphQL solves this by allowing clients to request exactly what they need in a single query.\nConsider a typical REST scenario where you need user information and their recent posts:\nGET /users/123 GET /users/123/posts?limit=5 With GraphQL, this becomes a single request:\nquery { user(id: 123) { name email posts(limit: 5) { title createdAt } } } Why Golang Excels at GraphQL Implementation Go\u0026rsquo;s characteristics make it particularly suitable for GraphQL server development. The language\u0026rsquo;s strong typing system aligns perfectly with GraphQL\u0026rsquo;s schema-first approach. Go\u0026rsquo;s compilation speed and runtime performance ensure that GraphQL resolvers execute efficiently, even under heavy load.\nThe Go ecosystem offers several excellent GraphQL libraries, with gqlgen being the most popular choice for server-side development. This library generates type-safe Go code from GraphQL schemas, reducing boilerplate and minimizing runtime errors.\nBuilding Your First GraphQL Server in Go Let\u0026rsquo;s start by setting up a basic GraphQL server using gqlgen. First, initialize a new Go module and install the necessary dependencies:\ngo mod init graphql-server go get github.com/99designs/gqlgen go get github.com/99designs/gqlgen/graphql/handler go get github.com/99designs/gqlgen/graphql/playground Create a GraphQL schema file called schema.graphql:\ntype User { id: ID! name: String! email: String! posts: [Post!]! } type Post { id: ID! title: String! content: String! author: User! createdAt: String! } type Query { users: [User!]! user(id: ID!): User posts: [Post!]! } type Mutation { createUser(input: NewUser!): User! createPost(input: NewPost!): Post! } input NewUser { name: String! email: String! } input NewPost { title: String! content: String! authorId: ID! } Initialize the GraphQL configuration:\ngo run github.com/99designs/gqlgen init This command generates several files including resolvers and server configuration. The generated graph/resolver.go file contains the resolver struct where you\u0026rsquo;ll implement your business logic.\nHere\u0026rsquo;s how to implement the resolvers:\npackage graph import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; ) // User represents a user in our system type User struct { ID string `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` } // Post represents a blog post type Post struct { ID string `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Content string `json:\u0026#34;content\u0026#34;` AuthorID string `json:\u0026#34;authorId\u0026#34;` CreatedAt time.Time `json:\u0026#34;createdAt\u0026#34;` } // In-memory storage for demonstration var users = []User{ {ID: \u0026#34;1\u0026#34;, Name: \u0026#34;John Doe\u0026#34;, Email: \u0026#34;john@example.com\u0026#34;}, {ID: \u0026#34;2\u0026#34;, Name: \u0026#34;Jane Smith\u0026#34;, Email: \u0026#34;jane@example.com\u0026#34;}, } var posts = []Post{ {ID: \u0026#34;1\u0026#34;, Title: \u0026#34;Introduction to GraphQL\u0026#34;, Content: \u0026#34;GraphQL is a query language...\u0026#34;, AuthorID: \u0026#34;1\u0026#34;, CreatedAt: time.Now().AddDate(0, 0, -1)}, {ID: \u0026#34;2\u0026#34;, Title: \u0026#34;Building APIs with Go\u0026#34;, Content: \u0026#34;Go is excellent for API development...\u0026#34;, AuthorID: \u0026#34;2\u0026#34;, CreatedAt: time.Now()}, } // Query resolver implementation func (r *queryResolver) Users(ctx context.Context) ([]*User, error) { result := make([]*User, len(users)) for i, user := range users { result[i] = \u0026amp;user } return result, nil } func (r *queryResolver) User(ctx context.Context, id string) (*User, error) { for _, user := range users { if user.ID == id { return \u0026amp;user, nil } } return nil, fmt.Errorf(\u0026#34;user with id %s not found\u0026#34;, id) } func (r *queryResolver) Posts(ctx context.Context) ([]*Post, error) { result := make([]*Post, len(posts)) for i, post := range posts { result[i] = \u0026amp;post } return result, nil } // User resolver for posts field func (r *userResolver) Posts(ctx context.Context, obj *User) ([]*Post, error) { var userPosts []*Post for _, post := range posts { if post.AuthorID == obj.ID { userPosts = append(userPosts, \u0026amp;post) } } return userPosts, nil } // Post resolver for author field func (r *postResolver) Author(ctx context.Context, obj *Post) (*User, error) { for _, user := range users { if user.ID == obj.AuthorID { return \u0026amp;user, nil } } return nil, fmt.Errorf(\u0026#34;author not found\u0026#34;) } Advanced GraphQL Features in Go Once you have basic queries working, you can implement more sophisticated features. Mutations allow clients to modify data on the server:\nfunc (r *mutationResolver) CreateUser(ctx context.Context, input NewUser) (*User, error) { newUser := User{ ID: strconv.Itoa(len(users) + 1), Name: input.Name, Email: input.Email, } users = append(users, newUser) return \u0026amp;newUser, nil } func (r *mutationResolver) CreatePost(ctx context.Context, input NewPost) (*Post, error) { newPost := Post{ ID: strconv.Itoa(len(posts) + 1), Title: input.Title, Content: input.Content, AuthorID: input.AuthorID, CreatedAt: time.Now(), } posts = append(posts, newPost) return \u0026amp;newPost, nil } Subscriptions enable real-time functionality, allowing clients to receive updates when data changes. This is particularly useful for applications requiring live updates, such as chat applications or real-time dashboards.\nDatabase Integration and Data Loading For production applications, you\u0026rsquo;ll typically integrate with a database. Similar to how you might connect PostgreSQL with Go using sqlx , GraphQL resolvers can query databases efficiently.\nHowever, GraphQL introduces the N+1 query problem, where nested fields can trigger multiple database queries. The solution is implementing data loaders, which batch and cache database requests:\npackage dataloader import ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/graph-gophers/dataloader/v6\u0026#34; ) type UserLoader struct { loader *dataloader.Loader[string, *User] } func NewUserLoader() *UserLoader { return \u0026amp;UserLoader{ loader: dataloader.NewBatchedLoader( batchUsers, dataloader.WithWait[string, *User](10*time.Millisecond), dataloader.WithMaxBatch[string, *User](100), ), } } func (ul *UserLoader) Load(ctx context.Context, userID string) (*User, error) { return ul.loader.Load(ctx, userID)() } func batchUsers(ctx context.Context, userIDs []string) []*dataloader.Result[*User] { // Batch load users from database // This function would query all userIDs in a single database call results := make([]*dataloader.Result[*User], len(userIDs)) // Implementation would fetch users by IDs from database // For demonstration, we\u0026#39;ll use our in-memory storage for i, userID := range userIDs { var user *User for _, u := range users { if u.ID == userID { user = \u0026amp;u break } } if user != nil { results[i] = \u0026amp;dataloader.Result[*User]{Data: user} } else { results[i] = \u0026amp;dataloader.Result[*User]{Error: fmt.Errorf(\u0026#34;user %s not found\u0026#34;, userID)} } } return results } Performance Optimization Strategies GraphQL servers require careful attention to performance, especially as schemas grow larger. Query complexity analysis prevents expensive queries from overwhelming your server:\npackage main import ( \u0026#34;github.com/99designs/gqlgen/graphql/handler\u0026#34; \u0026#34;github.com/99designs/gqlgen/graphql/handler/extension\u0026#34; \u0026#34;github.com/99designs/gqlgen/graphql/handler/lru\u0026#34; \u0026#34;github.com/99designs/gqlgen/graphql/handler/transport\u0026#34; ) func createServer() *handler.Server { srv := handler.NewDefaultServer(generated.NewExecutableSchema(generated.Config{Resolvers: \u0026amp;graph.Resolver{}})) // Enable query caching srv.SetQueryCache(lru.New(1000)) // Enable automatic persisted queries srv.Use(extension.AutomaticPersistedQuery{ Cache: lru.New(100), }) // Add complexity limit srv.Use(extension.FixedComplexityLimit(300)) // Enable introspection in development only srv.Use(extension.Introspection{}) return srv } Query depth limiting prevents malicious queries from creating excessive nesting, while query complexity analysis assigns costs to different fields and operations.\nComparison with REST API Development When comparing GraphQL to building REST APIs with frameworks like Gin , several key differences emerge. REST APIs require multiple endpoints for different resources, while GraphQL uses a single endpoint with flexible querying capabilities.\nVersion management in REST often requires new endpoint versions (v1, v2), but GraphQL schemas can evolve without breaking existing clients through field deprecation and addition strategies.\nHowever, REST APIs have advantages in caching strategies, as HTTP caching mechanisms work naturally with REST endpoints. GraphQL requires more sophisticated caching approaches, typically involving query result caching and field-level caching.\nTesting GraphQL APIs in Go Testing GraphQL APIs requires both unit testing of resolvers and integration testing of complete queries. Here\u0026rsquo;s how to test resolver functions:\npackage graph_test import ( \u0026#34;context\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;github.com/stretchr/testify/assert\u0026#34; \u0026#34;your-project/graph\u0026#34; ) func TestUserResolver(t *testing.T) { resolver := \u0026amp;graph.Resolver{} queryResolver := resolver.Query() ctx := context.Background() t.Run(\u0026#34;should return all users\u0026#34;, func(t *testing.T) { users, err := queryResolver.Users(ctx) assert.NoError(t, err) assert.NotEmpty(t, users) }) t.Run(\u0026#34;should return specific user\u0026#34;, func(t *testing.T) { user, err := queryResolver.User(ctx, \u0026#34;1\u0026#34;) assert.NoError(t, err) assert.Equal(t, \u0026#34;1\u0026#34;, user.ID) }) t.Run(\u0026#34;should return error for non-existent user\u0026#34;, func(t *testing.T) { user, err := queryResolver.User(ctx, \u0026#34;999\u0026#34;) assert.Error(t, err) assert.Nil(t, user) }) } For integration testing, you can create test clients that execute complete GraphQL queries against your server.\nSecurity Considerations GraphQL introduces unique security challenges that differ from traditional REST APIs. Query depth limiting and complexity analysis protect against resource exhaustion attacks. Additionally, field-level authorization ensures that users can only access data they\u0026rsquo;re permitted to see.\nImplementing authentication middleware in your GraphQL server follows similar patterns to other Go web applications :\nfunc authMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { token := r.Header.Get(\u0026#34;Authorization\u0026#34;) if token == \u0026#34;\u0026#34; { next.ServeHTTP(w, r) return } // Validate token and extract user information userID, err := validateToken(token) if err != nil { http.Error(w, \u0026#34;Invalid token\u0026#34;, http.StatusUnauthorized) return } // Add user to context ctx := context.WithValue(r.Context(), \u0026#34;userID\u0026#34;, userID) r = r.WithContext(ctx) next.ServeHTTP(w, r) }) } Production Deployment Considerations When deploying GraphQL APIs to production, several factors require attention. Query monitoring and analytics help understand how clients use your API and identify performance bottlenecks. Tools like Apollo Studio or custom monitoring solutions can provide insights into query performance and usage patterns.\nRate limiting in GraphQL differs from REST because clients can construct queries of varying complexity. Instead of simple request-per-second limits, implement query complexity-based rate limiting.\nSchema management becomes crucial as your API evolves. Consider implementing schema versioning strategies and maintaining backward compatibility through field deprecation rather than removing fields immediately.\nGraphQL Ecosystem and Tooling The GraphQL ecosystem in Go continues to expand with tools for schema management, testing, and monitoring. Libraries like gqlgen provide code generation from schemas, ensuring type safety and reducing manual coding errors.\nDevelopment tools such as GraphQL Playground and GraphiQL create interactive environments for testing queries during development. These tools generate documentation automatically from your schema, making API exploration intuitive for frontend developers.\nConclusion GraphQL with Golang offers a powerful combination for building modern APIs that address the limitations of traditional REST approaches. While the learning curve may be steeper than REST, the benefits of precise data fetching, strong typing, and flexible querying make GraphQL an excellent choice for complex applications.\nThe decision between GraphQL and REST depends on your specific requirements. GraphQL excels in scenarios with complex data relationships, multiple client types, and requirements for efficient data fetching. REST remains simpler for basic CRUD operations and scenarios where HTTP caching provides significant benefits.\nAs you continue exploring Go for API development, consider how GraphQL fits into your architecture alongside other patterns and frameworks. The combination of Go\u0026rsquo;s performance characteristics and GraphQL\u0026rsquo;s flexible querying capabilities creates a foundation for building scalable, maintainable APIs that can adapt to evolving client requirements.\nWhether you\u0026rsquo;re building mobile applications, web interfaces, or microservice architectures, GraphQL with Go provides the tools and performance needed for modern API development. The investment in learning GraphQL patterns and best practices pays dividends in reduced development time and improved application performance.\n","href":"/2025/09/graphql-golang-modern-alternative-rest-api.html","title":"GraphQL with Golang - A Modern Alternative to REST API"},{"content":"Choosing the right web framework can make or break your Go project. I\u0026rsquo;ve spent the last three years working with different Go frameworks across various production systems, and the three names that consistently come up in every discussion are Gin, Fiber, and Echo. Each has its passionate advocates, but which one should you actually choose in 2025?\nThe landscape has evolved significantly since these frameworks first appeared. Performance gaps have narrowed, feature sets have matured, and the ecosystem around each has grown substantially. What used to be clear-cut decisions based on pure speed are now more nuanced choices that depend on your specific use case, team experience, and architectural requirements.\nIf you\u0026rsquo;ve been building REST APIs with Go\u0026rsquo;s standard library or are considering moving from another language\u0026rsquo;s web framework, this comparison will help you understand exactly what each framework brings to the table and which one aligns best with your project goals.\nThe Current State of Go Web Frameworks Before diving into specific comparisons, let\u0026rsquo;s understand where these frameworks stand in 2025. Gin remains the most popular with over 75,000 GitHub stars, making it the de facto choice for many developers. Echo has carved out a solid niche with nearly 30,000 stars, particularly among enterprise developers who value its structure and type safety. Fiber, the newest of the three, has rapidly gained traction with its Express.js-inspired API and impressive performance claims.\nWhat\u0026rsquo;s interesting is how the performance differences have become less pronounced over time. While early benchmarks showed significant gaps between frameworks, real-world testing in 2025 reveals that the differences are often negligible for most applications. This shift means your decision should focus more on developer experience, ecosystem, and architectural fit rather than raw performance numbers.\nThe middleware ecosystem has also matured considerably. All three frameworks now offer comprehensive middleware libraries, robust authentication solutions, and production-ready features that eliminate most of the custom code you\u0026rsquo;d otherwise need to write.\nGin Framework Deep Dive Gin has earned its reputation as the most battle-tested framework in the Go ecosystem. Its design philosophy centers around simplicity and performance, built on top of the httprouter package for lightning-fast route matching. What makes Gin special is how it strikes a balance between being lightweight and feature-complete.\nThe framework\u0026rsquo;s middleware system is particularly elegant. You can chain middleware functions effortlessly, and the context passing mechanism makes it easy to share data between middleware and handlers. JSON binding works seamlessly out of the box, and the validation integration with go-playground/validator provides comprehensive request validation with minimal boilerplate.\nHere\u0026rsquo;s what a typical Gin application structure looks like:\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { r := gin.Default() // Middleware r.Use(gin.Logger()) r.Use(gin.Recovery()) // Routes api := r.Group(\u0026#34;/api/v1\u0026#34;) { api.GET(\u0026#34;/users\u0026#34;, getUsers) api.POST(\u0026#34;/users\u0026#34;, createUser) api.GET(\u0026#34;/users/:id\u0026#34;, getUser) } r.Run(\u0026#34;:8080\u0026#34;) } func getUsers(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;users\u0026#34;: []string{\u0026#34;user1\u0026#34;, \u0026#34;user2\u0026#34;}, }) } Gin\u0026rsquo;s biggest strength lies in its maturity and extensive community. You\u0026rsquo;ll find solutions for almost any problem you encounter, extensive middleware libraries, and comprehensive documentation. The learning curve is gentle, making it an excellent choice for teams transitioning from other languages or frameworks.\nHowever, Gin\u0026rsquo;s simplicity can also be a limitation for complex applications. While you can build sophisticated APIs, you\u0026rsquo;ll often find yourself implementing custom solutions for advanced features that other frameworks provide out of the box.\nEcho Framework Analysis Echo positions itself as the enterprise-ready framework with a focus on high performance and developer productivity. Its API design is more opinionated than Gin\u0026rsquo;s, providing more structure out of the box while maintaining flexibility where it matters.\nThe framework\u0026rsquo;s strength lies in its comprehensive feature set. Built-in data binding, validation, rendering, and middleware support cover most common web development needs. Echo\u0026rsquo;s context package is particularly well-designed, providing type-safe parameter binding and powerful middleware composition capabilities.\nEcho\u0026rsquo;s approach to middleware is worth highlighting:\npackage main import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/labstack/echo/v4\u0026#34; \u0026#34;github.com/labstack/echo/v4/middleware\u0026#34; ) func main() { e := echo.New() // Middleware e.Use(middleware.Logger()) e.Use(middleware.Recover()) e.Use(middleware.CORS()) // Routes api := e.Group(\u0026#34;/api/v1\u0026#34;) api.GET(\u0026#34;/users\u0026#34;, getUsers) api.POST(\u0026#34;/users\u0026#34;, createUser) e.Logger.Fatal(e.Start(\u0026#34;:8080\u0026#34;)) } func getUsers(c echo.Context) error { return c.JSON(http.StatusOK, map[string][]string{ \u0026#34;users\u0026#34;: {\u0026#34;user1\u0026#34;, \u0026#34;user2\u0026#34;}, }) } Echo\u0026rsquo;s error handling mechanism is more sophisticated than Gin\u0026rsquo;s, with built-in support for HTTP errors and custom error handling middleware. This makes building robust APIs easier, especially for larger applications where consistent error handling is crucial.\nThe framework also provides excellent support for HTTP/2, WebSocket connections, and automatic TLS, making it suitable for modern web applications that require these features. The built-in template rendering engine and static file serving capabilities mean you can build full web applications, not just APIs.\nEcho\u0026rsquo;s main drawback is its steeper learning curve compared to Gin. The more opinionated design means there are more concepts to learn upfront, though this pays dividends in larger, more complex projects.\nFiber Framework Examination Fiber takes a different approach entirely, drawing heavy inspiration from Express.js to create a familiar experience for developers coming from the Node.js ecosystem. Built on top of fasthttp rather than the standard net/http package, Fiber prioritizes raw performance above all else.\nThe Express.js-like API makes Fiber immediately familiar to many developers:\npackage main import ( \u0026#34;github.com/gofiber/fiber/v2\u0026#34; \u0026#34;github.com/gofiber/fiber/v2/middleware/logger\u0026#34; \u0026#34;github.com/gofiber/fiber/v2/middleware/recover\u0026#34; ) func main() { app := fiber.New() // Middleware app.Use(logger.New()) app.Use(recover.New()) // Routes api := app.Group(\u0026#34;/api/v1\u0026#34;) api.Get(\u0026#34;/users\u0026#34;, getUsers) api.Post(\u0026#34;/users\u0026#34;, createUser) app.Listen(\u0026#34;:8080\u0026#34;) } func getUsers(c *fiber.Ctx) error { return c.JSON(fiber.Map{ \u0026#34;users\u0026#34;: []string{\u0026#34;user1\u0026#34;, \u0026#34;user2\u0026#34;}, }) } Fiber\u0026rsquo;s performance characteristics are impressive. In benchmark tests, it consistently delivers higher requests per second and lower latency compared to Gin and Echo. The framework achieves this through its use of fasthttp, which provides zero-allocation routing and request handling in many scenarios.\nThe middleware ecosystem around Fiber has grown rapidly, with official middleware packages covering everything from CORS and compression to JWT authentication and rate limiting. The framework\u0026rsquo;s modular design makes it easy to add only the features you need, keeping your application lean.\nHowever, Fiber\u0026rsquo;s use of fasthttp instead of Go\u0026rsquo;s standard net/http comes with trade-offs. Some third-party libraries designed for standard HTTP handlers won\u0026rsquo;t work directly with Fiber. While adapters exist, this can create integration challenges, especially when working with existing codebases or specific monitoring tools.\nPerformance Benchmarks and Real-World Testing The performance conversation around these frameworks deserves careful examination. While micro-benchmarks often show dramatic differences, real-world performance depends heavily on your specific use case, database interactions, and business logic complexity.\nRecent 2025 benchmarks using Go 1.23.5 show interesting results. In synthetic \u0026ldquo;Hello World\u0026rdquo; tests, Echo slightly edges out both Gin and Fiber for raw throughput. However, when testing real-world scenarios with database interactions, JSON processing, and middleware chains, the differences become much smaller.\nFiber\u0026rsquo;s performance advantage is most noticeable in high-concurrency scenarios with simple request processing. Its architecture shines when handling thousands of simultaneous connections with minimal processing per request. In a real-world API benchmark, Fiber achieved approximately 36,000 requests per second compared to 34,000 for both Gin and Echo.\nMemory usage patterns also differ significantly. Fiber\u0026rsquo;s zero-allocation design results in lower memory pressure under high load, while Gin and Echo show more predictable memory patterns that are easier to profile and optimize. For most applications, these differences won\u0026rsquo;t impact user experience, but they matter for high-scale deployments.\nThe median latency differences are minimal across all three frameworks, typically varying by less than a millisecond in production scenarios. Where you\u0026rsquo;ll see more significant performance differences is in CPU utilization under sustained load, where Fiber\u0026rsquo;s optimizations provide measurable benefits.\nMiddleware Ecosystem and Extensibility The middleware ecosystem can make or break a framework\u0026rsquo;s productivity benefits. All three frameworks have mature middleware libraries, but they differ in approach and coverage.\nGin\u0026rsquo;s middleware ecosystem is the most extensive, benefiting from its longer presence in the market. The gin-contrib organization provides official middleware for common needs like CORS, sessions, and rate limiting. Third-party middleware is abundant, and the simple interface makes custom middleware development straightforward.\nEcho\u0026rsquo;s built-in middleware is comprehensive, covering most production needs without external dependencies. The framework includes rate limiting, CORS, JWT authentication, compression, and request logging out of the box. Custom middleware development follows a clean pattern, and the typed context makes middleware more robust than Gin\u0026rsquo;s approach.\nFiber\u0026rsquo;s middleware collection is growing rapidly and follows Express.js patterns that many developers find intuitive. The official gofiber organization maintains high-quality middleware packages, and the community has contributed adapters for popular Go libraries. However, the fasthttp dependency sometimes requires special versions of middleware that work with Fiber\u0026rsquo;s request/response model.\nAuthentication and authorization patterns differ across frameworks. Gin typically relies on third-party JWT libraries and custom middleware. Echo provides built-in JWT middleware with flexible configuration options. Fiber offers both built-in JWT support and compatibility with popular authentication libraries through adapters.\nDevelopment Experience and Learning Curve The developer experience varies significantly between these frameworks, affecting both initial learning time and long-term productivity.\nGin offers the gentlest learning curve. Its API closely resembles Go\u0026rsquo;s standard library patterns, making it intuitive for developers already familiar with Go. Documentation is extensive, community resources are abundant, and most developers can be productive within a few hours of first exposure.\nThe framework\u0026rsquo;s simplicity means fewer abstractions to learn, but this can lead to more boilerplate code in complex applications. Error handling follows Go\u0026rsquo;s standard patterns, which keeps things familiar but sometimes verbose.\nEcho provides more structure upfront, which translates to a steeper initial learning curve but potentially higher productivity in complex projects. The framework\u0026rsquo;s opinions about request handling, error management, and middleware composition create consistency across applications.\nEcho\u0026rsquo;s typed context and parameter binding reduce runtime errors and improve IDE support. The built-in validation and error handling create more predictable application behavior, though they require understanding Echo\u0026rsquo;s specific patterns.\nFiber\u0026rsquo;s Express.js-inspired API creates an interesting dynamic. Developers with JavaScript/Node.js background find it immediately familiar, while Go-native developers might find some patterns unusual. The framework\u0026rsquo;s approach to contexts and middleware follows JavaScript conventions more than Go conventions.\nThe documentation quality for Fiber has improved significantly, though it still lags behind Gin and Echo in terms of community resources and third-party tutorials.\nProduction Readiness and Deployment All three frameworks are production-ready, but they differ in their operational characteristics and deployment patterns.\nGin\u0026rsquo;s maturity shows in its production deployment patterns. The framework has been tested in countless production environments, and best practices are well-established. Memory usage is predictable, and the standard net/http foundation means excellent compatibility with Go\u0026rsquo;s tooling ecosystem.\nMonitoring and observability work seamlessly with standard Go tools. Gin applications integrate well with prometheus metrics, distributed tracing systems, and standard logging frameworks. The production deployment patterns are well-documented and battle-tested.\nEcho\u0026rsquo;s production characteristics are similarly robust. The framework\u0026rsquo;s built-in middleware handles many production concerns like request logging, panic recovery, and CORS out of the box. HTTP/2 support is excellent, and the framework handles WebSocket connections reliably.\nEcho applications tend to be more structured, which can simplify maintenance and debugging in production environments. The comprehensive error handling makes troubleshooting easier, and the framework\u0026rsquo;s middleware system provides good visibility into request processing.\nFiber\u0026rsquo;s production deployment requires more consideration due to its fasthttp foundation. While performance is excellent, some monitoring tools and middleware designed for standard net/http handlers require adapters. Memory profiling works differently, and debugging tools might need special configuration.\nHowever, Fiber\u0026rsquo;s performance characteristics can be a significant advantage in high-throughput scenarios. Applications that need to handle extreme load with minimal resource usage benefit from Fiber\u0026rsquo;s optimizations.\nWhich Framework Should You Choose in 2025 The decision between Gin, Echo, and Fiber depends on several factors specific to your project and team.\nChoose Gin if you\u0026rsquo;re building your first Go web application, need maximum compatibility with Go\u0026rsquo;s ecosystem, prefer simple and familiar patterns, or are migrating from a different language and want minimal learning curve. Gin excels for small to medium-sized REST APIs, microservices where simplicity matters, and teams that value proven, stable technology.\nGin is also the safest choice for long-term projects where maintainability and community support matter more than cutting-edge features. The extensive middleware ecosystem and abundant documentation make it easy to find solutions for common problems.\nChoose Echo if you\u0026rsquo;re building enterprise applications that need structure, require comprehensive built-in features, value type safety and robust error handling, or need HTTP/2 and WebSocket support. Echo works well for larger teams where consistency matters, complex APIs with sophisticated middleware requirements, and applications where developer productivity improvements justify a steeper learning curve.\nEcho\u0026rsquo;s opinionated design creates more maintainable code in large projects, and its comprehensive feature set reduces the need for third-party dependencies.\nChoose Fiber if raw performance is critical to your application, you\u0026rsquo;re migrating from Node.js/Express.js, need to handle extremely high concurrent loads, or want cutting-edge performance optimizations. Fiber excels in high-throughput APIs, real-time applications, microservices where performance matters more than ecosystem compatibility, and scenarios where every millisecond counts.\nHowever, consider Fiber carefully if you\u0026rsquo;re building applications that need extensive integration with Go\u0026rsquo;s standard ecosystem or if your team isn\u0026rsquo;t comfortable with the fasthttp trade-offs.\nIntegration with Other Go Technologies The choice of web framework affects how easily you can integrate with other Go technologies and patterns. This becomes particularly important as your application grows and you need to incorporate databases, message queues, monitoring systems, and other infrastructure components.\nGin\u0026rsquo;s use of standard net/http makes it compatible with virtually any Go library or tool. Whether you\u0026rsquo;re using GORM for database operations , implementing gRPC services , or adding structured logging , integration is typically straightforward.\nEcho similarly benefits from standard library compatibility while providing additional abstractions that can simplify integration. The framework\u0026rsquo;s context system plays well with Go\u0026rsquo;s context patterns, making it easy to implement request timeouts, cancellation, and distributed tracing.\nFiber\u0026rsquo;s fasthttp foundation occasionally creates integration challenges. While adapters exist for most popular libraries, you might encounter situations where custom integration work is required. This is most noticeable with monitoring and observability tools that expect standard HTTP handlers.\nFuture Outlook and Community Trends Looking ahead, all three frameworks continue active development with strong community support. Gin\u0026rsquo;s development has stabilized around maintaining backward compatibility while incorporating essential new features. The focus has shifted to ecosystem improvements and performance optimizations rather than major API changes.\nEcho maintains steady development with regular feature additions and performance improvements. The framework\u0026rsquo;s enterprise focus means continued investment in stability, security, and developer productivity features.\nFiber\u0026rsquo;s development pace is the most aggressive, with frequent releases adding new features and performance optimizations. The framework benefits from rapid adoption and an active community contributing middleware and extensions.\nThe broader Go ecosystem trend toward standardization around certain patterns benefits all three frameworks. As the community converges on best practices for areas like error handling and testing , the frameworks adapt to support these patterns consistently.\nMaking the Final Decision After working with all three frameworks across different projects, I\u0026rsquo;ve found that the \u0026ldquo;best\u0026rdquo; framework is the one that matches your team\u0026rsquo;s experience level, project requirements, and long-term maintenance goals.\nFor most developers starting new projects in 2025, Gin remains the safest choice. Its maturity, extensive ecosystem, and gentle learning curve make it suitable for a wide range of applications. You\u0026rsquo;re unlikely to encounter insurmountable problems, and solutions for common challenges are well-documented.\nEcho makes sense when you\u0026rsquo;re building larger, more structured applications where the framework\u0026rsquo;s opinions help maintain consistency across a larger codebase. The comprehensive built-in features reduce dependencies and create more predictable applications.\nFiber is worth considering when performance is genuinely critical to your application\u0026rsquo;s success. The trade-offs in ecosystem compatibility are real, but the performance benefits can be substantial for specific use cases.\nRemember that framework choice isn\u0026rsquo;t permanent. Go\u0026rsquo;s excellent tooling and clean separation of concerns make it relatively straightforward to migrate between frameworks if your requirements change. Focus on building great applications rather than endlessly debating framework choice.\nThe most important factor is getting started and building something valuable. Any of these three frameworks will serve you well for building modern web applications in Go. Choose based on your current needs, start building, and adapt as you learn more about your specific requirements.\nWhether you choose Gin\u0026rsquo;s simplicity, Echo\u0026rsquo;s structure, or Fiber\u0026rsquo;s performance, you\u0026rsquo;ll be working with a solid foundation that can grow with your application\u0026rsquo;s needs. The Go web development ecosystem in 2025 is mature, stable, and ready to support whatever you\u0026rsquo;re building.\n","href":"/2025/09/fiber-vs-gin-vs-echo-golang-framework-comparison-2025.html","title":"Fiber vs Gin vs Echo - Go Framework Comparison 2025"},{"content":"Building a REST API might seem straightforward at first glance, but creating one that\u0026rsquo;s actually ready for production is a different beast entirely. After spending years working with various Go frameworks, I can tell you that the Gin framework hits that sweet spot between developer productivity and performance that makes it perfect for building robust APIs.\nIf you\u0026rsquo;ve been building basic REST APIs with Go\u0026rsquo;s net/http package , you\u0026rsquo;ve probably noticed how much boilerplate code you need to write for routing, middleware, and request handling. That\u0026rsquo;s where Gin shines - it provides all the essential features you need while maintaining the performance advantages that make Go special.\nToday, we\u0026rsquo;re going to build a complete user management API that includes everything you\u0026rsquo;d expect in a production system: proper authentication, validation, error handling, logging, and structured responses. By the end of this guide, you\u0026rsquo;ll have a solid foundation for building any REST API in Go.\nWhy Choose Gin Over Standard net/http Let me be clear about something - Go\u0026rsquo;s standard library is incredibly powerful. You can absolutely build production APIs using just net/http, and many companies do. However, unless you have very specific performance requirements or need complete control over every HTTP interaction, Gin offers significant advantages.\nGin provides up to 40 times better performance compared to other frameworks like Martini, thanks to its custom router implementation. But more importantly for day-to-day development, it eliminates tons of boilerplate code while still giving you the flexibility to drop down to lower-level HTTP handling when needed.\nThe framework includes built-in support for JSON binding, validation, middleware chains, route grouping, and error handling - all the stuff you\u0026rsquo;d end up implementing yourself anyway. Plus, it has excellent middleware ecosystem and plays well with Go\u0026rsquo;s standard patterns.\nSetting Up Your Development Environment Before we dive into building our API, let\u0026rsquo;s make sure you have everything set up correctly. First, ensure you have Go installed on your system (if not, check out our installation guide for Linux ).\nCreate a new project directory and initialize your Go module:\nmkdir gin-user-api cd gin-user-api go mod init gin-user-api Install the required dependencies:\ngo get github.com/gin-gonic/gin go get github.com/go-playground/validator/v10 go get golang.org/x/crypto/bcrypt go get github.com/golang-jwt/jwt/v4 go get github.com/joho/godotenv These packages provide everything we need for a production-ready API: the Gin framework, request validation, password hashing, JWT authentication, and environment variable management.\nProject Structure and Architecture A well-organized project structure is crucial for maintainability, especially as your API grows. Here\u0026rsquo;s the structure we\u0026rsquo;ll use:\ngin-user-api/ ├── main.go # Application entry point ├── .env # Environment variables ├── config/ │ └── config.go # Configuration management ├── controllers/ │ └── user_controller.go # HTTP handlers ├── middleware/ │ ├── auth.go # Authentication middleware │ ├── cors.go # CORS middleware │ └── logger.go # Request logging ├── models/ │ └── user.go # Data models ├── routes/ │ └── routes.go # Route definitions ├── services/ │ └── user_service.go # Business logic └── utils/ ├── jwt.go # JWT utilities ├── password.go # Password utilities └── response.go # Response utilities This structure follows the common pattern of separating concerns into different layers: controllers handle HTTP requests, services contain business logic, and models define data structures.\nCreating the User Model and Validation Let\u0026rsquo;s start by defining our user model with proper validation tags. Create models/user.go:\npackage models import ( \u0026#34;time\u0026#34; ) type User struct { ID uint `json:\u0026#34;id\u0026#34; gorm:\u0026#34;primaryKey\u0026#34;` Username string `json:\u0026#34;username\u0026#34; gorm:\u0026#34;uniqueIndex\u0026#34; binding:\u0026#34;required,min=3,max=50\u0026#34;` Email string `json:\u0026#34;email\u0026#34; gorm:\u0026#34;uniqueIndex\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;-\u0026#34; gorm:\u0026#34;not null\u0026#34; binding:\u0026#34;required,min=6\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34; binding:\u0026#34;required,min=2,max=50\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34; binding:\u0026#34;required,min=2,max=50\u0026#34;` IsActive bool `json:\u0026#34;is_active\u0026#34; gorm:\u0026#34;default:true\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34;` } type UserResponse struct { ID uint `json:\u0026#34;id\u0026#34;` Username string `json:\u0026#34;username\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34;` IsActive bool `json:\u0026#34;is_active\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` } type CreateUserRequest struct { Username string `json:\u0026#34;username\u0026#34; binding:\u0026#34;required,min=3,max=50\u0026#34;` Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required,min=6\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34; binding:\u0026#34;required,min=2,max=50\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34; binding:\u0026#34;required,min=2,max=50\u0026#34;` } type UpdateUserRequest struct { Username string `json:\u0026#34;username\u0026#34; binding:\u0026#34;omitempty,min=3,max=50\u0026#34;` Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;omitempty,email\u0026#34;` FirstName string `json:\u0026#34;first_name\u0026#34; binding:\u0026#34;omitempty,min=2,max=50\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34; binding:\u0026#34;omitempty,min=2,max=50\u0026#34;` } type LoginRequest struct { Email string `json:\u0026#34;email\u0026#34; binding:\u0026#34;required,email\u0026#34;` Password string `json:\u0026#34;password\u0026#34; binding:\u0026#34;required\u0026#34;` } type LoginResponse struct { Token string `json:\u0026#34;token\u0026#34;` User UserResponse `json:\u0026#34;user\u0026#34;` } Notice how we\u0026rsquo;re using different structs for different purposes - this gives us better control over what data gets exposed through our API and what validation rules apply in different contexts.\nUtility Functions for Security Before building our controllers, let\u0026rsquo;s create some utility functions for handling passwords and JWT tokens. Create utils/password.go:\npackage utils import ( \u0026#34;golang.org/x/crypto/bcrypt\u0026#34; ) func HashPassword(password string) (string, error) { hashedBytes, err := bcrypt.GenerateFromPassword([]byte(password), bcrypt.DefaultCost) if err != nil { return \u0026#34;\u0026#34;, err } return string(hashedBytes), nil } func CheckPassword(hashedPassword, password string) error { return bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(password)) } Create utils/jwt.go:\npackage utils import ( \u0026#34;errors\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/golang-jwt/jwt/v4\u0026#34; ) var jwtSecret = []byte(\u0026#34;your-secret-key-change-this-in-production\u0026#34;) type Claims struct { UserID uint `json:\u0026#34;user_id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` jwt.RegisteredClaims } func GenerateToken(userID uint, email string) (string, error) { expirationTime := time.Now().Add(24 * time.Hour) claims := \u0026amp;Claims{ UserID: userID, Email: email, RegisteredClaims: jwt.RegisteredClaims{ ExpiresAt: jwt.NewNumericDate(expirationTime), IssuedAt: jwt.NewNumericDate(time.Now()), }, } token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims) return token.SignedString(jwtSecret) } func ValidateToken(tokenString string) (*Claims, error) { claims := \u0026amp;Claims{} token, err := jwt.ParseWithClaims(tokenString, claims, func(token *jwt.Token) (interface{}, error) { return jwtSecret, nil }) if err != nil { return nil, err } if !token.Valid { return nil, errors.New(\u0026#34;invalid token\u0026#34;) } return claims, nil } Create utils/response.go for standardized API responses:\npackage utils import ( \u0026#34;net/http\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) type APIResponse struct { Success bool `json:\u0026#34;success\u0026#34;` Message string `json:\u0026#34;message\u0026#34;` Data interface{} `json:\u0026#34;data,omitempty\u0026#34;` Error string `json:\u0026#34;error,omitempty\u0026#34;` } func SuccessResponse(c *gin.Context, statusCode int, message string, data interface{}) { c.JSON(statusCode, APIResponse{ Success: true, Message: message, Data: data, }) } func ErrorResponse(c *gin.Context, statusCode int, message string, err string) { c.JSON(statusCode, APIResponse{ Success: false, Message: message, Error: err, }) } func ValidationErrorResponse(c *gin.Context, err error) { ErrorResponse(c, http.StatusBadRequest, \u0026#34;Validation failed\u0026#34;, err.Error()) } Building Production-Ready Middleware Middleware is what transforms a basic API into a production-ready system. Let\u0026rsquo;s create essential middleware for authentication, logging, and CORS handling.\nCreate middleware/auth.go:\npackage middleware import ( \u0026#34;gin-user-api/utils\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func AuthMiddleware() gin.HandlerFunc { return func(c *gin.Context) { authHeader := c.GetHeader(\u0026#34;Authorization\u0026#34;) if authHeader == \u0026#34;\u0026#34; { utils.ErrorResponse(c, http.StatusUnauthorized, \u0026#34;Authorization header required\u0026#34;, \u0026#34;missing_token\u0026#34;) c.Abort() return } tokenParts := strings.Split(authHeader, \u0026#34; \u0026#34;) if len(tokenParts) != 2 || tokenParts[0] != \u0026#34;Bearer\u0026#34; { utils.ErrorResponse(c, http.StatusUnauthorized, \u0026#34;Invalid authorization header format\u0026#34;, \u0026#34;invalid_token_format\u0026#34;) c.Abort() return } claims, err := utils.ValidateToken(tokenParts[1]) if err != nil { utils.ErrorResponse(c, http.StatusUnauthorized, \u0026#34;Invalid token\u0026#34;, err.Error()) c.Abort() return } c.Set(\u0026#34;user_id\u0026#34;, claims.UserID) c.Set(\u0026#34;user_email\u0026#34;, claims.Email) c.Next() } } Create middleware/cors.go:\npackage middleware import ( \u0026#34;time\u0026#34; \u0026#34;github.com/gin-contrib/cors\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func CORSMiddleware() gin.HandlerFunc { return cors.New(cors.Config{ AllowOrigins: []string{\u0026#34;http://localhost:3000\u0026#34;, \u0026#34;https://yourdomain.com\u0026#34;}, AllowMethods: []string{\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;OPTIONS\u0026#34;}, AllowHeaders: []string{\u0026#34;Origin\u0026#34;, \u0026#34;Content-Type\u0026#34;, \u0026#34;Accept\u0026#34;, \u0026#34;Authorization\u0026#34;}, ExposeHeaders: []string{\u0026#34;Content-Length\u0026#34;}, AllowCredentials: true, MaxAge: 12 * time.Hour, }) } Create middleware/logger.go:\npackage middleware import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func LoggerMiddleware() gin.HandlerFunc { return gin.LoggerWithFormatter(func(param gin.LogFormatterParams) string { return fmt.Sprintf(\u0026#34;%s - [%s] \\\u0026#34;%s %s %s %d %s \\\u0026#34;%s\\\u0026#34; %s\\\u0026#34;\\n\u0026#34;, param.ClientIP, param.TimeStamp.Format(time.RFC1123), param.Method, param.Path, param.Request.Proto, param.StatusCode, param.Latency, param.Request.UserAgent(), param.ErrorMessage, ) }) } Implementing the User Service Layer The service layer contains our business logic, keeping it separated from HTTP concerns. Create services/user_service.go:\npackage services import ( \u0026#34;errors\u0026#34; \u0026#34;gin-user-api/models\u0026#34; \u0026#34;gin-user-api/utils\u0026#34; ) type UserService struct { users []models.User nextID uint } func NewUserService() *UserService { return \u0026amp;UserService{ users: make([]models.User, 0), nextID: 1, } } func (s *UserService) CreateUser(req models.CreateUserRequest) (*models.User, error) { // Check if user already exists for _, user := range s.users { if user.Email == req.Email || user.Username == req.Username { return nil, errors.New(\u0026#34;user with this email or username already exists\u0026#34;) } } // Hash password hashedPassword, err := utils.HashPassword(req.Password) if err != nil { return nil, errors.New(\u0026#34;failed to hash password\u0026#34;) } // Create user user := models.User{ ID: s.nextID, Username: req.Username, Email: req.Email, Password: hashedPassword, FirstName: req.FirstName, LastName: req.LastName, IsActive: true, } s.users = append(s.users, user) s.nextID++ return \u0026amp;user, nil } func (s *UserService) GetUserByID(id uint) (*models.User, error) { for _, user := range s.users { if user.ID == id { return \u0026amp;user, nil } } return nil, errors.New(\u0026#34;user not found\u0026#34;) } func (s *UserService) GetUserByEmail(email string) (*models.User, error) { for _, user := range s.users { if user.Email == email { return \u0026amp;user, nil } } return nil, errors.New(\u0026#34;user not found\u0026#34;) } func (s *UserService) UpdateUser(id uint, req models.UpdateUserRequest) (*models.User, error) { for i, user := range s.users { if user.ID == id { if req.Username != \u0026#34;\u0026#34; { // Check username uniqueness for j, otherUser := range s.users { if j != i \u0026amp;\u0026amp; otherUser.Username == req.Username { return nil, errors.New(\u0026#34;username already taken\u0026#34;) } } s.users[i].Username = req.Username } if req.Email != \u0026#34;\u0026#34; { // Check email uniqueness for j, otherUser := range s.users { if j != i \u0026amp;\u0026amp; otherUser.Email == req.Email { return nil, errors.New(\u0026#34;email already taken\u0026#34;) } } s.users[i].Email = req.Email } if req.FirstName != \u0026#34;\u0026#34; { s.users[i].FirstName = req.FirstName } if req.LastName != \u0026#34;\u0026#34; { s.users[i].LastName = req.LastName } return \u0026amp;s.users[i], nil } } return nil, errors.New(\u0026#34;user not found\u0026#34;) } func (s *UserService) DeleteUser(id uint) error { for i, user := range s.users { if user.ID == id { s.users = append(s.users[:i], s.users[i+1:]...) return nil } } return errors.New(\u0026#34;user not found\u0026#34;) } func (s *UserService) GetAllUsers() []models.User { return s.users } func (s *UserService) AuthenticateUser(req models.LoginRequest) (*models.User, error) { user, err := s.GetUserByEmail(req.Email) if err != nil { return nil, errors.New(\u0026#34;invalid email or password\u0026#34;) } if !user.IsActive { return nil, errors.New(\u0026#34;account is deactivated\u0026#34;) } if err := utils.CheckPassword(user.Password, req.Password); err != nil { return nil, errors.New(\u0026#34;invalid email or password\u0026#34;) } return user, nil } Creating HTTP Controllers Now let\u0026rsquo;s build the HTTP handlers that tie everything together. Create controllers/user_controller.go:\npackage controllers import ( \u0026#34;gin-user-api/models\u0026#34; \u0026#34;gin-user-api/services\u0026#34; \u0026#34;gin-user-api/utils\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) type UserController struct { userService *services.UserService } func NewUserController(userService *services.UserService) *UserController { return \u0026amp;UserController{ userService: userService, } } func (uc *UserController) Register(c *gin.Context) { var req models.CreateUserRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { utils.ValidationErrorResponse(c, err) return } user, err := uc.userService.CreateUser(req) if err != nil { utils.ErrorResponse(c, http.StatusBadRequest, \u0026#34;Failed to create user\u0026#34;, err.Error()) return } userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } utils.SuccessResponse(c, http.StatusCreated, \u0026#34;User created successfully\u0026#34;, userResponse) } func (uc *UserController) Login(c *gin.Context) { var req models.LoginRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { utils.ValidationErrorResponse(c, err) return } user, err := uc.userService.AuthenticateUser(req) if err != nil { utils.ErrorResponse(c, http.StatusUnauthorized, \u0026#34;Authentication failed\u0026#34;, err.Error()) return } token, err := utils.GenerateToken(user.ID, user.Email) if err != nil { utils.ErrorResponse(c, http.StatusInternalServerError, \u0026#34;Failed to generate token\u0026#34;, err.Error()) return } userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } response := models.LoginResponse{ Token: token, User: userResponse, } utils.SuccessResponse(c, http.StatusOK, \u0026#34;Login successful\u0026#34;, response) } func (uc *UserController) GetProfile(c *gin.Context) { userID, exists := c.Get(\u0026#34;user_id\u0026#34;) if !exists { utils.ErrorResponse(c, http.StatusUnauthorized, \u0026#34;User not authenticated\u0026#34;, \u0026#34;missing_user_id\u0026#34;) return } user, err := uc.userService.GetUserByID(userID.(uint)) if err != nil { utils.ErrorResponse(c, http.StatusNotFound, \u0026#34;User not found\u0026#34;, err.Error()) return } userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } utils.SuccessResponse(c, http.StatusOK, \u0026#34;Profile retrieved successfully\u0026#34;, userResponse) } func (uc *UserController) GetUser(c *gin.Context) { idParam := c.Param(\u0026#34;id\u0026#34;) id, err := strconv.ParseUint(idParam, 10, 32) if err != nil { utils.ErrorResponse(c, http.StatusBadRequest, \u0026#34;Invalid user ID\u0026#34;, err.Error()) return } user, err := uc.userService.GetUserByID(uint(id)) if err != nil { utils.ErrorResponse(c, http.StatusNotFound, \u0026#34;User not found\u0026#34;, err.Error()) return } userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } utils.SuccessResponse(c, http.StatusOK, \u0026#34;User retrieved successfully\u0026#34;, userResponse) } func (uc *UserController) UpdateUser(c *gin.Context) { idParam := c.Param(\u0026#34;id\u0026#34;) id, err := strconv.ParseUint(idParam, 10, 32) if err != nil { utils.ErrorResponse(c, http.StatusBadRequest, \u0026#34;Invalid user ID\u0026#34;, err.Error()) return } userID, exists := c.Get(\u0026#34;user_id\u0026#34;) if !exists || userID.(uint) != uint(id) { utils.ErrorResponse(c, http.StatusForbidden, \u0026#34;You can only update your own profile\u0026#34;, \u0026#34;unauthorized_update\u0026#34;) return } var req models.UpdateUserRequest if err := c.ShouldBindJSON(\u0026amp;req); err != nil { utils.ValidationErrorResponse(c, err) return } user, err := uc.userService.UpdateUser(uint(id), req) if err != nil { utils.ErrorResponse(c, http.StatusBadRequest, \u0026#34;Failed to update user\u0026#34;, err.Error()) return } userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } utils.SuccessResponse(c, http.StatusOK, \u0026#34;User updated successfully\u0026#34;, userResponse) } func (uc *UserController) DeleteUser(c *gin.Context) { idParam := c.Param(\u0026#34;id\u0026#34;) id, err := strconv.ParseUint(idParam, 10, 32) if err != nil { utils.ErrorResponse(c, http.StatusBadRequest, \u0026#34;Invalid user ID\u0026#34;, err.Error()) return } userID, exists := c.Get(\u0026#34;user_id\u0026#34;) if !exists || userID.(uint) != uint(id) { utils.ErrorResponse(c, http.StatusForbidden, \u0026#34;You can only delete your own account\u0026#34;, \u0026#34;unauthorized_delete\u0026#34;) return } err = uc.userService.DeleteUser(uint(id)) if err != nil { utils.ErrorResponse(c, http.StatusNotFound, \u0026#34;User not found\u0026#34;, err.Error()) return } utils.SuccessResponse(c, http.StatusOK, \u0026#34;User deleted successfully\u0026#34;, nil) } func (uc *UserController) GetAllUsers(c *gin.Context) { users := uc.userService.GetAllUsers() var userResponses []models.UserResponse for _, user := range users { userResponse := models.UserResponse{ ID: user.ID, Username: user.Username, Email: user.Email, FirstName: user.FirstName, LastName: user.LastName, IsActive: user.IsActive, CreatedAt: user.CreatedAt, } userResponses = append(userResponses, userResponse) } utils.SuccessResponse(c, http.StatusOK, \u0026#34;Users retrieved successfully\u0026#34;, userResponses) } Setting Up Routes and Server Create routes/routes.go to organize all our API routes:\npackage routes import ( \u0026#34;gin-user-api/controllers\u0026#34; \u0026#34;gin-user-api/middleware\u0026#34; \u0026#34;gin-user-api/services\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func SetupRoutes() *gin.Engine { r := gin.New() // Add middleware r.Use(middleware.LoggerMiddleware()) r.Use(middleware.CORSMiddleware()) r.Use(gin.Recovery()) // Initialize services and controllers userService := services.NewUserService() userController := controllers.NewUserController(userService) // Health check r.GET(\u0026#34;/health\u0026#34;, func(c *gin.Context) { c.JSON(200, gin.H{ \u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;API is running\u0026#34;, }) }) // API v1 routes v1 := r.Group(\u0026#34;/api/v1\u0026#34;) { // Public routes auth := v1.Group(\u0026#34;/auth\u0026#34;) { auth.POST(\u0026#34;/register\u0026#34;, userController.Register) auth.POST(\u0026#34;/login\u0026#34;, userController.Login) } // Protected routes users := v1.Group(\u0026#34;/users\u0026#34;).Use(middleware.AuthMiddleware()) { users.GET(\u0026#34;/profile\u0026#34;, userController.GetProfile) users.GET(\u0026#34;\u0026#34;, userController.GetAllUsers) users.GET(\u0026#34;/:id\u0026#34;, userController.GetUser) users.PUT(\u0026#34;/:id\u0026#34;, userController.UpdateUser) users.DELETE(\u0026#34;/:id\u0026#34;, userController.DeleteUser) } } return r } Finally, create the main application file main.go:\npackage main import ( \u0026#34;gin-user-api/routes\u0026#34; \u0026#34;log\u0026#34; ) func main() { // Setup routes r := routes.SetupRoutes() // Start server log.Println(\u0026#34;Starting server on port 8080...\u0026#34;) if err := r.Run(\u0026#34;:8080\u0026#34;); err != nil { log.Fatal(\u0026#34;Failed to start server:\u0026#34;, err) } } Testing Your Production-Ready API Let\u0026rsquo;s test our API to make sure everything works correctly. First, start the server:\ngo run main.go Test user registration:\ncurl -X POST http://localhost:8080/api/v1/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;johndoe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;securepassword\u0026#34;, \u0026#34;first_name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;last_name\u0026#34;: \u0026#34;Doe\u0026#34; }\u0026#39; Test user login:\ncurl -X POST http://localhost:8080/api/v1/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;john@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;securepassword\u0026#34; }\u0026#39; Use the token from login to access protected endpoints:\n# Replace YOUR_TOKEN with the actual token from login response curl -X GET http://localhost:8080/api/v1/users/profile \\ -H \u0026#34;Authorization: Bearer YOUR_TOKEN\u0026#34; Test getting all users:\ncurl -X GET http://localhost:8080/api/v1/users \\ -H \u0026#34;Authorization: Bearer YOUR_TOKEN\u0026#34; Production Deployment Considerations When you\u0026rsquo;re ready to deploy this API to production, there are several important considerations you need to address. First, never use hardcoded secrets like we did for demonstration purposes. Use environment variables for JWT secrets, database credentials, and other sensitive configuration.\nConsider implementing rate limiting to prevent abuse and DDoS attacks. The gin-contrib package provides excellent rate limiting middleware that you can easily integrate into your existing middleware chain.\nFor data persistence, you\u0026rsquo;ll want to replace our in-memory storage with a proper database. Consider using GORM with PostgreSQL or MySQL - check out our guide on connecting PostgreSQL with Go using sqlx for database integration patterns.\nImplement proper logging using structured logging libraries like logrus or zap. You\u0026rsquo;ll also want to add metrics collection and health checks for monitoring in production environments.\nPerformance Optimization and Best Practices Gin\u0026rsquo;s performance is already excellent out of the box, but there are several optimization techniques you can apply. First, consider implementing response caching for frequently accessed data. Gin plays well with Redis for caching strategies.\nWhen dealing with large datasets, implement pagination properly rather than returning all records at once. Our example shows basic pagination structure that you can extend based on your needs.\nFor error handling , consider implementing a global error handler middleware that can catch panics and return consistent error responses to your clients.\nAlways validate input data thoroughly - Gin\u0026rsquo;s binding and validation features make this straightforward, but remember to validate business logic constraints in your service layer as well.\nSecurity Hardening Security should be a top priority for any production API. Beyond basic authentication, consider implementing role-based access control (RBAC) for different user types and permissions.\nUse HTTPS in production with proper TLS certificates. Never transmit sensitive data over unencrypted connections. The middleware we created includes CORS configuration, but make sure to restrict origins to only trusted domains in production.\nImplement request size limits to prevent memory exhaustion attacks. Gin provides built-in middleware for this purpose.\nConsider adding request ID tracking through your middleware chain - this makes debugging production issues much easier when you can trace a request through your entire system.\nExtending Your API The foundation we\u0026rsquo;ve built today is solid, but there are many directions you can take it. Consider adding features like email verification for new accounts, password reset functionality, user profile images with file upload handling, and API documentation using tools like Swagger.\nYou might also want to explore microservices architecture if your application grows complex enough to warrant service separation.\nFor real-time features, you could integrate WebSocket support for notifications or live updates. Gin handles WebSocket upgrades gracefully while maintaining the same familiar API patterns.\nConclusion Building production-ready REST APIs with Gin strikes an excellent balance between developer productivity and performance. The framework provides all the essential features you need while staying out of your way when you need to implement custom logic.\nWhat we\u0026rsquo;ve built today includes proper authentication, validation, error handling, and a clean architecture that can scale with your application\u0026rsquo;s growth. The middleware system makes it easy to add cross-cutting concerns, and the service layer keeps your business logic separated from HTTP concerns.\nThe next time you\u0026rsquo;re building a REST API in Go, give Gin serious consideration. It\u0026rsquo;ll save you tons of development time while delivering the performance characteristics that make Go special for backend development.\nIf you\u0026rsquo;re interested in exploring more advanced Go web development patterns, check out our guide on structuring Go projects for clean architecture or learn about advanced concurrency patterns for handling high-traffic scenarios.\nGot questions about building production APIs with Gin? Drop a comment below - I love discussing different approaches to API architecture and the challenges you run into when scaling Go applications.\n","href":"/2025/09/building-rest-api-gin-framework-golang-production-ready.html","title":"Building REST API with Gin Framework Golang - Production Ready"},{"content":"Ever had a user complain that your app takes forever to send an email or process an image upload? Or maybe you\u0026rsquo;ve watched your response times crawl to a halt because you\u0026rsquo;re trying to do too much work during a single request? Laravel queues are the solution you\u0026rsquo;ve been looking for, and they\u0026rsquo;re easier to set up than you might think.\nThink of Laravel queues as your app\u0026rsquo;s personal assistant. Instead of making users wait while you send emails, resize images, or generate reports, you hand these tasks off to the background and let users continue with their day. The work still gets done, but it doesn\u0026rsquo;t block the user experience.\nQueue jobs are perfect for any task that doesn\u0026rsquo;t need to happen immediately - and honestly, that\u0026rsquo;s most tasks. Whether you\u0026rsquo;re sending welcome emails, processing file uploads, generating PDFs, or hitting external APIs, queues can make your app feel snappy and responsive while handling the heavy lifting behind the scenes.\nUnderstanding Laravel Queues Before we dive into the code, let\u0026rsquo;s understand what\u0026rsquo;s actually happening when you use Laravel queues. When a user triggers an action that normally takes time (like sending an email), instead of processing it immediately, Laravel puts that task into a queue - basically a to-do list.\nMeanwhile, queue workers (separate processes) are constantly checking this to-do list and processing tasks one by one. The user gets an immediate response, and the work happens in the background without them having to wait.\nHere\u0026rsquo;s a simple example to illustrate the difference:\nWithout Queues:\nUser submits a form App sends email (takes 3 seconds) App resizes uploaded image (takes 2 seconds) App saves data to database User finally sees success message (after 5+ seconds) With Queues:\nUser submits a form App queues email job App queues image resize job App saves data to database User sees success message (under 1 second) Background workers handle email and image tasks The difference in user experience is night and day.\nQueue Drivers and Configuration Laravel supports several queue drivers, each with different strengths depending on your needs.\nDatabase Driver (Perfect for Starting Out) The database driver stores jobs in your database - it\u0026rsquo;s simple, requires no additional services, and perfect for getting started:\n// config/queue.php \u0026#39;default\u0026#39; =\u0026gt; env(\u0026#39;QUEUE_CONNECTION\u0026#39;, \u0026#39;database\u0026#39;), \u0026#39;connections\u0026#39; =\u0026gt; [ \u0026#39;database\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;database\u0026#39;, \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;jobs\u0026#39;, \u0026#39;queue\u0026#39; =\u0026gt; \u0026#39;default\u0026#39;, \u0026#39;retry_after\u0026#39; =\u0026gt; 90, \u0026#39;after_commit\u0026#39; =\u0026gt; false, ], ], Set up the database tables:\nphp artisan queue:table php artisan queue:failed-table php artisan migrate Redis Driver (Great for Production) Redis is faster and more feature-rich, perfect for high-traffic applications:\n# Install Redis PHP extension composer require predis/predis // config/queue.php \u0026#39;redis\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;redis\u0026#39;, \u0026#39;connection\u0026#39; =\u0026gt; \u0026#39;default\u0026#39;, \u0026#39;queue\u0026#39; =\u0026gt; env(\u0026#39;REDIS_QUEUE\u0026#39;, \u0026#39;default\u0026#39;), \u0026#39;retry_after\u0026#39; =\u0026gt; 90, \u0026#39;block_for\u0026#39; =\u0026gt; null, \u0026#39;after_commit\u0026#39; =\u0026gt; false, ], Amazon SQS Driver (Scalable Cloud Solution) For applications that need to scale automatically:\ncomposer require aws/aws-sdk-php // config/queue.php \u0026#39;sqs\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;sqs\u0026#39;, \u0026#39;key\u0026#39; =\u0026gt; env(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), \u0026#39;secret\u0026#39; =\u0026gt; env(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;), \u0026#39;prefix\u0026#39; =\u0026gt; env(\u0026#39;SQS_PREFIX\u0026#39;, \u0026#39;https://sqs.us-east-1.amazonaws.com/your-account-id\u0026#39;), \u0026#39;queue\u0026#39; =\u0026gt; env(\u0026#39;SQS_QUEUE\u0026#39;, \u0026#39;default\u0026#39;), \u0026#39;suffix\u0026#39; =\u0026gt; env(\u0026#39;SQS_SUFFIX\u0026#39;), \u0026#39;region\u0026#39; =\u0026gt; env(\u0026#39;AWS_DEFAULT_REGION\u0026#39;, \u0026#39;us-east-1\u0026#39;), \u0026#39;after_commit\u0026#39; =\u0026gt; false, ], Creating Your First Job Let\u0026rsquo;s create a job that sends a welcome email to new users. This is a perfect example because emails can be slow and users shouldn\u0026rsquo;t have to wait for them.\nGenerate a new job:\nphp artisan make:job SendWelcomeEmail This creates a job class in app/Jobs/SendWelcomeEmail.php:\n\u0026lt;?php namespace App\\Jobs; use App\\Models\\User; use App\\Mail\\WelcomeEmail; use Illuminate\\Bus\\Queueable; use Illuminate\\Contracts\\Queue\\ShouldQueue; use Illuminate\\Foundation\\Bus\\Dispatchable; use Illuminate\\Queue\\InteractsWithQueue; use Illuminate\\Queue\\SerializesModels; use Illuminate\\Support\\Facades\\Mail; class SendWelcomeEmail implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $user; public function __construct(User $user) { $this-\u0026gt;user = $user; } public function handle() { // Send the welcome email Mail::to($this-\u0026gt;user-\u0026gt;email)-\u0026gt;send(new WelcomeEmail($this-\u0026gt;user)); } } Now, instead of sending the email directly in your controller, dispatch the job:\n// In your controller public function register(Request $request) { $user = User::create($request-\u0026gt;validated()); // Instead of: Mail::to($user-\u0026gt;email)-\u0026gt;send(new WelcomeEmail($user)); SendWelcomeEmail::dispatch($user); return redirect()-\u0026gt;route(\u0026#39;dashboard\u0026#39;)-\u0026gt;with(\u0026#39;success\u0026#39;, \u0026#39;Account created successfully!\u0026#39;); } The user sees the success message immediately, and the email gets sent in the background.\nJob Properties and Configuration Laravel jobs are highly configurable. Here are the most important properties you should know about:\nQueue Assignment You can organize jobs into different queues based on priority or type:\nclass SendWelcomeEmail implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; // Specify which queue this job should go to public $queue = \u0026#39;emails\u0026#39;; // Or set it when dispatching // SendWelcomeEmail::dispatch($user)-\u0026gt;onQueue(\u0026#39;high-priority\u0026#39;); } Retry Configuration Control how many times a job should be retried if it fails:\nclass ProcessPayment implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; // Retry up to 3 times public $tries = 3; // Wait 30 seconds between retries public $backoff = 30; // Or use exponential backoff public function backoff() { return [1, 5, 10, 30]; // Seconds between retries } } Timeout Configuration Prevent jobs from running too long:\nclass GenerateReport implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; // Job will be killed if it runs longer than 120 seconds public $timeout = 120; // Handle timeout gracefully public function timeoutAt() { return now()-\u0026gt;addMinutes(5); } } Advanced Job Types and Patterns Job Batching Sometimes you need to process many related jobs and know when they\u0026rsquo;re all done:\nuse Illuminate\\Bus\\Batch; use Illuminate\\Support\\Facades\\Bus; // Process 1000 emails in batches $jobs = []; foreach ($users as $user) { $jobs[] = new SendNewsletterEmail($user); } $batch = Bus::batch($jobs) -\u0026gt;then(function (Batch $batch) { // All jobs completed successfully Log::info(\u0026#39;Newsletter batch completed\u0026#39;, [\u0026#39;batch_id\u0026#39; =\u0026gt; $batch-\u0026gt;id]); }) -\u0026gt;catch(function (Batch $batch, Throwable $e) { // First batch job failure Log::error(\u0026#39;Newsletter batch failed\u0026#39;, [\u0026#39;batch_id\u0026#39; =\u0026gt; $batch-\u0026gt;id, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage()]); }) -\u0026gt;finally(function (Batch $batch) { // Batch has finished executing (success or failure) NotificationService::notifyAdmins(\u0026#39;Newsletter batch finished\u0026#39;); }) -\u0026gt;dispatch(); return response()-\u0026gt;json([\u0026#39;batch_id\u0026#39; =\u0026gt; $batch-\u0026gt;id]); Job Chains When jobs need to run in a specific order:\nuse Illuminate\\Support\\Facades\\Bus; // Process an order: charge payment -\u0026gt; update inventory -\u0026gt; send confirmation Bus::chain([ new ProcessPayment($order), new UpdateInventory($order), new SendOrderConfirmation($order), ])-\u0026gt;dispatch(); If any job in the chain fails, the remaining jobs won\u0026rsquo;t run.\nConditional Job Dispatching Only queue jobs when certain conditions are met:\nclass SendPromotionalEmail implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $user; public function __construct(User $user) { $this-\u0026gt;user = $user; } // Don\u0026#39;t queue if user has unsubscribed public function shouldQueue() { return $this-\u0026gt;user-\u0026gt;email_notifications_enabled; } public function handle() { if (!$this-\u0026gt;user-\u0026gt;email_notifications_enabled) { return; // Exit early if user has unsubscribed since queuing } Mail::to($this-\u0026gt;user-\u0026gt;email)-\u0026gt;send(new PromotionalEmail($this-\u0026gt;user)); } } Queue Workers and Processing Queue workers are the engines that actually process your jobs. Understanding how they work helps you optimize performance and avoid common pitfalls.\nStarting Workers Start a worker to process jobs:\n# Process jobs from the default queue php artisan queue:work # Process specific queues in order of priority php artisan queue:work --queue=high-priority,emails,default # Process jobs with memory and timeout limits php artisan queue:work --memory=512 --timeout=60 Worker Configuration Configure workers for your specific needs:\n# Process only 10 jobs before restarting (prevents memory leaks) php artisan queue:work --max-jobs=10 # Process jobs for 1 hour before restarting php artisan queue:work --max-time=3600 # Sleep for 5 seconds when no jobs are available php artisan queue:work --sleep=5 # Restart workers gracefully when they finish current job php artisan queue:restart Production Worker Setup In production, use a process manager like Supervisor to keep workers running:\n[program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /path/to/your/app/artisan queue:work redis --sleep=3 --tries=3 --max-time=3600 directory=/path/to/your/app user=www-data numprocs=8 redirect_stderr=true stdout_logfile=/var/log/laravel-worker.log stopwaitsecs=3600 This configuration runs 8 worker processes, automatically restarts them if they crash, and logs their output.\nError Handling and Failed Jobs Not all jobs succeed on the first try. Laravel provides robust error handling to deal with failures gracefully.\nHandling Job Failures Add a failed method to handle job failures:\nclass ProcessPayment implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $order; public $tries = 3; public function __construct(Order $order) { $this-\u0026gt;order = $order; } public function handle() { $paymentService = new PaymentService(); try { $paymentService-\u0026gt;charge($this-\u0026gt;order); $this-\u0026gt;order-\u0026gt;update([\u0026#39;status\u0026#39; =\u0026gt; \u0026#39;paid\u0026#39;]); } catch (PaymentException $e) { // Log the error with context Log::error(\u0026#39;Payment processing failed\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;id, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), \u0026#39;attempt\u0026#39; =\u0026gt; $this-\u0026gt;attempts(), ]); // Re-throw to trigger retry mechanism throw $e; } } public function failed(Throwable $exception) { // Handle job failure after all retries are exhausted $this-\u0026gt;order-\u0026gt;update([\u0026#39;status\u0026#39; =\u0026gt; \u0026#39;payment_failed\u0026#39;]); // Notify the user Mail::to($this-\u0026gt;order-\u0026gt;user-\u0026gt;email)-\u0026gt;send(new PaymentFailedEmail($this-\u0026gt;order)); // Alert administrators Log::alert(\u0026#39;Payment processing failed permanently\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;id, \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;order-\u0026gt;user_id, \u0026#39;error\u0026#39; =\u0026gt; $exception-\u0026gt;getMessage(), ]); } } Managing Failed Jobs Laravel tracks failed jobs automatically. You can view and manage them:\n# List all failed jobs php artisan queue:failed # Retry a specific failed job php artisan queue:retry 1 # Retry all failed jobs php artisan queue:retry all # Delete a failed job php artisan queue:forget 1 # Clear all failed jobs php artisan queue:flush Custom Failed Job Handling Create custom logic for handling failed jobs:\n// In a controller or command public function retryFailedJobs() { $failedJobs = DB::table(\u0026#39;failed_jobs\u0026#39;) -\u0026gt;where(\u0026#39;failed_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subDay()) // Only recent failures -\u0026gt;get(); foreach ($failedJobs as $failedJob) { $payload = json_decode($failedJob-\u0026gt;payload, true); // Only retry certain types of jobs if (str_contains($payload[\u0026#39;displayName\u0026#39;], \u0026#39;SendEmail\u0026#39;)) { Artisan::call(\u0026#39;queue:retry\u0026#39;, [\u0026#39;id\u0026#39; =\u0026gt; $failedJob-\u0026gt;id]); } } } Real-World Job Examples Let\u0026rsquo;s look at some practical job examples you\u0026rsquo;ll likely need in real applications.\nImage Processing Job class ProcessImageUpload implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $imagePath; protected $userId; public $timeout = 300; // 5 minutes for large images public function __construct($imagePath, $userId) { $this-\u0026gt;imagePath = $imagePath; $this-\u0026gt;userId = $userId; } public function handle() { $image = Image::make(storage_path(\u0026#39;app/\u0026#39; . $this-\u0026gt;imagePath)); // Create different sizes $sizes = [ \u0026#39;thumbnail\u0026#39; =\u0026gt; [150, 150], \u0026#39;medium\u0026#39; =\u0026gt; [500, 500], \u0026#39;large\u0026#39; =\u0026gt; [1200, 1200], ]; foreach ($sizes as $name =\u0026gt; $dimensions) { $resized = $image-\u0026gt;fit($dimensions[0], $dimensions[1]); $filename = $name . \u0026#39;_\u0026#39; . basename($this-\u0026gt;imagePath); $resized-\u0026gt;save(storage_path(\u0026#39;app/images/\u0026#39; . $filename)); } // Update user\u0026#39;s profile with processed images User::find($this-\u0026gt;userId)-\u0026gt;update([ \u0026#39;avatar_processed\u0026#39; =\u0026gt; true, \u0026#39;processing_completed_at\u0026#39; =\u0026gt; now(), ]); } public function failed(Throwable $exception) { User::find($this-\u0026gt;userId)-\u0026gt;update([ \u0026#39;avatar_processing_failed\u0026#39; =\u0026gt; true, ]); Log::error(\u0026#39;Image processing failed\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;userId, \u0026#39;image_path\u0026#39; =\u0026gt; $this-\u0026gt;imagePath, \u0026#39;error\u0026#39; =\u0026gt; $exception-\u0026gt;getMessage(), ]); } } PDF Generation Job class GenerateInvoicePDF implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $invoice; public $queue = \u0026#39;reports\u0026#39;; // Use a dedicated queue for reports public function __construct(Invoice $invoice) { $this-\u0026gt;invoice = $invoice; } public function handle() { $pdf = PDF::loadView(\u0026#39;invoices.pdf\u0026#39;, [ \u0026#39;invoice\u0026#39; =\u0026gt; $this-\u0026gt;invoice, \u0026#39;company\u0026#39; =\u0026gt; $this-\u0026gt;invoice-\u0026gt;company, \u0026#39;items\u0026#39; =\u0026gt; $this-\u0026gt;invoice-\u0026gt;items, ]); $filename = \u0026#34;invoice-{$this-\u0026gt;invoice-\u0026gt;number}.pdf\u0026#34;; $path = \u0026#34;invoices/{$filename}\u0026#34;; // Save PDF to storage Storage::put($path, $pdf-\u0026gt;output()); // Update invoice with PDF path $this-\u0026gt;invoice-\u0026gt;update([ \u0026#39;pdf_path\u0026#39; =\u0026gt; $path, \u0026#39;pdf_generated_at\u0026#39; =\u0026gt; now(), ]); // Email PDF to customer Mail::to($this-\u0026gt;invoice-\u0026gt;customer-\u0026gt;email) -\u0026gt;send(new InvoicePDFReady($this-\u0026gt;invoice, $path)); } } Data Export Job class ExportUsersToCSV implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $filters; protected $requestedBy; public $timeout = 600; // 10 minutes for large exports public function __construct(array $filters, User $requestedBy) { $this-\u0026gt;filters = $filters; $this-\u0026gt;requestedBy = $requestedBy; } public function handle() { $filename = \u0026#39;users_export_\u0026#39; . now()-\u0026gt;format(\u0026#39;Y_m_d_H_i_s\u0026#39;) . \u0026#39;.csv\u0026#39;; $path = \u0026#34;exports/{$filename}\u0026#34;; $query = User::query(); // Apply filters if (!empty($this-\u0026gt;filters[\u0026#39;created_after\u0026#39;])) { $query-\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;=\u0026#39;, $this-\u0026gt;filters[\u0026#39;created_after\u0026#39;]); } if (!empty($this-\u0026gt;filters[\u0026#39;role\u0026#39;])) { $query-\u0026gt;where(\u0026#39;role\u0026#39;, $this-\u0026gt;filters[\u0026#39;role\u0026#39;]); } // Stream large datasets to avoid memory issues $file = fopen(storage_path(\u0026#39;app/\u0026#39; . $path), \u0026#39;w\u0026#39;); fputcsv($file, [\u0026#39;ID\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;Email\u0026#39;, \u0026#39;Created At\u0026#39;, \u0026#39;Role\u0026#39;]); $query-\u0026gt;chunk(1000, function ($users) use ($file) { foreach ($users as $user) { fputcsv($file, [ $user-\u0026gt;id, $user-\u0026gt;name, $user-\u0026gt;email, $user-\u0026gt;created_at-\u0026gt;format(\u0026#39;Y-m-d H:i:s\u0026#39;), $user-\u0026gt;role, ]); } }); fclose($file); // Notify user that export is ready Mail::to($this-\u0026gt;requestedBy-\u0026gt;email) -\u0026gt;send(new ExportReady($filename, $path)); } } For comprehensive performance optimization when working with large datasets, check out our Laravel performance optimization guide .\nQueue Monitoring and Debugging Monitoring your queues is crucial for maintaining a healthy application. Here are the tools and techniques you need.\nBasic Queue Monitoring Check queue status and job counts:\n# Check queue status php artisan queue:monitor # View queue statistics php artisan queue:work --verbose # Check specific queue php artisan queue:size redis:high-priority Custom Queue Monitoring Create your own monitoring dashboard:\nclass QueueMonitoringController extends Controller { public function dashboard() { $queueStats = [ \u0026#39;pending_jobs\u0026#39; =\u0026gt; $this-\u0026gt;getPendingJobsCount(), \u0026#39;failed_jobs\u0026#39; =\u0026gt; $this-\u0026gt;getFailedJobsCount(), \u0026#39;processed_today\u0026#39; =\u0026gt; $this-\u0026gt;getProcessedJobsToday(), \u0026#39;average_processing_time\u0026#39; =\u0026gt; $this-\u0026gt;getAverageProcessingTime(), \u0026#39;queue_sizes\u0026#39; =\u0026gt; $this-\u0026gt;getQueueSizes(), ]; return view(\u0026#39;admin.queue-dashboard\u0026#39;, compact(\u0026#39;queueStats\u0026#39;)); } protected function getPendingJobsCount() { return DB::table(\u0026#39;jobs\u0026#39;)-\u0026gt;count(); } protected function getFailedJobsCount() { return DB::table(\u0026#39;failed_jobs\u0026#39;) -\u0026gt;where(\u0026#39;failed_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subDay()) -\u0026gt;count(); } protected function getQueueSizes() { $queues = [\u0026#39;default\u0026#39;, \u0026#39;emails\u0026#39;, \u0026#39;high-priority\u0026#39;, \u0026#39;reports\u0026#39;]; $sizes = []; foreach ($queues as $queue) { $sizes[$queue] = DB::table(\u0026#39;jobs\u0026#39;) -\u0026gt;where(\u0026#39;queue\u0026#39;, $queue) -\u0026gt;count(); } return $sizes; } } Queue Health Checks Monitor queue health automatically:\nclass QueueHealthCheck extends Command { protected $signature = \u0026#39;queue:health-check\u0026#39;; protected $description = \u0026#39;Check queue health and alert if issues found\u0026#39;; public function handle() { $this-\u0026gt;checkQueueSize(); $this-\u0026gt;checkFailedJobs(); $this-\u0026gt;checkOldJobs(); $this-\u0026gt;checkWorkerStatus(); } protected function checkQueueSize() { $pendingJobs = DB::table(\u0026#39;jobs\u0026#39;)-\u0026gt;count(); if ($pendingJobs \u0026gt; 1000) { $this-\u0026gt;alert(\u0026#34;High queue backlog: {$pendingJobs} pending jobs\u0026#34;); // Send notification to team Notification::route(\u0026#39;slack\u0026#39;, config(\u0026#39;monitoring.slack_webhook\u0026#39;)) -\u0026gt;notify(new QueueBacklogAlert($pendingJobs)); } } protected function checkFailedJobs() { $recentFailures = DB::table(\u0026#39;failed_jobs\u0026#39;) -\u0026gt;where(\u0026#39;failed_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;count(); if ($recentFailures \u0026gt; 10) { $this-\u0026gt;alert(\u0026#34;High failure rate: {$recentFailures} jobs failed in the last hour\u0026#34;); } } protected function checkOldJobs() { $oldJobs = DB::table(\u0026#39;jobs\u0026#39;) -\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026lt;\u0026#39;, now()-\u0026gt;subHours(6)) -\u0026gt;count(); if ($oldJobs \u0026gt; 0) { $this-\u0026gt;warn(\u0026#34;Found {$oldJobs} jobs older than 6 hours - workers may not be running\u0026#34;); } } } For detailed monitoring and alerting strategies, explore our Laravel production monitoring guide .\nPerformance Optimization As your application grows, you\u0026rsquo;ll need to optimize queue performance. Here are proven strategies.\nQueue Prioritization Process important jobs first:\n// In your worker command php artisan queue:work --queue=critical,high,normal,low // Or in your job class UrgentNotification implements ShouldQueue { public $queue = \u0026#39;critical\u0026#39;; // This job will be processed before others } Memory Management Prevent memory leaks in long-running workers:\nclass ProcessLargeDataset implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $datasetId; public function handle() { $dataset = Dataset::find($this-\u0026gt;datasetId); // Process in chunks to manage memory $dataset-\u0026gt;records()-\u0026gt;chunk(1000, function ($records) { foreach ($records as $record) { $this-\u0026gt;processRecord($record); } // Force garbage collection for large datasets if (memory_get_usage() \u0026gt; 100 * 1024 * 1024) { // 100MB gc_collect_cycles(); } }); // Clear any loaded relationships to free memory $dataset-\u0026gt;unsetRelations(); } } Database Connection Management Handle database connections properly in queued jobs:\nclass DatabaseIntensiveJob implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; public function handle() { try { // Your database operations here $this-\u0026gt;performDatabaseOperations(); } finally { // Disconnect to prevent connection leaks DB::disconnect(); } } } Horizon for Redis Queues If you\u0026rsquo;re using Redis, Laravel Horizon provides a beautiful dashboard and auto-scaling:\ncomposer require laravel/horizon php artisan horizon:install php artisan migrate Configure Horizon for auto-scaling:\n// config/horizon.php \u0026#39;environments\u0026#39; =\u0026gt; [ \u0026#39;production\u0026#39; =\u0026gt; [ \u0026#39;supervisor-1\u0026#39; =\u0026gt; [ \u0026#39;connection\u0026#39; =\u0026gt; \u0026#39;redis\u0026#39;, \u0026#39;queue\u0026#39; =\u0026gt; [\u0026#39;default\u0026#39;], \u0026#39;balance\u0026#39; =\u0026gt; \u0026#39;auto\u0026#39;, \u0026#39;autoScalingStrategy\u0026#39; =\u0026gt; \u0026#39;time\u0026#39;, \u0026#39;minProcesses\u0026#39; =\u0026gt; 1, \u0026#39;maxProcesses\u0026#39; =\u0026gt; 10, \u0026#39;balanceMaxShift\u0026#39; =\u0026gt; 1, \u0026#39;balanceCooldown\u0026#39; =\u0026gt; 3, \u0026#39;tries\u0026#39; =\u0026gt; 3, ], ], ], Testing Queue Jobs Testing queued jobs requires special considerations since they run asynchronously.\nTesting Job Dispatch Test that jobs are queued correctly:\nclass UserRegistrationTest extends TestCase { public function test_welcome_email_is_queued_after_registration() { Queue::fake(); $userData = [ \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;John Doe\u0026#39;, \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;john@example.com\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;password\u0026#39;, ]; $response = $this-\u0026gt;post(\u0026#39;/register\u0026#39;, $userData); $response-\u0026gt;assertStatus(302); Queue::assertPushed(SendWelcomeEmail::class); } public function test_welcome_email_job_has_correct_user() { Queue::fake(); $user = User::factory()-\u0026gt;create(); SendWelcomeEmail::dispatch($user); Queue::assertPushed(SendWelcomeEmail::class, function ($job) use ($user) { return $job-\u0026gt;user-\u0026gt;id === $user-\u0026gt;id; }); } } Testing Job Execution Test the actual job logic:\nclass SendWelcomeEmailTest extends TestCase { public function test_welcome_email_is_sent() { Mail::fake(); $user = User::factory()-\u0026gt;create(); $job = new SendWelcomeEmail($user); $job-\u0026gt;handle(); Mail::assertSent(WelcomeEmail::class, function ($mail) use ($user) { return $mail-\u0026gt;hasTo($user-\u0026gt;email); }); } public function test_job_handles_invalid_user() { $user = User::factory()-\u0026gt;create(); $user-\u0026gt;delete(); // Simulate deleted user $job = new SendWelcomeEmail($user); // Should not throw exception $this-\u0026gt;assertNull($job-\u0026gt;handle()); } } Testing Failed Jobs Test failure scenarios:\nclass ProcessPaymentTest extends TestCase { public function test_job_fails_gracefully_with_invalid_payment() { $order = Order::factory()-\u0026gt;create(); $job = new ProcessPayment($order); // Mock payment service to throw exception $this-\u0026gt;mock(PaymentService::class, function ($mock) { $mock-\u0026gt;shouldReceive(\u0026#39;charge\u0026#39;)-\u0026gt;andThrow(new PaymentException(\u0026#39;Invalid card\u0026#39;)); }); $this-\u0026gt;expectException(PaymentException::class); $job-\u0026gt;handle(); } public function test_failed_method_updates_order_status() { $order = Order::factory()-\u0026gt;create(); $job = new ProcessPayment($order); $exception = new PaymentException(\u0026#39;Payment failed\u0026#39;); $job-\u0026gt;failed($exception); $this-\u0026gt;assertEquals(\u0026#39;payment_failed\u0026#39;, $order-\u0026gt;fresh()-\u0026gt;status); } } Security Considerations Queue jobs can access sensitive data and perform critical operations, so security is important.\nInput Validation Always validate data in your jobs:\nclass ProcessUserData implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $userData; public function __construct(array $userData) { $this-\u0026gt;userData = $userData; } public function handle() { // Validate data even in background jobs $validator = Validator::make($this-\u0026gt;userData, [ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email\u0026#39;, \u0026#39;name\u0026#39; =\u0026gt; \u0026#39;required|string|max:255\u0026#39;, \u0026#39;age\u0026#39; =\u0026gt; \u0026#39;integer|min:0|max:150\u0026#39;, ]); if ($validator-\u0026gt;fails()) { Log::error(\u0026#39;Invalid data in job\u0026#39;, [ \u0026#39;data\u0026#39; =\u0026gt; $this-\u0026gt;userData, \u0026#39;errors\u0026#39; =\u0026gt; $validator-\u0026gt;errors(), ]); return; } // Process validated data $this-\u0026gt;processValidatedData($validator-\u0026gt;validated()); } } Authorization Checks Ensure jobs respect user permissions:\nclass DeleteUserAccount implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $userId; protected $requestedByUserId; public function __construct($userId, $requestedByUserId) { $this-\u0026gt;userId = $userId; $this-\u0026gt;requestedByUserId = $requestedByUserId; } public function handle() { $user = User::find($this-\u0026gt;userId); $requestedBy = User::find($this-\u0026gt;requestedByUserId); // Check if user still exists and requester has permission if (!$user || !$requestedBy) { Log::warning(\u0026#39;User deletion job failed - user not found\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;userId, \u0026#39;requested_by\u0026#39; =\u0026gt; $this-\u0026gt;requestedByUserId, ]); return; } // Only admins or the user themselves can delete accounts if ($requestedBy-\u0026gt;id !== $user-\u0026gt;id \u0026amp;\u0026amp; !$requestedBy-\u0026gt;isAdmin()) { Log::warning(\u0026#39;Unauthorized user deletion attempt\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;userId, \u0026#39;requested_by\u0026#39; =\u0026gt; $this-\u0026gt;requestedByUserId, ]); return; } // Proceed with deletion $user-\u0026gt;delete(); } } Sensitive Data Handling Be careful with sensitive data in jobs:\nclass ProcessCreditCard implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $encryptedCardData; protected $orderId; public function __construct($cardData, $orderId) { // Encrypt sensitive data before queuing $this-\u0026gt;encryptedCardData = encrypt($cardData); $this-\u0026gt;orderId = $orderId; } public function handle() { try { // Decrypt data when processing $cardData = decrypt($this-\u0026gt;encryptedCardData); // Process payment $this-\u0026gt;processPayment($cardData); } finally { // Clear sensitive data from memory $this-\u0026gt;encryptedCardData = null; $cardData = null; } } // Don\u0026#39;t store sensitive data in failed jobs table public function failed(Throwable $exception) { Log::error(\u0026#39;Payment processing failed\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $this-\u0026gt;orderId, \u0026#39;error\u0026#39; =\u0026gt; $exception-\u0026gt;getMessage(), // Don\u0026#39;t log card data! ]); } } For comprehensive security practices, review our Laravel security best practices guide .\nCommon Pitfalls and Solutions Here are the most common issues developers face with Laravel queues and how to solve them.\nMemory Leaks in Workers Long-running workers can accumulate memory. Solution:\n# Restart workers periodically php artisan queue:work --max-jobs=1000 --max-time=3600 # Monitor memory usage php artisan queue:work --memory=512 Database Connection Timeouts Workers that run for hours may lose database connections:\nclass LongRunningJob implements ShouldQueue { public function handle() { try { // Check connection before database operations DB::reconnect(); // Your database operations $this-\u0026gt;performDatabaseWork(); } catch (QueryException $e) { // Retry with fresh connection DB::reconnect(); $this-\u0026gt;performDatabaseWork(); } } } Job Serialization Issues Be careful with what you pass to jobs:\n// Bad - Eloquent models can become stale class BadJob implements ShouldQueue { protected $user; // This user data can become outdated public function __construct(User $user) { $this-\u0026gt;user = $user; // Serializes entire model } } // Good - Pass IDs and reload fresh data class GoodJob implements ShouldQueue { protected $userId; public function __construct($userId) { $this-\u0026gt;userId = $userId; // Only store ID } public function handle() { $user = User::find($this-\u0026gt;userId); // Load fresh data if (!$user) { Log::warning(\u0026#39;User not found in job\u0026#39;, [\u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;userId]); return; } // Work with fresh user data } } Duplicate Job Prevention Prevent the same job from being queued multiple times:\nclass SendDailyReport implements ShouldQueue, ShouldBeUnique { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $date; public function __construct($date) { $this-\u0026gt;date = $date; } // Define uniqueness public function uniqueId() { return \u0026#39;daily-report-\u0026#39; . $this-\u0026gt;date; } // How long to maintain uniqueness lock public function uniqueFor() { return 3600; // 1 hour } } Conclusion Laravel queues are one of those features that seem complex at first but become indispensable once you understand them. They\u0026rsquo;re the key to building responsive applications that can handle heavy workloads without making users wait.\nStart simple with the database driver for development and testing. As your needs grow, move to Redis for better performance or SQS for cloud scalability. Focus on these fundamentals:\nQueue time-consuming tasks like emails, file processing, and API calls Use proper error handling and retry logic Monitor your queues to catch issues early Test your jobs thoroughly Be mindful of security when handling sensitive data The difference between a sluggish app and a snappy one often comes down to smart use of background processing. With Laravel\u0026rsquo;s queue system, you have all the tools you need to build applications that feel fast and responsive, no matter how much work they\u0026rsquo;re doing behind the scenes.\nRemember, the best queue implementation is one that your users never notice - they just know your app feels fast and reliable. Start implementing queues in your next project, and your users (and your servers) will thank you for it.\n","href":"/2025/09/laravel-queue-jobs-background-processing-tutorial.html","title":"Easy Background Processing Tutorial"},{"content":"Running a Laravel application in production without proper monitoring is like driving blindfolded - you won\u0026rsquo;t know there\u0026rsquo;s a problem until you crash. The moment your app goes live, dozens of things can go wrong: database connections can fail, APIs can timeout, memory can run out, or users might trigger unexpected errors you never saw during development.\nGood monitoring isn\u0026rsquo;t just about knowing when things break - it\u0026rsquo;s about catching issues before they affect users, understanding performance trends, and having the data you need to fix problems quickly. Whether you\u0026rsquo;re running a small business site or a high-traffic application, the right monitoring setup can save you countless sleepless nights and frustrated customer calls.\nWhy Production Monitoring Matters Picture this: it\u0026rsquo;s Friday evening, you\u0026rsquo;re having dinner with family, and suddenly your phone starts buzzing with angry customer emails about your app being down. You frantically open your laptop to find that your database has been throwing connection errors for the past three hours, but you had no idea because there was no monitoring in place.\nThis scenario plays out more often than you\u0026rsquo;d think. Without proper monitoring, you\u0026rsquo;re always reactive instead of proactive. You only learn about problems when users complain, which means:\nRevenue loss from downtime you didn\u0026rsquo;t know about Damaged reputation from poor user experience Hours spent debugging without proper context Stress from constant uncertainty about your app\u0026rsquo;s health Production monitoring changes this completely. Instead of waiting for problems to surface, you get real-time insights into your application\u0026rsquo;s health, performance trends, and potential issues before they impact users.\nEssential Monitoring Categories Effective Laravel monitoring covers several key areas, each providing different insights into your application\u0026rsquo;s health.\nApplication Performance Monitoring This tracks how fast your application responds to requests, how much memory it uses, and where bottlenecks occur:\n// Simple performance tracking middleware class PerformanceMonitoring { public function handle($request, Closure $next) { $start = microtime(true); $startMemory = memory_get_usage(); $response = $next($request); $duration = microtime(true) - $start; $memoryUsed = memory_get_usage() - $startMemory; if ($duration \u0026gt; 1.0) { // Log slow requests Log::warning(\u0026#39;Slow request detected\u0026#39;, [ \u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;url(), \u0026#39;method\u0026#39; =\u0026gt; $request-\u0026gt;method(), \u0026#39;duration\u0026#39; =\u0026gt; round($duration, 3), \u0026#39;memory_mb\u0026#39; =\u0026gt; round($memoryUsed / 1024 / 1024, 2), \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), ]); } return $response; } } Error and Exception Tracking Catching and analyzing errors before they become bigger problems:\n// Custom exception handler for better error tracking class Handler extends ExceptionHandler { public function report(Throwable $exception) { if ($this-\u0026gt;shouldReport($exception)) { // Add context to error reports $context = [ \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;url\u0026#39; =\u0026gt; request()-\u0026gt;url(), \u0026#39;ip\u0026#39; =\u0026gt; request()-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; request()-\u0026gt;userAgent(), \u0026#39;session_id\u0026#39; =\u0026gt; session()-\u0026gt;getId(), ]; Log::error($exception-\u0026gt;getMessage(), array_merge($context, [ \u0026#39;exception\u0026#39; =\u0026gt; $exception, \u0026#39;trace\u0026#39; =\u0026gt; $exception-\u0026gt;getTraceAsString(), ])); // Send to external service (Sentry, Bugsnag, etc.) if (app()-\u0026gt;bound(\u0026#39;sentry\u0026#39;)) { app(\u0026#39;sentry\u0026#39;)-\u0026gt;captureException($exception); } } parent::report($exception); } } Database Performance Monitoring Keeping an eye on query performance and database health:\n// Monitor slow database queries DB::listen(function ($query) { if ($query-\u0026gt;time \u0026gt; 1000) { // Queries taking more than 1 second Log::warning(\u0026#39;Slow database query detected\u0026#39;, [ \u0026#39;sql\u0026#39; =\u0026gt; $query-\u0026gt;sql, \u0026#39;bindings\u0026#39; =\u0026gt; $query-\u0026gt;bindings, \u0026#39;time\u0026#39; =\u0026gt; $query-\u0026gt;time, \u0026#39;connection\u0026#39; =\u0026gt; $query-\u0026gt;connectionName, ]); } }); // Check for N+1 query problems $queryCount = 0; DB::listen(function ($query) use (\u0026amp;$queryCount) { $queryCount++; if ($queryCount \u0026gt; 50) { // Too many queries in one request Log::warning(\u0026#39;Potential N+1 query problem\u0026#39;, [ \u0026#39;query_count\u0026#39; =\u0026gt; $queryCount, \u0026#39;url\u0026#39; =\u0026gt; request()-\u0026gt;url(), ]); } }); For detailed strategies on optimizing database queries, check out our guide on Laravel N+1 query problem solutions .\nQueue and Job Monitoring Making sure your background jobs are running smoothly:\n// Monitor failed jobs class MonitorFailedJobs extends Command { public function handle() { $failedJobs = DB::table(\u0026#39;failed_jobs\u0026#39;) -\u0026gt;where(\u0026#39;failed_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;count(); if ($failedJobs \u0026gt; 10) { Log::alert(\u0026#39;High number of failed jobs\u0026#39;, [ \u0026#39;failed_count\u0026#39; =\u0026gt; $failedJobs, \u0026#39;time_window\u0026#39; =\u0026gt; \u0026#39;1 hour\u0026#39;, ]); // Send notification to team Notification::route(\u0026#39;slack\u0026#39;, config(\u0026#39;monitoring.slack_webhook\u0026#39;)) -\u0026gt;notify(new HighFailedJobsAlert($failedJobs)); } } } // Add this to your schedule $schedule-\u0026gt;command(\u0026#39;monitor:failed-jobs\u0026#39;)-\u0026gt;everyFiveMinutes(); Setting Up Laravel Telescope for Development Insights Laravel Telescope is like having X-ray vision for your application. While it\u0026rsquo;s primarily a development tool, understanding how to use it effectively helps you identify issues that might appear in production.\nInstall Telescope in your development environment:\ncomposer require laravel/telescope --dev php artisan telescope:install php artisan migrate Configure Telescope to capture the data you need:\n// config/telescope.php \u0026#39;watchers\u0026#39; =\u0026gt; [ Watchers\\DumpWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_DUMP_WATCHER\u0026#39;, true), Watchers\\FrameworkWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_FRAMEWORK_WATCHER\u0026#39;, true), Watchers\\DatabaseWatcher::class =\u0026gt; [ \u0026#39;enabled\u0026#39; =\u0026gt; env(\u0026#39;TELESCOPE_DB_WATCHER\u0026#39;, true), \u0026#39;slow\u0026#39; =\u0026gt; 100, // Log queries slower than 100ms ], Watchers\\EloquentWatcher::class =\u0026gt; [ \u0026#39;enabled\u0026#39; =\u0026gt; env(\u0026#39;TELESCOPE_ELOQUENT_WATCHER\u0026#39;, true), \u0026#39;hydrations\u0026#39; =\u0026gt; true, ], Watchers\\ExceptionWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_EXCEPTION_WATCHER\u0026#39;, true), Watchers\\JobWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_JOB_WATCHER\u0026#39;, true), Watchers\\LogWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_LOG_WATCHER\u0026#39;, true), Watchers\\MailWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_MAIL_WATCHER\u0026#39;, true), ], Never run Telescope in production - it\u0026rsquo;s a development tool that can impact performance and expose sensitive data.\nError Tracking with Sentry Sentry is probably the most popular error tracking service for Laravel applications, and for good reason. It gives you detailed error reports with context, user impact analysis, and powerful filtering capabilities.\nSetting up Sentry is straightforward:\ncomposer require sentry/sentry-laravel php artisan vendor:publish --provider=\u0026#34;Sentry\\Laravel\\ServiceProvider\u0026#34; Configure your Sentry DSN in your environment file:\nSENTRY_LARAVEL_DSN=https://your-dsn@sentry.io/project-id SENTRY_ENVIRONMENT=production Customize error reporting to add useful context:\n// In your exception handler public function report(Throwable $exception) { if (app()-\u0026gt;bound(\u0026#39;sentry\u0026#39;) \u0026amp;\u0026amp; $this-\u0026gt;shouldReport($exception)) { app(\u0026#39;sentry\u0026#39;)-\u0026gt;configureScope(function (Scope $scope) { $scope-\u0026gt;setUser([ \u0026#39;id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;email\u0026#39; =\u0026gt; auth()-\u0026gt;user()-\u0026gt;email ?? null, ]); $scope-\u0026gt;setTag(\u0026#39;feature\u0026#39;, request()-\u0026gt;route()-\u0026gt;getName()); $scope-\u0026gt;setContext(\u0026#39;request\u0026#39;, [ \u0026#39;url\u0026#39; =\u0026gt; request()-\u0026gt;url(), \u0026#39;method\u0026#39; =\u0026gt; request()-\u0026gt;method(), \u0026#39;ip\u0026#39; =\u0026gt; request()-\u0026gt;ip(), ]); }); app(\u0026#39;sentry\u0026#39;)-\u0026gt;captureException($exception); } parent::report($exception); } Custom Error Context Add business-specific context to your error reports:\n// In a controller or service try { $order = $this-\u0026gt;processPayment($paymentData); } catch (PaymentException $e) { // Add context before the exception bubbles up app(\u0026#39;sentry\u0026#39;)-\u0026gt;addBreadcrumb([ \u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Payment processing failed\u0026#39;, \u0026#39;category\u0026#39; =\u0026gt; \u0026#39;payment\u0026#39;, \u0026#39;data\u0026#39; =\u0026gt; [ \u0026#39;amount\u0026#39; =\u0026gt; $paymentData[\u0026#39;amount\u0026#39;], \u0026#39;currency\u0026#39; =\u0026gt; $paymentData[\u0026#39;currency\u0026#39;], \u0026#39;payment_method\u0026#39; =\u0026gt; $paymentData[\u0026#39;method\u0026#39;], ], ]); throw $e; } Alternative Error Tracking Solutions While Sentry is popular, you have several other excellent options:\nBugsnag Bugsnag offers similar functionality with a different interface and pricing model:\ncomposer require bugsnag/bugsnag-laravel php artisan vendor:publish --provider=\u0026#34;Bugsnag\\BugsnagLaravel\\BugsnagServiceProvider\u0026#34; Rollbar Rollbar provides real-time error tracking with good Laravel integration:\ncomposer require rollbar/rollbar-laravel php artisan vendor:publish --provider=\u0026#34;Rollbar\\Laravel\\RollbarServiceProvider\u0026#34; Flare (by Spatie) Flare is specifically designed for Laravel and offers beautiful error pages:\ncomposer require facade/ignition Application Performance Monitoring (APM) Tools APM tools help you understand not just when errors occur, but why your application might be slow or consuming too many resources.\nNew Relic New Relic provides comprehensive APM for PHP applications:\n# Install New Relic PHP agent # Follow platform-specific installation guide # Add to your .env NEW_RELIC_ENABLED=true NEW_RELIC_APP_NAME=\u0026#34;Your Laravel App\u0026#34; DataDog APM DataDog offers powerful monitoring with great visualization:\ncomposer require datadog/php-datadogstatsd Configure custom metrics:\n// Track custom business metrics class OrderService { protected $statsd; public function __construct() { $this-\u0026gt;statsd = new \\DataDogStatsd(); } public function createOrder($orderData) { $start = microtime(true); try { $order = Order::create($orderData); // Track successful orders $this-\u0026gt;statsd-\u0026gt;increment(\u0026#39;orders.created\u0026#39;); $this-\u0026gt;statsd-\u0026gt;histogram(\u0026#39;orders.value\u0026#39;, $order-\u0026gt;total); return $order; } catch (Exception $e) { $this-\u0026gt;statsd-\u0026gt;increment(\u0026#39;orders.failed\u0026#39;); throw $e; } finally { $duration = microtime(true) - $start; $this-\u0026gt;statsd-\u0026gt;timing(\u0026#39;orders.creation_time\u0026#39;, $duration * 1000); } } } Health Check Endpoints Health checks are simple endpoints that let you (and your monitoring tools) quickly verify that your application is working properly.\nBasic Health Check // routes/web.php Route::get(\u0026#39;/health\u0026#39;, function () { $checks = [ \u0026#39;database\u0026#39; =\u0026gt; false, \u0026#39;redis\u0026#39; =\u0026gt; false, \u0026#39;storage\u0026#39; =\u0026gt; false, ]; // Check database connection try { DB::connection()-\u0026gt;getPdo(); $checks[\u0026#39;database\u0026#39;] = true; } catch (Exception $e) { Log::error(\u0026#39;Database health check failed\u0026#39;, [\u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage()]); } // Check Redis connection try { Redis::ping(); $checks[\u0026#39;redis\u0026#39;] = true; } catch (Exception $e) { Log::error(\u0026#39;Redis health check failed\u0026#39;, [\u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage()]); } // Check storage access try { Storage::disk(\u0026#39;local\u0026#39;)-\u0026gt;put(\u0026#39;health-check\u0026#39;, \u0026#39;test\u0026#39;); Storage::disk(\u0026#39;local\u0026#39;)-\u0026gt;delete(\u0026#39;health-check\u0026#39;); $checks[\u0026#39;storage\u0026#39;] = true; } catch (Exception $e) { Log::error(\u0026#39;Storage health check failed\u0026#39;, [\u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage()]); } $allHealthy = !in_array(false, $checks); return response()-\u0026gt;json([ \u0026#39;status\u0026#39; =\u0026gt; $allHealthy ? \u0026#39;healthy\u0026#39; : \u0026#39;unhealthy\u0026#39;, \u0026#39;checks\u0026#39; =\u0026gt; $checks, \u0026#39;timestamp\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ], $allHealthy ? 200 : 503); }); Advanced Health Checks Create more sophisticated health checks that test business-critical functionality:\nclass HealthCheckController extends Controller { public function comprehensive() { $checks = [ \u0026#39;database\u0026#39; =\u0026gt; $this-\u0026gt;checkDatabase(), \u0026#39;redis\u0026#39; =\u0026gt; $this-\u0026gt;checkRedis(), \u0026#39;external_apis\u0026#39; =\u0026gt; $this-\u0026gt;checkExternalAPIs(), \u0026#39;queue_workers\u0026#39; =\u0026gt; $this-\u0026gt;checkQueueWorkers(), \u0026#39;disk_space\u0026#39; =\u0026gt; $this-\u0026gt;checkDiskSpace(), ]; $overallHealth = collect($checks)-\u0026gt;every(fn($check) =\u0026gt; $check[\u0026#39;healthy\u0026#39;]); return response()-\u0026gt;json([ \u0026#39;status\u0026#39; =\u0026gt; $overallHealth ? \u0026#39;healthy\u0026#39; : \u0026#39;degraded\u0026#39;, \u0026#39;checks\u0026#39; =\u0026gt; $checks, \u0026#39;version\u0026#39; =\u0026gt; config(\u0026#39;app.version\u0026#39;), \u0026#39;environment\u0026#39; =\u0026gt; app()-\u0026gt;environment(), \u0026#39;timestamp\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ], $overallHealth ? 200 : 503); } protected function checkDatabase() { try { $start = microtime(true); $userCount = User::count(); $duration = microtime(true) - $start; return [ \u0026#39;healthy\u0026#39; =\u0026gt; true, \u0026#39;response_time\u0026#39; =\u0026gt; round($duration * 1000, 2) . \u0026#39;ms\u0026#39;, \u0026#39;details\u0026#39; =\u0026gt; \u0026#34;Connected successfully, {$userCount} users\u0026#34;, ]; } catch (Exception $e) { return [ \u0026#39;healthy\u0026#39; =\u0026gt; false, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), ]; } } protected function checkExternalAPIs() { $apis = [ \u0026#39;payment_gateway\u0026#39; =\u0026gt; \u0026#39;https://api.stripe.com/v1/charges\u0026#39;, \u0026#39;email_service\u0026#39; =\u0026gt; \u0026#39;https://api.mailgun.net/v3/domains\u0026#39;, ]; $results = []; foreach ($apis as $name =\u0026gt; $url) { try { $start = microtime(true); $response = Http::timeout(5)-\u0026gt;get($url); $duration = microtime(true) - $start; $results[$name] = [ \u0026#39;healthy\u0026#39; =\u0026gt; $response-\u0026gt;successful(), \u0026#39;response_time\u0026#39; =\u0026gt; round($duration * 1000, 2) . \u0026#39;ms\u0026#39;, \u0026#39;status_code\u0026#39; =\u0026gt; $response-\u0026gt;status(), ]; } catch (Exception $e) { $results[$name] = [ \u0026#39;healthy\u0026#39; =\u0026gt; false, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), ]; } } return [ \u0026#39;healthy\u0026#39; =\u0026gt; collect($results)-\u0026gt;every(fn($result) =\u0026gt; $result[\u0026#39;healthy\u0026#39;]), \u0026#39;apis\u0026#39; =\u0026gt; $results, ]; } } Custom Logging and Alerting Sometimes you need monitoring that\u0026rsquo;s specific to your business logic. Custom logging and alerting help you track what matters most to your application.\nBusiness Metrics Monitoring // Monitor critical business events class OrderMetrics { public static function trackOrderCreated(Order $order) { Log::info(\u0026#39;Order created\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $order-\u0026gt;id, \u0026#39;user_id\u0026#39; =\u0026gt; $order-\u0026gt;user_id, \u0026#39;total\u0026#39; =\u0026gt; $order-\u0026gt;total, \u0026#39;items_count\u0026#39; =\u0026gt; $order-\u0026gt;items-\u0026gt;count(), \u0026#39;payment_method\u0026#39; =\u0026gt; $order-\u0026gt;payment_method, ]); // Track unusual order patterns if ($order-\u0026gt;total \u0026gt; 10000) { Log::warning(\u0026#39;High-value order created\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $order-\u0026gt;id, \u0026#39;total\u0026#39; =\u0026gt; $order-\u0026gt;total, \u0026#39;user_id\u0026#39; =\u0026gt; $order-\u0026gt;user_id, ]); } } public static function trackPaymentFailure($orderData, $error) { Log::error(\u0026#39;Payment processing failed\u0026#39;, [ \u0026#39;order_data\u0026#39; =\u0026gt; $orderData, \u0026#39;error\u0026#39; =\u0026gt; $error, \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;payment_method\u0026#39; =\u0026gt; $orderData[\u0026#39;payment_method\u0026#39;] ?? \u0026#39;unknown\u0026#39;, ]); // Alert if payment failures spike $recentFailures = Log::query() -\u0026gt;where(\u0026#39;level\u0026#39;, \u0026#39;error\u0026#39;) -\u0026gt;where(\u0026#39;message\u0026#39;, \u0026#39;Payment processing failed\u0026#39;) -\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;count(); if ($recentFailures \u0026gt; 10) { // Send alert to team Mail::to(config(\u0026#39;monitoring.alert_email\u0026#39;)) -\u0026gt;send(new PaymentFailureSpike($recentFailures)); } } } Real-time Alerting Set up alerts for critical issues:\n// Slack notification for critical errors class CriticalErrorNotification extends Notification { protected $exception; protected $context; public function __construct(Throwable $exception, array $context = []) { $this-\u0026gt;exception = $exception; $this-\u0026gt;context = $context; } public function via($notifiable) { return [\u0026#39;slack\u0026#39;]; } public function toSlack($notifiable) { return (new SlackMessage) -\u0026gt;error() -\u0026gt;content(\u0026#39;Critical error in production!\u0026#39;) -\u0026gt;attachment(function ($attachment) { $attachment-\u0026gt;title($this-\u0026gt;exception-\u0026gt;getMessage()) -\u0026gt;fields([ \u0026#39;File\u0026#39; =\u0026gt; $this-\u0026gt;exception-\u0026gt;getFile() . \u0026#39;:\u0026#39; . $this-\u0026gt;exception-\u0026gt;getLine(), \u0026#39;URL\u0026#39; =\u0026gt; $this-\u0026gt;context[\u0026#39;url\u0026#39;] ?? \u0026#39;N/A\u0026#39;, \u0026#39;User\u0026#39; =\u0026gt; $this-\u0026gt;context[\u0026#39;user_id\u0026#39;] ?? \u0026#39;Guest\u0026#39;, \u0026#39;Time\u0026#39; =\u0026gt; now()-\u0026gt;toDateTimeString(), ]); }); } } // Use in your exception handler if ($this-\u0026gt;isCriticalException($exception)) { Notification::route(\u0026#39;slack\u0026#39;, config(\u0026#39;monitoring.slack_webhook\u0026#39;)) -\u0026gt;notify(new CriticalErrorNotification($exception, $context)); } Performance Monitoring Strategies Understanding your application\u0026rsquo;s performance trends helps you optimize proactively rather than reactively.\nResponse Time Tracking class ResponseTimeMiddleware { public function handle($request, Closure $next) { $start = microtime(true); $response = $next($request); $duration = microtime(true) - $start; // Log response times for analysis Log::info(\u0026#39;Response time\u0026#39;, [ \u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;url(), \u0026#39;method\u0026#39; =\u0026gt; $request-\u0026gt;method(), \u0026#39;duration\u0026#39; =\u0026gt; round($duration, 3), \u0026#39;memory_peak\u0026#39; =\u0026gt; memory_get_peak_usage(true), \u0026#39;status_code\u0026#39; =\u0026gt; $response-\u0026gt;getStatusCode(), ]); // Add response time header for monitoring tools $response-\u0026gt;header(\u0026#39;X-Response-Time\u0026#39;, round($duration * 1000, 2)); return $response; } } Memory Usage Monitoring // Track memory usage patterns class MemoryMonitoring { public static function logMemoryUsage($context = \u0026#39;\u0026#39;) { $current = memory_get_usage(true); $peak = memory_get_peak_usage(true); if ($current \u0026gt; 100 * 1024 * 1024) { // \u0026gt; 100MB Log::warning(\u0026#39;High memory usage detected\u0026#39;, [ \u0026#39;context\u0026#39; =\u0026gt; $context, \u0026#39;current_mb\u0026#39; =\u0026gt; round($current / 1024 / 1024, 2), \u0026#39;peak_mb\u0026#39; =\u0026gt; round($peak / 1024 / 1024, 2), \u0026#39;url\u0026#39; =\u0026gt; request()-\u0026gt;url(), ]); } } } // Use in controllers or services public function processLargeDataset($data) { MemoryMonitoring::logMemoryUsage(\u0026#39;Before processing dataset\u0026#39;); // Your processing logic here $result = $this-\u0026gt;processData($data); MemoryMonitoring::logMemoryUsage(\u0026#39;After processing dataset\u0026#39;); return $result; } For comprehensive performance optimization techniques, explore our detailed guide on Laravel performance optimization .\nQueue and Job Monitoring Background jobs are critical for many Laravel applications, but they\u0026rsquo;re also easy to forget about until something goes wrong.\nQueue Health Monitoring // Monitor queue health class QueueHealthCheck extends Command { public function handle() { $connections = [\u0026#39;database\u0026#39;, \u0026#39;redis\u0026#39;, \u0026#39;sqs\u0026#39;]; // Your queue connections foreach ($connections as $connection) { $this-\u0026gt;checkQueueConnection($connection); } } protected function checkQueueConnection($connection) { try { $size = Queue::connection($connection)-\u0026gt;size(); // Alert if queue is backing up if ($size \u0026gt; 1000) { Log::warning(\u0026#39;Queue backup detected\u0026#39;, [ \u0026#39;connection\u0026#39; =\u0026gt; $connection, \u0026#39;size\u0026#39; =\u0026gt; $size, ]); // Send alert Notification::route(\u0026#39;slack\u0026#39;, config(\u0026#39;monitoring.slack_webhook\u0026#39;)) -\u0026gt;notify(new QueueBackupAlert($connection, $size)); } // Check for stuck jobs $oldestJob = DB::table(\u0026#39;jobs\u0026#39;) -\u0026gt;where(\u0026#39;queue\u0026#39;, $connection) -\u0026gt;orderBy(\u0026#39;created_at\u0026#39;) -\u0026gt;first(); if ($oldestJob \u0026amp;\u0026amp; now()-\u0026gt;diffInMinutes($oldestJob-\u0026gt;created_at) \u0026gt; 60) { Log::warning(\u0026#39;Stuck job detected\u0026#39;, [ \u0026#39;connection\u0026#39; =\u0026gt; $connection, \u0026#39;job_age_minutes\u0026#39; =\u0026gt; now()-\u0026gt;diffInMinutes($oldestJob-\u0026gt;created_at), ]); } } catch (Exception $e) { Log::error(\u0026#39;Queue health check failed\u0026#39;, [ \u0026#39;connection\u0026#39; =\u0026gt; $connection, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), ]); } } } Job Performance Tracking // Track job performance class JobPerformanceTracker { public function handle($job, $next) { $start = microtime(true); $startMemory = memory_get_usage(); try { $result = $next($job); $this-\u0026gt;logJobSuccess($job, $start, $startMemory); return $result; } catch (Exception $e) { $this-\u0026gt;logJobFailure($job, $start, $startMemory, $e); throw $e; } } protected function logJobSuccess($job, $start, $startMemory) { $duration = microtime(true) - $start; $memoryUsed = memory_get_usage() - $startMemory; Log::info(\u0026#39;Job completed\u0026#39;, [ \u0026#39;job_class\u0026#39; =\u0026gt; get_class($job), \u0026#39;duration\u0026#39; =\u0026gt; round($duration, 3), \u0026#39;memory_used_mb\u0026#39; =\u0026gt; round($memoryUsed / 1024 / 1024, 2), \u0026#39;queue\u0026#39; =\u0026gt; $job-\u0026gt;queue ?? \u0026#39;default\u0026#39;, ]); // Alert on slow jobs if ($duration \u0026gt; 300) { // 5 minutes Log::warning(\u0026#39;Slow job detected\u0026#39;, [ \u0026#39;job_class\u0026#39; =\u0026gt; get_class($job), \u0026#39;duration\u0026#39; =\u0026gt; round($duration, 3), ]); } } } Server and Infrastructure Monitoring Your Laravel application doesn\u0026rsquo;t exist in a vacuum - server health directly impacts application performance.\nSystem Resource Monitoring // Monitor system resources class SystemResourceCheck extends Command { public function handle() { $this-\u0026gt;checkDiskSpace(); $this-\u0026gt;checkMemoryUsage(); $this-\u0026gt;checkCPULoad(); } protected function checkDiskSpace() { $totalSpace = disk_total_space(\u0026#39;/\u0026#39;); $freeSpace = disk_free_space(\u0026#39;/\u0026#39;); $usedPercent = (($totalSpace - $freeSpace) / $totalSpace) * 100; if ($usedPercent \u0026gt; 90) { Log::alert(\u0026#39;Low disk space\u0026#39;, [ \u0026#39;used_percent\u0026#39; =\u0026gt; round($usedPercent, 2), \u0026#39;free_gb\u0026#39; =\u0026gt; round($freeSpace / 1024 / 1024 / 1024, 2), ]); } } protected function checkMemoryUsage() { $meminfo = file_get_contents(\u0026#39;/proc/meminfo\u0026#39;); preg_match(\u0026#39;/MemTotal:\\s+(\\d+)/\u0026#39;, $meminfo, $totalMatch); preg_match(\u0026#39;/MemAvailable:\\s+(\\d+)/\u0026#39;, $meminfo, $availableMatch); $total = $totalMatch[1] * 1024; // Convert to bytes $available = $availableMatch[1] * 1024; $usedPercent = (($total - $available) / $total) * 100; if ($usedPercent \u0026gt; 90) { Log::warning(\u0026#39;High memory usage\u0026#39;, [ \u0026#39;used_percent\u0026#39; =\u0026gt; round($usedPercent, 2), \u0026#39;available_gb\u0026#39; =\u0026gt; round($available / 1024 / 1024 / 1024, 2), ]); } } } Setting Up Alerting Rules Effective alerting prevents alert fatigue while ensuring you know about real problems quickly.\nSmart Alerting Strategy // Intelligent alerting that reduces noise class SmartAlerting { protected static $alertCooldowns = []; public static function sendAlert($type, $message, $data = [], $cooldownMinutes = 15) { $key = md5($type . $message); // Check if we\u0026#39;re in cooldown period if (isset(self::$alertCooldowns[$key])) { $lastSent = self::$alertCooldowns[$key]; if (now()-\u0026gt;diffInMinutes($lastSent) \u0026lt; $cooldownMinutes) { return; // Skip this alert } } // Send the alert self::$alertCooldowns[$key] = now(); Log::alert($message, array_merge($data, [ \u0026#39;alert_type\u0026#39; =\u0026gt; $type, \u0026#39;timestamp\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ])); // Send to external services if ($type === \u0026#39;critical\u0026#39;) { Notification::route(\u0026#39;slack\u0026#39;, config(\u0026#39;monitoring.critical_slack_webhook\u0026#39;)) -\u0026gt;notify(new CriticalAlert($message, $data)); } } } // Usage SmartAlerting::sendAlert(\u0026#39;database\u0026#39;, \u0026#39;Database connection failed\u0026#39;, [ \u0026#39;connection\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;error\u0026#39; =\u0026gt; $exception-\u0026gt;getMessage(), ], 30); // 30 minute cooldown Monitoring Dashboard Creation Having all your monitoring data in one place makes it easier to spot patterns and troubleshoot issues.\nSimple Laravel Dashboard // Create a monitoring dashboard class MonitoringDashboardController extends Controller { public function index() { $metrics = [ \u0026#39;error_rate\u0026#39; =\u0026gt; $this-\u0026gt;getErrorRate(), \u0026#39;response_times\u0026#39; =\u0026gt; $this-\u0026gt;getAverageResponseTimes(), \u0026#39;active_users\u0026#39; =\u0026gt; $this-\u0026gt;getActiveUsers(), \u0026#39;queue_size\u0026#39; =\u0026gt; $this-\u0026gt;getQueueSizes(), \u0026#39;system_health\u0026#39; =\u0026gt; $this-\u0026gt;getSystemHealth(), ]; return view(\u0026#39;monitoring.dashboard\u0026#39;, compact(\u0026#39;metrics\u0026#39;)); } protected function getErrorRate() { $total = Log::query() -\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;count(); $errors = Log::query() -\u0026gt;whereIn(\u0026#39;level\u0026#39;, [\u0026#39;error\u0026#39;, \u0026#39;critical\u0026#39;, \u0026#39;alert\u0026#39;, \u0026#39;emergency\u0026#39;]) -\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;count(); return $total \u0026gt; 0 ? round(($errors / $total) * 100, 2) : 0; } protected function getAverageResponseTimes() { return Log::query() -\u0026gt;where(\u0026#39;message\u0026#39;, \u0026#39;Response time\u0026#39;) -\u0026gt;where(\u0026#39;created_at\u0026#39;, \u0026#39;\u0026gt;\u0026#39;, now()-\u0026gt;subHour()) -\u0026gt;get() -\u0026gt;map(function ($log) { return $log-\u0026gt;context[\u0026#39;duration\u0026#39;] ?? 0; }) -\u0026gt;average(); } } For additional security considerations when setting up monitoring, review our Laravel security best practices guide .\nBest Practices and Common Pitfalls Getting monitoring right requires avoiding common mistakes and following proven practices.\nWhat to Monitor vs. What to Ignore Focus on metrics that directly impact user experience or business outcomes:\nMonitor These:\nError rates and critical exceptions Response times for key user journeys Database query performance Queue processing times Business-critical API failures Authentication and authorization failures Don\u0026rsquo;t Over-Monitor These:\nEvery single log entry Minor warnings that don\u0026rsquo;t affect functionality Development-only events Overly granular performance metrics Avoiding Alert Fatigue // Implement alert severity levels class AlertManager { const SEVERITY_INFO = \u0026#39;info\u0026#39;; const SEVERITY_WARNING = \u0026#39;warning\u0026#39;; const SEVERITY_CRITICAL = \u0026#39;critical\u0026#39;; const SEVERITY_EMERGENCY = \u0026#39;emergency\u0026#39;; public static function alert($severity, $message, $data = []) { switch ($severity) { case self::SEVERITY_EMERGENCY: // Page on-call engineer immediately self::sendPagerAlert($message, $data); self::sendSlackAlert($message, $data, \u0026#39;#critical-alerts\u0026#39;); break; case self::SEVERITY_CRITICAL: // Slack alert to team self::sendSlackAlert($message, $data, \u0026#39;#alerts\u0026#39;); break; case self::SEVERITY_WARNING: // Log for investigation during business hours Log::warning($message, $data); break; case self::SEVERITY_INFO: // Just log it Log::info($message, $data); break; } } } Performance Impact Considerations Monitoring shouldn\u0026rsquo;t slow down your application:\n// Async logging to reduce performance impact class AsyncMonitoring { public static function logAsync($level, $message, $data = []) { // Queue the logging operation dispatch(new LogMetricsJob($level, $message, $data))-\u0026gt;onQueue(\u0026#39;monitoring\u0026#39;); } } class LogMetricsJob implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; protected $level; protected $message; protected $data; public function __construct($level, $message, $data) { $this-\u0026gt;level = $level; $this-\u0026gt;message = $message; $this-\u0026gt;data = $data; } public function handle() { Log::log($this-\u0026gt;level, $this-\u0026gt;message, $this-\u0026gt;data); // Send to external monitoring services if (app()-\u0026gt;bound(\u0026#39;sentry\u0026#39;)) { app(\u0026#39;sentry\u0026#39;)-\u0026gt;addBreadcrumb([ \u0026#39;message\u0026#39; =\u0026gt; $this-\u0026gt;message, \u0026#39;data\u0026#39; =\u0026gt; $this-\u0026gt;data, \u0026#39;level\u0026#39; =\u0026gt; $this-\u0026gt;level, ]); } } } Conclusion Production monitoring isn\u0026rsquo;t just a nice-to-have feature - it\u0026rsquo;s essential for running reliable Laravel applications. The difference between a well-monitored app and one running blind is the difference between proactive problem-solving and reactive firefighting.\nStart with the basics: error tracking, performance monitoring, and health checks. As your application grows and your team becomes more comfortable with monitoring, you can add more sophisticated alerting, custom business metrics, and detailed performance analysis.\nRemember that good monitoring serves three main purposes: catching problems before users notice them, providing context when problems do occur, and giving you data to make informed decisions about performance improvements and capacity planning.\nThe tools and techniques covered in this guide will help you build a robust monitoring setup that grows with your application. Whether you\u0026rsquo;re using a simple custom solution or enterprise-grade monitoring services, the key is to start monitoring early and iterate based on what you learn about your application\u0026rsquo;s behavior in production.\nDon\u0026rsquo;t wait for problems to force you into implementing monitoring. Set it up now, tune it based on your real-world usage patterns, and sleep better knowing you\u0026rsquo;ll hear about issues before your users do.\n","href":"/2025/09/laravel-production-monitoring-error-tracking.html","title":"Error Tracking Tools and Techniques"},{"content":"If you\u0026rsquo;re tired of waiting for your Laravel app to respond and want to see some serious speed improvements, Laravel Octane might be exactly what you\u0026rsquo;re looking for. Think of it as giving your application a turbo boost - we\u0026rsquo;re talking about performance gains that can make your app 3x to 10x faster in many scenarios.\nLaravel Octane takes your regular Laravel application and runs it on high-performance application servers like Swoole or RoadRunner. Instead of booting up your entire application for every single request (which is what traditional PHP does), Octane keeps your app loaded in memory and reuses it for multiple requests. The result? Lightning-fast response times that will make your users happy.\nWhat Makes Laravel Octane So Fast? Traditional PHP applications follow a simple but inefficient pattern: for every request, the server boots up PHP, loads your entire application, processes the request, sends a response, and then throws everything away. It\u0026rsquo;s like starting your car from scratch every time you want to drive somewhere.\nOctane changes this game completely. It boots your Laravel application once and keeps it running in memory. When requests come in, they\u0026rsquo;re handled by the already-loaded application instance. No more constant bootstrapping, no more loading the same files over and over again.\nHere\u0026rsquo;s what happens under the hood:\nYour application starts once and stays in memory Database connections are pooled and reused Compiled views stay cached between requests Service container bindings remain intact Framework overhead is dramatically reduced The performance improvements are often dramatic. While traditional Laravel apps might handle 50-100 requests per second, Octane-powered applications can easily handle 500-2000 requests per second on the same hardware.\nInstalling Laravel Octane Getting started with Octane is surprisingly straightforward. You\u0026rsquo;ll need Laravel 8 or higher, and you can choose between two application servers: Swoole (PHP extension) or RoadRunner (Go-based server).\nLet\u0026rsquo;s start with the basic installation:\ncomposer require laravel/octane After installing the package, publish the configuration:\nphp artisan octane:install This command will ask you to choose between Swoole and RoadRunner. For most developers, Swoole is the easier choice since it\u0026rsquo;s a PHP extension, while RoadRunner requires downloading a separate binary but offers some unique features.\nInstalling with Swoole If you choose Swoole, you\u0026rsquo;ll need to install the PHP extension:\n# On Ubuntu/Debian sudo pecl install swoole # Using Docker (recommended for development) docker run --rm -v $(pwd):/var/www/html -w /var/www/html laravelsail/php81-composer:latest composer require laravel/octane Installing with RoadRunner For RoadRunner, the installation process downloads the binary for you:\nphp artisan octane:install --server=roadrunner This will download the RoadRunner binary and set up the necessary configuration files.\nBasic Configuration and Setup Once installed, you\u0026rsquo;ll find a new configuration file at config/octane.php. This file controls how Octane behaves, and understanding its options is crucial for getting the best performance.\nThe most important settings include:\nreturn [ \u0026#39;server\u0026#39; =\u0026gt; env(\u0026#39;OCTANE_SERVER\u0026#39;, \u0026#39;swoole\u0026#39;), \u0026#39;https\u0026#39; =\u0026gt; env(\u0026#39;OCTANE_HTTPS\u0026#39;, false), \u0026#39;listeners\u0026#39; =\u0026gt; [ WorkerStarting::class =\u0026gt; [ EnsureUploadedFilesAreValid::class, EnsureUploadedFilesCanBeMoved::class, ], RequestReceived::class =\u0026gt; [ ...Octane::prepareApplicationForNextOperation(), ...Octane::prepareApplicationForNextRequest(), ], RequestHandled::class =\u0026gt; [ FlushTemporaryState::class, ], ], ]; These listeners are crucial because they handle the cleanup between requests. Since your application stays in memory, you need to make sure that state from one request doesn\u0026rsquo;t leak into the next one.\nRunning Your First Octane Server Starting your Octane server is as simple as running:\nphp artisan octane:start By default, this starts the server on http://localhost:8000. You can customize the host and port:\nphp artisan octane:start --host=0.0.0.0 --port=9000 For production use, you\u0026rsquo;ll want to specify the number of workers:\nphp artisan octane:start --workers=4 --task-workers=6 The number of workers should generally match your CPU cores, while task workers handle background tasks and can be set higher.\nMemory Management and State Isolation Here\u0026rsquo;s where things get interesting - and where you need to be careful. Since your application stays in memory, you need to think about memory leaks and state isolation between requests.\nAvoiding Memory Leaks Octane automatically handles most cleanup, but you should be aware of common pitfalls:\n// Bad - this will accumulate data between requests class UserController extends Controller { protected static $cache = []; public function show(User $user) { self::$cache[] = $user; // This grows forever! return view(\u0026#39;user.show\u0026#39;, compact(\u0026#39;user\u0026#39;)); } } // Good - use proper caching mechanisms class UserController extends Controller { public function show(User $user) { $userData = Cache::remember(\u0026#34;user.{$user-\u0026gt;id}\u0026#34;, 3600, function () use ($user) { return $user-\u0026gt;toArray(); }); return view(\u0026#39;user.show\u0026#39;, compact(\u0026#39;userData\u0026#39;)); } } Managing Shared State Be extra careful with static variables and singletons:\n// Problematic - state persists between requests class OrderService { protected static $currentOrder; public function processOrder($orderData) { self::$currentOrder = $orderData; // Dangerous! // Process order... } } // Better approach class OrderService { public function processOrder($orderData) { // Use request-specific data, not static properties $order = new Order($orderData); // Process order... return $order; } } For proper memory management and performance optimization strategies, check out our detailed guide on Laravel performance optimization techniques .\nAdvanced Configuration Options Octane offers several advanced configuration options that can significantly impact performance.\nWorker Configuration The number of workers is crucial for performance. Too few workers and you\u0026rsquo;ll bottleneck on CPU. Too many and you\u0026rsquo;ll run out of memory:\n// config/octane.php \u0026#39;swoole\u0026#39; =\u0026gt; [ \u0026#39;options\u0026#39; =\u0026gt; [ \u0026#39;worker_num\u0026#39; =\u0026gt; env(\u0026#39;OCTANE_WORKERS\u0026#39;, 4), \u0026#39;task_worker_num\u0026#39; =\u0026gt; env(\u0026#39;OCTANE_TASK_WORKERS\u0026#39;, 6), \u0026#39;max_request\u0026#39; =\u0026gt; env(\u0026#39;OCTANE_MAX_REQUESTS\u0026#39;, 1000), \u0026#39;package_max_length\u0026#39; =\u0026gt; 10 * 1024 * 1024, // 10MB ], ], The max_request setting is particularly important. It determines how many requests a worker handles before being recycled. This helps prevent memory leaks from accumulating over time.\nDatabase Connection Pooling One of Octane\u0026rsquo;s biggest advantages is connection pooling. Instead of creating new database connections for each request, Octane maintains a pool of reusable connections:\n\u0026#39;swoole\u0026#39; =\u0026gt; [ \u0026#39;options\u0026#39; =\u0026gt; [ \u0026#39;db_pool\u0026#39; =\u0026gt; [ \u0026#39;min_connections\u0026#39; =\u0026gt; 1, \u0026#39;max_connections\u0026#39; =\u0026gt; 10, \u0026#39;connect_timeout\u0026#39; =\u0026gt; 10.0, \u0026#39;wait_timeout\u0026#39; =\u0026gt; 3.0, ], ], ], This dramatically reduces the overhead of establishing database connections, which can be one of the biggest performance bottlenecks in traditional PHP applications.\nProduction Deployment Strategies Running Octane in production requires some additional considerations compared to traditional Laravel deployments.\nProcess Management In production, you\u0026rsquo;ll want to use a process manager like Supervisor to ensure your Octane workers stay running:\n[program:octane] process_name=%(program_name)s_%(process_num)02d command=php /path/to/your/app/artisan octane:start --server=swoole --host=0.0.0.0 --port=8000 --workers=4 directory=/path/to/your/app user=www-data autostart=true autorestart=true redirect_stderr=true stdout_logfile=/var/log/octane.log Load Balancing For high-traffic applications, you can run multiple Octane instances behind a load balancer:\nupstream octane { server 127.0.0.1:8000; server 127.0.0.1:8001; server 127.0.0.1:8002; server 127.0.0.1:8003; } server { listen 80; server_name your-domain.com; location / { proxy_pass http://octane; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } Deployment and Hot Reloading When deploying updates, you\u0026rsquo;ll need to restart your Octane workers to pick up the changes:\nphp artisan octane:reload For zero-downtime deployments, you can use a blue-green deployment strategy or gradually restart workers.\nPerformance Testing and Monitoring Before and after implementing Octane, you should measure the performance impact. Here\u0026rsquo;s how to do proper performance testing:\nBenchmarking Tools Use tools like Apache Bench or wrk to measure performance:\n# Test traditional Laravel ab -n 1000 -c 10 http://your-app.com/ # Test with Octane ab -n 1000 -c 10 http://your-app.com:8000/ Application Performance Monitoring Implement monitoring to track key metrics:\n// Add to a middleware or service provider use Illuminate\\Support\\Facades\\Log; class PerformanceMonitoring { public function handle($request, Closure $next) { $start = microtime(true); $response = $next($request); $duration = microtime(true) - $start; if ($duration \u0026gt; 0.5) { // Log slow requests Log::warning(\u0026#39;Slow request detected\u0026#39;, [ \u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;url(), \u0026#39;duration\u0026#39; =\u0026gt; $duration, \u0026#39;memory\u0026#39; =\u0026gt; memory_get_peak_usage(true), ]); } return $response; } } For comprehensive monitoring strategies, explore our guide on Laravel production monitoring and error tracking .\nCommon Pitfalls and How to Avoid Them Octane introduces some new challenges that traditional Laravel developers might not be familiar with.\nSession and Cache Gotchas Since workers are persistent, be careful with session and cache usage:\n// Problematic - sessions might not work as expected class HomeController extends Controller { public function index() { session([\u0026#39;last_visit\u0026#39; =\u0026gt; now()]); // In Octane, this might not persist as expected } } // Better - be explicit about session handling class HomeController extends Controller { public function index() { session()-\u0026gt;put(\u0026#39;last_visit\u0026#39;, now()); session()-\u0026gt;save(); // Explicitly save return view(\u0026#39;home\u0026#39;); } } File Upload Handling File uploads need special attention in Octane:\npublic function uploadFile(Request $request) { $file = $request-\u0026gt;file(\u0026#39;upload\u0026#39;); // Make sure to move uploaded files properly $path = $file-\u0026gt;store(\u0026#39;uploads\u0026#39;); // Clean up temporary files if needed if (file_exists($file-\u0026gt;getPathname())) { unlink($file-\u0026gt;getPathname()); } return response()-\u0026gt;json([\u0026#39;path\u0026#39; =\u0026gt; $path]); } Database Connection Issues While connection pooling is great, be aware of potential issues:\n// Watch out for long-running queries that might timeout DB::statement(\u0026#39;SET SESSION wait_timeout = 300\u0026#39;); // Always use transactions properly DB::transaction(function () { // Your database operations }); Real-World Performance Examples Let\u0026rsquo;s look at some real performance improvements you might see with Octane:\nAPI Endpoints A typical API endpoint that fetches user data:\n// Before Octane: ~50ms response time // After Octane: ~5ms response time class UserApiController extends Controller { public function show(User $user) { return response()-\u0026gt;json([ \u0026#39;user\u0026#39; =\u0026gt; $user-\u0026gt;load([\u0026#39;posts\u0026#39;, \u0026#39;profile\u0026#39;]), \u0026#39;stats\u0026#39; =\u0026gt; $user-\u0026gt;calculateStats(), ]); } } Database-Heavy Operations Operations involving multiple database queries see dramatic improvements:\n// Before Octane: ~200ms for 100 records // After Octane: ~20ms for 100 records public function dashboard() { $recentPosts = Post::with(\u0026#39;author\u0026#39;)-\u0026gt;latest()-\u0026gt;take(10)-\u0026gt;get(); $userStats = User::selectRaw(\u0026#39;count(*) as total, avg(age) as avg_age\u0026#39;)-\u0026gt;first(); $topCategories = Category::withCount(\u0026#39;posts\u0026#39;)-\u0026gt;orderBy(\u0026#39;posts_count\u0026#39;, \u0026#39;desc\u0026#39;)-\u0026gt;take(5)-\u0026gt;get(); return view(\u0026#39;dashboard\u0026#39;, compact(\u0026#39;recentPosts\u0026#39;, \u0026#39;userStats\u0026#39;, \u0026#39;topCategories\u0026#39;)); } To further optimize database operations, make sure you\u0026rsquo;re avoiding N+1 query problems which can still impact performance even with Octane.\nSecurity Considerations Running long-lived processes introduces some security considerations that don\u0026rsquo;t exist in traditional PHP applications.\nMemory Exposure Since processes are long-lived, sensitive data might stay in memory longer:\n// Bad - sensitive data might persist class AuthController extends Controller { protected static $credentials = []; public function login(Request $request) { self::$credentials = $request-\u0026gt;only([\u0026#39;email\u0026#39;, \u0026#39;password\u0026#39;]); // This data stays in memory! } } // Good - don\u0026#39;t store sensitive data in static properties class AuthController extends Controller { public function login(Request $request) { $credentials = $request-\u0026gt;only([\u0026#39;email\u0026#39;, \u0026#39;password\u0026#39;]); if (Auth::attempt($credentials)) { // Clear sensitive data immediately $credentials = null; return redirect()-\u0026gt;intended(); } return back()-\u0026gt;withErrors([\u0026#39;email\u0026#39; =\u0026gt; \u0026#39;Invalid credentials\u0026#39;]); } } Process Isolation Make sure your application handles process isolation properly, especially if you\u0026rsquo;re processing user-uploaded content or executing dynamic code.\nFor comprehensive security practices when running high-performance Laravel applications, review our Laravel security best practices guide .\nWhen NOT to Use Octane While Octane offers impressive performance improvements, it\u0026rsquo;s not always the right choice:\nApplications with Heavy File I/O If your application does a lot of file processing or manipulation, the benefits might be limited:\n// This type of operation won\u0026#39;t see much improvement with Octane public function processLargeFile(Request $request) { $file = $request-\u0026gt;file(\u0026#39;data\u0026#39;); $data = file_get_contents($file-\u0026gt;path()); // Heavy processing... $processed = $this-\u0026gt;processData($data); return response()-\u0026gt;download($this-\u0026gt;generateReport($processed)); } Applications with Lots of External API Calls If your app spends most of its time waiting for external APIs, Octane won\u0026rsquo;t help much:\n// Octane can\u0026#39;t speed up external API calls public function fetchUserData($userId) { $userData = Http::get(\u0026#34;https://api.example.com/users/{$userId}\u0026#34;); $profileData = Http::get(\u0026#34;https://api.example.com/profiles/{$userId}\u0026#34;); $settingsData = Http::get(\u0026#34;https://api.example.com/settings/{$userId}\u0026#34;); return view(\u0026#39;user\u0026#39;, compact(\u0026#39;userData\u0026#39;, \u0026#39;profileData\u0026#39;, \u0026#39;settingsData\u0026#39;)); } Development Environments For local development, traditional Laravel is often more convenient because you don\u0026rsquo;t need to restart the server every time you make code changes.\nConclusion Laravel Octane represents a significant leap forward in PHP application performance. By keeping your application loaded in memory and reusing resources across requests, it can deliver performance improvements that were previously only possible with compiled languages or complex caching strategies.\nThe key to success with Octane is understanding how it changes the application lifecycle and adapting your coding practices accordingly. Pay attention to memory management, state isolation, and proper cleanup between requests.\nWhile there\u0026rsquo;s a learning curve and some additional complexity, the performance benefits are often worth it, especially for high-traffic applications or APIs. Start with a simple setup, measure your performance improvements, and gradually optimize based on your specific needs.\nRemember that Octane is just one part of a comprehensive performance strategy. Combine it with proper database optimization, caching strategies, and code optimization for the best results. The combination of these techniques can transform a slow Laravel application into a high-performance powerhouse that can handle thousands of requests per second.\n","href":"/2025/09/laravel-octane-boost-performance-tutorial.html","title":"Boost Performance with High-Speed Application Server"},{"content":"If you\u0026rsquo;ve ever wondered why your Laravel app suddenly becomes sluggish when displaying lists of data, you might be dealing with the dreaded N+1 query problem. It\u0026rsquo;s one of those sneaky performance killers that can turn a fast application into a slow, resource-hungry monster. Don\u0026rsquo;t worry though - once you understand what\u0026rsquo;s happening and how to fix it, you\u0026rsquo;ll never fall into this trap again.\nWhat is the N+1 Query Problem? Here\u0026rsquo;s what happens: your app makes one query to get a list of records, then fires off a separate query for each record to grab related data. Picture this - you want to show 100 blog posts with their authors\u0026rsquo; names. Instead of being smart about it, your app runs one query to get the posts, then 100 more queries to fetch each author. That\u0026rsquo;s 101 database hits when you could\u0026rsquo;ve done it with just 2!\nAs you can imagine, this creates a snowball effect. More records mean exponentially more queries, which translates to slower pages and angry users. Your server starts sweating, your database gets overwhelmed, and your app crawls to a halt. For more ways to speed up your Laravel app, check out our Laravel performance optimization techniques .\nIdentifying N+1 Queries in Laravel The good news? Laravel gives you some handy tools to catch these pesky queries before they become a problem.\nUsing Laravel Debugbar Laravel Debugbar is a lifesaver for catching query issues during development. Just install it with Composer:\ncomposer require barryvdh/laravel-debugbar --dev After installation, you\u0026rsquo;ll see a debug bar at the bottom of your browser that shows exactly how many queries each page runs. If that number looks suspiciously high, you\u0026rsquo;ve probably got an N+1 situation on your hands.\nUsing Query Logging Another approach is to turn on Laravel\u0026rsquo;s query logging to see exactly what\u0026rsquo;s happening under the hood:\nuse Illuminate\\Support\\Facades\\DB; // Enable query logging DB::enableQueryLog(); // Your code here $posts = Post::all(); foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; } // Get all executed queries $queries = DB::getQueryLog(); dd($queries); Using Laravel Telescope If you want to get really detailed about monitoring, Laravel Telescope gives you deep insights into what your app is doing, including all those database queries.\nCommon N+1 Query Scenarios Let\u0026rsquo;s look at some real-world examples where N+1 queries love to hide.\nBasic Relationship Access Here\u0026rsquo;s a classic example - showing blog posts with their authors:\n// This creates an N+1 query problem $posts = Post::all(); // 1 query foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; // N additional queries } With 50 posts, this innocent-looking code will hit your database 51 times. Ouch!\nNested Relationships Things get even uglier with nested relationships:\n$posts = Post::all(); // 1 query foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; // N queries for authors foreach ($post-\u0026gt;comments as $comment) { echo $comment-\u0026gt;user-\u0026gt;name; // N*M queries for comment users } } This kind of code can easily generate hundreds or even thousands of queries. Your database won\u0026rsquo;t be happy.\nSolving N+1 Queries with Eager Loading The magic solution to this mess? Eager loading with Laravel\u0026rsquo;s with() method.\nBasic Eager Loading // Solution: Use eager loading $posts = Post::with(\u0026#39;author\u0026#39;)-\u0026gt;get(); // 2 queries total foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; // No additional queries } Beautiful! This runs just two queries no matter how many posts you have.\nMultiple Relationships Need multiple relationships? No problem - load them all at once:\n$posts = Post::with([\u0026#39;author\u0026#39;, \u0026#39;category\u0026#39;, \u0026#39;tags\u0026#39;])-\u0026gt;get(); foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; echo $post-\u0026gt;category-\u0026gt;name; foreach ($post-\u0026gt;tags as $tag) { echo $tag-\u0026gt;name; } } Nested Eager Loading Got nested relationships? Dot notation is your friend:\n$posts = Post::with([ \u0026#39;author\u0026#39;, \u0026#39;comments.user\u0026#39;, \u0026#39;category\u0026#39; ])-\u0026gt;get(); foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; foreach ($post-\u0026gt;comments as $comment) { echo $comment-\u0026gt;user-\u0026gt;name; // No N+1 queries } } Advanced Eager Loading Techniques Conditional Eager Loading Sometimes you only want to load relationships when certain conditions are met:\n$posts = Post::with([ \u0026#39;author\u0026#39;, \u0026#39;comments\u0026#39; =\u0026gt; function ($query) { $query-\u0026gt;where(\u0026#39;approved\u0026#39;, true); } ])-\u0026gt;get(); Eager Loading Specific Columns Want to squeeze out even more performance? Only load the columns you actually need:\n$posts = Post::with([ \u0026#39;author:id,name,email\u0026#39;, \u0026#39;category:id,name\u0026#39; ])-\u0026gt;get(); Lazy Eager Loading Ever realize you need a relationship after you\u0026rsquo;ve already run your query? No worries:\n$posts = Post::all(); // Later in your code, you realize you need authors $posts-\u0026gt;load(\u0026#39;author\u0026#39;); foreach ($posts as $post) { echo $post-\u0026gt;author-\u0026gt;name; // No N+1 queries } Using Global Scopes for Automatic Eager Loading If you always need certain relationships, just set them to load automatically:\nclass Post extends Model { protected $with = [\u0026#39;author\u0026#39;, \u0026#39;category\u0026#39;]; // Your model code } Now every time you query posts, Laravel will automatically grab the author and category data too.\nPreventing N+1 Queries in Production Using strictLoading in Development Laravel 8.4 added a cool strictLoading feature that yells at you when you accidentally trigger lazy loading during development:\n// In AppServiceProvider boot method public function boot() { Model::preventLazyLoading(! app()-\u0026gt;isProduction()); } This will throw an exception whenever lazy loading happens in development, helping you catch N+1 problems early.\nDatabase Query Monitoring Here\u0026rsquo;s a simple middleware to keep an eye on query counts in production:\nclass QueryCountMiddleware { public function handle($request, Closure $next) { DB::enableQueryLog(); $response = $next($request); $queryCount = count(DB::getQueryLog()); if ($queryCount \u0026gt; 50) { // Set your threshold Log::warning(\u0026#34;High query count: {$queryCount} queries on \u0026#34; . $request-\u0026gt;url()); } return $response; } } Alternative Solutions Using Database Views For really complex data needs, sometimes a database view is the way to go:\nCREATE VIEW post_with_author AS SELECT posts.*, users.name as author_name, users.email as author_email FROM posts JOIN users ON posts.user_id = users.id; Caching Strategies Don\u0026rsquo;t forget about caching for data that doesn\u0026rsquo;t change often:\n$posts = Cache::remember(\u0026#39;posts_with_authors\u0026#39;, 3600, function () { return Post::with(\u0026#39;author\u0026#39;)-\u0026gt;get(); }); Check out our guide on Laravel production monitoring and error tracking for more caching strategies.\nUsing Raw Queries Sometimes a good old-fashioned raw query is exactly what you need:\n$postsWithAuthors = DB::select(\u0026#39; SELECT posts.*, users.name as author_name FROM posts JOIN users ON posts.user_id = users.id \u0026#39;); Performance Impact and Metrics Let\u0026rsquo;s talk numbers. Here\u0026rsquo;s what you might see with a typical N+1 scenario:\nWithout eager loading: 100ms for 100 posts (101 queries) With eager loading: 15ms for 100 posts (2 queries) That\u0026rsquo;s an 85% speed boost! And it gets even better as your data grows.\nBest Practices for Avoiding N+1 Queries Always use eager loading when you know you\u0026rsquo;ll need relationship data Monitor query counts during development and testing Use Laravel Debugbar or similar tools in development Implement query logging for production monitoring Consider using strictLoading in development environments Profile your application regularly to catch performance regressions Use database indexing appropriately for foreign keys Consider pagination for large datasets For additional security considerations when optimizing database queries, review our Laravel security best practices guide .\nTesting for N+1 Queries Here\u0026rsquo;s how to write tests that catch N+1 queries before they hit production:\npublic function test_posts_index_does_not_have_n_plus_one_queries() { $posts = Post::factory()-\u0026gt;count(10)-\u0026gt;create(); DB::enableQueryLog(); $response = $this-\u0026gt;get(\u0026#39;/posts\u0026#39;); // Assert maximum number of queries $this-\u0026gt;assertCount(2, DB::getQueryLog()); $response-\u0026gt;assertOk(); } Conclusion N+1 queries are a real pain, but they\u0026rsquo;re totally avoidable once you know what to look for. By using eager loading, keeping an eye on your query counts, and following the tips we\u0026rsquo;ve covered, you can keep your Laravel apps running fast and smooth.\nThe key isn\u0026rsquo;t just remembering to use with() - it\u0026rsquo;s developing an instinct for thinking about database efficiency. Always consider how your Eloquent code translates to actual SQL queries. Make monitoring and testing part of your routine, and your future self (and your users) will thank you.\nTrust me, the time you spend learning about N+1 queries now will save you countless headaches later. Your apps will be faster, your users will be happier, and your server bills will be lower. It\u0026rsquo;s a win-win-win situation.\nWant to take performance even further? Check out Laravel Octane for some serious speed improvements in production.\n","href":"/2025/09/laravel-n-plus-one-query-problem-solution.html","title":"Laravel N+1 Query Problem Solution Essential Database Optimization Guide"},{"content":"As your Laravel application grows, you might find yourself hitting the limitations of a monolithic architecture. Database bottlenecks, deployment challenges, and team coordination issues become increasingly common. The solution? Transitioning to a microservices architecture that breaks your monolith into smaller, manageable, and independently deployable services.\nThis comprehensive guide will walk you through the entire process of decomposing your Laravel monolith into microservices, from initial planning to practical implementation strategies.\nUnderstanding Monolith vs Microservices A monolithic application packages all functionality into a single deployable unit. While this approach works well for small to medium applications, it presents several challenges as your application scales:\nMonolith Limitations:\nSingle point of failure affects the entire application Difficult to scale individual components independently Technology stack limitations across the entire application Complex deployment processes for small changes Team coordination challenges in large development teams Microservices Benefits:\nIndependent deployment and scaling of services Technology diversity across different services Improved fault isolation and resilience Better team autonomy and faster development cycles Easier maintenance and testing of smaller codebases However, microservices also introduce complexity in terms of service communication, data consistency, and operational overhead. The key is knowing when and how to make the transition effectively.\nWhen to Consider Breaking Your Monolith Before diving into microservices, evaluate whether your application truly needs this architectural shift. Consider microservices when you experience:\nPerformance Issues:\nDatabase queries becoming increasingly complex and slow Specific modules requiring different scaling strategies Resource contention between different application features Development Challenges:\nMultiple teams working on the same codebase causing conflicts Deployment bottlenecks due to application size Difficulty implementing different technologies for specific features Business Requirements:\nNeed for independent feature releases Compliance requirements for data isolation Different availability requirements for various services Planning Your Microservices Architecture Successful decomposition requires careful planning and a clear understanding of your application\u0026rsquo;s domain boundaries.\nDomain-Driven Design Approach Start by identifying bounded contexts within your application. These represent distinct business capabilities that can operate independently:\n// Example: E-commerce bounded contexts - User Management (authentication, profiles, permissions) - Product Catalog (inventory, categories, search) - Order Management (cart, checkout, fulfillment) - Payment Processing (transactions, refunds, billing) - Notification Service (emails, SMS, push notifications) Data Decomposition Strategy One of the most challenging aspects of breaking a monolith is handling shared data. Each microservice should own its data completely:\nDatabase-per-Service Pattern:\n// Before: Single database with all tables users, products, orders, payments, notifications // After: Separate databases per service UserService: users, user_profiles, user_preferences ProductService: products, categories, inventory OrderService: orders, order_items, shipping PaymentService: transactions, payment_methods NotificationService: notifications, templates API Contract Design Define clear API contracts between services before implementation. This allows teams to work independently while ensuring compatibility.\n// User Service API Contract GET /api/users/{id} POST /api/users PUT /api/users/{id} DELETE /api/users/{id} // Product Service API Contract GET /api/products GET /api/products/{id} POST /api/products PUT /api/products/{id} Implementation Strategies The Strangler Fig Pattern Instead of a complete rewrite, gradually replace monolith functionality with microservices:\nStep 1: Identify the First Service Choose a bounded context with minimal dependencies. User authentication is often a good starting point.\nStep 2: Create the Service\n// routes/api.php in new Auth Service Route::middleware(\u0026#39;api\u0026#39;)-\u0026gt;group(function () { Route::post(\u0026#39;/login\u0026#39;, [AuthController::class, \u0026#39;login\u0026#39;]); Route::post(\u0026#39;/register\u0026#39;, [AuthController::class, \u0026#39;register\u0026#39;]); Route::middleware(\u0026#39;auth:sanctum\u0026#39;)-\u0026gt;group(function () { Route::get(\u0026#39;/user\u0026#39;, [AuthController::class, \u0026#39;user\u0026#39;]); Route::post(\u0026#39;/logout\u0026#39;, [AuthController::class, \u0026#39;logout\u0026#39;]); }); }); Step 3: Route Traffic Gradually Use a proxy or API gateway to route specific requests to the new service while maintaining existing functionality.\nDatabase Decomposition Separate shared data carefully to maintain data integrity:\n// Migration strategy for user data // 1. Create new user service database // 2. Set up data synchronization // 3. Update monolith to read from service API // 4. Remove user tables from monolith database // User Service Model class User extends Model { protected $fillable = [\u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;password\u0026#39;]; public function profile() { return $this-\u0026gt;hasOne(UserProfile::class); } } // Monolith integration class UserServiceClient { public function getUser(int $userId): array { $response = Http::get(\u0026#34;http://user-service/api/users/{$userId}\u0026#34;); return $response-\u0026gt;json(); } public function createUser(array $userData): array { $response = Http::post(\u0026#39;http://user-service/api/users\u0026#39;, $userData); return $response-\u0026gt;json(); } } Communication Patterns Synchronous Communication Use HTTP APIs for real-time data retrieval and immediate consistency requirements:\n// Product Service calling User Service class ProductController extends Controller { public function show(Product $product) { $user = app(UserServiceClient::class)-\u0026gt;getUser(auth()-\u0026gt;id()); return response()-\u0026gt;json([ \u0026#39;product\u0026#39; =\u0026gt; $product, \u0026#39;user_preferences\u0026#39; =\u0026gt; $user[\u0026#39;preferences\u0026#39;] ?? [] ]); } } Asynchronous Communication Implement event-driven architecture for loose coupling and better performance:\n// Event publishing in Order Service class OrderCreated extends Event { public $order; public function __construct(Order $order) { $this-\u0026gt;order = $order; } } // Event listener in Notification Service class SendOrderConfirmation { public function handle(OrderCreated $event) { Mail::to($event-\u0026gt;order-\u0026gt;customer_email) -\u0026gt;send(new OrderConfirmationMail($event-\u0026gt;order)); } } // Event listener in Inventory Service class UpdateInventory { public function handle(OrderCreated $event) { foreach ($event-\u0026gt;order-\u0026gt;items as $item) { $this-\u0026gt;inventoryService-\u0026gt;decreaseStock( $item-\u0026gt;product_id, $item-\u0026gt;quantity ); } } } Handling Cross-Cutting Concerns Authentication and Authorization Implement centralized authentication with distributed authorization:\n// JWT token validation middleware class ValidateJWTToken { public function handle($request, Closure $next) { $token = $request-\u0026gt;bearerToken(); if (!$token || !$this-\u0026gt;validateToken($token)) { return response()-\u0026gt;json([\u0026#39;error\u0026#39; =\u0026gt; \u0026#39;Unauthorized\u0026#39;], 401); } $request-\u0026gt;merge([\u0026#39;user\u0026#39; =\u0026gt; $this-\u0026gt;getUserFromToken($token)]); return $next($request); } private function validateToken(string $token): bool { // Validate JWT token against auth service $response = Http::get(\u0026#39;http://auth-service/api/validate\u0026#39;, [ \u0026#39;token\u0026#39; =\u0026gt; $token ]); return $response-\u0026gt;successful(); } } Logging and Monitoring Implement distributed tracing for better observability:\n// Correlation ID middleware class CorrelationIdMiddleware { public function handle($request, Closure $next) { $correlationId = $request-\u0026gt;header(\u0026#39;X-Correlation-ID\u0026#39;) ?? Str::uuid(); Log::withContext([\u0026#39;correlation_id\u0026#39; =\u0026gt; $correlationId]); $response = $next($request); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Correlation-ID\u0026#39;, $correlationId); return $response; } } Data Consistency and Transactions Eventual Consistency Accept that data will be eventually consistent across services:\n// Saga pattern for distributed transactions class OrderSaga { public function handle(CreateOrderCommand $command) { try { // Step 1: Reserve inventory $this-\u0026gt;inventoryService-\u0026gt;reserveItems($command-\u0026gt;items); // Step 2: Process payment $payment = $this-\u0026gt;paymentService-\u0026gt;charge($command-\u0026gt;paymentDetails); // Step 3: Create order $order = $this-\u0026gt;orderService-\u0026gt;create($command-\u0026gt;orderData); // Step 4: Send confirmation event(new OrderCreated($order)); } catch (Exception $e) { // Compensating actions $this-\u0026gt;rollbackSaga($command); throw $e; } } private function rollbackSaga(CreateOrderCommand $command) { $this-\u0026gt;inventoryService-\u0026gt;releaseReservation($command-\u0026gt;items); // Additional rollback actions... } } Performance Optimization Microservices can introduce latency due to network calls. Implement strategies to minimize performance impact:\nCaching Strategies // Service-level caching class UserServiceClient { public function getUser(int $userId): array { return Cache::remember(\u0026#34;user.{$userId}\u0026#34;, 3600, function () use ($userId) { $response = Http::get(\u0026#34;http://user-service/api/users/{$userId}\u0026#34;); return $response-\u0026gt;json(); }); } } // Database query optimization class ProductService { public function getProductsWithCategories(): Collection { return Cache::tags([\u0026#39;products\u0026#39;, \u0026#39;categories\u0026#39;]) -\u0026gt;remember(\u0026#39;products.with.categories\u0026#39;, 1800, function () { return Product::with(\u0026#39;category\u0026#39;)-\u0026gt;get(); }); } } Connection Pooling Configure connection pooling to reduce HTTP overhead:\n// config/http.php return [ \u0026#39;timeout\u0026#39; =\u0026gt; 30, \u0026#39;pool\u0026#39; =\u0026gt; [ \u0026#39;connections\u0026#39; =\u0026gt; 10, \u0026#39;max_requests\u0026#39; =\u0026gt; 100, ], ]; Testing Microservices Testing becomes more complex with distributed systems. Implement comprehensive testing strategies:\nContract Testing // User service contract test class UserServiceContractTest extends TestCase { public function test_get_user_returns_expected_structure() { $response = $this-\u0026gt;getJson(\u0026#39;/api/users/1\u0026#39;); $response-\u0026gt;assertStatus(200) -\u0026gt;assertJsonStructure([ \u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;, \u0026#39;created_at\u0026#39;, \u0026#39;profile\u0026#39; =\u0026gt; [ \u0026#39;avatar\u0026#39;, \u0026#39;bio\u0026#39; ] ]); } } Integration Testing // Service integration test class OrderCreationIntegrationTest extends TestCase { public function test_order_creation_updates_inventory() { // Arrange $product = Product::factory()-\u0026gt;create([\u0026#39;stock\u0026#39; =\u0026gt; 10]); $orderData = [\u0026#39;product_id\u0026#39; =\u0026gt; $product-\u0026gt;id, \u0026#39;quantity\u0026#39; =\u0026gt; 2]; // Act $response = $this-\u0026gt;postJson(\u0026#39;/api/orders\u0026#39;, $orderData); // Assert $response-\u0026gt;assertStatus(201); $this-\u0026gt;assertEquals(8, $product-\u0026gt;fresh()-\u0026gt;stock); } } Deployment and DevOps Microservices require robust deployment and monitoring strategies:\nContainerization # Dockerfile for a Laravel microservice FROM php:8.1-fpm WORKDIR /var/www COPY composer.json composer.lock ./ RUN composer install --no-dev --optimize-autoloader COPY . . RUN php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache EXPOSE 9000 CMD [\u0026#34;php-fpm\u0026#34;] Service Discovery Use service discovery for dynamic service location:\n// Service registry integration class ServiceRegistry { public function register(string $serviceName, string $host, int $port): void { Http::post(\u0026#39;http://consul:8500/v1/agent/service/register\u0026#39;, [ \u0026#39;Name\u0026#39; =\u0026gt; $serviceName, \u0026#39;Address\u0026#39; =\u0026gt; $host, \u0026#39;Port\u0026#39; =\u0026gt; $port, \u0026#39;Check\u0026#39; =\u0026gt; [ \u0026#39;HTTP\u0026#39; =\u0026gt; \u0026#34;http://{$host}:{$port}/health\u0026#34;, \u0026#39;Interval\u0026#39; =\u0026gt; \u0026#39;10s\u0026#39; ] ]); } public function discover(string $serviceName): array { $response = Http::get(\u0026#34;http://consul:8500/v1/health/service/{$serviceName}\u0026#34;); return $response-\u0026gt;json(); } } For more advanced deployment strategies, check out our comprehensive guide on Laravel Docker setup for development and production .\nMonitoring and Observability Implement comprehensive monitoring across all services:\n// Health check endpoint class HealthController extends Controller { public function check() { $checks = [ \u0026#39;database\u0026#39; =\u0026gt; $this-\u0026gt;checkDatabase(), \u0026#39;redis\u0026#39; =\u0026gt; $this-\u0026gt;checkRedis(), \u0026#39;external_services\u0026#39; =\u0026gt; $this-\u0026gt;checkExternalServices() ]; $overall = collect($checks)-\u0026gt;every(fn($check) =\u0026gt; $check[\u0026#39;status\u0026#39;] === \u0026#39;ok\u0026#39;); return response()-\u0026gt;json([ \u0026#39;status\u0026#39; =\u0026gt; $overall ? \u0026#39;ok\u0026#39; : \u0026#39;error\u0026#39;, \u0026#39;checks\u0026#39; =\u0026gt; $checks, \u0026#39;timestamp\u0026#39; =\u0026gt; now()-\u0026gt;toISOString() ], $overall ? 200 : 503); } } Common Pitfalls and Solutions Avoiding Distributed Monolith Don\u0026rsquo;t create a distributed monolith where services are too tightly coupled:\nProblem: Services calling each other synchronously for every operation Solution: Use asynchronous messaging and event-driven architecture\nManaging Data Consistency Problem: Maintaining ACID transactions across services Solution: Implement eventual consistency and compensating actions\nService Granularity Problem: Creating too many small services or too few large services Solution: Follow domain boundaries and business capabilities\nFor comprehensive performance optimization techniques and security best practices , make sure to implement proper monitoring and security measures across all your microservices.\nConclusion Breaking a Laravel monolith into microservices is a significant architectural decision that requires careful planning and execution. Start small, focus on domain boundaries, and gradually decompose your application while maintaining system reliability.\nThe key to successful microservices adoption lies in understanding your specific use case, implementing proper communication patterns, and maintaining strong DevOps practices. Remember that microservices are not a silver bullet \u0026ndash; they solve certain problems while introducing others.\nConsider complementing your microservices architecture with proper API authentication using Sanctum and implementing robust error tracking and monitoring to ensure your distributed system operates smoothly in production.\nTake your time with the transition, validate each step, and ensure your team is prepared for the operational complexity that microservices bring. With the right approach, you\u0026rsquo;ll build a scalable, maintainable system that can grow with your business needs.\n","href":"/2025/09/laravel-microservices-breaking-monolith.html","title":"Breaking Monolith into Scalable Services"},{"content":"Inertia.js lets you build a single‑page app on top of Laravel without maintaining a separate API. You keep server‑side routing, controllers, middleware, and validation, while rendering pages with React or Vue. The result feels like an SPA\u0026ndash;fast navigation, preserved state, and partial reloads\u0026ndash;without the overhead of duplicating server logic.\nThis guide walks through installation, page structure, forms, validation, shared data, server‑side rendering (optional), authentication with Sanctum, building assets with Vite, deployment, and fixes for the most common issues.\nWhy Inertia.js No separate API layer: controllers return Inertia responses instead of JSON. Keep Laravel features: policies, validation, flash messages, session auth. Modern client: React/Vue components for pages and layouts, fast navigation. Install and scaffold The fastest path is Laravel Breeze with the Inertia stack. Choose React or Vue.\ncomposer require laravel/breeze --dev php artisan breeze:install react # or: php artisan breeze:install vue npm install npm run dev php artisan migrate If you prefer a manual setup, install the server and client packages:\ncomposer require inertiajs/inertia-laravel npm install @inertiajs/core @inertiajs/react # or @inertiajs/vue3 Basic page flow Routes hit controllers. Controllers return Inertia::render() with a component name and props. The client bootstraps and renders the matching React/Vue component.\n// routes/web.php use Inertia\\Inertia; use App\\Models\\Post; Route::get(\u0026#39;/posts\u0026#39;, function () { return Inertia::render(\u0026#39;Posts/Index\u0026#39;, [ \u0026#39;filters\u0026#39; =\u0026gt; request()-\u0026gt;only(\u0026#39;search\u0026#39;), \u0026#39;posts\u0026#39; =\u0026gt; Post::query() -\u0026gt;when(request(\u0026#39;search\u0026#39;), fn($q,$s)=\u0026gt;$q-\u0026gt;where(\u0026#39;title\u0026#39;,\u0026#39;like\u0026#39;,\u0026#34;%$s%\u0026#34;)) -\u0026gt;latest()-\u0026gt;paginate(10) -\u0026gt;withQueryString(), ]); }); Example React component:\n// resources/js/Pages/Posts/Index.jsx import { Link, useForm } from \u0026#39;@inertiajs/react\u0026#39; export default function Index({ filters, posts }) { const { data, setData, get } = useForm({ search: filters?.search || \u0026#39;\u0026#39; }) return ( \u0026lt;div\u0026gt; \u0026lt;form onSubmit={e=\u0026gt;{e.preventDefault(); get(\u0026#39;/posts\u0026#39;)}}\u0026gt; \u0026lt;input value={data.search} onChange={e=\u0026gt;setData(\u0026#39;search\u0026#39;, e.target.value)} /\u0026gt; \u0026lt;button type=\u0026#34;submit\u0026#34;\u0026gt;Search\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;ul\u0026gt; {posts.data.map(p =\u0026gt; ( \u0026lt;li key={p.id}\u0026gt;\u0026lt;Link href={`/posts/${p.id}`}\u0026gt;{p.title}\u0026lt;/Link\u0026gt;\u0026lt;/li\u0026gt; ))} \u0026lt;/ul\u0026gt; \u0026lt;div\u0026gt; {posts.links.map(l =\u0026gt; ( \u0026lt;Link key={l.label} href={l.url || \u0026#39;#\u0026#39;} dangerouslySetInnerHTML={{__html: l.label}} preserveScroll/\u0026gt; ))} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; ) } Layouts, shared props, and meta Define a main layout once and reuse it. Pass global data (auth user, flash) via middleware.\n// app/Http/Middleware/HandleInertiaRequests.php use Inertia\\Middleware; class HandleInertiaRequests extends Middleware { protected $rootView = \u0026#39;app\u0026#39;; public function share($request) { return array_merge(parent::share($request), [ \u0026#39;auth\u0026#39; =\u0026gt; [ \u0026#39;user\u0026#39; =\u0026gt; fn() =\u0026gt; optional($request-\u0026gt;user())-\u0026gt;only(\u0026#39;id\u0026#39;,\u0026#39;name\u0026#39;,\u0026#39;email\u0026#39;), ], \u0026#39;flash\u0026#39; =\u0026gt; [ \u0026#39;success\u0026#39; =\u0026gt; fn() =\u0026gt; $request-\u0026gt;session()-\u0026gt;get(\u0026#39;success\u0026#39;), ], ]); } } On the client, use a top‑level layout and @inertiajs/react or @inertiajs/vue3 Head component to manage titles and meta tags.\nForms and validation Leverage Laravel validation in controllers and show errors in the page component.\n// app/Http/Controllers/PostController.php public function store(Request $r) { $validated = $r-\u0026gt;validate([ \u0026#39;title\u0026#39; =\u0026gt; [\u0026#39;required\u0026#39;,\u0026#39;max:120\u0026#39;], \u0026#39;body\u0026#39; =\u0026gt; [\u0026#39;required\u0026#39;], ]); Post::create($validated); return back()-\u0026gt;with(\u0026#39;success\u0026#39;, \u0026#39;Post created\u0026#39;); } React page snippet with useForm:\nconst { data, setData, post, processing, errors } = useForm({ title:\u0026#39;\u0026#39;, body:\u0026#39;\u0026#39; }) \u0026lt;form onSubmit={e=\u0026gt;{e.preventDefault(); post(\u0026#39;/posts\u0026#39;)}}\u0026gt; \u0026lt;input value={data.title} onChange={e=\u0026gt;setData(\u0026#39;title\u0026#39;, e.target.value)} /\u0026gt; {errors.title \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;error\u0026#34;\u0026gt;{errors.title}\u0026lt;/div\u0026gt;} \u0026lt;textarea value={data.body} onChange={e=\u0026gt;setData(\u0026#39;body\u0026#39;, e.target.value)} /\u0026gt; {errors.body \u0026amp;\u0026amp; \u0026lt;div className=\u0026#34;error\u0026#34;\u0026gt;{errors.body}\u0026lt;/div\u0026gt;} \u0026lt;button disabled={processing}\u0026gt;Save\u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; Partial reloads and performance Inertia only refreshes data you ask for. Use only to fetch specific props on visits and preserveState/preserveScroll for smooth UX. Split large components and lazy‑load where sensible. For broader tips, see: Laravel Performance Optimization .\nAuthentication with Sanctum Most Inertia apps use session‑based auth. Pair with Laravel Sanctum for cookie authentication. Ensure:\nCorrect cookie flags in config/session.php (domain, secure, same_site). CSRF cookie route /sanctum/csrf-cookie is accessible (for traditional form posts). Login/logout route handlers regenerate/invalidate sessions. If you encounter CSRF or cookie issues behind a proxy or different subdomains, refer to: Fixing Laravel Session and Cache Issues and Laravel Environment Configuration .\nSSR (optional) Server‑side rendering improves first paint and SEO for public pages. Breeze provides an SSR preset. Enable SSR in your Vite setup and run the SSR server process in production. Only render publicly visible routes; most dashboards are fine without SSR.\nAssets with Vite Vite handles builds. Typical commands:\nnpm run dev # HMR during development npm run build # production assets Keep the Vite manifest in sync and ensure your deployment copies built assets.\nFile uploads in Inertia pages Use regular multipart forms or FormData and apply the same validation and storage patterns described in: File Upload Best Practices .\nDeployment and caching Production checklist:\nServe from public/ and verify Nginx try_files points to index.php. See: Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide . Clear and rebuild caches after deploy; reload PHP‑FPM; restart workers if you use queues. See: Laravel Environment Configuration . Ensure ownership and permissions on storage/ and bootstrap/cache/ are correct: Fix Laravel Permission Issues . Troubleshooting “Back/forward shows stale data”: use only on visits and provide keys for lists to avoid stale renders. “Flash not showing”: share flash data in HandleInertiaRequests middleware. “Validation not appearing”: ensure controller returns back with errors (default behavior on validate failure) and page renders errors from Inertia props. \u0026ldquo;Build works locally, fails on server\u0026rdquo;: confirm Node/Vite run in CI and assets are deployed; avoid mixing old manifest files. “Slow initial load”: consider SSR for public pages, enable HTTP caching for static assets, and keep bundle sizes in check. For production diagnostics and clearer logs, see: Advanced Laravel Debugging with Logs . Summary Inertia lets Laravel own routing, validation, and policies while React/Vue own the view layer. Return Inertia::render() from controllers, use layouts and shared props for global state, handle forms with useForm, and improve UX with partial reloads and preserved state. Tie in Sanctum for auth, Vite for builds, and add SSR where it helps. With the deployment and troubleshooting patterns above, you get an SPA experience without running a separate API.\n","href":"/2025/09/laravel-integration-react-vue-inertia.html","title":"Laravel Integration with React Vue Complete Inertia.js Guide for Modern SPA"},{"content":"GraphQL shines when clients need flexible data shapes, fewer round trips, and typed contracts. For dashboards, mobile apps, or complex relationships, it can reduce API sprawl and speed up development. This tutorial uses Lighthouse, a mature GraphQL package for Laravel, and covers everything you need to go from a blank project to a production-ready API.\nWhy GraphQL (and when not to use it) Use GraphQL when clients need to query exactly the fields they need, combine multiple resources in one request, or evolve contracts without versioning endpoints. Prefer REST for simple, cacheable resources or when infrastructure, team skills, and tools already fit REST neatly. Install Lighthouse composer require nuwave/lighthouse php artisan vendor:publish --provider=\u0026#34;Nuwave\\Lighthouse\\LighthouseServiceProvider\u0026#34; The publish step creates graphql/schema.graphql and a config file. By default, the HTTP endpoint is /graphql and the playground is enabled in non‑production environments.\nModel and seed example data Assume a basic order system: User, Order, and OrderItem.\nphp artisan make:model Order -m php artisan make:model OrderItem -m Define relationships in Eloquent:\n// app/Models/Order.php class Order extends Model { public function user() { return $this-\u0026gt;belongsTo(User::class); } public function items() { return $this-\u0026gt;hasMany(OrderItem::class); } } Schema‑first design Lighthouse lets you write your schema in SDL and map it to Eloquent models and resolvers.\n# graphql/schema.graphql type User { id: ID! name: String! email: String! orders: [Order!]! @hasMany } type Order { id: ID! number: String! status: String! total: Float! user: User! @belongsTo items: [OrderItem!]! @hasMany created_at: DateTime! } type OrderItem { id: ID! order: Order! @belongsTo sku: String! qty: Int! price: Float! } type Query { me: User @guard(with: [\u0026#34;sanctum\u0026#34;]) @auth orders( status: String @eq orderBy: [OrderOrderBy!] ): [Order!]! @paginate(defaultCount: 20) @orderBy order(id: ID! @eq): Order @find } input OrderOrderBy { column: OrderOrderByColumn! order: SortOrder! = ASC } enum OrderOrderByColumn { id created_at total } type Mutation { createOrder(number: String!, items: [NewOrderItem!]!): Order @field(resolver: \u0026#34;App\\\\GraphQL\\\\Mutations\\\\CreateOrder@handle\u0026#34;) @guard } input NewOrderItem { sku: String!, qty: Int!, price: Float! } Resolvers and mutations You can implement resolvers as invokable classes.\nphp artisan lighthouse:mutation CreateOrder // app/GraphQL/Mutations/CreateOrder.php namespace App\\GraphQL\\Mutations; use App\\Models\\Order; use App\\Models\\OrderItem; use Illuminate\\Support\\Facades\\DB; class CreateOrder { public function handle($_, array $args) { return DB::transaction(function () use ($args) { $order = Order::create([ \u0026#39;number\u0026#39; =\u0026gt; $args[\u0026#39;number\u0026#39;], \u0026#39;status\u0026#39; =\u0026gt; \u0026#39;pending\u0026#39;, \u0026#39;total\u0026#39; =\u0026gt; collect($args[\u0026#39;items\u0026#39;])-\u0026gt;sum(fn($i) =\u0026gt; $i[\u0026#39;qty\u0026#39;] * $i[\u0026#39;price\u0026#39;]), \u0026#39;user_id\u0026#39;=\u0026gt; auth()-\u0026gt;id(), ]); foreach ($args[\u0026#39;items\u0026#39;] as $i) { OrderItem::create([ \u0026#39;order_id\u0026#39; =\u0026gt; $order-\u0026gt;id, \u0026#39;sku\u0026#39; =\u0026gt; $i[\u0026#39;sku\u0026#39;], \u0026#39;qty\u0026#39; =\u0026gt; $i[\u0026#39;qty\u0026#39;], \u0026#39;price\u0026#39; =\u0026gt; $i[\u0026#39;price\u0026#39;], ]); } return $order-\u0026gt;fresh(); }); } } Authentication and authorization For first‑party SPAs, pair GraphQL with Laravel Sanctum . Add @guard(with: [\u0026quot;sanctum\u0026quot;]) to protected fields and use @can or policies to enforce access.\ntype Query { me: User @guard(with: [\u0026#34;sanctum\u0026#34;]) @auth myOrders: [Order!]! @paginate(defaultCount: 20) @guard(with: [\u0026#34;sanctum\u0026#34;]) @whereAuth(relation: \u0026#34;user\u0026#34;) } For fine‑grained rules, Lighthouse can call policies:\ntype Order { id: ID! number: String! total: Float! user: User! @belongsTo @can(ability: \u0026#34;view\u0026#34;, find: \u0026#34;id\u0026#34;) } Avoiding N+1 queries GraphQL encourages nested selections, which can cause N+1 queries if resolvers call the database per row. Lighthouse integrates with Eloquent eager loading and DataLoader.\nPrefer relationship directives like @hasMany and @belongsTo so Lighthouse can eager load. Use @paginate for collections to keep results bounded. If you write custom resolvers, batch queries with -\u0026gt;with() and use loaders. Filtering and pagination Lighthouse offers @paginate, @orderBy, and helpers for simple filters (@eq, @where, @in). For complex filters, define input types and map them to query scopes.\nFile uploads over GraphQL Follow the GraphQL multipart request spec. Lighthouse supports it out of the box when you accept Upload in your schema and handle it in a resolver. Apply the same validation and storage practices as in: Laravel File Upload Best Practices .\nQuery complexity and depth limits Unbounded queries can be expensive. Set a max query depth/complexity in config/lighthouse.php. Keep introspection enabled unless you have a strong reason to disable it; rely on limits and auth for protection.\nError handling and logging GraphQL returns partial results with an errors array. Map exceptions to user‑friendly messages and log server errors with context. Improve logs using the patterns in: Advanced Laravel Debugging with Logs .\nCaching and performance Cache expensive resolvers with application cache and sensible keys (args + user id). Use ETag/HTTP caching at the gateway if your GraphQL layer sits behind Nginx/CloudFront. Persisted queries reduce payload size and help gateways cache by hash. For wider performance tips, see: Laravel Performance Optimization . Sample queries and mutations Query with filtering, ordering, and pagination:\nquery Orders($status: String, $orderBy: [OrderOrderBy!]) { orders(status: $status, orderBy: $orderBy) { paginatorInfo { currentPage lastPage total } data { id number status total created_at user { id name } } } } Variables\n{ \u0026#34;status\u0026#34;: \u0026#34;paid\u0026#34;, \u0026#34;orderBy\u0026#34;: [{\u0026#34;column\u0026#34;:\u0026#34;created_at\u0026#34;,\u0026#34;order\u0026#34;:\u0026#34;DESC\u0026#34;}] } Mutation with variables:\nmutation CreateOrder($number: String!, $items: [NewOrderItem!]!) { createOrder(number: $number, items: $items) { id number total items { sku qty price } } } Variables\n{ \u0026#34;number\u0026#34;: \u0026#34;SO-2025-0001\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;sku\u0026#34;:\u0026#34;SKU-1\u0026#34;,\u0026#34;qty\u0026#34;:2,\u0026#34;price\u0026#34;:19.9}, {\u0026#34;sku\u0026#34;:\u0026#34;SKU-2\u0026#34;,\u0026#34;qty\u0026#34;:1,\u0026#34;price\u0026#34;:49.0} ] } Using DataLoader via @batch For fields that cannot be covered by relationship directives, batch lookups to avoid N+1. Lighthouse supports @batch using a key field and a resolver that returns a map of results.\nSchema\ntype Query { skuInfo(sku: String!): Sku @field(resolver: \u0026#34;App\\\\GraphQL\\\\Queries\\\\SkuInfo@__invoke\u0026#34;) } type Sku { sku: String! title: String! price Float! } type OrderItem { id: ID! sku: String! qty: Int! price: Float! detail: Sku @batch(key: \u0026#34;sku\u0026#34;, resolver: \u0026#34;App\\\\GraphQL\\\\Loaders\\\\SkuByCode@load\u0026#34;) } Batch loader\n// app/GraphQL/Loaders/SkuByCode.php namespace App\\GraphQL\\Loaders; class SkuByCode { /** * @param array\u0026lt;string\u0026gt; $keys * @return array\u0026lt;string, array\u0026gt; Map from sku =\u0026gt; Sku payload */ public function load(array $keys): array { // Replace with a single query to your catalog service or database $rows = \\DB::table(\u0026#39;skus\u0026#39;)-\u0026gt;whereIn(\u0026#39;sku\u0026#39;, $keys)-\u0026gt;get([\u0026#39;sku\u0026#39;,\u0026#39;title\u0026#39;,\u0026#39;price\u0026#39;]); return $rows-\u0026gt;keyBy(\u0026#39;sku\u0026#39;)-\u0026gt;map(fn($r) =\u0026gt; [\u0026#39;sku\u0026#39;=\u0026gt;$r-\u0026gt;sku,\u0026#39;title\u0026#39;=\u0026gt;$r-\u0026gt;title,\u0026#39;price\u0026#39;=\u0026gt;(float)$r-\u0026gt;price])-\u0026gt;all(); } } With @batch, Lighthouse collects all requested detail fields and calls load() once per request, returning results keyed by the batch key. This collapses many small queries into one.\nSecurity limits configuration Set reasonable defaults in config/lighthouse.php:\nreturn [ \u0026#39;security\u0026#39; =\u0026gt; [ \u0026#39;max_query_complexity\u0026#39; =\u0026gt; 200, // keep within your app capacity \u0026#39;max_query_depth\u0026#39; =\u0026gt; 15, \u0026#39;disable_introspection\u0026#39; =\u0026gt; env(\u0026#39;LIGHTHOUSE_DISABLE_INTROSPECTION\u0026#39;, false), ], \u0026#39;route\u0026#39; =\u0026gt; [ \u0026#39;uri\u0026#39; =\u0026gt; \u0026#39;/graphql\u0026#39;, \u0026#39;middleware\u0026#39; =\u0026gt; [\u0026#39;api\u0026#39;], ], \u0026#39;guard\u0026#39; =\u0026gt; \u0026#39;sanctum\u0026#39;, \u0026#39;pagination\u0026#39; =\u0026gt; [ \u0026#39;default_count\u0026#39; =\u0026gt; 20, \u0026#39;max_count\u0026#39; =\u0026gt; 100 ], ]; Testing the API Use HTTP tests to send GraphQL queries and assert on JSON. Keep a set of smoke tests for critical fields and mutations.\npublic function test_orders_query() { $user = User::factory()-\u0026gt;create(); $this-\u0026gt;actingAs($user); $query = \u0026#39;{ orders { data { id number total } } }\u0026#39;; $this-\u0026gt;postJson(\u0026#39;/graphql\u0026#39;, [\u0026#39;query\u0026#39; =\u0026gt; $query]) -\u0026gt;assertOk() -\u0026gt;assertJsonStructure([\u0026#39;data\u0026#39; =\u0026gt; [\u0026#39;orders\u0026#39; =\u0026gt; [\u0026#39;data\u0026#39; =\u0026gt; [[\u0026#39;id\u0026#39;,\u0026#39;number\u0026#39;,\u0026#39;total\u0026#39;]]]]]); } Hardening for production Rate limit the /graphql endpoint and protect with WAF rules if exposed publicly. Enforce auth on sensitive types and fields. Deny by default; allow explicitly. Cap query depth/complexity and set generous timeouts for the PHP‑FPM pool handling GraphQL. Keep a repeatable deployment routine and clear/rebuild caches. Background: Laravel Environment Configuration and Deploy Laravel to VPS with Nginx . Ensure file permissions and symlinks are correct after deploys: Fix Laravel Permission Issues . Summary GraphQL gives clients the control to fetch what they need and nothing more. With Lighthouse, you define types and relationships in a schema, protect access with Sanctum and policies, avoid N+1 issues with eager loading, and keep costs in check with limits and caching. Tie it to your existing logging and deployment practices, and you have a modern API that scales with your application’s needs.\n","href":"/2025/09/laravel-graphql-tutorial-api-modern.html","title":"Modern API Tutorial for Complex Applications"},{"content":"File uploads are simple to build and easy to get wrong. The goal is to accept only what you expect, store files safely, and serve them without opening new risks. The checklist and examples below cover validation, storage, serving, limits, and common pitfalls.\nAccept only what you need Validate every request. If a feature requires only images, do not accept arbitrary files.\n// app/Http/Controllers/AvatarController.php public function store(Request $request) { $validated = $request-\u0026gt;validate([ \u0026#39;avatar\u0026#39; =\u0026gt; [ \u0026#39;required\u0026#39;, \u0026#39;file\u0026#39;, \u0026#39;image\u0026#39;, // jpeg, png, bmp, gif, svg, webp \u0026#39;max:2048\u0026#39;, // KB (2 MB) \u0026#39;mimetypes:image/jpeg,image/png,image/webp\u0026#39;, // or: \u0026#39;mimes:jpeg,png,webp\u0026#39; ], ]); $path = $request-\u0026gt;file(\u0026#39;avatar\u0026#39;)-\u0026gt;store(\u0026#39;avatars\u0026#39;, \u0026#39;public\u0026#39;); auth()-\u0026gt;user()-\u0026gt;update([\u0026#39;avatar_path\u0026#39; =\u0026gt; $path]); return back()-\u0026gt;with(\u0026#39;status\u0026#39;, \u0026#39;Avatar updated\u0026#39;); } Notes\nPrefer image plus specific types via mimetypes or mimes. Use size limits (max) appropriate for your use case. Use file to ensure an actual uploaded file is present. Never trust client MIME only Laravel’s validator checks MIME using PHP’s file info, but you can add a second check for sensitive paths. For example, block PHP or executable content even if the extension is renamed.\n$file = $request-\u0026gt;file(\u0026#39;upload\u0026#39;); $mime = $file-\u0026gt;getMimeType(); // from finfo $ext = strtolower($file-\u0026gt;getClientOriginalExtension()); if (in_array($ext, [\u0026#39;php\u0026#39;,\u0026#39;phtml\u0026#39;,\u0026#39;phar\u0026#39;])) { abort(422, \u0026#39;Invalid file type\u0026#39;); } // Optional: allowlist only $allowed = [\u0026#39;image/jpeg\u0026#39;,\u0026#39;image/png\u0026#39;,\u0026#39;image/webp\u0026#39;,\u0026#39;application/pdf\u0026#39;]; abort_unless(in_array($mime, $allowed, true), 422, \u0026#39;Unsupported file type\u0026#39;); Store files safely Use Laravel’s filesystem. It handles paths, hashing, and adapters.\n// Hash name avoids collisions and hides original names $path = $request-\u0026gt;file(\u0026#39;document\u0026#39;)-\u0026gt;store(\u0026#39;documents\u0026#39;); // default disk $pathPublic = $request-\u0026gt;file(\u0026#39;image\u0026#39;)-\u0026gt;store(\u0026#39;images\u0026#39;, \u0026#39;public\u0026#39;); // Or place with a custom name $name = Str::uuid()-\u0026gt;toString().\u0026#39;.\u0026#39;.$request-\u0026gt;file(\u0026#39;image\u0026#39;)-\u0026gt;extension(); $path = $request-\u0026gt;file(\u0026#39;image\u0026#39;)-\u0026gt;storeAs(\u0026#39;images\u0026#39;, $name, \u0026#39;public\u0026#39;); Tips\nUse hashName() or UUIDs, not user-supplied filenames. Do not build paths with user input. Let the storage layer resolve paths to avoid traversal (e.g., ../../ cases). Keep uploads outside the app code directory. Public vs private files Two broad patterns:\nPublic assets (e.g., avatars): store on the public disk and create a symlink with php artisan storage:link. Serve via the web server directly.\nPrivate files (e.g., invoices, reports): store on a private disk and stream through a controller, or generate a signed URL with expiry.\n// Stream a private file public function download(string $path) { $this-\u0026gt;authorize(\u0026#39;download\u0026#39;, $path); // apply your policy return Storage::disk(\u0026#39;private\u0026#39;)-\u0026gt;download($path); } // Or generate temporary access URL::temporarySignedRoute( \u0026#39;files.show\u0026#39;, now()-\u0026gt;addMinutes(10), [\u0026#39;path\u0026#39; =\u0026gt; $path] ); Block code execution in upload directories Even if uploads sit under public/, prevent PHP execution there. For Nginx, only pass real .php files in your app directory to PHP‑FPM.\nlocation ~ \\.php$ { include fastcgi_params; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; fastcgi_pass app:9000; } # Never treat arbitrary uploads as PHP location ~* /storage/.*\\.(php|phtml|phar)$ { return 403; } On Apache, use php_admin_flag engine off or deny execution in the uploads directory.\nServer limits that affect uploads Large files can fail before your controller runs.\nNginx: client_max_body_size 10m; Apache: LimitRequestBody PHP: upload_max_filesize, post_max_size, max_file_uploads Set these to realistic values for your application. When debugging production differences, confirm the environment values and caches as described in: Laravel Environment Configuration .\nImage processing without blocking requests Expensive work (resize, thumbnails, metadata stripping) belongs off the main request path.\n// dispatch a job to process the stored image ProcessAvatar::dispatch($path); Inside the job, use a library (e.g., Intervention Image or Imagick) to resize and strip metadata. Restart workers after changing env or config so they read fresh values: see Fixing Laravel Session and Cache Issues .\nServing from S3 or object storage S3 works well for both public and private files.\nStorage::disk(\u0026#39;s3\u0026#39;)-\u0026gt;putFileAs(\u0026#39;invoices\u0026#39;, $request-\u0026gt;file(\u0026#39;pdf\u0026#39;), $name); // Signed URL for private access $url = Storage::disk(\u0026#39;s3\u0026#39;)-\u0026gt;temporaryUrl( \u0026#39;invoices/\u0026#39;.$name, now()-\u0026gt;addMinutes(10) ); Harden S3 buckets: disable public ACLs unless required, use bucket policies, set correct Content-Type, and prefer signed URLs for sensitive content.\nQuotas, rate limits, and abuse controls Uploads need guardrails:\nLimit file size and types by route. Add rate limiting per user or IP to the upload endpoint. Enforce per-user quotas (table of used storage) and surface clear errors. Log upload attempts (success and rejection) with request context to spot abuse. For log patterns, see: Advanced Laravel Debugging with Logs . Validation messages and UX Return helpful validation errors and keep the form state so users can retry quickly. On SPAs, show progress bars for larger files and handle retries gracefully.\nTesting uploads Use Laravel’s helpers to test controllers and jobs.\npublic function test_avatar_upload() { Storage::fake(\u0026#39;public\u0026#39;); $file = UploadedFile::fake()-\u0026gt;image(\u0026#39;avatar.jpg\u0026#39;, 256, 256)-\u0026gt;size(1000); $this-\u0026gt;actingAs(User::factory()-\u0026gt;create()) -\u0026gt;post(\u0026#39;/profile/avatar\u0026#39;, [\u0026#39;avatar\u0026#39; =\u0026gt; $file]) -\u0026gt;assertSessionHasNoErrors(); Storage::disk(\u0026#39;public\u0026#39;)-\u0026gt;assertExists(\u0026#39;avatars/\u0026#39;.$file-\u0026gt;hashName()); } Common pitfalls Using original filenames directly. Use hashed names or UUIDs. Building file paths from user input. Always go through the storage API. Accepting */* MIME or no size limits. Doing heavy image work inside the request; use queues. Not clearing or rebuilding config caches after environment changes. Security checklist Validate file type, size, and presence with rules. Double‑check MIME with allowlists where needed. Hash or randomize filenames; avoid directory traversal by never concatenating user input into paths. Block code execution in upload directories. Use private storage and signed URLs for sensitive content. Set realistic server limits and monitor errors. Keep deployment routines predictable to avoid stale config. See: Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide and Laravel Security Best Practices for Production . Summary Secure uploads come down to strict validation, safe storage, careful serving, realistic server limits, and thoughtful background processing. With hashed names, private disks or signed URLs, non‑blocking image jobs, and clear logs, you minimize risk while keeping the experience smooth for users.\n","href":"/2025/09/laravel-file-upload-validation-security.html","title":"Laravel File Upload with Validation and Security Best Practices"},{"content":"Docker makes Laravel environments consistent across machines and stages. The steps below outline a clean setup for local development and a hardened build for production. Run PHP‑FPM behind Nginx, connect to MySQL/Postgres and Redis, toggle Xdebug when needed, and ship a small, cache‑friendly image.\nComponents PHP‑FPM container for the application Nginx container as the HTTP entry point MySQL or Postgres, plus Redis A docker-compose.yml for development with bind mounts A multi‑stage Dockerfile for a compact production image Project layout app/ # your Laravel app code docker/ nginx/ default.conf Dockerfile docker-compose.yml Dockerfile (multi‑stage) Build dependencies once, then copy only what you need into the runtime image. Enable OPcache for production; allow an Xdebug toggle for local work.\n# syntax=docker/dockerfile:1 ARG PHP_VERSION=8.2 FROM composer:2 AS vendor WORKDIR /app COPY composer.json composer.lock . RUN composer install --no-dev --prefer-dist --no-scripts --no-progress --no-interaction FROM php:${PHP_VERSION}-fpm AS base WORKDIR /var/www/html # System deps RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends \\ git unzip libzip-dev libpng-dev libonig-dev libicu-dev libpq-dev \\ \u0026amp;\u0026amp; docker-php-ext-install pdo pdo_mysql mysqli intl zip \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # OPcache for prod RUN docker-php-ext-install opcache \\ \u0026amp;\u0026amp; { echo \u0026#34;opcache.enable=1\u0026#34;; echo \u0026#34;opcache.enable_cli=0\u0026#34;; echo \u0026#34;opcache.jit_buffer_size=0\u0026#34;; } \u0026gt; /usr/local/etc/php/conf.d/opcache.ini # Xdebug (optional; enabled in dev via env) RUN pecl install xdebug \\ \u0026amp;\u0026amp; docker-php-ext-enable xdebug \\ \u0026amp;\u0026amp; { echo \u0026#34;xdebug.mode=off\u0026#34;; echo \u0026#34;xdebug.client_host=host.docker.internal\u0026#34;; } \u0026gt; /usr/local/etc/php/conf.d/docker-php-ext-xdebug.ini COPY --from=vendor /app/vendor /var/www/html/vendor COPY . /var/www/html # Permissions for storage/bootstrap/cache in container RUN chown -R www-data:www-data storage bootstrap/cache \\ \u0026amp;\u0026amp; find storage bootstrap/cache -type d -exec chmod 775 {} \\; \\ \u0026amp;\u0026amp; find storage bootstrap/cache -type f -exec chmod 664 {} \\; # Default to production settings; override in dev with env ENV APP_ENV=production \\ APP_DEBUG=false \\ PHP_IDE_CONFIG=\u0026#34;serverName=laravel-docker\u0026#34; CMD [\u0026#34;php-fpm\u0026#34;] Nginx config (docker/nginx/default.conf) server { listen 80; server_name _; root /var/www/html/public; index index.php index.html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { include fastcgi_params; fastcgi_intercept_errors on; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; fastcgi_pass app:9000; # php-fpm service name } location ~* \\.(?:css|js|jpg|jpeg|gif|png|svg|ico|webp)$ { expires 7d; access_log off; } } Development docker‑compose.yml Bind mount the source tree for instant reloads, enable Xdebug, and expose DB/Redis. Below uses MySQL; switch to Postgres if preferred.\nversion: \u0026#34;3.9\u0026#34; services: app: build: context: . args: PHP_VERSION: \u0026#34;8.2\u0026#34; image: laravel-app:dev container_name: laravel-app environment: APP_ENV: local APP_DEBUG: \u0026#34;true\u0026#34; XDEBUG_MODE: debug,develop volumes: - ./:/var/www/html depends_on: - db - redis web: image: nginx:1.25-alpine container_name: laravel-web ports: - \u0026#34;8080:80\u0026#34; volumes: - ./:/var/www/html:ro - ./docker/nginx/default.conf:/etc/nginx/conf.d/default.conf:ro depends_on: - app db: image: mysql:8.0 container_name: laravel-mysql environment: MYSQL_DATABASE: app MYSQL_USER: app MYSQL_PASSWORD: secret MYSQL_ROOT_PASSWORD: root ports: - \u0026#34;3306:3306\u0026#34; volumes: - dbdata:/var/lib/mysql healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;mysqladmin\u0026#34;, \u0026#34;ping\u0026#34;, \u0026#34;-h\u0026#34;, \u0026#34;localhost\u0026#34;] interval: 10s timeout: 5s retries: 10 redis: image: redis:7-alpine container_name: laravel-redis ports: - \u0026#34;6379:6379\u0026#34; volumes: - redisdata:/data volumes: dbdata: redisdata: Environment configuration Point your .env to container hosts and keep credentials in env variables.\nAPP_ENV=local APP_DEBUG=true APP_URL=http://localhost:8080 DB_CONNECTION=mysql DB_HOST=db DB_PORT=3306 DB_DATABASE=app DB_USERNAME=app DB_PASSWORD=secret CACHE_DRIVER=redis REDIS_HOST=redis REDIS_PORT=6379 SESSION_DRIVER=file Common commands in Docker # First-time setup docker compose up -d --build docker compose exec app php artisan key:generate # Run migrations and seeders docker compose exec app php artisan migrate --seed # Clear and rebuild caches docker compose exec app php artisan cache:clear \u0026amp;\u0026amp; \\ php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear \u0026amp;\u0026amp; \\ php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache # Run queue worker (dev) docker compose exec -d app php artisan queue:work --tries=3 Permissions and file ownership On Linux hosts, UID/GID mismatches can create root‑owned files on bind mounts. One solution is to build the image with a matching UID, another is to keep writes inside storage/ and bootstrap/cache and set group‑writable modes. For production servers outside Docker, follow: Fix Laravel Permission Issues .\nProduction build In production, bake dependencies and your code into the image. Avoid bind mounts; use read‑only file systems where possible, and send logs to stdout/stderr.\nExample production compose (excerpt):\nservices: app: build: context: . args: PHP_VERSION: \u0026#34;8.2\u0026#34; image: registry.example.com/laravel-app:2025-09-16 environment: APP_ENV: production APP_DEBUG: \u0026#34;false\u0026#34; deploy: replicas: 2 restart_policy: condition: on-failure healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;php -v || exit 1\u0026#34;] interval: 30s timeout: 5s retries: 3 web: image: nginx:1.25-alpine ports: - \u0026#34;80:80\u0026#34; depends_on: - app Deployment routine Use a predictable sequence to avoid stale caches and mismatched environments:\ndocker compose pull \u0026amp;\u0026amp; docker compose build docker compose up -d --no-deps --scale app=2 --build app web docker compose exec app php artisan migrate --force docker compose exec app php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear docker compose exec app php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache docker compose exec app php artisan queue:restart || true Security and hardening Never bake secrets into images. Pass them at runtime (env vars, orchestrator secrets). Serve over HTTPS and set secure cookies. Review: Laravel Security Best Practices for Production . Keep the Nginx container minimal and stateless; store user uploads in object storage or mounted volumes. Limit token scopes for API access; Sanctum is a good fit for first‑party clients: Laravel API Authentication with Sanctum . Troubleshooting If the app reads old values, you likely cached config earlier. Clear and rebuild. Background: Laravel Environment Configuration . If sessions or cookies fail behind a proxy, configure trusted proxies and cookie settings. See: Fixing Laravel Session and Cache Issues . For error spikes or 500 responses, check application and service logs first, then Nginx/PHP‑FPM. Patterns: Advanced Laravel Debugging with Logs . CPU spikes while building assets? Run composer install --no-dev and only what you need in images; keep build artifacts out of runtime layers. Summary A small set of containers\u0026ndash;PHP‑FPM, Nginx, a database, and Redis\u0026ndash;lets you develop locally and deploy consistently. Use bind mounts and Xdebug in development, but ship a multi‑stage, cached image in production with OPcache on. Keep secrets out of images, send logs to stdout, and follow a clear post‑deploy routine to rebuild caches and restart workers. Tie the setup to your existing operational practices to reduce surprises.\n","href":"/2025/09/laravel-docker-setup-development-production.html","title":"Development and Production Environment Setup"},{"content":"Migrations let you evolve your schema alongside the code. Done well, they are repeatable and safe. Done poorly, they lock tables, drop data, and take your site down. This guide focuses on practical patterns that reduce risk in production and make rollouts predictable.\nGround rules Treat migrations as immutable once deployed. If a mistake gets to production, add a new migration to correct it instead of editing history. Keep schema and data changes separate. Data backfills belong in their own migration or a job/command so you can control runtime and retries. Don’t rely on application models inside migrations. Models can drift as your app evolves. Prefer DB::table() or raw SQL that doesn’t depend on future code. Test locally and in staging with the same DB engine and major version you run in production. Always run with php artisan migrate --force in CI/production. Check status with php artisan migrate:status. Naming and versioning Use descriptive names that read like a change log: 2025_09_15_100001_add_status_to_orders_table.php. One concern per migration. If a change requires several steps (add column -\u0026gt; backfill -\u0026gt; enforce NOT NULL), use separate migrations in the right order.\nZero‑downtime mindset Your new code must work before, during, and after the migration. The safest pattern is a two‑step rollout:\nDeploy backward‑compatible code that does not depend on the new schema yet. Run the migration. Flip the code to use the new column/constraint. For larger changes, consider feature flags and a staged rollout. For server setup and permissions that avoid 403/500 during deploys, see: Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide and Fix Laravel Permission Issues .\nAdding columns safely Adding a NOT NULL column with a default can lock a big table or backfill every row inside a single statement. Safer pattern:\nStep 1: add the column as nullable without default. Step 2: backfill in batches. Step 3: add the default and the NOT NULL constraint. Example:\nSchema::table(\u0026#39;orders\u0026#39;, function (Blueprint $table) { $table-\u0026gt;unsignedTinyInteger(\u0026#39;status\u0026#39;)-\u0026gt;nullable(); }); DB::table(\u0026#39;orders\u0026#39;)-\u0026gt;whereNull(\u0026#39;status\u0026#39;) -\u0026gt;orderBy(\u0026#39;id\u0026#39;) -\u0026gt;chunkById(10_000, function ($rows) { foreach ($rows as $row) { DB::table(\u0026#39;orders\u0026#39;)-\u0026gt;where(\u0026#39;id\u0026#39;, $row-\u0026gt;id)-\u0026gt;update([\u0026#39;status\u0026#39; =\u0026gt; 0]); } }); Schema::table(\u0026#39;orders\u0026#39;, function (Blueprint $table) { $table-\u0026gt;unsignedTinyInteger(\u0026#39;status\u0026#39;)-\u0026gt;default(0)-\u0026gt;nullable(false)-\u0026gt;change(); }); Backfilling large tables Avoid long transactions and table scans. Use chunkById, update by primary key ranges, and run during off‑peak hours. If the backfill can take minutes, make it a queued job/command so it can resume on failure. For environment consistency and config caching pitfalls during deploys, review: Laravel Environment Configuration .\nIndexes without blocking traffic Indexes speed reads but can block writes if created the wrong way.\nPostgreSQL: use CREATE INDEX CONCURRENTLY (cannot run inside a transaction). In Laravel, set public $withinTransaction = false; on the migration class and run a raw statement. MySQL 8 / InnoDB: many operations are online; prefer ALGORITHM=INPLACE/INSTANT where possible. Avoid operations that copy the table. Example (PostgreSQL):\nclass AddIndexToOrdersOnCreatedAt extends Migration { public $withinTransaction = false; // required for CONCURRENTLY public function up(): void { DB::statement(\u0026#39;CREATE INDEX CONCURRENTLY idx_orders_created_at ON orders (created_at)\u0026#39;); } public function down(): void { DB::statement(\u0026#39;DROP INDEX CONCURRENTLY IF EXISTS idx_orders_created_at\u0026#39;); } } Foreign keys and data integrity Before adding a foreign key, clean the data. A simple SELECT for orphaned rows saves a failed deployment. Pick the right action for your lifecycle: ON DELETE CASCADE for true dependents (e.g., order items), RESTRICT when deletion should be explicit, or SET NULL for optional relationships.\nRenaming columns and tables Laravel needs doctrine/dbal to rename existing columns. Even then, renames can be disruptive for large tables.\nSafer alternative: add a new column, backfill, update code to read the new column, then drop the old one later. If you must rename, schedule a window and ensure your code can handle both names during the transition. Example rename with DBAL:\ncomposer require doctrine/dbal --dev Schema::table(\u0026#39;users\u0026#39;, function (Blueprint $table) { $table-\u0026gt;renameColumn(\u0026#39;fullname\u0026#39;, \u0026#39;name\u0026#39;); }); Transactions in migrations Laravel wraps migrations in a transaction when the driver supports it. Some operations (like Postgres CONCURRENTLY) cannot run inside one. Use the $withinTransaction = false; property on the migration class for those cases. For MySQL, avoid wrapping very long backfills in a single transaction; commit in batches instead.\nRolling forward vs rolling back In production, prefer forward‑only fixes. Rollbacks can fail if data has changed since the migration ran. Keep down() accurate for local/staging, but if a production migration goes wrong, ship a new forward migration to correct course.\nAvoid logic in migrations Migrations should change schema, not business rules. Don’t call application services or rely on model events/scopes. If you must move data across tables, use the query builder or raw SQL and keep scope explicit.\nSeeders, data fixes, and schema dumps Use seeders for initial content or reference tables. For long‑lived projects, prune ancient migrations with a schema dump so new installs are fast:\nphp artisan schema:dump --prune This stores the current schema as a SQL dump and removes old migrations that are already included in the dump. Keep recent migrations that were created after the dump.\nOperational checklist (copy/paste) # Pre‑deploy php artisan test --testsuite=Unit,Feature php artisan migrate:status # Deploy composer install --no-dev --prefer-dist --optimize-autoloader php artisan migrate --force # Post‑deploy php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache php artisan queue:restart || true Troubleshooting and observability If a migration fails on production, read the database error first. Then check application and service logs:\ntail -f storage/logs/laravel.log sudo journalctl -u php8.2-fpm -f sudo tail -f /var/log/nginx/error.log Adopt consistent, structured logging so you can see when a deployment slows queries or increases lock wait time. For patterns and examples, see: Advanced Laravel Debugging with Logs . If migrations affect performance, revisit indexes and caching strategies: Laravel Performance Optimization: 15 Techniques .\nSummary Ship schema changes safely by keeping migrations small and explicit, separating schema from data, backfilling in batches, using online index strategies, and choosing foreign‑key actions deliberately. Prefer forward fixes over rollbacks in production, and make deployments repeatable with a clear checklist. With these habits, migrations become dependable instead of risky.\n","href":"/2025/09/laravel-database-migration-best-practices.html","title":"Avoiding Fatal Mistakes"},{"content":"Laravel Sanctum offers two simple authentication modes that cover most applications:\nCookie-based auth for SPAs that live on the same top-level domain as your backend. Personal access tokens for mobile apps, third‑party clients, or server‑to‑server use. This tutorial walks through both flows end‑to‑end, including the necessary configuration (CORS, cookies, stateful domains), how to issue and revoke tokens, how to protect routes, and how to test the result. You’ll also find production notes to avoid common pitfalls.\nWhen to use Sanctum (and when not) Use Sanctum if you need a lightweight, first‑party SPA login or simple tokens with optional abilities. It integrates cleanly with Laravel’s session and guard system. Use Passport or OAuth 2.0 only if you must support third‑party OAuth clients, authorization codes, refresh tokens, and full OAuth flows. Install and prepare Sanctum ships with Laravel, but ensure the package and provider are present:\ncomposer require laravel/sanctum php artisan vendor:publish --provider=\u0026#34;Laravel\\Sanctum\\SanctumServiceProvider\u0026#34; php artisan migrate Add the middleware to app/Http/Kernel.php so Sanctum can manage cookies for SPAs:\n// app/Http/Kernel.php protected $middlewareGroups = [ \u0026#39;web\u0026#39; =\u0026gt; [ // ... \\Laravel\\Sanctum\\Http\\Middleware\\EnsureFrontendRequestsAreStateful::class, ], \u0026#39;api\u0026#39; =\u0026gt; [ // ... \\Illuminate\\Routing\\Middleware\\SubstituteBindings::class, ], ]; Configuration for SPA cookie auth Cookie mode gives you simple, session‑style auth for a first‑party SPA (for example, app.example.com and api.example.com). Configure the following:\nCORS (config/cors.php) return [ \u0026#39;paths\u0026#39; =\u0026gt; [\u0026#39;api/*\u0026#39;, \u0026#39;sanctum/csrf-cookie\u0026#39;, \u0026#39;login\u0026#39;, \u0026#39;logout\u0026#39;], \u0026#39;allowed_methods\u0026#39; =\u0026gt; [\u0026#39;*\u0026#39;], \u0026#39;allowed_origins\u0026#39; =\u0026gt; [\u0026#39;https://app.example.com\u0026#39;], \u0026#39;allowed_headers\u0026#39; =\u0026gt; [\u0026#39;*\u0026#39;], \u0026#39;supports_credentials\u0026#39; =\u0026gt; true, ]; Session and cookies (config/session.php) \u0026#39;domain\u0026#39; =\u0026gt; \u0026#39;.example.com\u0026#39;, \u0026#39;secure\u0026#39; =\u0026gt; env(\u0026#39;SESSION_SECURE_COOKIE\u0026#39;, true), \u0026#39;same_site\u0026#39; =\u0026gt; \u0026#39;lax\u0026#39;, Sanctum stateful domains (config/sanctum.php) \u0026#39;stateful\u0026#39; =\u0026gt; [ \u0026#39;app.example.com\u0026#39;, // SPA origin(s) ], .env highlights SESSION_DOMAIN=.example.com SESSION_SECURE_COOKIE=true APP_URL=https://api.example.com SANCTUM_STATEFUL_DOMAINS=app.example.com Login and logout endpoints (SPA) Flow overview:\nSPA requests /sanctum/csrf-cookie to prime the CSRF cookie. SPA posts credentials to /login (the default Laravel endpoint) with X-XSRF-TOKEN header. Server sets the session cookie; subsequent requests to /api/* include it automatically. Example controller for login/logout using Fortify or the default auth scaffolding works out of the box. If you roll your own:\n// routes/api.php (or routes/web.php for auth endpoints) use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Auth; Route::post(\u0026#39;/login\u0026#39;, function (Request $request) { $request-\u0026gt;validate([ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;required\u0026#39;, ]); if (! Auth::attempt($request-\u0026gt;only(\u0026#39;email\u0026#39;,\u0026#39;password\u0026#39;), true)) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Invalid credentials\u0026#39;], 422); } $request-\u0026gt;session()-\u0026gt;regenerate(); return response()-\u0026gt;noContent(); }); Route::post(\u0026#39;/logout\u0026#39;, function (Request $request) { Auth::guard(\u0026#39;web\u0026#39;)-\u0026gt;logout(); $request-\u0026gt;session()-\u0026gt;invalidate(); $request-\u0026gt;session()-\u0026gt;regenerateToken(); return response()-\u0026gt;noContent(); }); Protect routes with auth:sanctum Use the Sanctum guard to protect API routes:\n// routes/api.php Route::middleware(\u0026#39;auth:sanctum\u0026#39;)-\u0026gt;group(function () { Route::get(\u0026#39;/profile\u0026#39;, fn(Request $r) =\u0026gt; $r-\u0026gt;user()); Route::post(\u0026#39;/orders\u0026#39;, [OrderController::class, \u0026#39;store\u0026#39;]); }); Personal access tokens (mobile and third‑party) For mobile apps or server‑to‑server calls, use personal access tokens. Users can have multiple tokens with abilities (scopes).\nIssue a token:\n$token = $user-\u0026gt;createToken(\u0026#39;mobile\u0026#39;, [\u0026#39;orders:create\u0026#39;,\u0026#39;orders:read\u0026#39;]); return [\u0026#39;token\u0026#39; =\u0026gt; $token-\u0026gt;plainTextToken]; Send the token with API requests:\nAuthorization: Bearer \u0026lt;token\u0026gt; Check abilities inside controllers/policies:\nif ($request-\u0026gt;user()-\u0026gt;tokenCan(\u0026#39;orders:create\u0026#39;)) { // proceed } Revoke and rotate tokens Revoke the current access token:\n$request-\u0026gt;user()-\u0026gt;currentAccessToken()-\u0026gt;delete(); Revoke all tokens for a user:\n$request-\u0026gt;user()-\u0026gt;tokens()-\u0026gt;delete(); Token expiry and pruning Sanctum does not expire tokens by default. You can implement rotation or periodic pruning via a scheduled job. With Laravel 11+ you can prune with built‑in commands or write a custom command to delete old tokens based on last_used_at.\nTesting the flow Write basic feature tests to lock in behavior:\npublic function test_spa_login_and_profile() { $user = User::factory()-\u0026gt;create([\u0026#39;password\u0026#39; =\u0026gt; bcrypt(\u0026#39;secret\u0026#39;)]); $this-\u0026gt;get(\u0026#39;/sanctum/csrf-cookie\u0026#39;); $this-\u0026gt;post(\u0026#39;/login\u0026#39;, [\u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;secret\u0026#39;]) -\u0026gt;assertNoContent(); $this-\u0026gt;getJson(\u0026#39;/api/profile\u0026#39;)-\u0026gt;assertOk()-\u0026gt;assertJson([\u0026#39;id\u0026#39; =\u0026gt; $user-\u0026gt;id]); } public function test_pat_flow() { $user = User::factory()-\u0026gt;create(); $token = $user-\u0026gt;createToken(\u0026#39;test\u0026#39;, [\u0026#39;orders:read\u0026#39;])-\u0026gt;plainTextToken; $this-\u0026gt;withHeader(\u0026#39;Authorization\u0026#39;, \u0026#39;Bearer \u0026#39;.$token) -\u0026gt;getJson(\u0026#39;/api/orders\u0026#39;) -\u0026gt;assertOk(); } Troubleshooting 419 or CSRF mismatch: your SPA likely missed the /sanctum/csrf-cookie call, or CORS/credentials are off. Ensure supports_credentials=true, allow your SPA origin, and send X-XSRF-TOKEN on state‑changing requests. Unauthenticated on protected routes with cookies: check SESSION_DOMAIN, SESSION_SECURE_COOKIE, and SANCTUM_STATEFUL_DOMAINS. Cookies must be sent back to the API domain. See: Fixing Laravel Session and Cache Issues . Bearer token rejected: confirm the Authorization: Bearer header is present and not stripped by proxies. If using Nginx behind another proxy, validate forwarded headers. When debugging, add structured logs: Advanced Laravel Debugging with Logs . Works locally, fails in production: compare environments and cached config. Clear caches, rebuild, and reload PHP‑FPM. Background: Laravel Environment Configuration . 403 from server: verify DocumentRoot points to public/ and writable paths are correct: Fix Laravel Permission Issues . Production notes Always serve over HTTPS. Set SESSION_SECURE_COOKIE=true and pick a proper same_site value. Avoid exposing tokens in URLs. Limit token abilities to the minimum required and rotate when appropriate. Log authentication events and token usage. Use structured logs to identify misuse. Keep your deployment routine predictable and clear caches after environment changes. See: Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide and Laravel Security Best Practices for Production . If you need performance tuning (for example, lots of token checks), review cache strategy and DB indexes: Laravel Performance Optimization: 15 Techniques . Summary Use cookie‑based Sanctum auth for first‑party SPAs and personal access tokens for mobile or server‑to‑server calls. Configure CORS and cookies correctly, declare stateful domains, protect API routes with auth:sanctum, and keep tokens tight with abilities, rotation, and revocation. Test the end‑to‑end flow, monitor logs, and follow production hardening guidelines. With these in place, you get a secure, maintainable authentication system without the weight of full OAuth.\n","href":"/2025/09/laravel-api-authentication-sanctum-2025.html","title":"Laravel API Authentication with Sanctum Complete Tutorial 2025"},{"content":"Laravel applications fail for a handful of predictable reasons: missing or stale configuration, broken cache, database schema drift, misconfigured cookies, permissions, or plain coding mistakes. The sections below show fast, reliable ways to identify the root cause and ship a clean fix without guesswork.\nStart with a clean baseline Run these commands from the project root to eliminate stale build artifacts before you investigate further:\nphp artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear composer dump-autoload -o When the problem is related to file permissions (very common after deploy), follow the safe defaults in this companion article: Fix Laravel Permission Issues .\n“No application encryption key has been specified” Cause: APP_KEY is empty, truncated, or the app is using a cached config from a previous environment.\nFix:\nphp artisan key:generate --force php artisan config:clear \u0026amp;\u0026amp; php artisan config:cache Generate the key once and keep it stable across releases. Regenerating on a live site invalidates encrypted cookies and sessions.\n419 Page Expired or CSRF token mismatch Cause: the session cookie is not sent back or expires too soon, the domain/secure flags are wrong, or a form is missing the token.\nChecklist:\nForms must include @csrf. On HTTPS, set SESSION_SECURE_COOKIE=true. If you use subdomains, set SESSION_DOMAIN=.example.com. For cross‑site embeds, SESSION_SAME_SITE=none requires secure cookies. See also the cookie and driver section in: Fixing Laravel Session and Cache Issues .\n403 Forbidden Two very different sources:\nFrom Laravel: policies/gates or custom middleware deny the action. Verify policies are registered and the current user has the required ability. From the server: wrong document root (not public/), missing try_files, or permissions/SELinux in production. If it’s the server, check your Nginx/Apache config and file ownership. Reference: Fix Laravel Permission Issues and Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide . 404 Not Found or “Route [name] not defined.” Causes:\nTypo in route name or missing route import. Cached routes out of sync with code. Wrong HTTP verb. Fix:\nphp artisan route:list | grep -i users php artisan route:clear \u0026amp;\u0026amp; php artisan route:cache Confirm controller namespaces and route groups. Make sure HTTP verbs match the route definitions.\n500 Internal Server Error This is a category, not a single error. Look at logs first:\ntail -f storage/logs/laravel.log Common root causes and quick checks:\nSyntax/runtime errors: the stack trace points to the file and line. Config cache mismatch: php artisan config:clear and retry. Missing PHP extensions (mbstring, intl, pdo_*): install matching extensions for your PHP version. Permissions on storage/ or bootstrap/cache/: fix ownership and mode, then retry caches. Wrong .env values not being read: see the environment section below. “Class not found”, “Target class does not exist”, or autoload issues Causes: missing composer install, incorrect namespace, class renamed without updating references, or PSR‑4 path mismatch.\nFix:\ncomposer install --no-dev --prefer-dist --optimize-autoloader composer dump-autoload -o Check composer.json for correct PSR‑4 paths, and confirm the namespace matches the filesystem.\nSQLSTATE errors (e.g., Base table or view not found, Unknown column) Causes: pending migrations, wrong connection, or mismatched schema between web and CLI.\nFix:\nphp artisan migrate --force php artisan tinker \u0026gt;\u0026gt;\u0026gt; DB::connection()-\u0026gt;getDatabaseName() Ensure the app, queue workers, and CLI all point to the same database. Verify credentials in config/database.php and your environment.\nStorage and file errors (missing links, cannot write) Symptoms: 404 for uploaded files, image not found, Unable to create directory.\nFix:\nphp artisan storage:link sudo chown -R www-data:www-data storage bootstrap/cache sudo find storage bootstrap/cache -type d -exec chmod 775 {} \\; sudo find storage bootstrap/cache -type f -exec chmod 664 {} \\; Prefer group‑writable permissions over 777. For production patterns, see: Fix Laravel Permission Issues .\nEnvironment values not applying after deploy Cause: configuration was cached earlier and the app is still reading stale values. Another frequent trap is web requests vs CLI/workers using different environments.\nFix sequence:\nphp artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache php artisan queue:restart || true Set environment variables at the process level (PHP‑FPM, systemd, or your container orchestrator) instead of editing .env manually on servers. Details: Laravel Environment Configuration .\n405 Method Not Allowed Cause: the route exists but the HTTP method doesn’t match (GET vs POST), or middleware blocks the verb.\nFix:\nphp artisan route:list | grep -i your-endpoint Check JavaScript calls and HTML forms to ensure they use the expected verb and include _method when necessary.\n“Page works locally but not behind a proxy or load balancer” Cause: trusted proxy headers not configured, HTTPS offloading, or sticky sessions disabled.\nFix:\n// app/Http/Middleware/TrustProxies.php protected $proxies = \u0026#39;*\u0026#39;; protected $headers = \\Illuminate\\Http\\Request::HEADER_X_FORWARDED_AWS_ELB; // or HEADER_X_FORWARDED_ALL Restart workers after changing environment or config:\nphp artisan queue:restart “CORS policy” errors for APIs Cause: browser blocks cross‑origin requests. Configure allowed origins and headers.\nQuick check: publish the CORS config (config/cors.php) and set allowed origins for your environments. Ensure preflight requests (OPTIONS) are handled by your server.\nWhat to look for in logs Laravel’s application log usually has the answer. If the file is empty during a 500, check the PHP‑FPM and web server logs to catch fatal errors before Laravel handles them.\ntail -f storage/logs/laravel.log sudo journalctl -u php8.2-fpm -f sudo tail -f /var/log/nginx/error.log For stronger diagnostics and clean log patterns in production, see: Advanced Laravel Debugging with Logs .\nA dependable release routine Small, repeatable steps prevent most production incidents:\ncomposer install --no-dev --prefer-dist --optimize-autoloader php artisan migrate --force php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache php artisan queue:restart || true sudo systemctl reload php8.2-fpm || true Summary Resolve Laravel errors quickly by checking configuration cache, environment values, routes, migrations, file storage, and permissions first. Read the application log, then confirm server logs when needed. Keep releases predictable, restart workers after changes, and serve the app from public/ with correct ownership and modes. With these habits in place, most 419/403/404/500 incidents become straightforward to diagnose and fix.\n","href":"/2025/09/how-to-fix-common-laravel-errors.html","title":"How to Fix Common Laravel Errors Complete Troubleshooting Guide for Developers"},{"content":"Sessions and cache power many core features in Laravel\u0026ndash;from authentication to performance. When they break, symptoms can be confusing: users get logged out randomly, “remember me” does nothing, flash messages disappear, or recent cache writes don’t show up. Use the checklist below to quickly find and fix the cause.\nHow sessions and cache fail Sessions persist state across requests. Laravel can store them in files, database, Redis, Memcached, or array (for tests). If the storage can’t be written or the cookie can’t be read back, the user appears “logged out”. Cache stores computed data for speed. If the driver points to a different backend than you expect, or the key gets namespaced differently, you’ll read stale or missing values. Quick fixes that solve most cases Run these from your app root (adjust user/group):\n# Writable directories for file sessions/view cache sudo chown -R www-data:www-data storage bootstrap/cache sudo find storage bootstrap/cache -type d -exec chmod 775 {} \\; sudo find storage bootstrap/cache -type f -exec chmod 664 {} \\; # Clear stale caches before re-testing php artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear # Rebuild when stable php artisan config:cache php artisan route:cache php artisan view:cache # Reload FPM so workers/OPcache see changes sudo systemctl reload php8.2-fpm || sudo systemctl reload php8.1-fpm || true If permissions were the problem, random logouts and “unable to create directory” errors should be gone. See: Fix Laravel Permission Issues .\nVerify your drivers and stores Open your .env and confirm the intended drivers:\nSESSION_DRIVER=file # file|cookie|database|redis|memcached SESSION_LIFETIME=120 # minutes CACHE_DRIVER=file # file|redis|memcached|database|array CACHE_PREFIX=laravel_ # especially important on shared Redis Common pitfalls by driver File (default):\nMake sure storage/framework/sessions is writable by PHP-FPM. On high-traffic setups, file sessions can become slow; consider Redis or database. Database:\nRun php artisan session:table \u0026amp;\u0026amp; php artisan migrate. Verify the connection used in config/database.php matches what workers and web use. Redis:\nEnsure the same Redis host/port/db is used by all app processes (web, queue, scheduler). Use a CACHE_PREFIX and SESSION_CONNECTION/SESSION_PREFIX to avoid key collisions. If you run multiple apps on one Redis, prefixes are essential. Cookie:\nSession data lives in the cookie itself; if it exceeds browser limits (~4 KB), data may be truncated. Use another driver for larger payloads. Cookie settings that break logins Check these keys in .env and config/session.php:\nAPP_URL=https://example.com SESSION_DOMAIN=.example.com # include leading dot to cover subdomains SESSION_SECURE_COOKIE=true # true if you use HTTPS SESSION_SAME_SITE=lax # lax|strict|none (none requires secure cookie) SESSION_PATH=/ Guidelines:\nOn HTTPS, set SESSION_SECURE_COOKIE=true. If not, browsers may refuse to send cookies back. For subdomains, use .example.com as SESSION_DOMAIN. Mismatches cause “works on www, breaks on root” issues. If you embed cross-site (rare for app UIs), SAME_SITE=none requires SECURE_COOKIE=true. Proxies, load balancers, and sticky sessions Behind a load balancer, two things can break sessions:\nCookies stripped or altered by proxy headers. Configure trusted proxies so Laravel reads headers correctly. // app/Http/Middleware/TrustProxies.php protected $proxies = \u0026#39;*\u0026#39;; protected $headers = \\Illuminate\\Http\\Request::HEADER_X_FORWARDED_AWS_ELB; Non-sticky load balancing with file/database sessions. If each request hits a different server with different session store, users appear logged out. Solutions: Use a centralized store (Redis) for sessions. Or enable sticky sessions at the load balancer. Cache not updating (stale data) First confirm which store you’re reading from. Quick tinker check:\nphp artisan tinker \u0026gt;\u0026gt;\u0026gt; cache()-\u0026gt;put(\u0026#39;probe\u0026#39;, now()-\u0026gt;timestamp, 600) \u0026gt;\u0026gt;\u0026gt; cache()-\u0026gt;get(\u0026#39;probe\u0026#39;) If this works in CLI but not over HTTP, your web and CLI are using different PHP environments or configs. Check which php, PHP versions, and .env visibility for both. Also ensure workers (Horizon/queue) were restarted after changing config.\nRedis tips:\nUse CACHE_PREFIX to prevent collisions, especially when multiple apps share Redis. Check the selected database index (database in config/database.php for Redis) and that all processes agree. Inspect keys via redis-cli KEYS \u0026quot;laravel_*\u0026quot; | head for a quick sanity check. Don’t mix up config cache with app cache php artisan config:cache caches configuration, not your application cache keys. If .env changes “don’t work”, clear and rebuild config cache. For app data, use php artisan cache:clear or invalidate specific keys.\nQueues and schedulers read stale config After changing .env or config, restart workers so they reload the container:\nphp artisan queue:restart sudo systemctl restart supervisor || true For background on environment handling (and why CLI and web can differ), see: Laravel Environment Configuration .\nTest route to prove where the problem is Add a temporary route and controller/closure to reproduce:\n// routes/web.php Route::get(\u0026#39;/session-test\u0026#39;, function (\\Illuminate\\Http\\Request $request) { $count = session()-\u0026gt;get(\u0026#39;count\u0026#39;, 0) + 1; session([\u0026#39;count\u0026#39; =\u0026gt; $count]); cache()-\u0026gt;put(\u0026#39;session_probe\u0026#39;, $count, 600); return [ \u0026#39;count\u0026#39; =\u0026gt; $count, \u0026#39;session_id\u0026#39; =\u0026gt; $request-\u0026gt;session()-\u0026gt;getId(), \u0026#39;cache_probe\u0026#39; =\u0026gt; cache()-\u0026gt;get(\u0026#39;session_probe\u0026#39;), \u0026#39;driver\u0026#39; =\u0026gt; config(\u0026#39;session.driver\u0026#39;), \u0026#39;domain\u0026#39; =\u0026gt; config(\u0026#39;session.domain\u0026#39;), \u0026#39;secure\u0026#39; =\u0026gt; config(\u0026#39;session.secure\u0026#39;), \u0026#39;same_site\u0026#39; =\u0026gt; config(\u0026#39;session.same_site\u0026#39;), ]; }); Reload the page several times. If count resets to 1, the session cookie never comes back or the backend can’t persist. Use devtools (Application -\u0026gt; Cookies) to inspect cookie domain/secure flags.\nClean up stale sessions and caches Over time, old files and keys pile up:\nFile sessions: schedule a cleanup via Laravel’s scheduler or systemd timer that runs php artisan session:prune (Laravel 11+) or a custom command to delete expired files. Redis: set TTLs (sessions already expire), and occasionally sample keys with SCAN to ensure prefixes are consistent. Views/cache: add a deploy step that clears and then rebuilds caches to avoid mixing stale artifacts with new code. Production checklist # 1) Permissions (file sessions/views) sudo chown -R www-data:www-data storage bootstrap/cache sudo find storage bootstrap/cache -type d -exec chmod 775 {} \\; sudo find storage bootstrap/cache -type f -exec chmod 664 {} \\; # 2) Drivers and cookies grep -E \u0026#34;^(SESSION_|CACHE_|APP_URL=)\u0026#34; .env || true # 3) Clear/rebuild caches php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache # 4) Restart workers and reload FPM php artisan queue:restart || true sudo systemctl reload php8.2-fpm || true # 5) Verify Redis if used redis-cli PING || true Summary Most session and cache issues boil down to: wrong permissions or drivers, cookie settings (domain/secure/same-site), different environments between web and CLI/workers, or stale caches. Fix storage/permissions first, confirm drivers, set cookies correctly, clear then rebuild caches, restart workers, and reload PHP-FPM. Use the small test route to see exactly where it fails.\n","href":"/2025/09/fixing-laravel-session-cache-issues.html","title":"Fixing Laravel Session and Cache Issues Complete Troubleshooting Guide"},{"content":"Env problems often show up right after a deploy or a cache command: the app works locally but fails in production with “No application encryption key has been specified”, wrong database credentials, missing API keys, or stale config even after you edited .env. This happens because of how Laravel loads environment variables and how config caching freezes values.\nHow Laravel reads environment variables Laravel reads environment variables in two ways:\nDuring bootstrap from .env via Dotenv for local/dev and many simple servers. From the real process environment (what PHP‑FPM/Apache passes to PHP) when .env is not available or when you deploy containerized systems and set vars externally. When you run php artisan config:cache, Laravel compiles your configuration into a single PHP file (bootstrap/cache/config.php). From that point on, your app no longer looks at .env on each request. Any change in .env will not take effect until you clear and rebuild the config cache.\nCommon symptoms You updated .env but the app still uses old values. APP_KEY error (HTTP 500) after config:cache or fresh deploy. Queue/cron jobs using different values compared to web requests. Env vars set in Nginx/Apache don’t appear in config(). Fix sequence (production) From your project root on the server:\n# 1) Ensure the PHP user can read .env (but do not make it world-readable) sudo chown deploy:www-data .env # adjust users/groups sudo chmod 640 .env # 2) Clear stale caches php artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear # 3) Rebuild caches only after verifying .env and real environment php artisan config:cache php artisan route:cache php artisan view:cache # 4) Reload PHP-FPM to refresh OPcache and workers sudo systemctl reload php8.2-fpm || sudo systemctl reload php8.1-fpm || true APP_KEY and encryption If you see “No application encryption key has been specified”, your APP_KEY is empty, truncated, or the cached config is stale.\nphp artisan key:generate --force php artisan config:clear \u0026amp;\u0026amp; php artisan config:cache sudo systemctl reload php8.2-fpm || true Generate the key only once; persist it across deployments. Regenerating on a live app will invalidate encrypted data (sessions/cookies).\nWhere to set variables in production Prefer setting environment variables at the process level in production instead of editing .env on the server. This reduces drift and surprises:\nsystemd for PHP‑FPM: set variables in the PHP‑FPM pool or unit file. Nginx: pass variables to PHP via fastcgi_param. Apache: use SetEnv (mod_env) or PassEnv. Containers: set ENV at runtime (not in the image) and use secrets for sensitive values. Examples PHP‑FPM pool (Ubuntu) at /etc/php/8.2/fpm/pool.d/www.conf:\n; Make sure clear_env is disabled so PHP sees the environment clear_env = no env[APP_ENV] = production env[APP_DEBUG] = false env[DB_HOST] = 127.0.0.1 env[DB_DATABASE] = app env[DB_USERNAME] = app env[DB_PASSWORD] = secret Nginx location (complementary, sometimes used for a few vars):\nlocation ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php8.2-fpm.sock; fastcgi_param APP_ENV production; fastcgi_param APP_DEBUG 0; } Apache vhost:\n\u0026lt;VirtualHost *:80\u0026gt; DocumentRoot /var/www/app/current/public SetEnv APP_ENV production SetEnv APP_DEBUG false \u0026lt;/VirtualHost\u0026gt; Note: after you cache config, Laravel reads from the cached array, not from .env. Keep a single source of truth.\nQueues, Horizon, and cron may use different environments Common trap: web requests see the new env, but queue workers and scheduled jobs still use old values because they were started earlier.\nFix by restarting workers after changing any env or config:\nphp artisan queue:restart sudo systemctl restart supervisor || true # if you use Supervisor If you deploy via a script, add these steps right after cache rebuild and migrations.\nConfig cache pitfalls and tips Cache after setting env: Don’t run config:cache if your .env is incomplete; you’ll freeze the wrong values. Don’t hardcode env in config/*.php: Keep env('KEY') calls only in config files, not in application code. Immutable config between releases: Avoid editing .env on servers; use your deploy tool or infrastructure to inject env consistently. Clear first, then rebuild: config:clear before config:cache helps avoid stale entries. Match PHP versions across web/CLI: A different PHP binary used by CLI might look at a different php.ini or FPM pool. Troubleshooting checklist Print what the app sees (in a tinker shell or temporary route): php artisan tinker \u0026gt;\u0026gt;\u0026gt; config(\u0026#39;app.env\u0026#39;) \u0026gt;\u0026gt;\u0026gt; env(\u0026#39;APP_ENV\u0026#39;) // only reliable during bootstrap and in tinker Compare web vs CLI: php -v php -i | grep -i fpm which php Review logs for clues (permissions, parse errors, missing vars): tail -f storage/logs/laravel.log sudo journalctl -u php8.2-fpm -f For deeper diagnostics and structured logging techniques, read: Advanced Laravel Debugging with Logs .\nDeployment flow that avoids env drift Use a predictable deployment script:\n#!/usr/bin/env bash set -euo pipefail APP_DIR=/var/www/app/current PHP_SVC=php8.2-fpm cd \u0026#34;$APP_DIR\u0026#34; composer install --no-dev --prefer-dist --optimize-autoloader php artisan migrate --force php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan config:cache \u0026amp;\u0026amp; php artisan route:cache \u0026amp;\u0026amp; php artisan view:cache php artisan queue:restart || true sudo systemctl reload $PHP_SVC || true Security considerations for .env Never commit .env to git. Keep a .env.example without secrets. Least privilege: limit read access to the web/process user (e.g., chmod 640). Use secrets managers when possible, or OS‑level environment variables for production. Don’t expose .env via web root. Ensure your DocumentRoot points to public/. If you’re setting up from scratch, follow: Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide . Performance notes Configuration caching helps performance. Reload FPM so OPcache and workers see fresh code/config. For more tuning: Laravel Performance Optimization: 15 Techniques .\nSummary If .env changes don’t apply or config:cache breaks the app, do this: keep a single source of truth for env vars, clear then rebuild caches in order, restart workers, and reload PHP‑FPM. Prefer real environment variables in production over editing .env by hand. A small, repeatable deploy routine prevents most surprises.\n","href":"/2025/09/laravel-environment-configuration-env-issues.html","title":"Laravel Environment Configuration Fixing .env and Config Cache Issues"},{"content":"If a fresh deploy returns 403 or 500, the cause is usually predictable: wrong ownership/permissions, web server misconfig, missing PHP extensions, or SELinux. Use the checklist below to find and fix it quickly. Examples cover Ubuntu/Debian (Nginx/Apache with PHP‑FPM) and CentOS/RHEL (SELinux).\nWhy 403 vs 500 403 Forbidden from the web server: The server blocked access before Laravel ran. Common causes: wrong document root (not pointing to public/), missing try_files, directory or file not readable, SELinux contexts, or a security module (WAF/mod_security/Cloudflare) rejecting the request. 403 from Laravel: Authorization middleware/policies, CSRF token failures, or custom gates deny the action. 500 Internal Server Error: PHP crashed or threw an exception. Common causes: wrong permissions on storage/ or bootstrap/cache, missing PHP extensions, invalid .env, wrong APP_KEY, or syntax/runtime errors. Quick fix checklist (safe defaults) Run these commands from your project root (adjust the PHP‑FPM user for your distro):\n# 1) Identify your web user ps aux | egrep \u0026#34;php-fpm|php-fpm8|php7|php8|apache2|httpd\u0026#34; | grep -v grep # Ubuntu/Debian (Nginx/Apache): usually www-data # CentOS/RHEL (Nginx): usually nginx # 2) Set correct ownership for writable paths sudo chown -R www-data:www-data storage bootstrap/cache # 3) Apply safe, group-writable permissions sudo find storage bootstrap/cache -type d -exec chmod 775 {} \\; sudo find storage bootstrap/cache -type f -exec chmod 664 {} \\; # 4) If you deploy as a different user (e.g., deploy), share write access via ACLs sudo setfacl -R -m u:www-data:rwx -m u:$(whoami):rwx storage bootstrap/cache sudo setfacl -dR -m u:www-data:rwx -m u:$(whoami):rwx storage bootstrap/cache # 5) Clear and rebuild caches (after fixing perms) php artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear php artisan optimize # 6) Reload PHP-FPM to refresh OPcache sudo systemctl reload php8.2-fpm || sudo systemctl reload php8.1-fpm || true Never use 777. Prefer 775 for directories and 664 for files, with correct ownership/ACLs.\nServe from public/ and use try_files Laravel must be served from the public/ directory. If you point Nginx/Apache to the project root, you’ll get 403/404 and expose sensitive files.\nFor an end‑to‑end walkthrough of provisioning and deploying with Nginx and PHP‑FPM, see Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide .\nNginx example (Ubuntu):\nserver { server_name example.com; root /var/www/app/current/public; index index.php index.html; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { include snippets/fastcgi-php.conf; fastcgi_pass unix:/run/php/php8.2-fpm.sock; } location ~* \\.(?!well-known).* { # optional security hardening access_log off; } } Apache example:\n\u0026lt;VirtualHost *:80\u0026gt; ServerName example.com DocumentRoot /var/www/app/current/public \u0026lt;Directory /var/www/app/current/public\u0026gt; AllowOverride All Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; If you moved configs, confirm the socket path (or host:port) matches your PHP‑FPM version. A wrong fastcgi_pass leads to 502/500.\nFix permissions the right way Laravel writes logs, cache, compiled views, and sessions to storage/ and bootstrap/cache. If PHP‑FPM cannot write there, you’ll see 500 and log errors like “Permission denied”.\nRecommended pattern:\nOwnership: www-data:www-data (Ubuntu/Debian) or nginx:nginx (CentOS/RHEL) on storage/ and bootstrap/cache. Permissions: 775 for directories, 664 for files. Shared deploy scenario: If you deploy as deploy user, keep the project owned by deploy:deploy, add www-data to the group, and use setgid or ACLs: sudo usermod -aG www-data deploy sudo chgrp -R www-data storage bootstrap/cache sudo chmod -R g+rwX storage bootstrap/cache # Ensure new files inherit the group (setgid bit) sudo find storage bootstrap/cache -type d -exec chmod g+s {} \\; Or prefer ACL (more explicit and less brittle across releases):\nsudo setfacl -R -m u:www-data:rwx storage bootstrap/cache sudo setfacl -dR -m u:www-data:rwx storage bootstrap/cache SELinux on CentOS/RHEL If SELinux is enforcing, standard chmod/chown may not be enough. Label writable paths for the web server context:\nsudo chcon -R -t httpd_sys_rw_content_t storage bootstrap/cache sudo setsebool -P httpd_unified 1 # optional; unify contexts for httpd/php-fpm Avoid disabling SELinux; prefer correct contexts. Check denials with sudo ausearch -m avc -ts recent or review /var/log/audit/audit.log.\nEnvironment sanity checks APP_KEY: Must be set in production. If missing, sessions and encryption fail and can trigger 500. Generate once and keep it stable: php artisan key:generate --force .env permissions: Make it readable by the PHP‑FPM user but not world‑readable: sudo chown deploy:www-data .env sudo chmod 640 .env APP_DEBUG=false: Always disable debug in production. Keep detailed errors in logs, not on screen. If you keep hitting configuration pitfalls, review your config cache and .env handling practices, and prefer immutable environment variables in your runtime (e.g., systemd or Docker) over editing files in production.\nCache and optimize properly After deployments, clear stale caches and rebuild:\nphp artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear php artisan config:cache php artisan route:cache php artisan view:cache # Reload FPM to refresh OPcache sudo systemctl reload php8.2-fpm || sudo systemctl reload php8.1-fpm || true Want to push performance further? Try this next:\nLaravel Performance Optimization: 15 Techniques Double‑check Composer and PHP extensions A 500 can also come from missing extensions (e.g., pdo_mysql, mbstring, openssl, intl, xml, ctype, json, tokenizer, bcmath). Install the right set for your PHP version:\n# Ubuntu example (adjust php version) sudo apt-get update sudo apt-get install -y php8.2-fpm php8.2-cli php8.2-mysql php8.2-xml php8.2-mbstring php8.2-curl php8.2-intl php8.2-zip php8.2-bcmath # Deploy dependencies without dev and optimize autoloader composer install --no-dev --prefer-dist --optimize-autoloader Read the right logs When you still get 403/500, the logs tell you why:\n# Laravel app errors tail -f storage/logs/laravel.log # Nginx errors sudo journalctl -u nginx -f sudo tail -f /var/log/nginx/error.log # Apache errors sudo journalctl -u apache2 -f sudo tail -f /var/log/apache2/error.log # PHP-FPM errors sudo journalctl -u php8.2-fpm -f sudo tail -f /var/log/php8.2-fpm.log 2\u0026gt;/dev/null || true For deeper diagnostics and better logs, see: Advanced Laravel Debugging with Logs .\n403 from Laravel vs server If the 403 is generated by Laravel (you’ll see it in laravel.log), check:\nPolicies/Gates: confirm the authenticated user really has access. Middleware: role checks or custom guards. CSRF: webhooks and third‑party callbacks often need to be excluded from VerifyCsrfToken. CORS: a failed preflight can look like a blocked request; verify your CORS settings if you serve APIs. Broader troubleshooting tip: keep error logs clean, avoid noisy debugging in production, and reproduce issues locally with the same PHP version and extensions.\nHardening and good practices Avoid chmod -R 777. Use group write with setgid or ACLs instead. Keep storage/ and bootstrap/cache writable only by the web user and deploy user. Rotate logs to avoid full disks (e.g., logrotate). A full disk yields 500s when Laravel cannot write logs. Run queues/scheduled jobs under the same user that has access to storage/. Validate symlinks if you deploy with releases: ensure current/ points to the latest and storage links are intact. Security hardening reference:\nLaravel Security Best Practices for Production Example deploy snippet This minimal script makes repeated deployments predictable:\n#!/usr/bin/env bash set -euo pipefail APP_DIR=/var/www/app/current PHP_SVC=php8.2-fpm WEB_USER=www-data cd \u0026#34;$APP_DIR\u0026#34; composer install --no-dev --prefer-dist --optimize-autoloader php artisan migrate --force # Permissions sudo chown -R $WEB_USER:$WEB_USER storage bootstrap/cache sudo find storage bootstrap/cache -type d -exec chmod 775 {} \\; sudo find storage bootstrap/cache -type f -exec chmod 664 {} \\; # Caches php artisan cache:clear \u0026amp;\u0026amp; php artisan config:clear \u0026amp;\u0026amp; php artisan route:clear \u0026amp;\u0026amp; php artisan view:clear php artisan optimize sudo systemctl reload $PHP_SVC If you run workers, ensure the worker user has the same permissions as your web user, and reload workers after deployments to pick up new code and config.\nSummary Most 403/500 issues after a Laravel deploy are solved by four things: serve from public/ with a correct try_files; give PHP‑FPM write access to storage/ and bootstrap/cache with safe permissions; ensure your environment and PHP extensions are correct; and, on CentOS/RHEL, fix SELinux contexts. With those in place\u0026ndash;and a small deploy script\u0026ndash;you’ll have stable, repeatable releases without resorting to risky 777 permissions.\nFurther reading Deploy Laravel to VPS with Nginx \u0026ndash; Complete Guide Laravel Performance Optimization: 15 Techniques Advanced Laravel Debugging with Logs Laravel Security Best Practices for Production ","href":"/2025/09/fix-laravel-permission-issues-production.html","title":"Fix Laravel Permission Issues Solving 403 and 500 Errors on Production Server"},{"content":"Performance optimization is crucial for creating successful Laravel applications that provide excellent user experiences. Slow applications frustrate users, hurt SEO rankings, and can significantly impact business revenue. This comprehensive guide covers 15 proven techniques to dramatically improve your Laravel application\u0026rsquo;s performance.\nModern web users expect applications to load quickly and respond instantly to interactions. Studies show that even a one-second delay in page load time can reduce conversions by 7%. Laravel provides powerful tools and features to help you build fast applications, but knowing how to use them effectively makes all the difference.\n1. Database Query Optimization The database is often the primary bottleneck in Laravel applications. Optimizing your database queries can provide the most significant performance improvements.\nEliminate N+1 Query Problems The N+1 query problem occurs when you load a collection of models and then access related data for each model individually. This results in executing N+1 queries instead of just 2 queries.\n\u0026lt;?php // Bad: N+1 Query Problem $posts = Post::all(); foreach ($posts as $post) { echo $post-\u0026gt;user-\u0026gt;name; // This executes a query for each post } // Good: Use Eager Loading $posts = Post::with(\u0026#39;user\u0026#39;)-\u0026gt;get(); foreach ($posts as $post) { echo $post-\u0026gt;user-\u0026gt;name; // No additional queries needed } For more complex relationships, use nested eager loading:\n\u0026lt;?php $posts = Post::with([ \u0026#39;user\u0026#39;, \u0026#39;comments.user\u0026#39;, \u0026#39;tags\u0026#39; ])-\u0026gt;get(); Use Specific Columns in Select Queries Only select the columns you actually need instead of loading all columns with select *:\n\u0026lt;?php // Bad: Loads all columns $users = User::all(); // Good: Only load specific columns $users = User::select([\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;email\u0026#39;])-\u0026gt;get(); // Even better for relationships $posts = Post::with([\u0026#39;user:id,name\u0026#39;])-\u0026gt;select([\u0026#39;id\u0026#39;, \u0026#39;title\u0026#39;, \u0026#39;user_id\u0026#39;])-\u0026gt;get(); Implement Proper Database Indexing Database indexes dramatically improve query performance. Create indexes for columns frequently used in WHERE, ORDER BY, and JOIN clauses:\n\u0026lt;?php // In your migration Schema::table(\u0026#39;posts\u0026#39;, function (Blueprint $table) { $table-\u0026gt;index(\u0026#39;status\u0026#39;); $table-\u0026gt;index(\u0026#39;created_at\u0026#39;); $table-\u0026gt;index([\u0026#39;user_id\u0026#39;, \u0026#39;status\u0026#39;]); // Composite index }); 2. Implement Effective Caching Strategies Caching is one of the most effective ways to improve application performance by storing frequently accessed data in memory.\nQuery Result Caching Cache expensive database queries to avoid repeated execution:\n\u0026lt;?php use Illuminate\\Support\\Facades\\Cache; class PostService { public function getFeaturedPosts(): Collection { return Cache::remember(\u0026#39;featured_posts\u0026#39;, 3600, function () { return Post::where(\u0026#39;is_featured\u0026#39;, true) -\u0026gt;with([\u0026#39;user\u0026#39;, \u0026#39;category\u0026#39;]) -\u0026gt;orderBy(\u0026#39;created_at\u0026#39;, \u0026#39;desc\u0026#39;) -\u0026gt;limit(10) -\u0026gt;get(); }); } public function getPopularPostsByCategory(int $categoryId): Collection { $cacheKey = \u0026#34;popular_posts_category_{$categoryId}\u0026#34;; return Cache::remember($cacheKey, 1800, function () use ($categoryId) { return Post::where(\u0026#39;category_id\u0026#39;, $categoryId) -\u0026gt;withCount(\u0026#39;comments\u0026#39;) -\u0026gt;orderBy(\u0026#39;comments_count\u0026#39;, \u0026#39;desc\u0026#39;) -\u0026gt;limit(5) -\u0026gt;get(); }); } } Model Caching with Cache Tags Use cache tags for more granular cache invalidation:\n\u0026lt;?php class Post extends Model { protected static function booted() { static::saved(function ($post) { Cache::tags([\u0026#39;posts\u0026#39;, \u0026#34;category_{$post-\u0026gt;category_id}\u0026#34;])-\u0026gt;flush(); }); static::deleted(function ($post) { Cache::tags([\u0026#39;posts\u0026#39;, \u0026#34;category_{$post-\u0026gt;category_id}\u0026#34;])-\u0026gt;flush(); }); } } class PostService { public function getPostsByCategory(int $categoryId): Collection { return Cache::tags([\u0026#39;posts\u0026#39;, \u0026#34;category_{$categoryId}\u0026#34;]) -\u0026gt;remember(\u0026#34;posts_category_{$categoryId}\u0026#34;, 3600, function () use ($categoryId) { return Post::where(\u0026#39;category_id\u0026#39;, $categoryId)-\u0026gt;get(); }); } } 3. Optimize Eloquent Relationships Properly managing Eloquent relationships can significantly impact performance, especially when dealing with large datasets.\nUse Lazy Eager Loading When you don\u0026rsquo;t know in advance which relationships you\u0026rsquo;ll need, use lazy eager loading:\n\u0026lt;?php $posts = Post::all(); if ($shouldLoadComments) { $posts-\u0026gt;load(\u0026#39;comments.user\u0026#39;); } if ($shouldLoadTags) { $posts-\u0026gt;load(\u0026#39;tags\u0026#39;); } Implement Efficient Pagination Use cursor pagination for better performance with large datasets:\n\u0026lt;?php // Traditional pagination (can be slow with large offsets) $posts = Post::paginate(15); // Cursor pagination (more efficient for large datasets) $posts = Post::cursorPaginate(15); // For API responses with better performance class PostController extends Controller { public function index(Request $request) { $posts = Post::with(\u0026#39;user\u0026#39;) -\u0026gt;when($request-\u0026gt;cursor, function ($query, $cursor) { return $query-\u0026gt;cursorPaginate(20); }, function ($query) { return $query-\u0026gt;simplePaginate(20); }); return response()-\u0026gt;json($posts); } } 4. Use Database Raw Queries for Complex Operations Sometimes raw queries are more efficient than Eloquent for complex operations:\n\u0026lt;?php class ReportService { public function getMonthlyUserStats(int $year, int $month): array { $result = DB::select(\u0026#34; SELECT DATE(created_at) as date, COUNT(*) as new_users, COUNT(CASE WHEN email_verified_at IS NOT NULL THEN 1 END) as verified_users FROM users WHERE YEAR(created_at) = ? AND MONTH(created_at) = ? GROUP BY DATE(created_at) ORDER BY date \u0026#34;, [$year, $month]); return collect($result)-\u0026gt;toArray(); } public function updatePostViewCounts(array $postIds): void { $placeholders = str_repeat(\u0026#39;?,\u0026#39;, count($postIds) - 1) . \u0026#39;?\u0026#39;; DB::update(\u0026#34; UPDATE posts SET view_count = view_count + 1, updated_at = NOW() WHERE id IN ({$placeholders}) \u0026#34;, $postIds); } } 5. Implement Queue Jobs for Heavy Operations Move time-consuming operations to background jobs to improve user experience:\n\u0026lt;?php use Illuminate\\Bus\\Queueable; use Illuminate\\Contracts\\Queue\\ShouldQueue; use Illuminate\\Foundation\\Bus\\Dispatchable; use Illuminate\\Queue\\InteractsWithQueue; use Illuminate\\Queue\\SerializesModels; class ProcessLargeDatasetJob implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; public function __construct( private array $data, private int $userId ) {} public function handle(): void { $chunks = array_chunk($this-\u0026gt;data, 1000); foreach ($chunks as $chunk) { $this-\u0026gt;processChunk($chunk); } $user = User::find($this-\u0026gt;userId); $user-\u0026gt;notify(new DataProcessingCompleteNotification()); } private function processChunk(array $chunk): void { DB::transaction(function () use ($chunk) { foreach ($chunk as $item) { // Process each item ProcessedData::create($item); } }); } } // Usage in controller class DataController extends Controller { public function processData(Request $request) { $data = $request-\u0026gt;input(\u0026#39;data\u0026#39;); ProcessLargeDatasetJob::dispatch($data, auth()-\u0026gt;id()); return response()-\u0026gt;json([ \u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Data processing started. You will be notified when complete.\u0026#39; ]); } } 6. Optimize Configuration and Route Caching Laravel provides several caching mechanisms to improve bootstrap performance:\n# Cache configuration files php artisan config:cache # Cache routes php artisan route:cache # Cache views php artisan view:cache # Cache events and listeners php artisan event:cache # For production, run all optimizations php artisan optimize Create a deployment script to automate this process:\n\u0026lt;?php // deploy.php class DeploymentOptimizer { public static function optimize(): void { $commands = [ \u0026#39;config:cache\u0026#39;, \u0026#39;route:cache\u0026#39;, \u0026#39;view:cache\u0026#39;, \u0026#39;event:cache\u0026#39;, \u0026#39;optimize\u0026#39; ]; foreach ($commands as $command) { echo \u0026#34;Running: php artisan {$command}\\n\u0026#34;; Artisan::call($command); } echo \u0026#34;Optimization complete!\\n\u0026#34;; } } 7. Use Appropriate HTTP Caching Headers Implement proper HTTP caching to reduce server load and improve user experience:\n\u0026lt;?php class CacheMiddleware { public function handle(Request $request, Closure $next, int $minutes = 60) { $response = $next($request); if ($request-\u0026gt;isMethod(\u0026#39;GET\u0026#39;) \u0026amp;\u0026amp; $response-\u0026gt;getStatusCode() === 200) { $response-\u0026gt;headers-\u0026gt;set(\u0026#39;Cache-Control\u0026#39;, \u0026#34;public, max-age=\u0026#34; . ($minutes * 60)); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;Expires\u0026#39;, now()-\u0026gt;addMinutes($minutes)-\u0026gt;toRfc7231String()); // Add ETag for conditional requests $etag = md5($response-\u0026gt;getContent()); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;ETag\u0026#39;, $etag); if ($request-\u0026gt;getETags() \u0026amp;\u0026amp; in_array($etag, $request-\u0026gt;getETags())) { return response(\u0026#39;\u0026#39;, 304); } } return $response; } } // Apply to routes Route::middleware([\u0026#39;cache:120\u0026#39;])-\u0026gt;group(function () { Route::get(\u0026#39;/api/posts\u0026#39;, [PostController::class, \u0026#39;index\u0026#39;]); Route::get(\u0026#39;/api/posts/{post}\u0026#39;, [PostController::class, \u0026#39;show\u0026#39;]); }); 8. Optimize Asset Loading and Compilation Use Laravel Mix or Vite for efficient asset compilation and optimization:\n// vite.config.js import { defineConfig } from \u0026#39;vite\u0026#39;; import laravel from \u0026#39;laravel-vite-plugin\u0026#39;; export default defineConfig({ plugins: [ laravel({ input: [\u0026#39;resources/css/app.css\u0026#39;, \u0026#39;resources/js/app.js\u0026#39;], refresh: true, }), ], build: { rollupOptions: { output: { manualChunks: { vendor: [\u0026#39;vue\u0026#39;, \u0026#39;axios\u0026#39;], utils: [\u0026#39;lodash\u0026#39;, \u0026#39;moment\u0026#39;], } } } } }); 9. Implement Efficient Session Management Optimize session storage for better performance:\n\u0026lt;?php // config/session.php return [ \u0026#39;driver\u0026#39; =\u0026gt; env(\u0026#39;SESSION_DRIVER\u0026#39;, \u0026#39;redis\u0026#39;), \u0026#39;lifetime\u0026#39; =\u0026gt; env(\u0026#39;SESSION_LIFETIME\u0026#39;, 120), \u0026#39;expire_on_close\u0026#39; =\u0026gt; false, \u0026#39;encrypt\u0026#39; =\u0026gt; false, \u0026#39;files\u0026#39; =\u0026gt; storage_path(\u0026#39;framework/sessions\u0026#39;), \u0026#39;connection\u0026#39; =\u0026gt; env(\u0026#39;SESSION_CONNECTION\u0026#39;), \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;sessions\u0026#39;, \u0026#39;store\u0026#39; =\u0026gt; env(\u0026#39;SESSION_STORE\u0026#39;), \u0026#39;lottery\u0026#39; =\u0026gt; [2, 100], \u0026#39;cookie\u0026#39; =\u0026gt; env(\u0026#39;SESSION_COOKIE\u0026#39;, \u0026#39;laravel_session\u0026#39;), \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;/\u0026#39;, \u0026#39;domain\u0026#39; =\u0026gt; env(\u0026#39;SESSION_DOMAIN\u0026#39;), \u0026#39;secure\u0026#39; =\u0026gt; env(\u0026#39;SESSION_SECURE_COOKIE\u0026#39;, false), \u0026#39;http_only\u0026#39; =\u0026gt; true, \u0026#39;same_site\u0026#39; =\u0026gt; \u0026#39;lax\u0026#39;, ]; 10. Use Response Caching Middleware Create middleware for intelligent response caching:\n\u0026lt;?php class ResponseCacheMiddleware { public function handle(Request $request, Closure $next, ...$tags) { if ($request-\u0026gt;isMethod(\u0026#39;GET\u0026#39;)) { $cacheKey = $this-\u0026gt;generateCacheKey($request); if ($cached = Cache::get($cacheKey)) { return response($cached[\u0026#39;content\u0026#39;]) -\u0026gt;withHeaders($cached[\u0026#39;headers\u0026#39;]); } } $response = $next($request); if ($request-\u0026gt;isMethod(\u0026#39;GET\u0026#39;) \u0026amp;\u0026amp; $response-\u0026gt;getStatusCode() === 200) { $cacheData = [ \u0026#39;content\u0026#39; =\u0026gt; $response-\u0026gt;getContent(), \u0026#39;headers\u0026#39; =\u0026gt; $response-\u0026gt;headers-\u0026gt;all() ]; Cache::put($cacheKey, $cacheData, 3600); } return $response; } private function generateCacheKey(Request $request): string { return \u0026#39;response_\u0026#39; . md5($request-\u0026gt;fullUrl() . serialize($request-\u0026gt;user()?-\u0026gt;id)); } } 11. Optimize Database Connections Configure database connections for optimal performance:\n\u0026lt;?php // config/database.php \u0026#39;mysql\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;mysql\u0026#39;, \u0026#39;url\u0026#39; =\u0026gt; env(\u0026#39;DATABASE_URL\u0026#39;), \u0026#39;host\u0026#39; =\u0026gt; env(\u0026#39;DB_HOST\u0026#39;, \u0026#39;127.0.0.1\u0026#39;), \u0026#39;port\u0026#39; =\u0026gt; env(\u0026#39;DB_PORT\u0026#39;, \u0026#39;3306\u0026#39;), \u0026#39;database\u0026#39; =\u0026gt; env(\u0026#39;DB_DATABASE\u0026#39;, \u0026#39;forge\u0026#39;), \u0026#39;username\u0026#39; =\u0026gt; env(\u0026#39;DB_USERNAME\u0026#39;, \u0026#39;forge\u0026#39;), \u0026#39;password\u0026#39; =\u0026gt; env(\u0026#39;DB_PASSWORD\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;unix_socket\u0026#39; =\u0026gt; env(\u0026#39;DB_SOCKET\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;charset\u0026#39; =\u0026gt; \u0026#39;utf8mb4\u0026#39;, \u0026#39;collation\u0026#39; =\u0026gt; \u0026#39;utf8mb4_unicode_ci\u0026#39;, \u0026#39;prefix\u0026#39; =\u0026gt; \u0026#39;\u0026#39;, \u0026#39;prefix_indexes\u0026#39; =\u0026gt; true, \u0026#39;strict\u0026#39; =\u0026gt; true, \u0026#39;engine\u0026#39; =\u0026gt; null, \u0026#39;options\u0026#39; =\u0026gt; extension_loaded(\u0026#39;pdo_mysql\u0026#39;) ? array_filter([ PDO::MYSQL_ATTR_SSL_CA =\u0026gt; env(\u0026#39;MYSQL_ATTR_SSL_CA\u0026#39;), PDO::ATTR_PERSISTENT =\u0026gt; env(\u0026#39;DB_PERSISTENT\u0026#39;, true), PDO::MYSQL_ATTR_USE_BUFFERED_QUERY =\u0026gt; true, ]) : [], \u0026#39;dump\u0026#39; =\u0026gt; [ \u0026#39;dump_binary_path\u0026#39; =\u0026gt; \u0026#39;/usr/bin\u0026#39;, ], ], 12. Use Lazy Collections for Large Datasets Process large datasets efficiently with lazy collections:\n\u0026lt;?php class DataExportService { public function exportLargeDataset(): void { $filename = storage_path(\u0026#39;exports/large_dataset.csv\u0026#39;); $file = fopen($filename, \u0026#39;w\u0026#39;); // Write CSV header fputcsv($file, [\u0026#39;ID\u0026#39;, \u0026#39;Name\u0026#39;, \u0026#39;Email\u0026#39;, \u0026#39;Created At\u0026#39;]); // Process data in chunks using lazy collection User::lazy(1000)-\u0026gt;each(function (User $user) use ($file) { fputcsv($file, [ $user-\u0026gt;id, $user-\u0026gt;name, $user-\u0026gt;email, $user-\u0026gt;created_at-\u0026gt;toDateString() ]); }); fclose($file); } public function processLargeDataset(): void { Post::lazy(500) -\u0026gt;filter(function (Post $post) { return $post-\u0026gt;created_at-\u0026gt;isLastMonth(); }) -\u0026gt;each(function (Post $post) { $post-\u0026gt;update([\u0026#39;processed\u0026#39; =\u0026gt; true]); }); } } 13. Implement Efficient File Uploads Optimize file upload handling for better performance:\n\u0026lt;?php class FileUploadService { public function uploadLargeFile(UploadedFile $file, string $disk = \u0026#39;public\u0026#39;): array { $filename = $this-\u0026gt;generateFilename($file); $path = $file-\u0026gt;storeAs(\u0026#39;uploads\u0026#39;, $filename, $disk); // Process file in background for large files if ($file-\u0026gt;getSize() \u0026gt; 10 * 1024 * 1024) { // 10MB ProcessLargeFileJob::dispatch($path, $disk); } return [ \u0026#39;filename\u0026#39; =\u0026gt; $filename, \u0026#39;path\u0026#39; =\u0026gt; $path, \u0026#39;size\u0026#39; =\u0026gt; $file-\u0026gt;getSize(), \u0026#39;mime_type\u0026#39; =\u0026gt; $file-\u0026gt;getMimeType() ]; } private function generateFilename(UploadedFile $file): string { $timestamp = now()-\u0026gt;format(\u0026#39;Y/m/d\u0026#39;); $hash = Str::random(40); $extension = $file-\u0026gt;getClientOriginalExtension(); return \u0026#34;{$timestamp}/{$hash}.{$extension}\u0026#34;; } } class ProcessLargeFileJob implements ShouldQueue { use Dispatchable, InteractsWithQueue, Queueable, SerializesModels; public function __construct( private string $path, private string $disk ) {} public function handle(): void { // Process the file: resize images, extract metadata, etc. $fullPath = Storage::disk($this-\u0026gt;disk)-\u0026gt;path($this-\u0026gt;path); if ($this-\u0026gt;isImage($fullPath)) { $this-\u0026gt;processImage($fullPath); } } private function isImage(string $path): bool { $imageTypes = [\u0026#39;image/jpeg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/gif\u0026#39;, \u0026#39;image/webp\u0026#39;]; return in_array(mime_content_type($path), $imageTypes); } private function processImage(string $path): void { // Create thumbnails, optimize images, etc. } } 14. Monitor and Profile Performance Use Laravel\u0026rsquo;s built-in tools and third-party packages for performance monitoring:\n\u0026lt;?php class PerformanceMiddleware { public function handle(Request $request, Closure $next) { $startTime = microtime(true); $startMemory = memory_get_usage(true); $response = $next($request); $executionTime = (microtime(true) - $startTime) * 1000; $memoryUsage = (memory_get_usage(true) - $startMemory) / 1024 / 1024; if ($executionTime \u0026gt; 1000) { // Log slow requests Log::warning(\u0026#39;Slow request detected\u0026#39;, [ \u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;fullUrl(), \u0026#39;method\u0026#39; =\u0026gt; $request-\u0026gt;method(), \u0026#39;execution_time\u0026#39; =\u0026gt; $executionTime . \u0026#39;ms\u0026#39;, \u0026#39;memory_usage\u0026#39; =\u0026gt; $memoryUsage . \u0026#39;MB\u0026#39;, \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id() ]); } // Add performance headers in debug mode if (config(\u0026#39;app.debug\u0026#39;)) { $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Execution-Time\u0026#39;, $executionTime . \u0026#39;ms\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Memory-Usage\u0026#39;, $memoryUsage . \u0026#39;MB\u0026#39;); } return $response; } } 15. Use Production-Optimized Server Configuration Optimize your server configuration for Laravel applications:\n# nginx.conf optimizations server { listen 80; server_name example.com; root /var/www/html/public; index index.php; # Gzip compression gzip on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml; gzip_min_length 1000; # Cache static assets location ~* \\.(jpg|jpeg|png|gif|ico|css|js|woff|woff2)$ { expires 1y; add_header Cache-Control \u0026#34;public, immutable\u0026#34;; add_header Vary Accept-Encoding; } # PHP-FPM configuration location ~ \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; # Optimization headers fastcgi_buffer_size 128k; fastcgi_buffers 4 256k; fastcgi_busy_buffers_size 256k; } } Performance Testing and Monitoring Create automated performance tests to ensure optimizations are working:\n\u0026lt;?php namespace Tests\\Performance; use Tests\\TestCase; use Illuminate\\Foundation\\Testing\\RefreshDatabase; class ApplicationPerformanceTest extends TestCase { public function test_homepage_loads_quickly(): void { $startTime = microtime(true); $response = $this-\u0026gt;get(\u0026#39;/\u0026#39;); $executionTime = (microtime(true) - $startTime) * 1000; $response-\u0026gt;assertStatus(200); $this-\u0026gt;assertLessThan(500, $executionTime, \u0026#39;Homepage should load in under 500ms\u0026#39;); } public function test_api_endpoints_perform_well(): void { $user = User::factory()-\u0026gt;create(); $startTime = microtime(true); $response = $this-\u0026gt;actingAs($user)-\u0026gt;getJson(\u0026#39;/api/posts\u0026#39;); $executionTime = (microtime(true) - $startTime) * 1000; $response-\u0026gt;assertStatus(200); $this-\u0026gt;assertLessThan(300, $executionTime, \u0026#39;API should respond in under 300ms\u0026#39;); } } Conclusion Performance optimization is an ongoing process that requires careful monitoring and continuous improvement. These 15 techniques provide a solid foundation for building fast Laravel applications, but the specific optimizations you need will depend on your application\u0026rsquo;s unique requirements and bottlenecks.\nStart with the techniques that provide the biggest impact for your specific use case, typically database query optimization and caching. Monitor your application\u0026rsquo;s performance regularly and apply optimizations systematically rather than trying to implement everything at once.\nRemember that premature optimization can sometimes make code more complex without providing significant benefits. Always measure performance before and after optimizations to ensure they\u0026rsquo;re actually improving your application\u0026rsquo;s speed and user experience.\nWant to take your Laravel skills to the next level? Discover proven strategies in our Clean Code Laravel: Project Structure Guide and essential Production Security Best Practices for bulletproof applications.\n","href":"/2025/09/laravel-performance-optimization-15-techniques.html","title":"15 Essential Techniques for Fast Applications"},{"content":"Security is paramount when deploying Laravel applications to production environments. A single vulnerability can compromise user data, damage your reputation, and result in significant financial losses. This comprehensive guide covers essential security practices to protect your Laravel applications from common threats and vulnerabilities.\nLaravel provides excellent security features out of the box, but proper implementation and additional security measures are crucial for production deployments. From authentication and authorization to data protection and server hardening, every layer of your application stack requires careful attention to security details.\n1. Authentication and Authorization Security Proper authentication and authorization form the foundation of application security. Laravel provides robust tools, but they must be configured correctly for production use.\nImplement Strong Password Policies Enforce strong password requirements to prevent brute force attacks and improve overall security:\n\u0026lt;?php namespace App\\Rules; use Illuminate\\Contracts\\Validation\\Rule; class StrongPassword implements Rule { public function passes($attribute, $value): bool { // At least 12 characters long if (strlen($value) \u0026lt; 12) { return false; } // Contains uppercase letter if (!preg_match(\u0026#39;/[A-Z]/\u0026#39;, $value)) { return false; } // Contains lowercase letter if (!preg_match(\u0026#39;/[a-z]/\u0026#39;, $value)) { return false; } // Contains number if (!preg_match(\u0026#39;/[0-9]/\u0026#39;, $value)) { return false; } // Contains special character if (!preg_match(\u0026#39;/[^A-Za-z0-9]/\u0026#39;, $value)) { return false; } // Check against common passwords $commonPasswords = [\u0026#39;password123\u0026#39;, \u0026#39;123456789\u0026#39;, \u0026#39;qwerty123\u0026#39;]; if (in_array(strtolower($value), $commonPasswords)) { return false; } return true; } public function message(): string { return \u0026#39;Password must be at least 12 characters and contain uppercase, lowercase, number, and special character.\u0026#39;; } } class RegisterRequest extends FormRequest { public function rules(): array { return [ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email|unique:users\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; [\u0026#39;required\u0026#39;, \u0026#39;confirmed\u0026#39;, new StrongPassword()], ]; } } Implement Rate Limiting for Authentication Protect against brute force attacks with intelligent rate limiting:\n\u0026lt;?php namespace App\\Http\\Controllers\\Auth; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\RateLimiter; use Illuminate\\Support\\Str; class LoginController extends Controller { public function login(Request $request) { $throttleKey = $this-\u0026gt;throttleKey($request); if (RateLimiter::tooManyAttempts($throttleKey, 5)) { $seconds = RateLimiter::availableIn($throttleKey); return response()-\u0026gt;json([ \u0026#39;message\u0026#39; =\u0026gt; \u0026#34;Too many login attempts. Please try again in {$seconds} seconds.\u0026#34; ], 429); } $credentials = $request-\u0026gt;validate([ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;required|email\u0026#39;, \u0026#39;password\u0026#39; =\u0026gt; \u0026#39;required\u0026#39; ]); if (Auth::attempt($credentials)) { RateLimiter::clear($throttleKey); // Log successful login Log::info(\u0026#39;User logged in\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;ip\u0026#39; =\u0026gt; $request-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; $request-\u0026gt;userAgent() ]); return redirect()-\u0026gt;intended(\u0026#39;/dashboard\u0026#39;); } RateLimiter::hit($throttleKey); // Log failed login attempt Log::warning(\u0026#39;Failed login attempt\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $request-\u0026gt;email, \u0026#39;ip\u0026#39; =\u0026gt; $request-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; $request-\u0026gt;userAgent() ]); return back()-\u0026gt;withErrors([ \u0026#39;email\u0026#39; =\u0026gt; \u0026#39;The provided credentials do not match our records.\u0026#39;, ]); } private function throttleKey(Request $request): string { return Str::lower($request-\u0026gt;input(\u0026#39;email\u0026#39;)) . \u0026#39;|\u0026#39; . $request-\u0026gt;ip(); } } Multi-Factor Authentication Implementation Add an extra layer of security with 2FA:\n\u0026lt;?php namespace App\\Services; use App\\Models\\User; use PragmaRX\\Google2FA\\Google2FA; use SimpleSoftwareIO\\QrCode\\Facades\\QrCode; class TwoFactorAuthService { private Google2FA $google2fa; public function __construct() { $this-\u0026gt;google2fa = new Google2FA(); } public function generateSecretKey(): string { return $this-\u0026gt;google2fa-\u0026gt;generateSecretKey(); } public function getQRCodeUrl(User $user, string $secret): string { return $this-\u0026gt;google2fa-\u0026gt;getQRCodeUrl( config(\u0026#39;app.name\u0026#39;), $user-\u0026gt;email, $secret ); } public function verifyCode(string $secret, string $code): bool { return $this-\u0026gt;google2fa-\u0026gt;verifyKey($secret, $code); } public function enable2FA(User $user, string $code): bool { if (!$this-\u0026gt;verifyCode($user-\u0026gt;two_factor_secret, $code)) { return false; } $user-\u0026gt;update([ \u0026#39;two_factor_enabled\u0026#39; =\u0026gt; true, \u0026#39;two_factor_confirmed_at\u0026#39; =\u0026gt; now() ]); return true; } } class TwoFactorMiddleware { public function handle(Request $request, Closure $next) { $user = auth()-\u0026gt;user(); if ($user \u0026amp;\u0026amp; $user-\u0026gt;two_factor_enabled \u0026amp;\u0026amp; !session(\u0026#39;2fa_verified\u0026#39;)) { return redirect()-\u0026gt;route(\u0026#39;2fa.verify\u0026#39;); } return $next($request); } } 2. Input Validation and Sanitization Proper input validation prevents many security vulnerabilities including SQL injection, XSS, and data corruption.\nComprehensive Request Validation Create robust validation rules for all user inputs:\n\u0026lt;?php namespace App\\Http\\Requests; use Illuminate\\Foundation\\Http\\FormRequest; class CreatePostRequest extends FormRequest { public function rules(): array { return [ \u0026#39;title\u0026#39; =\u0026gt; [ \u0026#39;required\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;max:255\u0026#39;, \u0026#39;regex:/^[a-zA-Z0-9\\s\\-_.,!?]+$/\u0026#39; // Only allow safe characters ], \u0026#39;content\u0026#39; =\u0026gt; [ \u0026#39;required\u0026#39;, \u0026#39;string\u0026#39;, \u0026#39;max:50000\u0026#39; ], \u0026#39;category_id\u0026#39; =\u0026gt; \u0026#39;required|exists:categories,id\u0026#39;, \u0026#39;tags\u0026#39; =\u0026gt; \u0026#39;array|max:10\u0026#39;, \u0026#39;tags.*\u0026#39; =\u0026gt; \u0026#39;string|max:50|regex:/^[a-zA-Z0-9\\-_]+$/\u0026#39;, \u0026#39;featured_image\u0026#39; =\u0026gt; \u0026#39;nullable|image|max:2048|mimes:jpeg,png,webp\u0026#39;, \u0026#39;publish_at\u0026#39; =\u0026gt; \u0026#39;nullable|date|after:now\u0026#39; ]; } public function sanitizeInput(): array { $input = $this-\u0026gt;validated(); // Sanitize HTML content $input[\u0026#39;content\u0026#39;] = $this-\u0026gt;sanitizeHtml($input[\u0026#39;content\u0026#39;]); // Sanitize title $input[\u0026#39;title\u0026#39;] = strip_tags(trim($input[\u0026#39;title\u0026#39;])); return $input; } private function sanitizeHtml(string $content): string { $allowedTags = \u0026#39;\u0026lt;p\u0026gt;\u0026lt;br\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a\u0026gt;\u0026lt;h2\u0026gt;\u0026lt;h3\u0026gt;\u0026lt;h4\u0026gt;\u0026lt;blockquote\u0026gt;\u0026#39;; $content = strip_tags($content, $allowedTags); // Remove potentially dangerous attributes $content = preg_replace(\u0026#39;/(\u0026lt;[^\u0026gt;]*) on\\w+=\u0026#34;[^\u0026#34;]*\u0026#34;/i\u0026#39;, \u0026#39;$1\u0026#39;, $content); $content = preg_replace(\u0026#39;/(\u0026lt;[^\u0026gt;]*) style=\u0026#34;[^\u0026#34;]*\u0026#34;/i\u0026#39;, \u0026#39;$1\u0026#39;, $content); return $content; } public function messages(): array { return [ \u0026#39;title.regex\u0026#39; =\u0026gt; \u0026#39;Title contains invalid characters.\u0026#39;, \u0026#39;tags.*.regex\u0026#39; =\u0026gt; \u0026#39;Tags can only contain letters, numbers, hyphens, and underscores.\u0026#39;, ]; } } SQL Injection Prevention Always use parameterized queries and Eloquent ORM properly:\n\u0026lt;?php namespace App\\Services; use App\\Models\\Post; use Illuminate\\Database\\Eloquent\\Collection; use Illuminate\\Support\\Facades\\DB; class PostSearchService { public function search(string $query, array $filters = []): Collection { // Good: Using Eloquent query builder (parameterized) $posts = Post::query() -\u0026gt;when($query, function ($q) use ($query) { $q-\u0026gt;where(\u0026#39;title\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#39;%\u0026#39; . $query . \u0026#39;%\u0026#39;) -\u0026gt;orWhere(\u0026#39;content\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#39;%\u0026#39; . $query . \u0026#39;%\u0026#39;); }) -\u0026gt;when($filters[\u0026#39;category\u0026#39;] ?? null, function ($q, $category) { $q-\u0026gt;where(\u0026#39;category_id\u0026#39;, $category); }) -\u0026gt;when($filters[\u0026#39;author\u0026#39;] ?? null, function ($q, $author) { $q-\u0026gt;where(\u0026#39;user_id\u0026#39;, $author); }) -\u0026gt;published() -\u0026gt;orderBy(\u0026#39;created_at\u0026#39;, \u0026#39;desc\u0026#39;) -\u0026gt;get(); return $posts; } public function complexSearch(array $criteria): array { // Good: Using parameterized raw queries when needed $results = DB::select(\u0026#34; SELECT p.*, u.name as author_name, c.name as category_name FROM posts p JOIN users u ON p.user_id = u.id JOIN categories c ON p.category_id = c.id WHERE p.status = \u0026#39;published\u0026#39; AND p.created_at \u0026gt;= ? AND (p.title LIKE ? OR p.content LIKE ?) ORDER BY p.created_at DESC LIMIT ? \u0026#34;, [ $criteria[\u0026#39;date_from\u0026#39;], \u0026#39;%\u0026#39; . $criteria[\u0026#39;search\u0026#39;] . \u0026#39;%\u0026#39;, \u0026#39;%\u0026#39; . $criteria[\u0026#39;search\u0026#39;] . \u0026#39;%\u0026#39;, $criteria[\u0026#39;limit\u0026#39;] ?? 20 ]); return collect($results)-\u0026gt;toArray(); } // Bad example - NEVER do this private function badSearchExample(string $query): Collection { // This is vulnerable to SQL injection return DB::select(\u0026#34;SELECT * FROM posts WHERE title LIKE \u0026#39;%{$query}%\u0026#39;\u0026#34;); } } 3. Cross-Site Request Forgery (CSRF) Protection Laravel\u0026rsquo;s CSRF protection is enabled by default, but proper implementation is crucial:\n\u0026lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Http\\Request; class VerifyCsrfToken extends \\Illuminate\\Foundation\\Http\\Middleware\\VerifyCsrfToken { protected $except = [ // Only add routes that absolutely need to be excluded \u0026#39;api/webhooks/*\u0026#39;, // External webhook endpoints ]; public function handle($request, Closure $next) { // Add additional CSRF checks for sensitive operations if ($this-\u0026gt;isSensitiveOperation($request)) { $this-\u0026gt;validateCsrfToken($request); } return parent::handle($request, $next); } private function isSensitiveOperation(Request $request): bool { $sensitiveRoutes = [ \u0026#39;user/delete\u0026#39;, \u0026#39;admin/*\u0026#39;, \u0026#39;payment/process\u0026#39; ]; foreach ($sensitiveRoutes as $route) { if ($request-\u0026gt;is($route)) { return true; } } return false; } } // API CSRF protection for SPA applications class ApiCsrfMiddleware { public function handle(Request $request, Closure $next) { // For API routes, use double submit cookies if ($request-\u0026gt;isMethod(\u0026#39;post\u0026#39;) || $request-\u0026gt;isMethod(\u0026#39;put\u0026#39;) || $request-\u0026gt;isMethod(\u0026#39;delete\u0026#39;)) { $headerToken = $request-\u0026gt;header(\u0026#39;X-CSRF-TOKEN\u0026#39;); $cookieToken = $request-\u0026gt;cookie(\u0026#39;XSRF-TOKEN\u0026#39;); if (!$headerToken || !$cookieToken || $headerToken !== $cookieToken) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;CSRF token mismatch\u0026#39;], 419); } } return $next($request); } } 4. Cross-Site Scripting (XSS) Protection Prevent XSS attacks through proper output encoding and Content Security Policy:\n\u0026lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Http\\Request; class ContentSecurityPolicy { public function handle(Request $request, Closure $next) { $response = $next($request); $csp = [ \u0026#34;default-src \u0026#39;self\u0026#39;\u0026#34;, \u0026#34;script-src \u0026#39;self\u0026#39; \u0026#39;unsafe-inline\u0026#39; https://cdn.jsdelivr.net https://unpkg.com\u0026#34;, \u0026#34;style-src \u0026#39;self\u0026#39; \u0026#39;unsafe-inline\u0026#39; https://fonts.googleapis.com https://cdn.jsdelivr.net\u0026#34;, \u0026#34;font-src \u0026#39;self\u0026#39; https://fonts.gstatic.com\u0026#34;, \u0026#34;img-src \u0026#39;self\u0026#39; data: https:\u0026#34;, \u0026#34;connect-src \u0026#39;self\u0026#39;\u0026#34;, \u0026#34;frame-ancestors \u0026#39;none\u0026#39;\u0026#34;, \u0026#34;base-uri \u0026#39;self\u0026#39;\u0026#34;, \u0026#34;form-action \u0026#39;self\u0026#39;\u0026#34; ]; $response-\u0026gt;headers-\u0026gt;set(\u0026#39;Content-Security-Policy\u0026#39;, implode(\u0026#39;; \u0026#39;, $csp)); // Additional security headers $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Content-Type-Options\u0026#39;, \u0026#39;nosniff\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Frame-Options\u0026#39;, \u0026#39;DENY\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-XSS-Protection\u0026#39;, \u0026#39;1; mode=block\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;Referrer-Policy\u0026#39;, \u0026#39;strict-origin-when-cross-origin\u0026#39;); return $response; } } // Helper for safe output in Blade templates class SecurityHelper { public static function sanitizeOutput(string $content, bool $allowHtml = false): string { if (!$allowHtml) { return htmlspecialchars($content, ENT_QUOTES, \u0026#39;UTF-8\u0026#39;); } // For HTML content, use a whitelist approach $allowedTags = \u0026#39;\u0026lt;p\u0026gt;\u0026lt;br\u0026gt;\u0026lt;strong\u0026gt;\u0026lt;em\u0026gt;\u0026lt;ul\u0026gt;\u0026lt;ol\u0026gt;\u0026lt;li\u0026gt;\u0026lt;a\u0026gt;\u0026lt;h2\u0026gt;\u0026lt;h3\u0026gt;\u0026lt;h4\u0026gt;\u0026#39;; $cleaned = strip_tags($content, $allowedTags); // Remove dangerous attributes $cleaned = preg_replace(\u0026#39;/(\u0026lt;[^\u0026gt;]*) on\\w+=\u0026#34;[^\u0026#34;]*\u0026#34;/i\u0026#39;, \u0026#39;$1\u0026#39;, $cleaned); $cleaned = preg_replace(\u0026#39;/(\u0026lt;[^\u0026gt;]*) style=\u0026#34;[^\u0026#34;]*\u0026#34;/i\u0026#39;, \u0026#39;$1\u0026#39;, $cleaned); $cleaned = preg_replace(\u0026#39;/javascript:/i\u0026#39;, \u0026#39;\u0026#39;, $cleaned); return $cleaned; } } 5. File Upload Security Secure file upload handling prevents malicious file execution and server compromise:\n\u0026lt;?php namespace App\\Services; use Illuminate\\Http\\UploadedFile; use Illuminate\\Support\\Facades\\Storage; use Illuminate\\Support\\Str; class SecureFileUploadService { private array $allowedMimeTypes = [ \u0026#39;image/jpeg\u0026#39;, \u0026#39;image/png\u0026#39;, \u0026#39;image/webp\u0026#39;, \u0026#39;application/pdf\u0026#39;, \u0026#39;text/plain\u0026#39; ]; private array $allowedExtensions = [ \u0026#39;jpg\u0026#39;, \u0026#39;jpeg\u0026#39;, \u0026#39;png\u0026#39;, \u0026#39;webp\u0026#39;, \u0026#39;pdf\u0026#39;, \u0026#39;txt\u0026#39; ]; private int $maxFileSize = 5 * 1024 * 1024; // 5MB public function uploadFile(UploadedFile $file, string $directory = \u0026#39;uploads\u0026#39;): array { $this-\u0026gt;validateFile($file); $filename = $this-\u0026gt;generateSecureFilename($file); $path = $directory . \u0026#39;/\u0026#39; . $filename; // Store file outside web root $disk = config(\u0026#39;app.env\u0026#39;) === \u0026#39;production\u0026#39; ? \u0026#39;private\u0026#39; : \u0026#39;public\u0026#39;; // Scan file for malware (if antivirus service available) $this-\u0026gt;scanFile($file); $storedPath = $file-\u0026gt;storeAs($directory, $filename, $disk); // Log file upload Log::info(\u0026#39;File uploaded\u0026#39;, [ \u0026#39;filename\u0026#39; =\u0026gt; $filename, \u0026#39;size\u0026#39; =\u0026gt; $file-\u0026gt;getSize(), \u0026#39;mime_type\u0026#39; =\u0026gt; $file-\u0026gt;getMimeType(), \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;ip\u0026#39; =\u0026gt; request()-\u0026gt;ip() ]); return [ \u0026#39;filename\u0026#39; =\u0026gt; $filename, \u0026#39;path\u0026#39; =\u0026gt; $storedPath, \u0026#39;size\u0026#39; =\u0026gt; $file-\u0026gt;getSize(), \u0026#39;mime_type\u0026#39; =\u0026gt; $file-\u0026gt;getMimeType() ]; } private function validateFile(UploadedFile $file): void { // Check file size if ($file-\u0026gt;getSize() \u0026gt; $this-\u0026gt;maxFileSize) { throw new \\InvalidArgumentException(\u0026#39;File size exceeds maximum allowed size.\u0026#39;); } // Verify MIME type $mimeType = $file-\u0026gt;getMimeType(); if (!in_array($mimeType, $this-\u0026gt;allowedMimeTypes)) { throw new \\InvalidArgumentException(\u0026#39;File type not allowed.\u0026#39;); } // Verify file extension $extension = strtolower($file-\u0026gt;getClientOriginalExtension()); if (!in_array($extension, $this-\u0026gt;allowedExtensions)) { throw new \\InvalidArgumentException(\u0026#39;File extension not allowed.\u0026#39;); } // Additional checks for image files if (str_starts_with($mimeType, \u0026#39;image/\u0026#39;)) { $this-\u0026gt;validateImageFile($file); } } private function validateImageFile(UploadedFile $file): void { // Verify it\u0026#39;s actually an image $imageInfo = getimagesize($file-\u0026gt;getRealPath()); if (!$imageInfo) { throw new \\InvalidArgumentException(\u0026#39;Invalid image file.\u0026#39;); } // Check image dimensions [$width, $height] = $imageInfo; if ($width \u0026gt; 4000 || $height \u0026gt; 4000) { throw new \\InvalidArgumentException(\u0026#39;Image dimensions too large.\u0026#39;); } } private function generateSecureFilename(UploadedFile $file): string { $extension = $file-\u0026gt;getClientOriginalExtension(); $hash = hash(\u0026#39;sha256\u0026#39;, $file-\u0026gt;getClientOriginalName() . time() . Str::random(10)); return substr($hash, 0, 32) . \u0026#39;.\u0026#39; . $extension; } private function scanFile(UploadedFile $file): void { // Implement virus scanning if available // This could integrate with ClamAV or similar service $content = file_get_contents($file-\u0026gt;getRealPath()); // Basic malicious pattern detection $maliciousPatterns = [ \u0026#39;/\u0026lt;\\?php/i\u0026#39;, \u0026#39;/\u0026lt;script/i\u0026#39;, \u0026#39;/eval\\(/i\u0026#39;, \u0026#39;/exec\\(/i\u0026#39;, \u0026#39;/system\\(/i\u0026#39; ]; foreach ($maliciousPatterns as $pattern) { if (preg_match($pattern, $content)) { throw new \\InvalidArgumentException(\u0026#39;File contains potentially malicious content.\u0026#39;); } } } } 6. Session Security Configure sessions securely to prevent session hijacking and fixation:\n\u0026lt;?php // config/session.php return [ \u0026#39;driver\u0026#39; =\u0026gt; env(\u0026#39;SESSION_DRIVER\u0026#39;, \u0026#39;redis\u0026#39;), \u0026#39;lifetime\u0026#39; =\u0026gt; env(\u0026#39;SESSION_LIFETIME\u0026#39;, 120), \u0026#39;expire_on_close\u0026#39; =\u0026gt; true, \u0026#39;encrypt\u0026#39; =\u0026gt; true, \u0026#39;files\u0026#39; =\u0026gt; storage_path(\u0026#39;framework/sessions\u0026#39;), \u0026#39;connection\u0026#39; =\u0026gt; env(\u0026#39;SESSION_CONNECTION\u0026#39;), \u0026#39;table\u0026#39; =\u0026gt; \u0026#39;sessions\u0026#39;, \u0026#39;store\u0026#39; =\u0026gt; env(\u0026#39;SESSION_STORE\u0026#39;), \u0026#39;lottery\u0026#39; =\u0026gt; [2, 100], \u0026#39;cookie\u0026#39; =\u0026gt; env( \u0026#39;SESSION_COOKIE\u0026#39;, Str::slug(env(\u0026#39;APP_NAME\u0026#39;, \u0026#39;laravel\u0026#39;), \u0026#39;_\u0026#39;).\u0026#39;_session\u0026#39; ), \u0026#39;path\u0026#39; =\u0026gt; \u0026#39;/\u0026#39;, \u0026#39;domain\u0026#39; =\u0026gt; env(\u0026#39;SESSION_DOMAIN\u0026#39;), \u0026#39;secure\u0026#39; =\u0026gt; env(\u0026#39;SESSION_SECURE_COOKIE\u0026#39;, true), \u0026#39;http_only\u0026#39; =\u0026gt; true, \u0026#39;same_site\u0026#39; =\u0026gt; \u0026#39;strict\u0026#39;, ]; namespace App\\Http\\Middleware; use Closure; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Auth; class SecureSession { public function handle(Request $request, Closure $next) { // Regenerate session ID on login if ($request-\u0026gt;user() \u0026amp;\u0026amp; !session(\u0026#39;session_regenerated\u0026#39;)) { $request-\u0026gt;session()-\u0026gt;regenerate(); session([\u0026#39;session_regenerated\u0026#39; =\u0026gt; true]); } // Check for session hijacking $this-\u0026gt;checkSessionSecurity($request); return $next($request); } private function checkSessionSecurity(Request $request): void { // Check if user agent changed $currentUserAgent = $request-\u0026gt;userAgent(); $sessionUserAgent = session(\u0026#39;user_agent\u0026#39;); if ($sessionUserAgent \u0026amp;\u0026amp; $sessionUserAgent !== $currentUserAgent) { Auth::logout(); session()-\u0026gt;invalidate(); throw new \\Exception(\u0026#39;Session security violation detected.\u0026#39;); } if (!$sessionUserAgent) { session([\u0026#39;user_agent\u0026#39; =\u0026gt; $currentUserAgent]); } // Check IP address changes (optional, can be problematic with mobile users) if (config(\u0026#39;security.check_ip_changes\u0026#39;)) { $currentIp = $request-\u0026gt;ip(); $sessionIp = session(\u0026#39;ip_address\u0026#39;); if ($sessionIp \u0026amp;\u0026amp; $sessionIp !== $currentIp) { Auth::logout(); session()-\u0026gt;invalidate(); throw new \\Exception(\u0026#39;IP address changed during session.\u0026#39;); } if (!$sessionIp) { session([\u0026#39;ip_address\u0026#39; =\u0026gt; $currentIp]); } } } } 7. Environment Configuration Security Secure your environment configuration and sensitive data:\n\u0026lt;?php namespace App\\Console\\Commands; use Illuminate\\Console\\Command; class SecurityAuditCommand extends Command { protected $signature = \u0026#39;security:audit\u0026#39;; protected $description = \u0026#39;Run security audit on the application\u0026#39;; public function handle(): void { $this-\u0026gt;info(\u0026#39;Running security audit...\u0026#39;); $checks = [ \u0026#39;checkEnvironmentVariables\u0026#39;, \u0026#39;checkFilePermissions\u0026#39;, \u0026#39;checkDatabaseSecurity\u0026#39;, \u0026#39;checkCacheConfiguration\u0026#39;, \u0026#39;checkLoggingConfiguration\u0026#39; ]; $passed = 0; $failed = 0; foreach ($checks as $check) { if ($this-\u0026gt;$check()) { $passed++; $this-\u0026gt;line(\u0026#34;✓ {$check}\u0026#34;, \u0026#39;fg=green\u0026#39;); } else { $failed++; $this-\u0026gt;line(\u0026#34;✗ {$check}\u0026#34;, \u0026#39;fg=red\u0026#39;); } } $this-\u0026gt;info(\u0026#34;\\nSecurity Audit Complete\u0026#34;); $this-\u0026gt;line(\u0026#34;Passed: {$passed}\u0026#34;); $this-\u0026gt;line(\u0026#34;Failed: {$failed}\u0026#34;); } private function checkEnvironmentVariables(): bool { $required = [ \u0026#39;APP_KEY\u0026#39;, \u0026#39;DB_PASSWORD\u0026#39;, \u0026#39;REDIS_PASSWORD\u0026#39; ]; $issues = []; foreach ($required as $var) { if (!env($var)) { $issues[] = \u0026#34;Missing {$var}\u0026#34;; } } // Check for default/weak values if (env(\u0026#39;APP_KEY\u0026#39;) === \u0026#39;base64:your-secret-key-here\u0026#39;) { $issues[] = \u0026#39;APP_KEY is using default value\u0026#39;; } if (env(\u0026#39;DB_PASSWORD\u0026#39;) === \u0026#39;password\u0026#39; || env(\u0026#39;DB_PASSWORD\u0026#39;) === \u0026#39;\u0026#39;) { $issues[] = \u0026#39;Weak database password\u0026#39;; } if (!empty($issues)) { $this-\u0026gt;warn(implode(\u0026#39;, \u0026#39;, $issues)); return false; } return true; } private function checkFilePermissions(): bool { $files = [ \u0026#39;.env\u0026#39; =\u0026gt; \u0026#39;600\u0026#39;, \u0026#39;storage\u0026#39; =\u0026gt; \u0026#39;755\u0026#39;, \u0026#39;bootstrap/cache\u0026#39; =\u0026gt; \u0026#39;755\u0026#39; ]; $issues = []; foreach ($files as $file =\u0026gt; $expectedPerm) { $path = base_path($file); if (file_exists($path)) { $currentPerm = substr(sprintf(\u0026#39;%o\u0026#39;, fileperms($path)), -3); if ($currentPerm !== $expectedPerm) { $issues[] = \u0026#34;{$file}: {$currentPerm} (expected {$expectedPerm})\u0026#34;; } } } if (!empty($issues)) { $this-\u0026gt;warn(\u0026#39;File permission issues: \u0026#39; . implode(\u0026#39;, \u0026#39;, $issues)); return false; } return true; } private function checkDatabaseSecurity(): bool { // Check database connection encryption try { $pdo = DB::getPdo(); $stmt = $pdo-\u0026gt;query(\u0026#34;SHOW STATUS LIKE \u0026#39;Ssl_cipher\u0026#39;\u0026#34;); $result = $stmt-\u0026gt;fetch(); if (!$result || empty($result[1])) { $this-\u0026gt;warn(\u0026#39;Database connection is not encrypted\u0026#39;); return false; } } catch (\\Exception $e) { $this-\u0026gt;warn(\u0026#39;Could not verify database encryption\u0026#39;); return false; } return true; } private function checkCacheConfiguration(): bool { $driver = config(\u0026#39;cache.default\u0026#39;); if ($driver === \u0026#39;file\u0026#39; \u0026amp;\u0026amp; app()-\u0026gt;environment(\u0026#39;production\u0026#39;)) { $this-\u0026gt;warn(\u0026#39;Using file cache driver in production\u0026#39;); return false; } return true; } private function checkLoggingConfiguration(): bool { $logLevel = config(\u0026#39;logging.level\u0026#39;); if ($logLevel === \u0026#39;debug\u0026#39; \u0026amp;\u0026amp; app()-\u0026gt;environment(\u0026#39;production\u0026#39;)) { $this-\u0026gt;warn(\u0026#39;Debug logging enabled in production\u0026#39;); return false; } return true; } } 8. API Security Best Practices Secure your APIs with proper authentication and rate limiting:\n\u0026lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Hash; class ApiSecurityMiddleware { public function handle(Request $request, Closure $next) { // Validate API key if required if (!$this-\u0026gt;validateApiKey($request)) { return response()-\u0026gt;json([\u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Invalid API key\u0026#39;], 401); } // Add security headers for APIs $response = $next($request); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Content-Type-Options\u0026#39;, \u0026#39;nosniff\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Frame-Options\u0026#39;, \u0026#39;DENY\u0026#39;); $response-\u0026gt;headers-\u0026gt;set(\u0026#39;Cache-Control\u0026#39;, \u0026#39;no-store, no-cache, must-revalidate\u0026#39;); return $response; } private function validateApiKey(Request $request): bool { $apiKey = $request-\u0026gt;header(\u0026#39;X-API-Key\u0026#39;); if (!$apiKey) { return false; } // Validate against stored API keys return Hash::check($apiKey, config(\u0026#39;api.key_hash\u0026#39;)); } } // JWT Token Security namespace App\\Services; use App\\Models\\User; use Firebase\\JWT\\JWT; use Firebase\\JWT\\Key; use Illuminate\\Support\\Facades\\Log; use Illuminate\\Support\\Str; class JwtTokenService { private string $secretKey; private string $algorithm = \u0026#39;HS256\u0026#39;; private int $expirationTime = 3600; // 1 hour public function __construct() { $this-\u0026gt;secretKey = config(\u0026#39;jwt.secret\u0026#39;); } public function generateToken(User $user): string { $payload = [ \u0026#39;sub\u0026#39; =\u0026gt; $user-\u0026gt;id, \u0026#39;email\u0026#39; =\u0026gt; $user-\u0026gt;email, \u0026#39;iat\u0026#39; =\u0026gt; time(), \u0026#39;exp\u0026#39; =\u0026gt; time() + $this-\u0026gt;expirationTime, \u0026#39;jti\u0026#39; =\u0026gt; Str::uuid()-\u0026gt;toString(), // JWT ID for tracking \u0026#39;aud\u0026#39; =\u0026gt; config(\u0026#39;app.url\u0026#39;), \u0026#39;iss\u0026#39; =\u0026gt; config(\u0026#39;app.name\u0026#39;) ]; return JWT::encode($payload, $this-\u0026gt;secretKey, $this-\u0026gt;algorithm); } public function validateToken(string $token): ?array { try { $decoded = JWT::decode($token, new Key($this-\u0026gt;secretKey, $this-\u0026gt;algorithm)); return (array) $decoded; } catch (\\Exception $e) { Log::warning(\u0026#39;Invalid JWT token\u0026#39;, [ \u0026#39;token\u0026#39; =\u0026gt; substr($token, 0, 20) . \u0026#39;...\u0026#39;, \u0026#39;error\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), \u0026#39;ip\u0026#39; =\u0026gt; request()-\u0026gt;ip() ]); return null; } } public function refreshToken(string $token): ?string { $payload = $this-\u0026gt;validateToken($token); if (!$payload) { return null; } $user = User::find($payload[\u0026#39;sub\u0026#39;]); if (!$user) { return null; } return $this-\u0026gt;generateToken($user); } } 9. Security Monitoring and Logging Implement comprehensive security monitoring:\n\u0026lt;?php namespace App\\Services; use Illuminate\\Http\\Request; use Illuminate\\Support\\Facades\\Cache; use Illuminate\\Support\\Facades\\Log; use Illuminate\\Support\\Facades\\Mail; class SecurityMonitoringService { public function logSecurityEvent(string $event, array $context = []): void { $securityLog = [ \u0026#39;event\u0026#39; =\u0026gt; $event, \u0026#39;timestamp\u0026#39; =\u0026gt; now(), \u0026#39;ip\u0026#39; =\u0026gt; request()-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; request()-\u0026gt;userAgent(), \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;session_id\u0026#39; =\u0026gt; session()-\u0026gt;getId(), \u0026#39;context\u0026#39; =\u0026gt; $context ]; Log::channel(\u0026#39;security\u0026#39;)-\u0026gt;warning($event, $securityLog); // Alert on critical events if ($this-\u0026gt;isCriticalEvent($event)) { $this-\u0026gt;sendSecurityAlert($securityLog); } } public function detectSuspiciousActivity(Request $request): bool { $suspicious = false; // Check for rapid requests from same IP if ($this-\u0026gt;isRapidRequests($request-\u0026gt;ip())) { $this-\u0026gt;logSecurityEvent(\u0026#39;Rapid requests detected\u0026#39;, [\u0026#39;ip\u0026#39; =\u0026gt; $request-\u0026gt;ip()]); $suspicious = true; } // Check for suspicious user agents if ($this-\u0026gt;isSuspiciousUserAgent($request-\u0026gt;userAgent())) { $this-\u0026gt;logSecurityEvent(\u0026#39;Suspicious user agent\u0026#39;, [\u0026#39;user_agent\u0026#39; =\u0026gt; $request-\u0026gt;userAgent()]); $suspicious = true; } // Check for common attack patterns in URLs if ($this-\u0026gt;containsAttackPatterns($request-\u0026gt;fullUrl())) { $this-\u0026gt;logSecurityEvent(\u0026#39;Attack pattern in URL\u0026#39;, [\u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;fullUrl()]); $suspicious = true; } return $suspicious; } private function isCriticalEvent(string $event): bool { $criticalEvents = [ \u0026#39;Multiple failed login attempts\u0026#39;, \u0026#39;Admin account accessed\u0026#39;, \u0026#39;Database query error\u0026#39;, \u0026#39;File upload violation\u0026#39;, \u0026#39;Potential SQL injection\u0026#39; ]; return in_array($event, $criticalEvents); } private function sendSecurityAlert(array $logData): void { // Send notification to security team // This could be email, Slack, or external security service if (config(\u0026#39;security.alerts.email\u0026#39;)) { Mail::to(config(\u0026#39;security.alerts.email\u0026#39;)) -\u0026gt;send(new SecurityAlertMail($logData)); } // Note: You need to create SecurityAlertMail class: // php artisan make:mail SecurityAlertMail } private function isRapidRequests(string $ip): bool { $key = \u0026#34;rapid_requests:{$ip}\u0026#34;; $requests = Cache::get($key, 0); Cache::put($key, $requests + 1, 60); // Track for 1 minute return $requests \u0026gt; 100; // More than 100 requests per minute } private function isSuspiciousUserAgent(string $userAgent): bool { $suspiciousPatterns = [ \u0026#39;/sqlmap/i\u0026#39;, \u0026#39;/nmap/i\u0026#39;, \u0026#39;/nikto/i\u0026#39;, \u0026#39;/curl/i\u0026#39;, \u0026#39;/wget/i\u0026#39;, \u0026#39;/python/i\u0026#39; ]; foreach ($suspiciousPatterns as $pattern) { if (preg_match($pattern, $userAgent)) { return true; } } return false; } private function containsAttackPatterns(string $url): bool { $attackPatterns = [ \u0026#39;/\\.\\./i\u0026#39;, // Directory traversal \u0026#39;/union\\s+select/i\u0026#39;, // SQL injection \u0026#39;/\u0026lt;script/i\u0026#39;, // XSS \u0026#39;/eval\\(/i\u0026#39;, // Code injection \u0026#39;/base64_decode/i\u0026#39; // Potential malicious code ]; foreach ($attackPatterns as $pattern) { if (preg_match($pattern, $url)) { return true; } } return false; } } 10. Production Deployment Security Checklist Final security checklist for production deployment:\n# 1. Environment Configuration php artisan config:cache php artisan route:cache php artisan view:cache # 2. File Permissions chmod 644 .env chmod -R 755 storage chmod -R 755 bootstrap/cache # 3. Remove Development Tools composer install --no-dev --optimize-autoloader # 4. Clear Sensitive Caches php artisan cache:clear php artisan config:clear php artisan route:clear php artisan view:clear # 5. Set Proper Directory Permissions find storage -type f -exec chmod 644 {} \\; find storage -type d -exec chmod 755 {} \\; Create a deployment security script:\n\u0026lt;?php namespace App\\Console\\Commands; class SecurityDeployCommand extends Command { protected $signature = \u0026#39;security:deploy\u0026#39;; protected $description = \u0026#39;Run security checks before deployment\u0026#39;; public function handle(): void { $this-\u0026gt;info(\u0026#39;Running pre-deployment security checks...\u0026#39;); $checks = [ \u0026#39;Environment variables are secure\u0026#39;, \u0026#39;Debug mode is disabled\u0026#39;, \u0026#39;APP_KEY is properly set\u0026#39;, \u0026#39;Database credentials are secure\u0026#39;, \u0026#39;File permissions are correct\u0026#39;, \u0026#39;Sensitive files are protected\u0026#39; ]; foreach ($checks as $check) { if ($this-\u0026gt;runSecurityCheck($check)) { $this-\u0026gt;line(\u0026#34;✓ {$check}\u0026#34;, \u0026#39;fg=green\u0026#39;); } else { $this-\u0026gt;line(\u0026#34;✗ {$check}\u0026#34;, \u0026#39;fg=red\u0026#39;); $this-\u0026gt;error(\u0026#39;Security check failed. Deployment aborted.\u0026#39;); return; } } $this-\u0026gt;info(\u0026#39;All security checks passed. Ready for deployment.\u0026#39;); } private function runSecurityCheck(string $check): bool { switch ($check) { case \u0026#39;Debug mode is disabled\u0026#39;: return !config(\u0026#39;app.debug\u0026#39;); case \u0026#39;APP_KEY is properly set\u0026#39;: return config(\u0026#39;app.key\u0026#39;) \u0026amp;\u0026amp; config(\u0026#39;app.key\u0026#39;) !== \u0026#39;base64:your-secret-key-here\u0026#39;; default: return true; } } } Conclusion Security is not a one-time setup but an ongoing process that requires constant vigilance and updates. These best practices provide a solid foundation for securing your Laravel applications in production environments.\nRegular security audits, monitoring, and staying updated with the latest security patches are essential for maintaining a secure application. Remember that security is only as strong as its weakest link, so ensure all team members understand and follow these practices.\nThe investment in proper security measures pays dividends in protecting your users\u0026rsquo; data, maintaining trust, and avoiding costly security breaches. Start implementing these practices early in your development process rather than trying to retrofit security into an existing application.\nReady to build better Laravel applications? Master the art of Clean Code and Project Structure or supercharge your apps with our comprehensive Performance Optimization Guide .\n","href":"/2025/09/laravel-security-best-practices-production.html","title":"Complete Production Security Guide"},{"content":"Writing clean, maintainable code in Laravel applications requires more than just understanding the framework\u0026rsquo;s features. It demands a systematic approach to organizing your project structure, implementing proven design patterns, and following established best practices that make your codebase scalable and readable.\nLaravel provides excellent flexibility, but this freedom can sometimes lead to messy codebases if developers don\u0026rsquo;t establish clear conventions early on. This comprehensive guide will walk you through proven strategies for creating professional Laravel applications that are easy to maintain, test, and scale.\nUnderstanding Clean Code Principles in Laravel Clean code isn\u0026rsquo;t just about making your code look pretty. It\u0026rsquo;s about creating applications that other developers can easily understand, modify, and extend. In the Laravel ecosystem, this means leveraging the framework\u0026rsquo;s conventions while adding your own organizational patterns.\nThe foundation of clean Laravel code rests on several key principles: single responsibility, proper naming conventions, consistent file organization, and strategic use of Laravel\u0026rsquo;s built-in features. These principles become especially important as your application grows beyond a simple CRUD interface.\nEssential Laravel Project Structure A well-organized Laravel project goes beyond the default directory structure. While Laravel\u0026rsquo;s default organization works well for small applications, larger projects benefit from additional layers of organization that separate concerns more clearly.\nService Layer Architecture Implementing a service layer helps separate business logic from your controllers, making your code more testable and maintainable. Here\u0026rsquo;s how to structure this approach:\n\u0026lt;?php namespace App\\Services; use App\\Models\\User; use App\\Models\\Order; use Illuminate\\Support\\Facades\\DB; use Illuminate\\Support\\Facades\\Log; class OrderService { public function createOrder(User $user, array $orderData): Order { return DB::transaction(function () use ($user, $orderData) { $order = new Order(); $order-\u0026gt;user_id = $user-\u0026gt;id; $order-\u0026gt;total_amount = $this-\u0026gt;calculateTotal($orderData[\u0026#39;items\u0026#39;]); $order-\u0026gt;status = \u0026#39;pending\u0026#39;; $order-\u0026gt;save(); $this-\u0026gt;attachOrderItems($order, $orderData[\u0026#39;items\u0026#39;]); $this-\u0026gt;sendOrderConfirmation($order); Log::info(\u0026#39;Order created successfully\u0026#39;, [\u0026#39;order_id\u0026#39; =\u0026gt; $order-\u0026gt;id]); return $order; }); } private function calculateTotal(array $items): float { return collect($items)-\u0026gt;sum(function ($item) { return $item[\u0026#39;price\u0026#39;] * $item[\u0026#39;quantity\u0026#39;]; }); } private function attachOrderItems(Order $order, array $items): void { foreach ($items as $item) { $order-\u0026gt;items()-\u0026gt;create([ \u0026#39;product_id\u0026#39; =\u0026gt; $item[\u0026#39;product_id\u0026#39;], \u0026#39;quantity\u0026#39; =\u0026gt; $item[\u0026#39;quantity\u0026#39;], \u0026#39;price\u0026#39; =\u0026gt; $item[\u0026#39;price\u0026#39;] ]); } } private function sendOrderConfirmation(Order $order): void { // Implementation for sending order confirmation } } This service class encapsulates all order-related business logic, making it reusable across different parts of your application. Your controller becomes much simpler:\n\u0026lt;?php namespace App\\Http\\Controllers; use App\\Services\\OrderService; use App\\Http\\Requests\\CreateOrderRequest; use Illuminate\\Http\\JsonResponse; class OrderController extends Controller { private OrderService $orderService; public function __construct(OrderService $orderService) { $this-\u0026gt;orderService = $orderService; } public function store(CreateOrderRequest $request): JsonResponse { $order = $this-\u0026gt;orderService-\u0026gt;createOrder( auth()-\u0026gt;user(), $request-\u0026gt;validated() ); return response()-\u0026gt;json([ \u0026#39;message\u0026#39; =\u0026gt; \u0026#39;Order created successfully\u0026#39;, \u0026#39;order\u0026#39; =\u0026gt; $order ], 201); } } Repository Pattern Implementation The Repository pattern provides an abstraction layer between your business logic and data access logic. This pattern becomes invaluable when you need to switch data sources or implement complex querying logic.\n\u0026lt;?php namespace App\\Repositories; use App\\Models\\Product; use Illuminate\\Database\\Eloquent\\Collection; interface ProductRepositoryInterface { public function findById(int $id): ?Product; public function findByCategory(string $category): Collection; public function findFeaturedProducts(int $limit = 10): Collection; public function searchByName(string $name): Collection; } class ProductRepository implements ProductRepositoryInterface { public function findById(int $id): ?Product { return Product::with([\u0026#39;category\u0026#39;, \u0026#39;images\u0026#39;])-\u0026gt;find($id); } public function findByCategory(string $category): Collection { return Product::whereHas(\u0026#39;category\u0026#39;, function ($query) use ($category) { $query-\u0026gt;where(\u0026#39;slug\u0026#39;, $category); })-\u0026gt;with([\u0026#39;category\u0026#39;, \u0026#39;images\u0026#39;])-\u0026gt;get(); } public function findFeaturedProducts(int $limit = 10): Collection { return Product::where(\u0026#39;is_featured\u0026#39;, true) -\u0026gt;with([\u0026#39;category\u0026#39;, \u0026#39;images\u0026#39;]) -\u0026gt;limit($limit) -\u0026gt;get(); } public function searchByName(string $name): Collection { return Product::where(\u0026#39;name\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#34;%{$name}%\u0026#34;) -\u0026gt;with([\u0026#39;category\u0026#39;, \u0026#39;images\u0026#39;]) -\u0026gt;get(); } } Don\u0026rsquo;t forget to bind your repository in a service provider:\n\u0026lt;?php namespace App\\Providers; use Illuminate\\Support\\ServiceProvider; use App\\Repositories\\ProductRepositoryInterface; use App\\Repositories\\ProductRepository; class RepositoryServiceProvider extends ServiceProvider { public function register(): void { $this-\u0026gt;app-\u0026gt;bind( ProductRepositoryInterface::class, ProductRepository::class ); } } Advanced Directory Organization As your Laravel application grows, the default directory structure might not be sufficient. Consider creating additional directories that reflect your application\u0026rsquo;s domain:\napp/ ├── Actions/ │ ├── Orders/ │ │ ├── CreateOrderAction.php │ │ └── UpdateOrderStatusAction.php │ └── Users/ │ ├── RegisterUserAction.php │ └── UpdateUserProfileAction.php ├── DataTransferObjects/ │ ├── OrderDTO.php │ └── UserDTO.php ├── Repositories/ │ ├── Contracts/ │ │ ├── OrderRepositoryInterface.php │ │ └── UserRepositoryInterface.php │ ├── OrderRepository.php │ └── UserRepository.php ├── Services/ │ ├── OrderService.php │ ├── PaymentService.php │ └── NotificationService.php └── ValueObjects/ ├── Money.php └── Email.php Data Transfer Objects (DTOs) DTOs help you maintain clean interfaces between different layers of your application:\n\u0026lt;?php namespace App\\DataTransferObjects; class OrderDTO { public function __construct( public readonly int $userId, public readonly array $items, public readonly string $shippingAddress, public readonly ?string $notes = null ) {} public static function fromRequest(array $data): self { return new self( userId: $data[\u0026#39;user_id\u0026#39;], items: $data[\u0026#39;items\u0026#39;], shippingAddress: $data[\u0026#39;shipping_address\u0026#39;], notes: $data[\u0026#39;notes\u0026#39;] ?? null ); } public function toArray(): array { return [ \u0026#39;user_id\u0026#39; =\u0026gt; $this-\u0026gt;userId, \u0026#39;items\u0026#39; =\u0026gt; $this-\u0026gt;items, \u0026#39;shipping_address\u0026#39; =\u0026gt; $this-\u0026gt;shippingAddress, \u0026#39;notes\u0026#39; =\u0026gt; $this-\u0026gt;notes, ]; } } Action Classes for Single Responsibility Action classes encapsulate single business operations, making your code more focused and testable:\n\u0026lt;?php namespace App\\Actions\\Orders; use App\\Models\\Order; use App\\DataTransferObjects\\OrderDTO; use App\\Services\\PaymentService; use App\\Services\\InventoryService; use App\\Services\\NotificationService; class CreateOrderAction { public function __construct( private PaymentService $paymentService, private InventoryService $inventoryService, private NotificationService $notificationService ) {} public function execute(OrderDTO $orderDTO): Order { // Check inventory availability $this-\u0026gt;inventoryService-\u0026gt;checkAvailability($orderDTO-\u0026gt;items); // Create the order $order = Order::create($orderDTO-\u0026gt;toArray()); // Process payment $payment = $this-\u0026gt;paymentService-\u0026gt;processPayment($order); // Update inventory $this-\u0026gt;inventoryService-\u0026gt;reserveItems($orderDTO-\u0026gt;items); // Send notifications $this-\u0026gt;notificationService-\u0026gt;sendOrderConfirmation($order); return $order-\u0026gt;fresh(); } } Model Organization and Relationships Proper model organization extends beyond just defining relationships. Consider implementing model concerns, observers, and custom collections to keep your models clean and focused.\nUsing Model Concerns Organize common model behavior into reusable concerns:\n\u0026lt;?php namespace App\\Models\\Concerns; use Illuminate\\Database\\Eloquent\\Builder; trait HasActiveScope { public function scopeActive(Builder $query): Builder { return $query-\u0026gt;where(\u0026#39;is_active\u0026#39;, true); } public function scopeInactive(Builder $query): Builder { return $query-\u0026gt;where(\u0026#39;is_active\u0026#39;, false); } public function activate(): bool { return $this-\u0026gt;update([\u0026#39;is_active\u0026#39; =\u0026gt; true]); } public function deactivate(): bool { return $this-\u0026gt;update([\u0026#39;is_active\u0026#39; =\u0026gt; false]); } } Custom Collections for Enhanced Functionality Create custom collections to add domain-specific methods:\n\u0026lt;?php namespace App\\Collections; use Illuminate\\Database\\Eloquent\\Collection; use App\\Models\\Order; class OrderCollection extends Collection { public function pending(): self { return $this-\u0026gt;filter(fn(Order $order) =\u0026gt; $order-\u0026gt;status === \u0026#39;pending\u0026#39;); } public function completed(): self { return $this-\u0026gt;filter(fn(Order $order) =\u0026gt; $order-\u0026gt;status === \u0026#39;completed\u0026#39;); } public function totalRevenue(): float { return $this-\u0026gt;sum(\u0026#39;total_amount\u0026#39;); } public function averageOrderValue(): float { return $this-\u0026gt;avg(\u0026#39;total_amount\u0026#39;); } } Then use it in your model:\n\u0026lt;?php namespace App\\Models; use Illuminate\\Database\\Eloquent\\Model; use App\\Collections\\OrderCollection; class Order extends Model { public function newCollection(array $models = []): OrderCollection { return new OrderCollection($models); } } Testing Clean Code Architecture Clean architecture makes testing easier. Here\u0026rsquo;s how to test your service layer:\n\u0026lt;?php namespace Tests\\Unit\\Services; use Tests\\TestCase; use App\\Services\\OrderService; use App\\Models\\User; use App\\Models\\Product; use Illuminate\\Foundation\\Testing\\RefreshDatabase; class OrderServiceTest extends TestCase { use RefreshDatabase; private OrderService $orderService; protected function setUp(): void { parent::setUp(); $this-\u0026gt;orderService = app(OrderService::class); } public function test_creates_order_successfully(): void { $user = User::factory()-\u0026gt;create(); $product = Product::factory()-\u0026gt;create([\u0026#39;price\u0026#39; =\u0026gt; 100]); $orderData = [ \u0026#39;items\u0026#39; =\u0026gt; [ [ \u0026#39;product_id\u0026#39; =\u0026gt; $product-\u0026gt;id, \u0026#39;quantity\u0026#39; =\u0026gt; 2, \u0026#39;price\u0026#39; =\u0026gt; $product-\u0026gt;price ] ] ]; $order = $this-\u0026gt;orderService-\u0026gt;createOrder($user, $orderData); $this-\u0026gt;assertEquals($user-\u0026gt;id, $order-\u0026gt;user_id); $this-\u0026gt;assertEquals(200, $order-\u0026gt;total_amount); $this-\u0026gt;assertEquals(\u0026#39;pending\u0026#39;, $order-\u0026gt;status); $this-\u0026gt;assertCount(1, $order-\u0026gt;items); } } Performance Considerations in Clean Architecture While clean architecture provides many benefits, it\u0026rsquo;s important to consider performance implications. Use Laravel\u0026rsquo;s query optimization features strategically:\n\u0026lt;?php namespace App\\Services; use App\\Models\\Order; use Illuminate\\Database\\Eloquent\\Collection; class OrderReportService { public function getMonthlyReport(int $year, int $month): array { $orders = Order::with([\u0026#39;items.product\u0026#39;, \u0026#39;user\u0026#39;]) -\u0026gt;whereYear(\u0026#39;created_at\u0026#39;, $year) -\u0026gt;whereMonth(\u0026#39;created_at\u0026#39;, $month) -\u0026gt;get(); return [ \u0026#39;total_orders\u0026#39; =\u0026gt; $orders-\u0026gt;count(), \u0026#39;total_revenue\u0026#39; =\u0026gt; $orders-\u0026gt;sum(\u0026#39;total_amount\u0026#39;), \u0026#39;average_order_value\u0026#39; =\u0026gt; $orders-\u0026gt;avg(\u0026#39;total_amount\u0026#39;), \u0026#39;top_products\u0026#39; =\u0026gt; $this-\u0026gt;getTopProducts($orders), ]; } private function getTopProducts(Collection $orders): array { return $orders-\u0026gt;flatMap-\u0026gt;items -\u0026gt;groupBy(\u0026#39;product_id\u0026#39;) -\u0026gt;map(fn($items) =\u0026gt; [ \u0026#39;product\u0026#39; =\u0026gt; $items-\u0026gt;first()-\u0026gt;product, \u0026#39;quantity_sold\u0026#39; =\u0026gt; $items-\u0026gt;sum(\u0026#39;quantity\u0026#39;), \u0026#39;revenue\u0026#39; =\u0026gt; $items-\u0026gt;sum(fn($item) =\u0026gt; $item-\u0026gt;price * $item-\u0026gt;quantity) ]) -\u0026gt;sortByDesc(\u0026#39;quantity_sold\u0026#39;) -\u0026gt;take(10) -\u0026gt;values() -\u0026gt;toArray(); } } Configuration and Environment Management Proper configuration management is crucial for clean code. Create custom configuration files for complex settings:\n\u0026lt;?php // config/business.php return [ \u0026#39;order\u0026#39; =\u0026gt; [ \u0026#39;max_items_per_order\u0026#39; =\u0026gt; env(\u0026#39;MAX_ITEMS_PER_ORDER\u0026#39;, 50), \u0026#39;auto_cancel_hours\u0026#39; =\u0026gt; env(\u0026#39;AUTO_CANCEL_HOURS\u0026#39;, 24), \u0026#39;minimum_order_amount\u0026#39; =\u0026gt; env(\u0026#39;MINIMUM_ORDER_AMOUNT\u0026#39;, 10.00), ], \u0026#39;payment\u0026#39; =\u0026gt; [ \u0026#39;default_gateway\u0026#39; =\u0026gt; env(\u0026#39;PAYMENT_GATEWAY\u0026#39;, \u0026#39;stripe\u0026#39;), \u0026#39;timeout_seconds\u0026#39; =\u0026gt; env(\u0026#39;PAYMENT_TIMEOUT\u0026#39;, 30), \u0026#39;retry_attempts\u0026#39; =\u0026gt; env(\u0026#39;PAYMENT_RETRY_ATTEMPTS\u0026#39;, 3), ], ]; Conclusion Implementing clean code practices in Laravel requires discipline and planning, but the benefits are substantial. A well-structured Laravel application with proper separation of concerns, consistent naming conventions, and strategic use of design patterns becomes easier to maintain, test, and scale.\nThe key to success lies in starting with good practices from the beginning rather than trying to refactor a messy codebase later. Use Laravel\u0026rsquo;s built-in features as your foundation, but don\u0026rsquo;t hesitate to add your own organizational layers when they serve your application\u0026rsquo;s specific needs.\nRemember that clean code isn\u0026rsquo;t about following every pattern perfectly, but about creating code that serves your team and your project\u0026rsquo;s long-term goals. Start with the basics covered in this guide, and gradually introduce more advanced patterns as your application grows in complexity.\nLooking to optimize your Laravel application further? Learn about 15 Essential Performance Optimization Techniques or explore comprehensive Security Best Practices for Production environments.\n","href":"/2025/09/clean-code-laravel-project-structure.html","title":"Clean Code Laravel Project Structure and Design Patterns Guide"},{"content":"When your Laravel application starts acting up in production, proper logging becomes your lifeline. Unlike development environments where you can use tools like dd() or dump(), production debugging requires a more sophisticated approach. This comprehensive guide walks you through advanced Laravel debugging techniques using logs that will help you identify, track, and resolve production issues efficiently.\nUnderstanding Laravel\u0026rsquo;s Logging Architecture Laravel provides a robust logging system built on top of the Monolog library. The framework offers multiple logging channels, each designed for specific use cases. Before diving into advanced debugging techniques, you need to understand how Laravel handles logging under the hood.\nThe logging configuration lives in config/logging.php, where you can define various channels such as single file, daily rotation, syslog, and even custom channels. Each channel can have different log levels, from emergency down to debug, giving you fine-grained control over what gets logged and where.\nWhen debugging production issues, the key is to log the right information at the right time without overwhelming your storage or degrading performance. This means understanding when to use each log level and structuring your log messages for maximum clarity.\nSetting Up Structured Logging for Better Debugging Structured logging is crucial for production debugging. Instead of writing plain text messages, structured logs contain additional context that makes searching and filtering much more effective. Laravel\u0026rsquo;s logging system supports structured logging out of the box.\nHere\u0026rsquo;s how to implement structured logging in your Laravel application:\n\u0026lt;?php use Illuminate\\Support\\Facades\\Log; class OrderService { public function processOrder($orderId, $userId) { Log::info(\u0026#39;Order processing started\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $orderId, \u0026#39;user_id\u0026#39; =\u0026gt; $userId, \u0026#39;timestamp\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), \u0026#39;memory_usage\u0026#39; =\u0026gt; memory_get_usage(true), \u0026#39;request_id\u0026#39; =\u0026gt; request()-\u0026gt;header(\u0026#39;X-Request-ID\u0026#39;) ]); try { // Your order processing logic here $result = $this-\u0026gt;executeOrderLogic($orderId); Log::info(\u0026#39;Order processing completed successfully\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $orderId, \u0026#39;processing_time\u0026#39; =\u0026gt; $this-\u0026gt;calculateProcessingTime(), \u0026#39;result\u0026#39; =\u0026gt; $result ]); return $result; } catch (Exception $e) { Log::error(\u0026#39;Order processing failed\u0026#39;, [ \u0026#39;order_id\u0026#39; =\u0026gt; $orderId, \u0026#39;error_message\u0026#39; =\u0026gt; $e-\u0026gt;getMessage(), \u0026#39;error_code\u0026#39; =\u0026gt; $e-\u0026gt;getCode(), \u0026#39;stack_trace\u0026#39; =\u0026gt; $e-\u0026gt;getTraceAsString(), \u0026#39;file\u0026#39; =\u0026gt; $e-\u0026gt;getFile(), \u0026#39;line\u0026#39; =\u0026gt; $e-\u0026gt;getLine() ]); throw $e; } } } This structured approach provides context that makes debugging significantly easier. You can quickly filter logs by order ID, user ID, or any other relevant parameter.\nCreating Custom Log Channels for Different Purposes Different types of issues require different logging strategies. Creating custom log channels allows you to separate concerns and make debugging more targeted. Here\u0026rsquo;s how to set up specialized log channels:\n// config/logging.php \u0026#39;channels\u0026#39; =\u0026gt; [ \u0026#39;performance\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;daily\u0026#39;, \u0026#39;path\u0026#39; =\u0026gt; storage_path(\u0026#39;logs/performance.log\u0026#39;), \u0026#39;level\u0026#39; =\u0026gt; \u0026#39;info\u0026#39;, \u0026#39;days\u0026#39; =\u0026gt; 14, ], \u0026#39;security\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;daily\u0026#39;, \u0026#39;path\u0026#39; =\u0026gt; storage_path(\u0026#39;logs/security.log\u0026#39;), \u0026#39;level\u0026#39; =\u0026gt; \u0026#39;warning\u0026#39;, \u0026#39;days\u0026#39; =\u0026gt; 30, ], \u0026#39;database\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;daily\u0026#39;, \u0026#39;path\u0026#39; =\u0026gt; storage_path(\u0026#39;logs/database.log\u0026#39;), \u0026#39;level\u0026#39; =\u0026gt; \u0026#39;debug\u0026#39;, \u0026#39;days\u0026#39; =\u0026gt; 7, ], ], Now you can log to specific channels based on the type of issue:\nLog::channel(\u0026#39;performance\u0026#39;)-\u0026gt;info(\u0026#39;Slow query detected\u0026#39;, [ \u0026#39;query\u0026#39; =\u0026gt; $query, \u0026#39;execution_time\u0026#39; =\u0026gt; $executionTime, \u0026#39;affected_rows\u0026#39; =\u0026gt; $affectedRows ]); Log::channel(\u0026#39;security\u0026#39;)-\u0026gt;warning(\u0026#39;Failed login attempt\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $email, \u0026#39;ip_address\u0026#39; =\u0026gt; request()-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; request()-\u0026gt;userAgent() ]); Implementing Context-Aware Logging Context is everything when debugging production issues. Laravel provides several ways to add context to your logs automatically. The most effective approach is to create a logging middleware that adds request-specific context to every log entry.\n\u0026lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Support\\Facades\\Log; use Illuminate\\Support\\Str; class LoggingContext { public function handle($request, Closure $next) { $requestId = Str::uuid()-\u0026gt;toString(); $request-\u0026gt;headers-\u0026gt;set(\u0026#39;X-Request-ID\u0026#39;, $requestId); Log::withContext([ \u0026#39;request_id\u0026#39; =\u0026gt; $requestId, \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;ip_address\u0026#39; =\u0026gt; $request-\u0026gt;ip(), \u0026#39;route\u0026#39; =\u0026gt; $request-\u0026gt;route()?-\u0026gt;getName(), \u0026#39;method\u0026#39; =\u0026gt; $request-\u0026gt;method(), \u0026#39;url\u0026#39; =\u0026gt; $request-\u0026gt;fullUrl(), ]); return $next($request); } } This middleware ensures every log entry includes essential debugging information, making it much easier to trace issues across multiple requests or user sessions.\nAdvanced Error Tracking and Exception Handling Exception handling is where most production debugging begins. Laravel\u0026rsquo;s exception handler is your first line of defense, but you need to customize it for effective debugging.\n\u0026lt;?php namespace App\\Exceptions; use Illuminate\\Foundation\\Exceptions\\Handler as ExceptionHandler; use Illuminate\\Support\\Facades\\Log; use Throwable; class Handler extends ExceptionHandler { public function report(Throwable $exception) { if ($this-\u0026gt;shouldReport($exception)) { Log::error(\u0026#39;Exception occurred\u0026#39;, [ \u0026#39;exception_class\u0026#39; =\u0026gt; get_class($exception), \u0026#39;message\u0026#39; =\u0026gt; $exception-\u0026gt;getMessage(), \u0026#39;code\u0026#39; =\u0026gt; $exception-\u0026gt;getCode(), \u0026#39;file\u0026#39; =\u0026gt; $exception-\u0026gt;getFile(), \u0026#39;line\u0026#39; =\u0026gt; $exception-\u0026gt;getLine(), \u0026#39;trace\u0026#39; =\u0026gt; $exception-\u0026gt;getTraceAsString(), \u0026#39;request_data\u0026#39; =\u0026gt; request()-\u0026gt;except([\u0026#39;password\u0026#39;, \u0026#39;password_confirmation\u0026#39;]), \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;session_id\u0026#39; =\u0026gt; session()-\u0026gt;getId(), \u0026#39;occurred_at\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ]); } parent::report($exception); } } Database Query Debugging and N+1 Problem Detection Database-related issues are common in production applications. Laravel provides excellent tools for debugging database queries, but you need to set them up properly for production use.\nEnable query logging in your service provider:\n\u0026lt;?php namespace App\\Providers; use Illuminate\\Support\\Facades\\DB; use Illuminate\\Support\\Facades\\Log; use Illuminate\\Support\\ServiceProvider; class AppServiceProvider extends ServiceProvider { public function boot() { if (config(\u0026#39;app.debug\u0026#39;) || config(\u0026#39;logging.log_queries\u0026#39;)) { DB::listen(function ($query) { if ($query-\u0026gt;time \u0026gt; 1000) { // Log slow queries (\u0026gt; 1 second) Log::channel(\u0026#39;database\u0026#39;)-\u0026gt;warning(\u0026#39;Slow query detected\u0026#39;, [ \u0026#39;sql\u0026#39; =\u0026gt; $query-\u0026gt;sql, \u0026#39;bindings\u0026#39; =\u0026gt; $query-\u0026gt;bindings, \u0026#39;time\u0026#39; =\u0026gt; $query-\u0026gt;time, \u0026#39;connection\u0026#39; =\u0026gt; $query-\u0026gt;connectionName, ]); } }); } } } For detecting N+1 problems and other performance issues, you should explore tools mentioned in our 5 Laravel Extensions for Visual Studio Code guide, which includes debugging extensions that can help during development.\nPerformance Monitoring Through Logging Performance issues often surface in production first. Implementing performance logging helps you identify bottlenecks before they become critical problems.\n\u0026lt;?php namespace App\\Http\\Middleware; use Closure; use Illuminate\\Support\\Facades\\Log; class PerformanceMonitoring { public function handle($request, Closure $next) { $startTime = microtime(true); $startMemory = memory_get_usage(true); $response = $next($request); $endTime = microtime(true); $endMemory = memory_get_usage(true); $executionTime = ($endTime - $startTime) * 1000; // Convert to milliseconds $memoryUsed = $endMemory - $startMemory; if ($executionTime \u0026gt; 2000 || $memoryUsed \u0026gt; 50 * 1024 * 1024) { // 2 seconds or 50MB Log::channel(\u0026#39;performance\u0026#39;)-\u0026gt;warning(\u0026#39;Performance threshold exceeded\u0026#39;, [ \u0026#39;route\u0026#39; =\u0026gt; $request-\u0026gt;route()?-\u0026gt;getName(), \u0026#39;method\u0026#39; =\u0026gt; $request-\u0026gt;method(), \u0026#39;execution_time_ms\u0026#39; =\u0026gt; round($executionTime, 2), \u0026#39;memory_used_mb\u0026#39; =\u0026gt; round($memoryUsed / 1024 / 1024, 2), \u0026#39;response_status\u0026#39; =\u0026gt; $response-\u0026gt;getStatusCode(), ]); } return $response; } } Log Analysis and Monitoring Best Practices Having logs is only useful if you can analyze them effectively. Here are best practices for log analysis in production environments:\nFirst, implement log rotation to prevent disk space issues. Laravel\u0026rsquo;s daily driver handles this automatically, but you should monitor disk usage regularly.\nSecond, consider using log aggregation tools. While not strictly Laravel-specific, tools like ELK Stack (Elasticsearch, Logstash, Kibana) or more modern solutions like Grafana Loki can make log analysis much more powerful.\nThird, implement alerting based on log patterns. Critical errors should trigger immediate notifications, while performance degradations might warrant daily summaries.\nSecurity-Focused Logging for Production Security incidents require immediate attention, so your logging strategy should include security-specific considerations:\n\u0026lt;?php namespace App\\Listeners; use Illuminate\\Auth\\Events\\Failed; use Illuminate\\Auth\\Events\\Login; use Illuminate\\Auth\\Events\\Logout; use Illuminate\\Support\\Facades\\Log; class SecurityEventLogger { public function handleFailedLogin(Failed $event) { Log::channel(\u0026#39;security\u0026#39;)-\u0026gt;warning(\u0026#39;Failed login attempt\u0026#39;, [ \u0026#39;email\u0026#39; =\u0026gt; $event-\u0026gt;credentials[\u0026#39;email\u0026#39;] ?? \u0026#39;unknown\u0026#39;, \u0026#39;ip_address\u0026#39; =\u0026gt; request()-\u0026gt;ip(), \u0026#39;user_agent\u0026#39; =\u0026gt; request()-\u0026gt;userAgent(), \u0026#39;attempted_at\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ]); } public function handleSuccessfulLogin(Login $event) { Log::channel(\u0026#39;security\u0026#39;)-\u0026gt;info(\u0026#39;Successful login\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; $event-\u0026gt;user-\u0026gt;id, \u0026#39;email\u0026#39; =\u0026gt; $event-\u0026gt;user-\u0026gt;email, \u0026#39;ip_address\u0026#39; =\u0026gt; request()-\u0026gt;ip(), \u0026#39;logged_in_at\u0026#39; =\u0026gt; now()-\u0026gt;toISOString(), ]); } } Debugging Production Deployment Issues When deployment issues occur, having proper logging around your deployment process is crucial. If you\u0026rsquo;re following our Deploy Laravel Application to VPS with Nginx: Complete Production Guide , you\u0026rsquo;ll want to ensure your deployment scripts include logging at each critical step.\nConsider logging configuration changes, migration results, cache clearing operations, and queue worker status. This information becomes invaluable when troubleshooting deployment-related issues.\nTesting Your Logging Strategy Your logging strategy is only as good as your ability to use it when problems occur. Regularly test your logging setup by:\nSimulating different types of errors and verifying they\u0026rsquo;re logged correctly Ensuring log rotation works as expected Testing your log analysis and alerting systems Verifying that sensitive information is properly excluded from logs Remember that effective logging is a balance between having enough information to debug issues and not overwhelming your system with unnecessary data. Start with essential information and gradually add more context as you identify gaps in your debugging process.\nConclusion Advanced Laravel debugging with logs requires a systematic approach that considers the unique challenges of production environments. By implementing structured logging, creating targeted log channels, adding proper context, and following security best practices, you create a debugging system that helps you resolve issues quickly and efficiently.\nThe key to successful production debugging is preparation. Set up your logging infrastructure before you need it, test it regularly, and continuously refine your approach based on the types of issues you encounter. With proper logging in place, production issues become manageable challenges rather than emergency fire drills.\nRemember that debugging is an iterative process. As your application grows and changes, so should your logging strategy. Stay proactive, monitor your logs regularly, and don\u0026rsquo;t wait for problems to surface before implementing better logging practices.\nAdvanced Third-Party Logging and Monitoring Solutions While Laravel\u0026rsquo;s built-in logging capabilities are powerful, production applications often benefit from dedicated monitoring and error tracking services. These tools provide advanced features like real-time alerting, error aggregation, performance monitoring, and team collaboration features.\nSentry: Real-time Error Tracking Sentry is one of the most popular error tracking platforms that integrates seamlessly with Laravel. It provides real-time error tracking, performance monitoring, and release tracking.\nInstallation and Setup:\ncomposer require sentry/sentry-laravel php artisan sentry:install Configuration in Laravel:\n// config/sentry.php return [ \u0026#39;dsn\u0026#39; =\u0026gt; env(\u0026#39;SENTRY_LARAVEL_DSN\u0026#39;, env(\u0026#39;SENTRY_DSN\u0026#39;)), \u0026#39;release\u0026#39; =\u0026gt; env(\u0026#39;SENTRY_RELEASE\u0026#39;), \u0026#39;environment\u0026#39; =\u0026gt; env(\u0026#39;SENTRY_ENVIRONMENT\u0026#39;, env(\u0026#39;APP_ENV\u0026#39;, \u0026#39;production\u0026#39;)), // Breadcrumbs for better debugging context \u0026#39;breadcrumbs\u0026#39; =\u0026gt; [ \u0026#39;logs\u0026#39; =\u0026gt; true, \u0026#39;cache\u0026#39; =\u0026gt; true, \u0026#39;sql_queries\u0026#39; =\u0026gt; true, ], // Performance monitoring \u0026#39;traces_sample_rate\u0026#39; =\u0026gt; env(\u0026#39;SENTRY_TRACES_SAMPLE_RATE\u0026#39;, 0.1), ]; Custom Context and Tags:\nuse Sentry\\Laravel\\Integration; class OrderController extends Controller { public function store(Request $request) { Integration::addBreadcrumb( new \\Sentry\\Breadcrumb( \\Sentry\\Breadcrumb::LEVEL_INFO, \\Sentry\\Breadcrumb::TYPE_DEFAULT, \u0026#39;order.processing\u0026#39;, \u0026#39;Starting order processing\u0026#39;, [\u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id()] ) ); \\Sentry\\withScope(function (\\Sentry\\State\\Scope $scope) use ($request) { $scope-\u0026gt;setTag(\u0026#39;order_type\u0026#39;, $request-\u0026gt;get(\u0026#39;type\u0026#39;)); $scope-\u0026gt;setUser([ \u0026#39;id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;email\u0026#39; =\u0026gt; auth()-\u0026gt;user()-\u0026gt;email, ]); try { // Your order processing logic } catch (Exception $e) { \\Sentry\\captureException($e); throw $e; } }); } } Laravel Telescope: Development Debugging For development environments, Laravel Telescope provides an elegant debug assistant that gives you insight into requests, exceptions, database queries, queued jobs, and more.\ncomposer require laravel/telescope --dev php artisan telescope:install php artisan migrate Custom Watchers Configuration:\n// config/telescope.php \u0026#39;watchers\u0026#39; =\u0026gt; [ Watchers\\CacheWatcher::class =\u0026gt; env(\u0026#39;TELESCOPE_CACHE_WATCHER\u0026#39;, true), Watchers\\CommandWatcher::class =\u0026gt; [ \u0026#39;enabled\u0026#39; =\u0026gt; env(\u0026#39;TELESCOPE_COMMAND_WATCHER\u0026#39;, true), \u0026#39;ignore\u0026#39; =\u0026gt; [\u0026#39;schedule:run\u0026#39;], ], Watchers\\QueryWatcher::class =\u0026gt; [ \u0026#39;enabled\u0026#39; =\u0026gt; env(\u0026#39;TELESCOPE_QUERY_WATCHER\u0026#39;, true), \u0026#39;slow\u0026#39; =\u0026gt; 100, // Log queries slower than 100ms ], ], Flare: Laravel-specific Error Tracking Flare is specifically designed for Laravel applications and provides detailed error context including stack traces, user information, and environment details.\ncomposer require facade/ignition Integration with Custom Error Context:\nuse Facade\\FlareClient\\Flare; class CustomExceptionHandler extends Handler { public function report(Throwable $exception) { if (app()-\u0026gt;bound(\u0026#39;flare\u0026#39;)) { app(\u0026#39;flare\u0026#39;)-\u0026gt;context(\u0026#39;Order Processing\u0026#39;, [ \u0026#39;user_id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;session_id\u0026#39; =\u0026gt; session()-\u0026gt;getId(), \u0026#39;request_id\u0026#39; =\u0026gt; request()-\u0026gt;header(\u0026#39;X-Request-ID\u0026#39;), ]); } parent::report($exception); } } Rollbar: Comprehensive Error Monitoring Rollbar provides real-time error alerting and detailed error analysis with team collaboration features.\ncomposer require rollbar/rollbar-laravel Configuration:\n// config/logging.php \u0026#39;rollbar\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;monolog\u0026#39;, \u0026#39;handler\u0026#39; =\u0026gt; \\Rollbar\\Laravel\\MonologHandler::class, \u0026#39;access_token\u0026#39; =\u0026gt; env(\u0026#39;ROLLBAR_TOKEN\u0026#39;), \u0026#39;level\u0026#39; =\u0026gt; \u0026#39;debug\u0026#39;, \u0026#39;check_ignore\u0026#39; =\u0026gt; function($isUncaught, $exception, $payload) { return false; // Log all errors }, ], Bugsnag: Enterprise Error Monitoring Bugsnag offers advanced error monitoring with stability scoring and release tracking.\ncomposer require bugsnag/bugsnag-laravel Advanced Configuration:\n// config/bugsnag.php return [ \u0026#39;api_key\u0026#39; =\u0026gt; env(\u0026#39;BUGSNAG_API_KEY\u0026#39;), \u0026#39;release_stage\u0026#39; =\u0026gt; env(\u0026#39;APP_ENV\u0026#39;), \u0026#39;filters\u0026#39; =\u0026gt; [\u0026#39;password\u0026#39;, \u0026#39;password_confirmation\u0026#39;], \u0026#39;project_root\u0026#39; =\u0026gt; base_path(), \u0026#39;callbacks\u0026#39; =\u0026gt; [ function($report) { $report-\u0026gt;setUser([ \u0026#39;id\u0026#39; =\u0026gt; auth()-\u0026gt;id(), \u0026#39;name\u0026#39; =\u0026gt; auth()-\u0026gt;user()-\u0026gt;name ?? \u0026#39;Guest\u0026#39;, \u0026#39;email\u0026#39; =\u0026gt; auth()-\u0026gt;user()-\u0026gt;email ?? null, ]); } ], ]; Log Management with ELK Stack (Self-hosted) For organizations preferring self-hosted solutions, the ELK Stack (Elasticsearch, Logstash, Kibana) provides powerful log aggregation and analysis.\nLaravel Configuration for ELK:\n// config/logging.php \u0026#39;elk\u0026#39; =\u0026gt; [ \u0026#39;driver\u0026#39; =\u0026gt; \u0026#39;monolog\u0026#39;, \u0026#39;handler\u0026#39; =\u0026gt; Monolog\\Handler\\ElasticsearchHandler::class, \u0026#39;formatter\u0026#39; =\u0026gt; Monolog\\Formatter\\ElasticsearchFormatter::class, \u0026#39;handler_with\u0026#39; =\u0026gt; [ \u0026#39;client\u0026#39; =\u0026gt; new Elasticsearch\\Client([ \u0026#39;hosts\u0026#39; =\u0026gt; [env(\u0026#39;ELASTICSEARCH_HOST\u0026#39;, \u0026#39;localhost:9200\u0026#39;)] ]), \u0026#39;options\u0026#39; =\u0026gt; [ \u0026#39;index\u0026#39; =\u0026gt; \u0026#39;laravel-logs\u0026#39;, \u0026#39;type\u0026#39; =\u0026gt; \u0026#39;_doc\u0026#39;, ], ], ], Choosing the Right Solution For Small to Medium Applications:\nSentry (free tier available) - Best overall solution with great Laravel integration Flare - Laravel-specific with excellent debugging context Laravel Telescope - Essential for development environments For Enterprise Applications:\nRollbar or Bugsnag - Advanced features, team collaboration, SLA support ELK Stack - Full control, self-hosted, advanced querying capabilities New Relic or Datadog - Full application performance monitoring beyond just errors Budget Considerations:\nFree options: Sentry (limited), Laravel Telescope (dev only) Paid tiers start: $26/month (Sentry), $49/month (Rollbar) Enterprise: Custom pricing for high-volume applications Integration Best Practices When implementing third-party logging solutions alongside Laravel\u0026rsquo;s native logging:\nLayer your monitoring: Use native Laravel logging for application flow, third-party services for error tracking Configure appropriate sampling: Don\u0026rsquo;t log every single event to avoid overwhelming your monitoring service Set up proper alerting: Configure notifications for critical errors only to prevent alert fatigue Use correlation IDs: Track requests across different services and logs Implement feature flags: Easily enable/disable monitoring features without code changes These third-party solutions complement Laravel\u0026rsquo;s native logging capabilities and provide production-grade monitoring that scales with your application\u0026rsquo;s growth and complexity.\n","href":"/2025/09/advanced-laravel-debugging-with-logs.html","title":"Advanced Laravel Debugging with Logs Production Issues Troubleshooting"},{"content":"Go modules revolutionized dependency management in the Go ecosystem when they were introduced in Go 1.11. While most developers are familiar with basic module operations like go mod init and go get, there are several advanced features that can significantly improve your development workflow. In this comprehensive guide, we\u0026rsquo;ll explore three critical advanced concepts: working with private repositories, handling semantic import versioning v2 and beyond, and leveraging go.work for multi-module projects.\nUnderstanding Go Modules Foundation Before diving into advanced features, let\u0026rsquo;s quickly review the fundamentals. Go modules provide a way to manage dependencies and versioning in Go projects. Each module is defined by a go.mod file that declares the module path and its dependencies.\nmodule github.com/yourorg/yourproject go 1.21 require ( github.com/gin-gonic/gin v1.9.1 github.com/lib/pq v1.10.9 ) The module system uses semantic versioning and provides excellent tooling for dependency resolution. However, as your projects grow in complexity, you\u0026rsquo;ll encounter scenarios that require more sophisticated approaches.\nWorking with Private Repositories One of the most common challenges developers face is working with private repositories. By default, Go tries to fetch modules through public proxies and version control systems, which doesn\u0026rsquo;t work for private code.\nConfiguring GOPRIVATE The GOPRIVATE environment variable tells Go which module paths should be treated as private. Set this to prevent Go from attempting to fetch your private modules through public proxies:\nexport GOPRIVATE=github.com/yourcompany/*,gitlab.com/yourorg/* You can also set it globally:\ngo env -w GOPRIVATE=github.com/yourcompany/*,gitlab.com/yourorg/* Authentication for Private Repositories For Git-based private repositories, you\u0026rsquo;ll need to configure authentication. The most secure approach is using SSH keys:\ngit config --global url.\u0026#34;git@github.com:\u0026#34;.insteadOf \u0026#34;https://github.com/\u0026#34; This configuration redirects HTTPS URLs to SSH, allowing Go to use your SSH key for authentication.\nFor corporate environments using access tokens, you can configure Git credentials:\ngit config --global url.\u0026#34;https://username:token@github.com/\u0026#34;.insteadOf \u0026#34;https://github.com/\u0026#34; Private Module Proxies Large organizations often set up private module proxies for better control and caching. You can configure Go to use your private proxy:\nexport GOPROXY=https://your-private-proxy.com,https://proxy.golang.org,direct The comma-separated list tells Go to try your private proxy first, then fall back to the public proxy, and finally attempt direct version control access.\nSemantic Import Versioning v2 and Beyond Go\u0026rsquo;s approach to semantic versioning becomes more complex when dealing with major version changes. The language enforces a specific convention for major versions v2 and higher.\nThe v2+ Import Path Rule When a module reaches version 2.0.0 or higher, Go requires the major version to be included in the module path. This ensures import compatibility and prevents confusion between different major versions.\nHere\u0026rsquo;s how it works:\n// v0 and v1 (traditional) module github.com/yourorg/yourproject // v2 and higher module github.com/yourorg/yourproject/v2 Creating a v2 Module Let\u0026rsquo;s walk through creating a v2 module. Suppose you have an existing v1 module that needs breaking changes:\nCreate a new directory structure: yourproject/ ├── go.mod # v1 module ├── main.go ├── v2/ │ ├── go.mod # v2 module │ └── main.go Update the v2 go.mod file: module github.com/yourorg/yourproject/v2 go 1.21 require ( // your dependencies ) Import the v2 module: import \u0026#34;github.com/yourorg/yourproject/v2\u0026#34; Gradual Migration Strategy When upgrading to v2+, you often need to maintain backward compatibility. Here\u0026rsquo;s a practical approach:\n// In your v2 module package main import ( v1 \u0026#34;github.com/yourorg/yourproject\u0026#34; \u0026#34;github.com/yourorg/yourproject/v2/internal\u0026#34; ) // NewClient creates a v2 client while maintaining v1 compatibility func NewClient(config interface{}) *Client { switch cfg := config.(type) { case v1.Config: return \u0026amp;Client{legacy: true, v1Config: cfg} case internal.ConfigV2: return \u0026amp;Client{legacy: false, v2Config: cfg} default: panic(\u0026#34;unsupported config type\u0026#34;) } } This pattern allows users to gradually migrate from v1 to v2 without breaking existing code.\nVersion Selection and Compatibility Go\u0026rsquo;s module system can handle multiple major versions of the same module simultaneously. This is particularly useful for large codebases:\nrequire ( github.com/yourorg/yourproject v1.2.3 github.com/yourorg/yourproject/v2 v2.1.0 github.com/yourorg/yourproject/v3 v3.0.1 ) Each major version is treated as a separate module, allowing for careful migration and testing.\nMastering go.work for Multi-Module Projects The go.work file, introduced in Go 1.18, provides workspace support for multi-module development. This feature is invaluable for large projects spanning multiple modules or when developing modules that depend on each other.\nCreating a Workspace Initialize a workspace in your project root:\ngo work init ./module1 ./module2 ./module3 This creates a go.work file:\ngo 1.21 use ( ./module1 ./module2 ./module3 ) Workspace Benefits The workspace mode offers several advantages:\nLocal Development: Changes in one module are immediately visible to others without publishing. Consistent Versions: All modules in the workspace use the same dependency versions. Simplified Testing: Test interactions between modules without complex setup. Practical Workspace Example Consider a microservices architecture with shared libraries:\nmyproject/ ├── go.work ├── shared/ │ ├── go.mod │ └── auth/ ├── userservice/ │ ├── go.mod │ └── main.go ├── orderservice/ │ ├── go.mod │ └── main.go The go.work file enables seamless development:\ngo 1.21 use ( ./shared ./userservice ./orderservice ) replace github.com/myorg/shared =\u0026gt; ./shared Working with External Dependencies You can also use workspaces to work on forks or local versions of external dependencies:\n# Clone the dependency locally git clone https://github.com/external/library.git # Add it to your workspace go work use ./library # Your go.work file now includes the local version This is particularly useful when you need to debug or contribute to external libraries while working on your project.\nWorkspace Commands Go provides several commands for workspace management:\n# Add a module to workspace go work use ./newmodule # Remove a module from workspace go work use -r ./oldmodule # Update workspace modules go work sync Integration with Development Workflows CI/CD Considerations When using advanced module features in CI/CD pipelines, consider these best practices:\nEnvironment Variables: Set GOPRIVATE and GOPROXY in your CI environment. Authentication: Use service accounts or deploy keys for private repository access. Workspace Handling: Disable workspace mode in CI by removing go.work or using -workfile=off. # GitHub Actions example steps: - name: Setup Go uses: actions/setup-go@v4 with: go-version: \u0026#39;1.21\u0026#39; - name: Configure private modules run: | go env -w GOPRIVATE=github.com/yourorg/* - name: Build without workspace run: go build -workfile=off ./... Development Best Practices Version Pinning: Use specific versions in production but allow flexibility in development. Regular Updates: Keep dependencies updated and monitor for security vulnerabilities. Module Structure: Organize related functionality into logical modules. Troubleshooting Common Issues Private Repository Access Issues When encountering authentication problems:\n# Debug module resolution go env GOPROXY go env GOPRIVATE # Test authentication git ls-remote https://github.com/yourorg/private-repo.git Version Resolution Conflicts For complex dependency scenarios, use go mod graph to understand the dependency tree:\ngo mod graph | grep yourmodule Workspace Confusion If workspace behavior seems unexpected:\n# Check active workspace go env GOWORK # Disable workspace temporarily go build -workfile=off Advanced Patterns and Tips Module Replacement for Development Use replace directives for local development:\nreplace github.com/external/module =\u0026gt; ../local/module replace github.com/external/module =\u0026gt; github.com/yourfork/module v1.0.0 Conditional Builds with Modules Combine modules with build tags for environment-specific builds:\n//go:build development // +build development package config import \u0026#34;github.com/yourorg/dev-tools/v2\u0026#34; Testing Module Versions Create comprehensive tests for version compatibility:\nfunc TestVersionCompatibility(t *testing.T) { // Test v1 behavior v1Client := v1.NewClient(v1.Config{}) // Test v2 behavior v2Client := v2.NewClient(v2.Config{}) // Verify compatibility assert.Equal(t, v1Client.Process(), v2Client.ProcessLegacy()) } Performance and Security Considerations When working with advanced module features, keep these aspects in mind:\nSecurity Regularly audit dependencies with go list -m -u all Use go mod verify to check module integrity Consider using tools like govulncheck for vulnerability scanning Performance Private proxies can significantly improve build times Workspace mode may slow down large builds; disable in CI when appropriate Use go mod download to pre-populate module cache Conclusion Advanced Go modules features unlock powerful capabilities for complex project management. Private repository support enables enterprise development workflows, semantic import versioning ensures long-term maintainability, and go.work simplifies multi-module development.\nBy mastering these concepts, you\u0026rsquo;ll be better equipped to handle sophisticated Go projects and contribute to large-scale applications. Remember to start small, test thoroughly, and gradually adopt these advanced features as your projects grow in complexity.\nFor more insights into Go development, check out our guides on structuring Go projects , error handling , and working with interfaces to build robust applications.\nThe journey of mastering Go modules is ongoing, but with these advanced techniques in your toolkit, you\u0026rsquo;re well-prepared to tackle any dependency management challenge that comes your way.\n","href":"/2025/09/advanced-go-modules-private-repos-semantic-import-v2-go-work.html","title":"Private Repos, Semantic Import v2+, and go.work"},{"content":"Ever wondered how your phone instantly recognizes your face to unlock, or how Tesla\u0026rsquo;s autopilot spots other cars on the highway? That\u0026rsquo;s computer vision at work, and honestly, it\u0026rsquo;s not as complicated as it looks. When I first managed to get a webcam to detect my face in real-time, I was blown away. It felt like I\u0026rsquo;d just taught my computer to see.\nThe crazy thing is, you can build this stuff yourself. No PhD required, no expensive equipment - just Python, OpenCV, and some patience. I\u0026rsquo;ve been working with computer vision for a few years now, and I still get excited every time I see a detection algorithm actually work on messy, real-world data.\nIf you\u0026rsquo;ve been curious about how face recognition works, or you want to add some computer vision magic to your projects, stick around. We\u0026rsquo;re going to build everything from scratch - starting with basic object detection and working our way up to a full face recognition system that actually works.\nWhy OpenCV Rules the Computer Vision World Look, there are tons of computer vision libraries out there, but OpenCV has been the king of the hill for over 20 years. Intel originally built it, and now thousands of developers worldwide keep improving it. What\u0026rsquo;s the big deal? It\u0026rsquo;s fast, it\u0026rsquo;s free, and it just works.\nThe thing about computer vision is that the math gets really complex really fast. Instead of spending months implementing edge detection algorithms or wrestling with image transformations, OpenCV gives you all that stuff for free. It\u0026rsquo;s like having a Swiss Army knife full of computer vision tools that smarter people than me have already perfected.\nPlus, it plays nice with NumPy, which means your images are just arrays of numbers that you can manipulate super efficiently. Unlike building REST APIs from scratch where you might want to understand every piece, with computer vision you often just want the algorithms to work so you can focus on solving your actual problem.\nGetting Everything Set Up Alright, before we start building anything cool, we need to get your environment ready. Don\u0026rsquo;t worry - this part is pretty painless, and once it\u0026rsquo;s done, you\u0026rsquo;ll never have to think about it again.\nFirst things first: you need Python. If you\u0026rsquo;re on Linux and feeling lost in the terminal, our essential Linux commands guide will get you up to speed quickly.\nInstall OpenCV and the required dependencies:\n# Install OpenCV with Python bindings pip install opencv-python # Install additional OpenCV contributions (needed for face recognition) pip install opencv-contrib-python # Install supporting libraries pip install numpy matplotlib pillow # For advanced features (optional) pip install scikit-image Note: If you\u0026rsquo;re having installation issues, try these alternatives:\n# On some systems, you might need: pip3 install opencv-python opencv-contrib-python # For headless servers (no display): pip install opencv-python-headless opencv-contrib-python-headless # If pip fails, try conda: conda install -c conda-forge opencv Let\u0026rsquo;s verify your installation with a quick test:\nimport cv2 import numpy as np print(f\u0026#34;OpenCV version: {cv2.__version__}\u0026#34;) print(\u0026#34;Installation successful!\u0026#34;) # Test camera access (optional) cap = cv2.VideoCapture(0) if cap.isOpened(): print(\u0026#34;Camera access: OK\u0026#34;) cap.release() else: print(\u0026#34;Camera access: Failed (this is normal if no camera is connected)\u0026#34;) How Computers Actually \u0026ldquo;See\u0026rdquo; Images Here\u0026rsquo;s the thing that blew my mind when I first started: computers don\u0026rsquo;t see images the way we do. To us, a photo of a cat is just\u0026hellip; a cat. To a computer, it\u0026rsquo;s thousands of tiny numbers arranged in a grid.\nEvery pixel in a grayscale image has a value from 0 (completely black) to 255 (completely white). Color images are trickier - they have three layers (red, green, blue), so each pixel actually has three numbers. When you stack these layers together, you get the full-color image we see.\nThis is why computer vision works so well with Python - images are basically just NumPy arrays, and Python is fantastic at manipulating arrays. Here\u0026rsquo;s how to load your first image and see what the computer sees:\nimport cv2 import matplotlib.pyplot as plt # Load an image image = cv2.imread(\u0026#39;your_image.jpg\u0026#39;) # OpenCV loads images in BGR format, convert to RGB for display image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Display the image plt.figure(figsize=(10, 8)) plt.imshow(image_rgb) plt.title(\u0026#39;Your First OpenCV Image\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.show() # Print image properties print(f\u0026#34;Image shape: {image.shape}\u0026#34;) print(f\u0026#34;Image data type: {image.dtype}\u0026#34;) Once you get this concept - that images are just numbers - everything else starts to make sense. Our job is to write code that finds patterns in those numbers that represent the stuff we care about.\nLet\u0026rsquo;s Build Something That Actually Detects Objects Object detection is where things get really interesting. It\u0026rsquo;s one thing to classify an image as \u0026ldquo;contains a dog\u0026rdquo; - it\u0026rsquo;s another thing entirely to point at the exact location where the dog is sitting. This is what makes computer vision so powerful for real applications.\nThere are a bunch of different ways to detect objects with OpenCV. The simplest approach is template matching - basically, you give it a small image of what you\u0026rsquo;re looking for, and it finds all the places in a larger image that look similar. It\u0026rsquo;s not the fanciest method, but it works great when you know exactly what you\u0026rsquo;re hunting for:\nimport cv2 import numpy as np def detect_objects_template_matching(image_path, template_path, threshold=0.8): try: # Load the main image and template image = cv2.imread(image_path) template = cv2.imread(template_path, cv2.IMREAD_GRAYSCALE) if image is None: raise ValueError(f\u0026#34;Could not load image: {image_path}\u0026#34;) if template is None: raise ValueError(f\u0026#34;Could not load template: {template_path}\u0026#34;) # Convert main image to grayscale gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Perform template matching result = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF_NORMED) # Find locations where matching exceeds threshold locations = np.where(result \u0026gt;= threshold) # Get template dimensions h, w = template.shape # Draw rectangles around detected objects for pt in zip(*locations[::-1]): cv2.rectangle(image, pt, (pt[0] + w, pt[1] + h), (0, 255, 0), 2) return image, len(locations[0]) except Exception as e: print(f\u0026#34;Error in template matching: {e}\u0026#34;) return None, 0 # Example usage detected_image, count = detect_objects_template_matching(\u0026#39;scene.jpg\u0026#39;, \u0026#39;object_template.jpg\u0026#39;) if detected_image is not None: print(f\u0026#34;Found {count} objects\u0026#34;) # Display result (comment out these lines if running on headless server) cv2.imshow(\u0026#39;Object Detection Results\u0026#39;, detected_image) cv2.waitKey(0) cv2.destroyAllWindows() else: print(\u0026#34;Could not load images. Make sure the file paths are correct.\u0026#34;) For more sophisticated object detection, OpenCV includes pre-trained models that can detect multiple object classes simultaneously. Here\u0026rsquo;s how to use the YOLO (You Only Look Once) detector:\nNote: You\u0026rsquo;ll need to download YOLO model files first:\nDownload yolov3.weights, yolov3.cfg, and coco.names from the official YOLO repository Or use YOLOv4/v5 for better performance def detect_objects_yolo(image_path, config_path, weights_path, names_path): # Load YOLO net = cv2.dnn.readNet(weights_path, config_path) # Load class names with open(names_path, \u0026#34;r\u0026#34;) as f: classes = [line.strip() for line in f.readlines()] # Load image image = cv2.imread(image_path) height, width, channels = image.shape # Prepare input for the network blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False) net.setInput(blob) # Run detection outputs = net.forward() boxes = [] confidences = [] class_ids = [] # Process detections for output in outputs: for detection in output: scores = detection[5:] class_id = np.argmax(scores) confidence = scores[class_id] if confidence \u0026gt; 0.5: # Object detected center_x = int(detection[0] * width) center_y = int(detection[1] * height) w = int(detection[2] * width) h = int(detection[3] * height) # Calculate top-left corner x = int(center_x - w / 2) y = int(center_y - h / 2) boxes.append([x, y, w, h]) confidences.append(float(confidence)) class_ids.append(class_id) # Apply non-maximum suppression to eliminate weak, overlapping boxes indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) # Draw bounding boxes and labels for i in range(len(boxes)): if i in indexes: x, y, w, h = boxes[i] label = str(classes[class_ids[i]]) confidence = confidences[i] cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2) cv2.putText(image, f\u0026#34;{label} {confidence:.2f}\u0026#34;, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2) return image What\u0026rsquo;s cool about these pre-trained models is they already know how to spot tons of different things - people, cars, bikes, animals, you name it. No training required on your end. Just download the model files and start detecting.\nNow Let\u0026rsquo;s Get Into Face Recognition Face detection is cool, but face recognition? That\u0026rsquo;s where the real magic happens. Instead of just saying \u0026ldquo;hey, there\u0026rsquo;s a face here,\u0026rdquo; we\u0026rsquo;re going to teach the computer to recognize specific people. Think Facebook\u0026rsquo;s photo tagging, but you built it yourself.\nOpenCV has a few different ways to handle faces. We\u0026rsquo;ll start with Haar Cascades for detection - they\u0026rsquo;re not the newest tech, but they\u0026rsquo;re rock solid and fast enough for most projects:\nimport cv2 import os class FaceDetector: def __init__(self): # Load the pre-trained Haar Cascade classifier for face detection self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_frontalface_default.xml\u0026#39;) self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_eye.xml\u0026#39;) def detect_faces(self, image_path): # Load image image = cv2.imread(image_path) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Detect faces faces = self.face_cascade.detectMultiScale( gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE ) # Draw rectangles around faces for (x, y, w, h) in faces: cv2.rectangle(image, (x, y), (x+w, y+h), (255, 0, 0), 2) # Detect eyes within the face region roi_gray = gray[y:y+h, x:x+w] eyes = self.eye_cascade.detectMultiScale(roi_gray) for (ex, ey, ew, eh) in eyes: cv2.rectangle(image, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0, 255, 0), 2) return image, faces def detect_faces_realtime(self): # Start video capture cap = cv2.VideoCapture(0) while True: ret, frame = cap.read() if not ret: break # Convert to grayscale for detection gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Detect faces faces = self.face_cascade.detectMultiScale(gray, 1.3, 5) # Draw rectangles around faces for (x, y, w, h) in faces: cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2) cv2.putText(frame, \u0026#39;Face Detected\u0026#39;, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2) # Display the frame cv2.imshow(\u0026#39;Face Detection\u0026#39;, frame) # Break loop on \u0026#39;q\u0026#39; key press if cv2.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break # Clean up cap.release() cv2.destroyAllWindows() # Example usage detector = FaceDetector() # Detect faces in a single image result_image, detected_faces = detector.detect_faces(\u0026#39;photo.jpg\u0026#39;) print(f\u0026#34;Detected {len(detected_faces)} faces\u0026#34;) # Start real-time face detection detector.detect_faces_realtime() For actual face recognition (identifying specific individuals), we need to train a model with known faces. Here\u0026rsquo;s a complete face recognition system:\nimport cv2 import numpy as np import os from PIL import Image class FaceRecognizer: def __init__(self): self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_frontalface_default.xml\u0026#39;) self.recognizer = cv2.face.LBPHFaceRecognizer_create() self.faces = [] self.labels = [] self.label_names = {} def prepare_training_data(self, data_folder): \u0026#34;\u0026#34;\u0026#34;Prepare training data from organized folder structure\u0026#34;\u0026#34;\u0026#34; current_label = 0 for person_name in os.listdir(data_folder): person_path = os.path.join(data_folder, person_name) if not os.path.isdir(person_path): continue self.label_names[current_label] = person_name for image_name in os.listdir(person_path): image_path = os.path.join(person_path, image_name) # Load and convert image image = cv2.imread(image_path) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Detect face faces = self.face_cascade.detectMultiScale(gray, 1.2, 5) for (x, y, w, h) in faces: face_roi = gray[y:y+h, x:x+w] self.faces.append(face_roi) self.labels.append(current_label) current_label += 1 def train_model(self, data_folder): \u0026#34;\u0026#34;\u0026#34;Train the face recognition model\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Preparing training data...\u0026#34;) self.prepare_training_data(data_folder) print(f\u0026#34;Training with {len(self.faces)} face samples...\u0026#34;) self.recognizer.train(self.faces, np.array(self.labels)) print(\u0026#34;Training completed!\u0026#34;) # Save the trained model self.recognizer.save(\u0026#39;face_recognizer.yml\u0026#39;) # Save label names with open(\u0026#39;label_names.txt\u0026#39;, \u0026#39;w\u0026#39;) as f: for label, name in self.label_names.items(): f.write(f\u0026#34;{label}:{name}\\n\u0026#34;) def load_model(self): \u0026#34;\u0026#34;\u0026#34;Load a previously trained model\u0026#34;\u0026#34;\u0026#34; self.recognizer.read(\u0026#39;face_recognizer.yml\u0026#39;) # Load label names self.label_names = {} with open(\u0026#39;label_names.txt\u0026#39;, \u0026#39;r\u0026#39;) as f: for line in f: label, name = line.strip().split(\u0026#39;:\u0026#39;) self.label_names[int(label)] = name def recognize_faces(self, image_path, confidence_threshold=50): \u0026#34;\u0026#34;\u0026#34;Recognize faces in an image\u0026#34;\u0026#34;\u0026#34; image = cv2.imread(image_path) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) faces = self.face_cascade.detectMultiScale(gray, 1.2, 5) for (x, y, w, h) in faces: face_roi = gray[y:y+h, x:x+w] # Predict the face label, confidence = self.recognizer.predict(face_roi) if confidence \u0026lt; confidence_threshold: name = self.label_names.get(label, \u0026#34;Unknown\u0026#34;) confidence_text = f\u0026#34;{confidence:.1f}\u0026#34; else: name = \u0026#34;Unknown\u0026#34; confidence_text = f\u0026#34;{confidence:.1f}\u0026#34; # Draw rectangle and label cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2) cv2.putText(image, f\u0026#34;{name} ({confidence_text})\u0026#34;, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2) return image def recognize_faces_realtime(self): \u0026#34;\u0026#34;\u0026#34;Real-time face recognition from webcam\u0026#34;\u0026#34;\u0026#34; cap = cv2.VideoCapture(0) while True: ret, frame = cap.read() if not ret: break gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) faces = self.face_cascade.detectMultiScale(gray, 1.3, 5) for (x, y, w, h) in faces: face_roi = gray[y:y+h, x:x+w] label, confidence = self.recognizer.predict(face_roi) if confidence \u0026lt; 50: name = self.label_names.get(label, \u0026#34;Unknown\u0026#34;) color = (0, 255, 0) # Green for recognized else: name = \u0026#34;Unknown\u0026#34; color = (0, 0, 255) # Red for unknown cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2) cv2.putText(frame, f\u0026#34;{name} ({confidence:.1f})\u0026#34;, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2) cv2.imshow(\u0026#39;Face Recognition\u0026#39;, frame) if cv2.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break cap.release() cv2.destroyAllWindows() # Example usage recognizer = FaceRecognizer() # Train the model (organize your training images in folders by person\u0026#39;s name) recognizer.train_model(\u0026#39;training_data/\u0026#39;) # Or load a previously trained model # recognizer.load_model() # Recognize faces in an image result = recognizer.recognize_faces(\u0026#39;test_image.jpg\u0026#39;) cv2.imshow(\u0026#39;Recognition Result\u0026#39;, result) cv2.waitKey(0) cv2.destroyAllWindows() # Start real-time recognition recognizer.recognize_faces_realtime() Taking It Up a Notch with Advanced Techniques Once you\u0026rsquo;ve got the basics down, there\u0026rsquo;s a whole world of more sophisticated techniques that can make your applications way more accurate and robust. We\u0026rsquo;re talking about deep learning models and feature-based detection that can handle tricky lighting, weird angles, and all the messy stuff you encounter in real-world applications.\nDeep Learning Models:\nThe heavy hitters in object detection these days are all deep learning models. YOLO, SSD, R-CNN - these aren\u0026rsquo;t just fancy acronyms, they\u0026rsquo;re genuinely better at spotting objects than the older methods. OpenCV plays nice with all of them:\ndef advanced_object_detection(image_path): # Load a pre-trained DNN model net = cv2.dnn.readNetFromDarknet(\u0026#39;yolo.cfg\u0026#39;, \u0026#39;yolo.weights\u0026#39;) # Load image image = cv2.imread(image_path) height, width = image.shape[:2] # Create blob from image blob = cv2.dnn.blobFromImage(image, 1/255.0, (608, 608), swapRB=True, crop=False) net.setInput(blob) # Get layer names layer_names = net.getLayerNames() output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()] # Run forward pass outputs = net.forward(output_layers) # Process detections boxes, confidences, class_ids = [], [], [] for output in outputs: for detection in output: scores = detection[5:] class_id = np.argmax(scores) confidence = scores[class_id] if confidence \u0026gt; 0.5: box = detection[0:4] * np.array([width, height, width, height]) center_x, center_y, w, h = box.astype(\u0026#39;int\u0026#39;) x = int(center_x - (w / 2)) y = int(center_y - (h / 2)) boxes.append([x, y, int(w), int(h)]) confidences.append(float(confidence)) class_ids.append(class_id) return boxes, confidences, class_ids Feature-Based Recognition:\nSometimes you need something that works even when the lighting is terrible or the object is rotated at a weird angle. That\u0026rsquo;s where feature-based methods shine - they look for distinctive patterns that stay consistent even when everything else changes:\ndef feature_based_recognition(image1_path, image2_path): # Load images img1 = cv2.imread(image1_path, cv2.IMREAD_GRAYSCALE) img2 = cv2.imread(image2_path, cv2.IMREAD_GRAYSCALE) # Initialize SIFT detector sift = cv2.SIFT_create() # Find keypoints and descriptors kp1, des1 = sift.detectAndCompute(img1, None) kp2, des2 = sift.detectAndCompute(img2, None) # Match features bf = cv2.BFMatcher() matches = bf.knnMatch(des1, des2, k=2) # Apply ratio test good_matches = [] for m, n in matches: if m.distance \u0026lt; 0.75 * n.distance: good_matches.append([m]) # Draw matches result = cv2.drawMatchesKnn(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS) return result, len(good_matches) Making Your Code Fast Enough for the Real World Here\u0026rsquo;s the thing nobody tells you about computer vision: the demo always works perfectly, but real-world performance is where things get tricky. You\u0026rsquo;ve got bad lighting, shaky cameras, and users who expect everything to work instantly. Here\u0026rsquo;s how to make your code actually usable in production.\nPerformance Optimization Tips:\ndef optimize_for_realtime(): # Use smaller input sizes for faster processing target_width, target_height = 320, 240 # Initialize video capture with optimal settings cap = cv2.VideoCapture(0) cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_width) cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_height) cap.set(cv2.CAP_PROP_FPS, 30) # Skip frames if processing is slow frame_skip = 2 frame_count = 0 while True: ret, frame = cap.read() if not ret: break frame_count += 1 # Process every nth frame if frame_count % frame_skip == 0: # Your computer vision processing here processed_frame = process_frame(frame) cv2.imshow(\u0026#39;Optimized Processing\u0026#39;, processed_frame) else: cv2.imshow(\u0026#39;Optimized Processing\u0026#39;, frame) if cv2.waitKey(1) \u0026amp; 0xFF == ord(\u0026#39;q\u0026#39;): break cap.release() cv2.destroyAllWindows() def process_frame(frame): # Resize frame for faster processing small_frame = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5) # Convert to grayscale (faster processing) gray = cv2.cvtColor(small_frame, cv2.COLOR_BGR2GRAY) # Example: Apply face detection face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \u0026#39;haarcascade_frontalface_default.xml\u0026#39;) faces = face_cascade.detectMultiScale(gray, 1.3, 5) # Draw rectangles around detected faces processed_small = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR) # Convert back to color for (x, y, w, h) in faces: cv2.rectangle(processed_small, (x, y), (x+w, y+h), (255, 0, 0), 2) # Resize back to original size result = cv2.resize(processed_small, (frame.shape[1], frame.shape[0])) return result Handling Different Lighting Conditions:\ndef improve_image_quality(image): # Histogram equalization for better contrast gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) equalized = cv2.equalizeHist(gray) # Convert back to BGR result = cv2.cvtColor(equalized, cv2.COLOR_GRAY2BGR) # Alternative: CLAHE (Contrast Limited Adaptive Histogram Equalization) clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8)) cl1 = clahe.apply(gray) return result When Things Go Wrong (And They Will) Computer vision is finicky. Your code will work perfectly on your test images and then completely fail when you point it at a real camera. Here are the most common issues I\u0026rsquo;ve run into and how to fix them:\nCamera Won\u0026rsquo;t Work: This happens a lot, especially on Linux. Sometimes it\u0026rsquo;s permissions, sometimes it\u0026rsquo;s drivers. If you\u0026rsquo;re struggling with terminal stuff, our Linux commands guide covers the basics of troubleshooting hardware access.\nEverything Runs Super Slow: Usually this means your images are too big or you\u0026rsquo;re using an overly complex algorithm. Start small - use 320x240 images instead of 4K, and get the simple stuff working first.\nFalse Detections Everywhere: Your threshold is probably too low. Bump it up gradually until the false positives go away. Sometimes it helps to combine multiple detection methods and only trust results that both agree on.\nIntegrating with Your Existing Projects Most of the time, you\u0026rsquo;re not building a standalone computer vision app - you\u0026rsquo;re adding vision capabilities to something bigger. Maybe it\u0026rsquo;s a web app that needs to process uploaded images, or a mobile backend that analyzes photos.\nIf you\u0026rsquo;re building web APIs, FastAPI works great for wrapping your OpenCV code in REST endpoints. Just remember that image processing can be CPU-intensive, so you might want to run it async or queue the work.\nFor production deployment, containerizing everything with Docker makes life easier. We\u0026rsquo;ve got guides on Docker setup and deployment strategies that\u0026rsquo;ll help you get your computer vision services running reliably.\nDon\u0026rsquo;t Forget About Security and Privacy Look, if you\u0026rsquo;re building anything that processes faces or personal images, you need to think seriously about security. Face data is biometric data, which means it\u0026rsquo;s regulated differently than regular user data in many places.\nA few things to keep in mind: never store raw face images if you don\u0026rsquo;t absolutely have to. If you\u0026rsquo;re storing face encodings, encrypt them. And please, please implement proper authentication - check our password security guide if you need help with that.\nIf you\u0026rsquo;re processing live video feeds, log everything and make sure only authorized people can access the system. Also, double-check your local privacy laws - some jurisdictions have strict rules about face recognition systems.\nWhere to Go From Here What we\u0026rsquo;ve covered today is really just scratching the surface. Computer vision is huge - there\u0026rsquo;s gesture recognition, medical imaging, autonomous vehicles, augmented reality, you name it. The cool thing is, everything builds on the same basic concepts we\u0026rsquo;ve been working with.\nIf you\u0026rsquo;re into robotics, start looking into stereo vision and 3D reconstruction. Healthcare applications? Medical imaging is a fascinating rabbit hole. Security-focused? Biometric systems and anomaly detection are where it\u0026rsquo;s at.\nThe computer vision community is pretty awesome too. There are tons of open-source projects you can contribute to, Kaggle competitions to enter, and research papers to implement. Half the breakthrough techniques we use today started as some researcher\u0026rsquo;s crazy idea in a paper.\nJust remember that this field moves fast. What\u0026rsquo;s hot today might be old news in six months. But that\u0026rsquo;s part of what makes it exciting - there\u0026rsquo;s always something new to learn and experiment with.\nThe best advice I can give you? Start building stuff. Real projects with messy, real-world data will teach you more than any tutorial ever could. Take the techniques we\u0026rsquo;ve covered here and apply them to problems you actually care about.\nSo, what\u0026rsquo;s your first computer vision project going to be?\n","href":"/2025/09/computer-vision-opencv-object-detection-face-recognition-tutorial.html","title":"Computer Vision with OpenCV Complete Guide to Object Detection and Face Recognition in Python"},{"content":"Go 1.21 introduced log/slog, a standard structured logging API that finally brings first‑class JSON and attribute‑based logging to the standard library. If you’ve used zap or logrus, the core ideas will feel familiar\u0026ndash;just simpler and standardized.\nThis guide takes you from zero to production-ready logging with slog. We\u0026rsquo;ll start with basic setup, then gradually build up to advanced patterns like HTTP middleware, security, testing, and observability integration. Each section includes working examples you can run immediately.\nWhy structured logging matters Plain text logs are easy to read but hard to search and analyze. Structured logs emit key\u0026ndash;value pairs (JSON), which makes it trivial to filter by traceID, aggregate by user_id, or alert on level=ERROR. For API work, check out how we build routes in Go here: How to Build a REST API in Go using net/http . Pairing a solid logging strategy with a clean project structure helps in the long run: Structuring Go Projects: Clean Project Structure and Best Practices .\nQuick start: Text vs JSON slog writes through a Handler. Use a colorful text output locally and JSON in production.\npackage main import ( \u0026#34;log/slog\u0026#34; \u0026#34;os\u0026#34; ) func main() { // Development: human‑friendly text textHandler := slog.NewTextHandler(os.Stdout, \u0026amp;slog.HandlerOptions{Level: slog.LevelInfo}) // Production: machine‑friendly JSON jsonHandler := slog.NewJSONHandler(os.Stdout, \u0026amp;slog.HandlerOptions{Level: slog.LevelInfo}) // Choose based on env var h slog.Handler = textHandler if os.Getenv(\u0026#34;ENV\u0026#34;) == \u0026#34;prod\u0026#34; { h = jsonHandler } logger := slog.New(h) slog.SetDefault(logger) // optional: use slog.Default() slog.Info(\u0026#34;server starting\u0026#34;, \u0026#34;addr\u0026#34;, \u0026#34;:8080\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0.0\u0026#34;) } Output examples:\nDevelopment (text format):\ntime=2025-09-01T10:30:15.123+07:00 level=INFO msg=\u0026#34;server starting\u0026#34; addr=:8080 version=1.0.0 Production (JSON format):\n{\u0026#34;time\u0026#34;:\u0026#34;2025-09-01T10:30:15.123456+07:00\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;server starting\u0026#34;,\u0026#34;addr\u0026#34;:\u0026#34;:8080\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;1.0.0\u0026#34;} Understanding slog core concepts Before diving deeper, let\u0026rsquo;s understand slog\u0026rsquo;s key building blocks:\nLogger: The main logging interface Handler: Controls output format (JSON/Text) and destination Attributes: Key-value pairs that add context (\u0026quot;user_id\u0026quot;, 42) Groups: Nested attributes under a common key Adding context with attributes Attributes are name\u0026ndash;value pairs that add context to your logs. You can add them per-call or create scoped loggers:\n// Method 1: Add attributes per call slog.Info(\u0026#34;user action\u0026#34;, \u0026#34;user_id\u0026#34;, 42, \u0026#34;action\u0026#34;, \u0026#34;login\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;203.0.113.10\u0026#34;) // Method 2: Create scoped logger with permanent attributes userLog := slog.Default().With(\u0026#34;user_id\u0026#34;, 42, \u0026#34;session\u0026#34;, \u0026#34;abc123\u0026#34;) userLog.Info(\u0026#34;logged in\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;203.0.113.10\u0026#34;) userLog.Info(\u0026#34;viewed profile\u0026#34;) // user_id and session automatically included Output:\n{\u0026#34;time\u0026#34;:\u0026#34;2025-09-01T10:30:15+07:00\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;logged in\u0026#34;,\u0026#34;user_id\u0026#34;:42,\u0026#34;session\u0026#34;:\u0026#34;abc123\u0026#34;,\u0026#34;ip\u0026#34;:\u0026#34;203.0.113.10\u0026#34;} {\u0026#34;time\u0026#34;:\u0026#34;2025-09-01T10:30:16+07:00\u0026#34;,\u0026#34;level\u0026#34;:\u0026#34;INFO\u0026#34;,\u0026#34;msg\u0026#34;:\u0026#34;viewed profile\u0026#34;,\u0026#34;user_id\u0026#34;:42,\u0026#34;session\u0026#34;:\u0026#34;abc123\u0026#34;} ### Organizing logs with groups Use `WithGroup` to nest attributes under a key. This keeps related fields organized. ```go l := slog.Default().WithGroup(\u0026#34;http\u0026#34;).With(\u0026#34;method\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;route\u0026#34;, \u0026#34;/users/:id\u0026#34;) l.Info(\u0026#34;request completed\u0026#34;, \u0026#34;status\u0026#34;, 200, \u0026#34;latency_ms\u0026#34;, 34) // JSON example: {\u0026#34;http\u0026#34;:{\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;route\u0026#34;:\u0026#34;/users/:id\u0026#34;},\u0026#34;status\u0026#34;:200,\u0026#34;latency_ms\u0026#34;:34} Levels and filtering slog supports DEBUG, INFO, WARN, ERROR. Configure in HandlerOptions.\nopts := \u0026amp;slog.HandlerOptions{Level: slog.LevelDebug} handler := slog.NewJSONHandler(os.Stdout, opts) logger := slog.New(handler) logger.Debug(\u0026#34;cache miss\u0026#34;, \u0026#34;key\u0026#34;, \u0026#34;user:42\u0026#34;) Dynamic levels (e.g., from env var) are common. Ensure noisy debug logs stay off in production.\nProduction-Ready Patterns Now that you understand the basics, let\u0026rsquo;s explore production patterns including security, middleware, and performance.\nHandler configuration for production HandlerOptions includes useful knobs:\nAddSource: attach source file/line (useful, slight overhead) ReplaceAttr: transform or redact attributes before emit Redact sensitive data Never log secrets or PII. Use ReplaceAttr to sanitize by key.\nredacting := \u0026amp;slog.HandlerOptions{ Level: slog.LevelInfo, ReplaceAttr: func(groups []string, a slog.Attr) slog.Attr { // Redact common sensitive keys switch a.Key { case \u0026#34;password\u0026#34;, \u0026#34;token\u0026#34;, \u0026#34;authorization\u0026#34;, \u0026#34;api_key\u0026#34;: return slog.String(a.Key, \u0026#34;REDACTED\u0026#34;) } return a }, } logger := slog.New(slog.NewJSONHandler(os.Stdout, redacting)) logger.Info(\u0026#34;signup\u0026#34;, \u0026#34;email\u0026#34;, \u0026#34;user@example.com\u0026#34;, \u0026#34;password\u0026#34;, \u0026#34;secret\u0026#34;) HTTP middleware with request IDs For APIs, enrich logs with request_id, method, path, and latency. A minimal net/http middleware:\npackage middleware import ( \u0026#34;context\u0026#34; \u0026#34;log/slog\u0026#34; \u0026#34;math/rand\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; ) type ctxKey string const RequestIDKey ctxKey = \u0026#34;request_id\u0026#34; func RequestLogger(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() rid := newRID() ctx := context.WithValue(r.Context(), RequestIDKey, rid) l := slog.Default().With( \u0026#34;request_id\u0026#34;, rid, \u0026#34;method\u0026#34;, r.Method, \u0026#34;path\u0026#34;, r.URL.Path, ) l.Info(\u0026#34;request started\u0026#34;) rw := \u0026amp;statusRecorder{ResponseWriter: w, status: 200} next.ServeHTTP(rw, r.WithContext(ctx)) l.Info(\u0026#34;request completed\u0026#34;, \u0026#34;status\u0026#34;, rw.status, \u0026#34;latency_ms\u0026#34;, time.Since(start).Milliseconds()) }) } type statusRecorder struct { http.ResponseWriter status int } func (w *statusRecorder) WriteHeader(code int) { w.status = code w.ResponseWriter.WriteHeader(code) } func newRID() string { const letters = \u0026#34;abcdefghijklmnopqrstuvwxyz0123456789\u0026#34; b := make([]byte, 12) for i := range b { b[i] = letters[rand.Intn(len(letters))] } return string(b) } Hook this into your server router. If you’re building your own HTTP server, our step‑by‑step REST tutorial may help: How to Build a REST API in Go using net/http .\nLibrary APIs: pass loggers or context? Two common approaches:\nPass a *slog.Logger to constructors and keep it on the type. type Store struct { log *slog.Logger } func NewStore(log *slog.Logger) *Store { return \u0026amp;Store{log: log} } Derive a logger from context.Context at call sites (attach fields per request). You can wrap this yourself by keeping a logger in context and retrieving it in functions. Choose one convention and stick to it to keep call sites clean. For clean separation of layers and testability, see our guide: Structuring Go Projects: Clean Project Structure and Best Practices .\nEmitting errors with details Log actionable error information: message, error type, and a few high‑signal attributes (IDs, sizes, counts). Avoid dumping full payloads.\nif err := svc.Do(ctx, job); err != nil { slog.Error(\u0026#34;process job failed\u0026#34;, \u0026#34;job_id\u0026#34;, job.ID, \u0026#34;err\u0026#34;, err) return err } Working with JSON payloads Keep payload logging minimal and scrubbed. For structured data handling basics in Go, revisit: Working with JSON in Go: Encode, Decode, and Tag Structs .\nEnvironment presets Create a small helper that picks sensible defaults based on ENV.\npackage logx import ( \u0026#34;log/slog\u0026#34; \u0026#34;os\u0026#34; ) func New() *slog.Logger { env := os.Getenv(\u0026#34;ENV\u0026#34;) opts := \u0026amp;slog.HandlerOptions{Level: slog.LevelInfo} var h slog.Handler if env == \u0026#34;prod\u0026#34; { h = slog.NewJSONHandler(os.Stdout, opts) } else { opts.AddSource = true h = slog.NewTextHandler(os.Stdout, opts) } return slog.New(h) } Rotation and shipping slog doesn’t rotate files\u0026ndash;it writes to an io.Writer. In containers, write to stdout/stderr and let the platform collect (Docker, systemd, Kubernetes). If you must write files, use external rotation (logrotate) or a service.\nObservability integrations Structured logs complement metrics and traces. If you’re adding tracing next, consider OpenTelemetry for Go; link request IDs between logs and traces for faster incident response.\nTesting logs You can capture output with a buffer for assertions. For broader testing patterns, see: Testing in Go: Writing Unit Tests with the Testing Package .\npackage mypkg import ( \u0026#34;bytes\u0026#34; \u0026#34;log/slog\u0026#34; \u0026#34;testing\u0026#34; ) func TestLogs(t *testing.T) { var buf bytes.Buffer l := slog.New(slog.NewJSONHandler(\u0026amp;buf, \u0026amp;slog.HandlerOptions{Level: slog.LevelDebug})) l.Info(\u0026#34;hello\u0026#34;, \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) out := buf.String() if want := \u0026#34;\\\u0026#34;hello\\\u0026#34;\u0026#34;; !bytes.Contains([]byte(out), []byte(want)) { t.Fatalf(\u0026#34;missing message: %s\u0026#34;, out) } } Performance benchmarking slog is designed to be fast with minimal allocations. Here\u0026rsquo;s how it compares to popular logging libraries.\npackage logging_test import ( \u0026#34;io\u0026#34; \u0026#34;log/slog\u0026#34; \u0026#34;testing\u0026#34; \u0026#34;go.uber.org/zap\u0026#34; \u0026#34;github.com/sirupsen/logrus\u0026#34; ) func BenchmarkSlog(b *testing.B) { logger := slog.New(slog.NewJSONHandler(io.Discard, \u0026amp;slog.HandlerOptions{Level: slog.LevelInfo})) b.ResetTimer() b.RunParallel(func(pb *testing.PB) { for pb.Next() { logger.Info(\u0026#34;benchmark message\u0026#34;, \u0026#34;user_id\u0026#34;, 12345, \u0026#34;action\u0026#34;, \u0026#34;login\u0026#34;, \u0026#34;ip\u0026#34;, \u0026#34;192.168.1.1\u0026#34;) } }) } func BenchmarkZap(b *testing.B) { logger := zap.New(zap.NewCore( zap.NewJSONEncoder(zap.NewProductionEncoderConfig()), zap.AddSync(io.Discard), zap.InfoLevel, )) b.ResetTimer() b.RunParallel(func(pb *testing.PB) { for pb.Next() { logger.Info(\u0026#34;benchmark message\u0026#34;, zap.Int(\u0026#34;user_id\u0026#34;, 12345), zap.String(\u0026#34;action\u0026#34;, \u0026#34;login\u0026#34;), zap.String(\u0026#34;ip\u0026#34;, \u0026#34;192.168.1.1\u0026#34;)) } }) } func BenchmarkLogrus(b *testing.B) { logger := logrus.New() logger.SetOutput(io.Discard) logger.SetFormatter(\u0026amp;logrus.JSONFormatter{}) b.ResetTimer() b.RunParallel(func(pb *testing.PB) { for pb.Next() { logger.WithFields(logrus.Fields{\u0026#34;user_id\u0026#34;: 12345, \u0026#34;action\u0026#34;: \u0026#34;login\u0026#34;, \u0026#34;ip\u0026#34;: \u0026#34;192.168.1.1\u0026#34;}).Info(\u0026#34;benchmark message\u0026#34;) } }) } Typical results (your mileage may vary):\nBenchmarkSlog-8 2000000 650 ns/op 48 B/op 1 allocs/op BenchmarkZap-8 3000000 420 ns/op 32 B/op 1 allocs/op BenchmarkLogrus-8 500000 3200 ns/op 280 B/op 10 allocs/op slog offers excellent performance while maintaining simplicity. Zap is still fastest for high-throughput scenarios, but slog\u0026rsquo;s standard library status and ease of use make it ideal for most applications.\nAdvanced: Observability Integration The following sections cover enterprise-grade logging patterns. If you\u0026rsquo;re just getting started, you can skip to the \u0026ldquo;Migration notes\u0026rdquo; section and return here when you need production observability.\nELK Stack and Grafana integration Production logging shines when paired with log aggregation. Here\u0026rsquo;s how to set up slog with popular observability stacks.\nElasticsearch + Logstash + Kibana (ELK) Docker Compose setup (docker-compose.yml):\nversion: \u0026#39;3.8\u0026#39; services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0 environment: - discovery.type=single-node - xpack.security.enabled=false ports: - \u0026#34;9200:9200\u0026#34; logstash: image: docker.elastic.co/logstash/logstash:8.11.0 volumes: - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf ports: - \u0026#34;5044:5044\u0026#34; depends_on: - elasticsearch kibana: image: docker.elastic.co/kibana/kibana:8.11.0 environment: - ELASTICSEARCH_HOSTS=http://elasticsearch:9200 ports: - \u0026#34;5601:5601\u0026#34; depends_on: - elasticsearch app: build: . environment: - ENV=prod - LOG_OUTPUT=json depends_on: - logstash Logstash configuration (logstash.conf):\ninput { beats { port =\u0026gt; 5044 } # Or direct TCP input for simple setups tcp { port =\u0026gt; 5000 codec =\u0026gt; json_lines } } filter { if [fields][service] == \u0026#34;go-api\u0026#34; { # Parse slog JSON output json { source =\u0026gt; \u0026#34;message\u0026#34; } # Convert slog timestamp date { match =\u0026gt; [ \u0026#34;time\u0026#34;, \u0026#34;ISO8601\u0026#34; ] } # Extract request_id for correlation if [request_id] { mutate { add_tag =\u0026gt; [ \u0026#34;has_request_id\u0026#34; ] } } # Create structured fields mutate { add_field =\u0026gt; { \u0026#34;service\u0026#34; =\u0026gt; \u0026#34;go-api\u0026#34; } add_field =\u0026gt; { \u0026#34;log_level\u0026#34; =\u0026gt; \u0026#34;%{level}\u0026#34; } } } } output { elasticsearch { hosts =\u0026gt; [\u0026#34;elasticsearch:9200\u0026#34;] index =\u0026gt; \u0026#34;go-logs-%{+YYYY.MM.dd}\u0026#34; } } Go application with structured logging:\npackage main import ( \u0026#34;log/slog\u0026#34; \u0026#34;net\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { // Configure slog for ELK opts := \u0026amp;slog.HandlerOptions{ Level: slog.LevelInfo, ReplaceAttr: func(groups []string, a slog.Attr) slog.Attr { // Ensure timestamp is in ISO8601 format for Logstash if a.Key == slog.TimeKey { return slog.String(slog.TimeKey, a.Value.Time().Format(time.RFC3339)) } return a }, } var handler slog.Handler if os.Getenv(\u0026#34;LOG_OUTPUT\u0026#34;) == \u0026#34;logstash\u0026#34; { // Send directly to Logstash TCP input conn, err := net.Dial(\u0026#34;tcp\u0026#34;, \u0026#34;logstash:5000\u0026#34;) if err != nil { panic(err) } handler = slog.NewJSONHandler(conn, opts) } else { handler = slog.NewJSONHandler(os.Stdout, opts) } logger := slog.New(handler) slog.SetDefault(logger) // Add service metadata baseLogger := slog.Default().With(\u0026#34;service\u0026#34;, \u0026#34;go-api\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0.0\u0026#34;) baseLogger.Info(\u0026#34;application started\u0026#34;, \u0026#34;port\u0026#34;, 8080) // Example business logic logging requestLogger := baseLogger.With(\u0026#34;request_id\u0026#34;, \u0026#34;req-123\u0026#34;, \u0026#34;user_id\u0026#34;, 456) requestLogger.Info(\u0026#34;processing order\u0026#34;, \u0026#34;order_id\u0026#34;, \u0026#34;ord-789\u0026#34;, \u0026#34;amount\u0026#34;, 99.99) requestLogger.Warn(\u0026#34;inventory low\u0026#34;, \u0026#34;product_id\u0026#34;, \u0026#34;prod-123\u0026#34;, \u0026#34;remaining\u0026#34;, 5) } Grafana + Loki setup Docker Compose for Grafana stack:\nversion: \u0026#39;3.8\u0026#39; services: loki: image: grafana/loki:2.9.0 ports: - \u0026#34;3100:3100\u0026#34; command: -config.file=/etc/loki/local-config.yaml promtail: image: grafana/promtail:2.9.0 volumes: - /var/log:/var/log:ro - ./promtail-config.yml:/etc/promtail/config.yml command: -config.file=/etc/promtail/config.yml grafana: image: grafana/grafana:10.2.0 ports: - \u0026#34;3000:3000\u0026#34; environment: - GF_SECURITY_ADMIN_PASSWORD=admin volumes: - grafana-storage:/var/lib/grafana volumes: grafana-storage: Promtail configuration (promtail-config.yml):\nserver: http_listen_port: 9080 grpc_listen_port: 0 positions: filename: /tmp/positions.yaml clients: - url: http://loki:3100/loki/api/v1/push scrape_configs: - job_name: go-app-logs static_configs: - targets: - localhost labels: job: go-app service: api __path__: /var/log/go-app/*.log pipeline_stages: - json: expressions: time: time level: level msg: msg request_id: request_id user_id: user_id - labels: level: request_id: - timestamp: source: time format: RFC3339 Go app configured for Loki:\npackage main import ( \u0026#34;log/slog\u0026#34; \u0026#34;os\u0026#34; ) func setupLogger() *slog.Logger { opts := \u0026amp;slog.HandlerOptions{ Level: slog.LevelInfo, ReplaceAttr: func(groups []string, a slog.Attr) slog.Attr { // Add environment and service labels return a }, } // Write to file for Promtail to collect if logFile := os.Getenv(\u0026#34;LOG_FILE\u0026#34;); logFile != \u0026#34;\u0026#34; { file, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err == nil { return slog.New(slog.NewJSONHandler(file, opts)) } } return slog.New(slog.NewJSONHandler(os.Stdout, opts)) } func main() { logger := setupLogger().With( \u0026#34;service\u0026#34;, \u0026#34;go-api\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;1.0.0\u0026#34;, \u0026#34;environment\u0026#34;, os.Getenv(\u0026#34;ENV\u0026#34;), ) slog.SetDefault(logger) slog.Info(\u0026#34;service started\u0026#34;, \u0026#34;config\u0026#34;, \u0026#34;loaded\u0026#34;) // Example with trace correlation traceLogger := logger.With(\u0026#34;trace_id\u0026#34;, \u0026#34;trace-abc123\u0026#34;, \u0026#34;span_id\u0026#34;, \u0026#34;span-456\u0026#34;) traceLogger.Info(\u0026#34;database query\u0026#34;, \u0026#34;table\u0026#34;, \u0026#34;users\u0026#34;, \u0026#34;duration_ms\u0026#34;, 45) traceLogger.Error(\u0026#34;connection failed\u0026#34;, \u0026#34;error\u0026#34;, \u0026#34;timeout\u0026#34;, \u0026#34;retry_count\u0026#34;, 3) } Grafana Dashboard queries LogQL queries for common patterns:\n# All errors in the last hour {service=\u0026#34;go-api\u0026#34;} |= \u0026#34;ERROR\u0026#34; | json # Request latency by endpoint {service=\u0026#34;go-api\u0026#34;} | json | __error__ = \u0026#34;\u0026#34; | unwrap duration_ms | rate(5m) # Error rate by request_id rate(({service=\u0026#34;go-api\u0026#34;} |= \u0026#34;ERROR\u0026#34; | json)[5m]) # Top users by request volume topk(10, count by (user_id) (rate({service=\u0026#34;go-api\u0026#34;} | json | __error__ = \u0026#34;\u0026#34; [5m]))) Key benefits of structured logging with observability:\nCorrelation: Link logs, metrics, and traces with request_id Alerting: Set up alerts on error rates or specific patterns Debugging: Filter by user, endpoint, or time range instantly Analytics: Aggregate business metrics from log data Production tip: Always include consistent field names (request_id, user_id, trace_id) across your microservices for easier correlation in your observability stack.\nMigration notes (zap/logrus -\u0026gt; slog) Message + fields map directly to slog.Info(\u0026quot;msg\u0026quot;, \u0026quot;key\u0026quot;, val, ...). Replace global usage with dependency injection or a single slog.SetDefault during bootstrap. If you relied on sampling or custom encoders, keep using your old logger behind an adapter until equivalent features are available or needed. Common pitfalls and tips Be consistent with key names (request_id, not requestId). Avoid logging entire structs or raw bodies in production. Use WithGroup for domains like http, db, queue. Keep error logs actionable; include IDs, not entire payloads. Prefer stdout in containers; let your platform ship logs. Putting it together (mini example) package main import ( \u0026#34;log/slog\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; ) func main() { env := os.Getenv(\u0026#34;ENV\u0026#34;) opts := \u0026amp;slog.HandlerOptions{Level: slog.LevelInfo} if env != \u0026#34;prod\u0026#34; { opts.AddSource = true } var h slog.Handler if env == \u0026#34;prod\u0026#34; { h = slog.NewJSONHandler(os.Stdout, opts) } else { h = slog.NewTextHandler(os.Stdout, opts) } slog.SetDefault(slog.New(h)) mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/healthz\u0026#34;, func(w http.ResponseWriter, r *http.Request) { slog.Default().WithGroup(\u0026#34;http\u0026#34;).Info(\u0026#34;health check\u0026#34;, \u0026#34;status\u0026#34;, \u0026#34;ok\u0026#34;, \u0026#34;time\u0026#34;, time.Now().Format(time.RFC3339)) w.WriteHeader(http.StatusOK) _, _ = w.Write([]byte(\u0026#34;ok\u0026#34;)) }) addr := \u0026#34;:8080\u0026#34; slog.Info(\u0026#34;server listening\u0026#34;, \u0026#34;addr\u0026#34;, addr) _ = http.ListenAndServe(addr, mux) } Where to go next Build a small REST API and add the middleware above: How to Build a REST API in Go using net/http Learn how to pass cancellation and deadlines with context: Using Context in Go: Cancellation, Timeout, and Deadlines Explained Organize your project for growth: Structuring Go Projects: Clean Project Structure and Best Practices Review JSON handling patterns in Go: Working with JSON in Go: Encode, Decode, and Tag Structs Add automated tests, including log checks: Testing in Go: Writing Unit Tests with the Testing Package With slog, you get a batteries‑included, standard way to emit clean, consistent logs. Start with text locally, JSON in production, add just enough context, and keep sensitive data out. Your future self\u0026ndash;and your on‑call teammates\u0026ndash;will thank you.\n","href":"/2025/09/complete-guide-slog-go-structured-logging-2025.html","title":"The Complete Guide to slog (Go 1.21+) Modern Structured Logging in Go (2025)"},{"content":"The way we write code is changing dramatically. AI coding assistants have moved from experimental tools to essential companions that can genuinely transform your development workflow. I\u0026rsquo;ve been testing AI coding tools since they first emerged, and honestly, the progress in 2025 has been mind-blowing.\nWhat used to take hours of debugging, researching documentation, or writing boilerplate code can now be done in minutes. But with so many AI coding assistants flooding the market, choosing the right one feels overwhelming. That\u0026rsquo;s why I\u0026rsquo;ve spent months testing every major AI coding tool to bring you this comprehensive guide.\nWhether you\u0026rsquo;re a seasoned developer looking to boost productivity or a beginner wanting to accelerate your learning, these AI coding assistants will revolutionize how you approach programming. Let\u0026rsquo;s dive into the 10 best options that are actually worth your time and money in 2025.\nWhy AI Coding Assistants Matter in 2025 Before we jump into the tools, let\u0026rsquo;s talk about why this matters. According to recent studies, developers using AI coding assistants report 55% faster task completion and significantly reduced debugging time. It\u0026rsquo;s not about replacing developers \u0026ndash; it\u0026rsquo;s about eliminating the tedious parts so you can focus on creative problem-solving and architecture decisions.\nThink of it this way: instead of spending 20 minutes writing repetitive CRUD operations or searching Stack Overflow for that regex pattern you always forget, you can focus on building features that actually matter to your users.\n1. GitHub Copilot - The Industry Standard Website: github.com/features/copilot Price: $10/month (Individual), $19/month (Business)\nBest For: All-around coding assistance across multiple languages\nGitHub Copilot remains the gold standard for AI coding assistance, and for good reason. Built on OpenAI\u0026rsquo;s Codex, it understands context incredibly well and provides suggestions that feel almost telepathic.\nWhat makes Copilot special is its deep integration with your development environment. It doesn\u0026rsquo;t just complete lines \u0026ndash; it understands your project structure, follows your coding patterns, and even generates entire functions based on comments. I\u0026rsquo;ve found it particularly brilliant for writing REST API endpoints and database queries.\nThe chat feature introduced in 2024 has been a game-changer. You can ask questions about your code, request explanations, or even get help with debugging directly in your IDE. It\u0026rsquo;s like having a senior developer looking over your shoulder, ready to help whenever you\u0026rsquo;re stuck.\nPros: Excellent context awareness, seamless IDE integration, strong community\nCons: Subscription cost, occasional overcomplicated suggestions\n2. Amazon CodeWhisperer - AWS Integration Champion Website: aws.amazon.com/codewhisperer Price: Free tier available, $19/month for Professional\nBest For: AWS development, cloud-native applications\nAmazon\u0026rsquo;s entry into the AI coding space focuses heavily on cloud development, and it shows. CodeWhisperer excels at generating AWS-optimized code and identifying security vulnerabilities in real-time.\nWhat impressed me most is its security scanning capabilities. It automatically flags potential security issues and suggests fixes, which is incredibly valuable when deploying applications to production . The integration with AWS services is seamless \u0026ndash; it understands Lambda functions, DynamoDB operations, and S3 interactions better than any other AI assistant.\nThe free tier is genuinely useful, making it accessible for individual developers and small teams. If you\u0026rsquo;re working primarily with AWS, this should be your first choice.\nPros: Strong AWS integration, security focus, generous free tier\nCons: Less effective outside AWS ecosystem, newer to market\n3. Tabnine - The Privacy-Focused Choice Website: tabnine.com Price: Free basic version, $12/month for Pro\nBest For: Enterprise environments, privacy-conscious developers\nTabnine takes a different approach by focusing on privacy and customization. Unlike cloud-based solutions, it can run entirely on your local machine, which is crucial for enterprise environments with strict data policies.\nWhat sets Tabnine apart is its ability to learn from your specific codebase. It adapts to your team\u0026rsquo;s coding standards, naming conventions, and architectural patterns. This personalization makes its suggestions feel more relevant and consistent with your project\u0026rsquo;s style.\nThe AI models are trained on permissively licensed code only, addressing copyright concerns that some developers have with other tools. This ethical approach has made it popular in corporate environments.\nPros: Privacy-focused, learns team patterns, ethical training data\nCons: Smaller suggestion database, requires setup time\n4. Codeium - The Free Powerhouse Website: codeium.com Price: Free for individuals, Enterprise pricing available\nBest For: Budget-conscious developers, students\nCodeium might be the most impressive free AI coding assistant available. It offers features that rival paid solutions without the subscription cost. I\u0026rsquo;ve been consistently surprised by the quality of its suggestions, especially for JavaScript and Python development .\nThe tool supports over 70 programming languages and integrates with all major IDEs. Its chat feature helps explain code, suggest improvements, and even refactor existing functions. For students or developers just starting with AI assistance, this is an excellent entry point.\nThe company\u0026rsquo;s business model focuses on enterprise features rather than limiting individual use, which means the free tier remains genuinely useful rather than a limited trial.\nPros: Completely free for individuals, wide language support, no usage limits\nCons: Enterprise features limited, newer company\n5. Cursor - The AI-First IDE Website: cursor.sh Price: $20/month for Pro features\nBest For: Developers wanting an AI-native editing experience\nCursor takes a revolutionary approach by building AI assistance directly into the editor rather than as a plugin. This isn\u0026rsquo;t just another code completion tool \u0026ndash; it\u0026rsquo;s an entire IDE designed around AI collaboration.\nThe standout feature is its ability to understand and modify entire codebases. You can ask it to implement features across multiple files, refactor large sections of code, or explain complex system interactions. It\u0026rsquo;s particularly powerful for understanding unfamiliar codebases quickly.\nThe AI can also generate commit messages, write tests, and even help with code reviews. It feels like the future of software development, where AI is a true collaborative partner rather than just a smart autocomplete.\nPros: Revolutionary AI integration, codebase understanding, innovative features\nCons: Requires learning new IDE, higher price point\n6. Claude Dev - The Reasoning Expert Website: claude.ai Price: Various pricing tiers based on usage\nBest For: Complex problem solving, architectural decisions\nClaude Dev, built on Anthropic\u0026rsquo;s Claude AI, excels at understanding context and providing thoughtful, well-reasoned coding suggestions. It\u0026rsquo;s particularly strong at explaining complex concepts and helping with architectural decisions.\nWhat I appreciate most about Claude Dev is its ability to engage in detailed technical discussions. You can ask about design patterns, performance implications, or security considerations, and get responses that feel like consulting with a senior architect.\nThe tool is excellent for code reviews, suggesting not just syntax improvements but also discussing the broader implications of different implementation approaches.\nPros: Excellent reasoning abilities, architectural insights, detailed explanations\nCons: Higher cost for heavy usage, focus on consultation over completion\n7. Replit Ghostwriter - The Collaborative Coder Website: replit.com/site/ghostwriter Price: $10/month as part of Replit Core\nBest For: Learning, prototyping, collaborative development\nReplit\u0026rsquo;s Ghostwriter shines in collaborative and educational environments. Integrated into the Replit platform, it provides contextual suggestions while you code in the browser, making it perfect for rapid prototyping and learning.\nThe tool excels at explaining code concepts and helping beginners understand programming patterns. It can generate complete applications from descriptions, making it excellent for proof-of-concepts and learning new technologies .\nThe collaborative features allow teams to work together with AI assistance, making it valuable for code reviews and pair programming sessions.\nPros: Excellent for learning, collaborative features, browser-based\nCons: Limited to Replit platform, less suitable for complex projects\n8. Sourcegraph Cody - The Enterprise Solution Website: sourcegraph.com/cody Price: Free tier, $9/month for Pro, Enterprise pricing\nBest For: Large codebases, enterprise development\nCody by Sourcegraph is designed for enterprise environments with massive codebases. It understands code across entire repositories and can help navigate complex system architectures.\nThe tool\u0026rsquo;s strength lies in its ability to understand relationships between different parts of large applications. It can suggest changes that maintain consistency across your entire codebase and help identify potential breaking changes.\nFor teams working on microservices or large monolithic applications, Cody\u0026rsquo;s repository-wide understanding is invaluable for maintaining code quality and consistency.\nPros: Enterprise-focused, large codebase understanding, team features\nCons: Overkill for small projects, enterprise-focused pricing\n9. CodeT5 - The Open Source Alternative Website: github.com/salesforce/CodeT5 Price: Free (open source)\nBest For: Researchers, privacy advocates, custom implementations\nCodeT5 represents the open-source approach to AI coding assistance. Based on the T5 transformer architecture, it provides transparency and customizability that proprietary solutions can\u0026rsquo;t match.\nWhile it requires more technical setup than commercial alternatives, CodeT5 offers complete control over your AI coding assistant. You can train it on your specific domain, modify its behavior, and integrate it into custom workflows.\nThis tool is perfect for organizations with strict privacy requirements or researchers wanting to experiment with AI-assisted development.\nPros: Open source, customizable, transparent\nCons: Requires technical setup, less polish than commercial tools\n10. JetBrains AI Assistant - The IDE Native Website: jetbrains.com/ai Price: Included with JetBrains IDEs subscription\nBest For: JetBrains IDE users, integrated development workflows\nJetBrains\u0026rsquo; AI Assistant integrates seamlessly with their popular IDE suite, providing context-aware suggestions that understand your project structure and dependencies.\nThe tool excels at code generation within the JetBrains ecosystem, understanding project templates, frameworks, and coding standards. It\u0026rsquo;s particularly strong for Java and Kotlin development , leveraging JetBrains\u0026rsquo; deep understanding of these languages.\nFor developers already using IntelliJ IDEA, PyCharm, or other JetBrains IDEs, this provides seamless AI assistance without changing your workflow.\nPros: Deep IDE integration, framework awareness, familiar interface\nCons: Limited to JetBrains IDEs, part of larger subscription\nChoosing the Right AI Coding Assistant The best AI coding assistant depends on your specific needs:\nFor Beginners: Start with Codeium (free) or GitHub Copilot (industry standard)\nFor AWS Development: Amazon CodeWhisperer is unmatched\nFor Privacy: Tabnine offers local processing and ethical training\nFor Innovation: Cursor provides a glimpse into the future of AI-assisted development\nFor Enterprise: Sourcegraph Cody handles large codebases effectively\nThe Future of AI-Assisted Development These tools are just the beginning. As AI models become more sophisticated and context-aware, we\u0026rsquo;ll see even more intelligent assistance. The key is starting now, learning how to effectively collaborate with AI, and staying updated with new developments.\nAI coding assistants aren\u0026rsquo;t about replacing developers \u0026ndash; they\u0026rsquo;re about amplifying human creativity and problem-solving abilities. By handling routine tasks, they free us to focus on architecture, user experience, and innovative solutions.\nGetting Started My recommendation? Try the free tiers of 2-3 different tools to see which fits your workflow best. Most developers end up using different AI assistants for different tasks \u0026ndash; GitHub Copilot for general development, CodeWhisperer for AWS projects, and Cursor for complex refactoring.\nRemember, these tools are most effective when you understand what you\u0026rsquo;re trying to build. They accelerate development but don\u0026rsquo;t replace the need to understand programming fundamentals and system design principles.\nThe AI coding revolution is here, and these 10 assistants represent the best tools available today. Choose the ones that fit your workflow, budget, and privacy requirements, then start building the future of software development.\nWhat\u0026rsquo;s your experience with AI coding assistants? Have you tried any of these tools? Let me know in the comments which ones have been most helpful in your development workflow!\n","href":"/2025/08/10-best-ai-coding-assistants-every-developer-should-try-2025.html","title":"10 Best AI Coding Assistants Every Developer Should Try in 2025"},{"content":"Whether you\u0026rsquo;re building web applications, managing servers, or working in DevOps, mastering Linux commands is absolutely essential for any developer in 2025. I\u0026rsquo;ve been working with Linux systems for years, and I can tell you that knowing the right commands at the right time can save you hours of work and make you incredibly productive.\nLinux dominates the server world, powers most cloud infrastructure, and is the backbone of modern development environments. From managing Docker containers to setting up secure web servers with HTTPS , these commands will be your daily companions.\nIn this comprehensive guide, I\u0026rsquo;ll walk you through the most essential Linux commands that every developer should master in 2025. These aren\u0026rsquo;t just theoretical examples - every command here has been tested and works in real-world scenarios.\nNavigation and File System Commands 1. pwd - Print Working Directory Before you can navigate anywhere, you need to know where you are. The pwd command shows your current directory location.\npwd Example output:\n/home/username/projects/myapp This is incredibly useful when you\u0026rsquo;re deep in a project structure and need to orient yourself quickly.\n2. ls - List Directory Contents The ls command is probably the most used command in Linux. It shows you what\u0026rsquo;s in your current directory.\n# Basic listing ls # Detailed listing with permissions and timestamps ls -la # List only directories ls -d */ # Sort by modification time (newest first) ls -lt # Show file sizes in human readable format ls -lh The -la flag is particularly useful as it shows hidden files (those starting with a dot), file permissions, ownership, and timestamps. Perfect for debugging permission issues or finding configuration files.\n3. cd - Change Directory Moving around the file system is fundamental. The cd command lets you navigate to different directories.\n# Go to a specific directory cd /var/log # Go to home directory cd ~ cd # Go back to previous directory cd - # Go up one directory level cd .. # Go up two directory levels cd ../.. Pro tip: cd - is a lifesaver when you\u0026rsquo;re switching between two directories frequently during development.\n4. find - Search for Files and Directories The find command is incredibly powerful for locating files based on various criteria.\n# Find files by name find . -name \u0026#34;*.js\u0026#34; # Find files modified in the last 7 days find . -mtime -7 # Find files larger than 100MB find . -size +100M # Find and execute command on results find . -name \u0026#34;*.log\u0026#34; -exec rm {} \\; # Find directories only find . -type d -name \u0026#34;node_modules\u0026#34; # Find files with specific permissions find . -perm 755 This is essential when working with large codebases or trying to clean up old files and dependencies.\nFile Operations and Management 5. cp - Copy Files and Directories Copying files and directories is a daily task for developers, whether backing up configurations or duplicating project structures.\n# Copy a file cp source.txt destination.txt # Copy with preserving timestamps and permissions cp -p config.json config.backup.json # Copy directory recursively cp -r project/ project-backup/ # Copy multiple files to directory cp *.txt backup/ # Interactive copy (asks before overwriting) cp -i important.conf important.conf.new 6. mv - Move/Rename Files The mv command both moves and renames files - it\u0026rsquo;s the same operation in Linux.\n# Rename a file mv old_name.txt new_name.txt # Move file to different directory mv myfile.txt /home/username/documents/ # Move and rename simultaneously mv temp.log /var/log/application.log # Move multiple files mv *.txt documents/ 7. rm - Remove Files and Directories Use with caution! The rm command permanently deletes files.\n# Remove a file rm unwanted.txt # Remove multiple files rm file1.txt file2.txt # Remove directory and all contents rm -rf old_project/ # Interactive removal (asks for confirmation) rm -i suspicious_file.txt # Remove all .log files in current directory rm *.log Warning: rm -rf is powerful but dangerous. Double-check your path before running it, especially with sudo privileges.\n8. mkdir - Create Directories Creating directories is straightforward with mkdir.\n# Create a single directory mkdir new_project # Create nested directories mkdir -p project/src/components # Create multiple directories at once mkdir backend frontend database # Create directory with specific permissions mkdir -m 755 public_folder The -p flag is particularly useful in development when you need to create entire directory structures in one command.\nFile Content Operations 9. cat - Display File Contents The cat command displays the entire content of a file.\n# Display file content cat package.json # Display multiple files cat file1.txt file2.txt # Display with line numbers cat -n app.js # Display non-printing characters cat -A config.txt 10. less and more - Page Through Files For large files, less and more allow you to scroll through content page by page.\n# View large log files less /var/log/syslog # Search within less (press / then type search term) less application.log # View file with more (simpler than less) more large_file.txt In less, use:\nSpace bar to go forward one page b to go back one page q to quit /search_term to search n to find next occurrence 11. head and tail - Show Beginning or End of Files Perfect for checking log files or large datasets.\n# Show first 10 lines (default) head error.log # Show first 20 lines head -n 20 access.log # Show last 10 lines tail error.log # Show last 20 lines and follow new additions (great for logs) tail -f -n 20 /var/log/nginx/access.log # Show last 50 lines tail -n 50 application.log The tail -f command is invaluable for monitoring log files in real-time during development and debugging.\n12. grep - Search Text Patterns grep is one of the most powerful tools for searching text patterns in files.\n# Search for text in a file grep \u0026#34;error\u0026#34; application.log # Case insensitive search grep -i \u0026#34;warning\u0026#34; system.log # Search recursively in all files grep -r \u0026#34;TODO\u0026#34; src/ # Show line numbers with matches grep -n \u0026#34;function\u0026#34; app.js # Search for exact word grep -w \u0026#34;user\u0026#34; database.log # Invert match (show lines that don\u0026#39;t contain pattern) grep -v \u0026#34;debug\u0026#34; error.log # Count matching lines grep -c \u0026#34;success\u0026#34; access.log # Show context (2 lines before and after match) grep -C 2 \u0026#34;exception\u0026#34; error.log 13. sed - Stream Editor sed is perfect for quick text replacements and file modifications.\n# Replace first occurrence in each line sed \u0026#39;s/old/new/\u0026#39; file.txt # Replace all occurrences sed \u0026#39;s/old/new/g\u0026#39; file.txt # Edit file in place sed -i \u0026#39;s/localhost/production.server.com/g\u0026#39; config.txt # Delete lines containing pattern sed \u0026#39;/debug/d\u0026#39; log.txt # Print specific line numbers sed -n \u0026#39;10,20p\u0026#39; large_file.txt Process and System Management 14. ps - Display Running Processes Understanding what\u0026rsquo;s running on your system is crucial for debugging and performance monitoring.\n# Show processes for current user ps # Show all processes with detailed info ps aux # Show processes in tree format ps auxf # Show processes for specific user ps -u username # Find specific process ps aux | grep nginx 15. top and htop - Real-time Process Monitoring Monitor system resources and running processes in real-time.\n# Basic system monitor top # Better alternative (if installed) htop # Show processes by CPU usage top -o %CPU # Show processes by memory usage top -o %MEM In top:\nPress q to quit Press k to kill a process Press M to sort by memory Press P to sort by CPU 16. kill and killall - Terminate Processes Stop problematic or unnecessary processes.\n# Kill process by PID kill 1234 # Force kill process kill -9 1234 # Kill process by name killall node # Kill all processes matching pattern pkill -f \u0026#34;python.*myapp\u0026#34; # List signals available kill -l 17. jobs, bg, and fg - Job Control Manage background and foreground processes.\n# List active jobs jobs # Put current process in background # (Press Ctrl+Z to suspend, then:) bg # Bring job to foreground fg # Run command in background from start nohup python long_running_script.py \u0026amp; # Bring specific job to foreground fg %1 Network and System Information 18. ping - Test Network Connectivity Test if you can reach other systems or websites.\n# Basic ping ping google.com # Ping with limited count ping -c 4 8.8.8.8 # Ping with specific interval ping -i 2 localhost # Ping with larger packet size ping -s 1000 server.com 19. wget and curl - Download Files and Test APIs Essential for downloading files and testing web services.\n# Download file with wget wget https://example.com/file.zip # Download and save with different name wget -O myfile.zip https://example.com/file.zip # Download recursively (be careful!) wget -r -np https://example.com/directory/ # Basic curl request curl https://api.example.com/users # POST request with data curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;name\u0026#34;:\u0026#34;John\u0026#34;}\u0026#39; https://api.example.com/users # Save response to file curl -o response.json https://api.example.com/data # Follow redirects curl -L https://bit.ly/shortened-url # Include headers in output curl -i https://api.example.com/status 20. netstat and ss - Network Statistics Monitor network connections and ports.\n# Show all connections netstat -a # Show listening ports netstat -l # Show TCP connections netstat -t # Show which process is using which port netstat -tulpn # Modern alternative to netstat ss -tulpn # Check specific port ss -tulpn | grep :80 File Permissions and Ownership 21. chmod - Change File Permissions Managing file permissions is critical for security and functionality.\n# Make file executable chmod +x script.sh # Set specific permissions (rwxr-xr-x) chmod 755 myfile.txt # Make file readable/writable for owner only chmod 600 private.key # Remove execute permission for group and others chmod go-x sensitive_script.sh # Recursively change permissions chmod -R 644 web_content/ Permission numbers:\n7 = rwx (read, write, execute) 6 = rw- (read, write) 5 = r-x (read, execute) 4 = r\u0026ndash; (read only) 22. chown - Change File Ownership Change who owns files and directories.\n# Change owner chown username file.txt # Change owner and group chown username:groupname file.txt # Recursively change ownership chown -R www-data:www-data /var/www/html/ # Change only group chown :developers project/ Archive and Compression 23. tar - Archive Files tar is essential for creating backups and distributing code.\n# Create archive tar -czf backup.tar.gz project/ # Extract archive tar -xzf backup.tar.gz # List archive contents tar -tzf backup.tar.gz # Extract to specific directory tar -xzf backup.tar.gz -C /tmp/ # Create archive excluding certain files tar --exclude=\u0026#39;*.log\u0026#39; -czf clean_backup.tar.gz project/ 24. zip and unzip - Create and Extract ZIP Files Sometimes ZIP format is more convenient, especially for sharing with non-Linux users.\n# Create zip archive zip -r project.zip project/ # Extract zip file unzip project.zip # List zip contents unzip -l project.zip # Extract to specific directory unzip project.zip -d /tmp/extracted/ # Create zip excluding certain files zip -r project.zip project/ -x \u0026#34;*.log\u0026#34; \u0026#34;*/node_modules/*\u0026#34; Text Processing and Data Manipulation 25. sort - Sort Lines of Text Sorting data is frequently needed in development and analysis.\n# Sort file contents sort names.txt # Sort numerically sort -n numbers.txt # Reverse sort sort -r file.txt # Sort by specific column (space-separated) sort -k2 data.txt # Remove duplicates while sorting sort -u duplicated.txt # Sort by file size ls -l | sort -k5 -n 26. uniq - Report or Filter Unique Lines Work with unique lines in files (usually used after sort).\n# Show unique lines only sort file.txt | uniq # Count occurrences of each line sort file.txt | uniq -c # Show only duplicated lines sort file.txt | uniq -d # Show only unique lines (no duplicates) sort file.txt | uniq -u 27. wc - Word, Line, Character, and Byte Count Count various aspects of file contents.\n# Count lines, words, and characters wc file.txt # Count only lines wc -l file.txt # Count only words wc -w file.txt # Count only characters wc -c file.txt # Count files in directory ls | wc -l System Monitoring and Disk Usage 28. df - Display Filesystem Disk Usage Monitor disk space usage across mounted filesystems.\n# Show disk usage for all filesystems df # Show in human readable format df -h # Show specific filesystem df -h /var # Show inode usage df -i 29. du - Display Directory Space Usage Check how much space directories and files are using.\n# Show directory sizes du -h # Show only directory totals du -sh */ # Show largest directories first du -h | sort -rh # Show size of specific directory du -sh project/ # Exclude certain file types du -h --exclude=\u0026#34;*.log\u0026#34; project/ 30. free - Display Memory Usage Monitor system memory usage.\n# Show memory usage free # Show in human readable format free -h # Update every 2 seconds free -h -s 2 # Show memory usage in MB free -m Environment and Variables 31. env and export - Environment Variables Manage environment variables for applications and scripts.\n# Show all environment variables env # Set environment variable for current session export DATABASE_URL=\u0026#34;postgresql://localhost:5432/mydb\u0026#34; # Show specific variable echo $PATH # Set variable for single command DATABASE_URL=\u0026#34;test://localhost\u0026#34; node app.js # Make variable available to child processes export NODE_ENV=production 32. which and whereis - Locate Commands Find where commands and programs are located.\n# Find command location which python # Find multiple locations and info whereis python # Check if command exists which docker || echo \u0026#34;Docker not installed\u0026#34; # Show all locations in PATH which -a python Advanced Tips and Combinations Command Chaining and Pipes Linux\u0026rsquo;s real power comes from combining commands:\n# Chain commands with pipes ps aux | grep node | awk \u0026#39;{print $2}\u0026#39; | xargs kill # Find large files and show details find . -size +100M | xargs ls -lh # Count unique IP addresses in log grep \u0026#34;GET\u0026#34; access.log | awk \u0026#39;{print $1}\u0026#39; | sort | uniq -c | sort -nr # Monitor log file for errors tail -f error.log | grep -i \u0026#34;critical\u0026#34; Using History and Shortcuts Make your terminal work more efficiently:\n# Show command history history # Re-run last command !! # Re-run command from history by number !123 # Search history interactively # Press Ctrl+R and type search term # Clear history history -c Useful Keyboard Shortcuts Ctrl+C: Kill current process Ctrl+Z: Suspend current process Ctrl+D: Exit current shell Ctrl+L: Clear screen Ctrl+A: Go to beginning of line Ctrl+E: Go to end of line Ctrl+U: Clear line before cursor Ctrl+K: Clear line after cursor Real-World Development Scenarios Debugging Web Applications When your web application isn\u0026rsquo;t working properly:\n# Check if service is running ps aux | grep nginx # Check what\u0026#39;s listening on port 80 ss -tulpn | grep :80 # Monitor error logs tail -f /var/log/nginx/error.log # Check disk space (common cause of issues) df -h # Find large log files eating disk space find /var/log -name \u0026#34;*.log\u0026#34; -size +100M Project Cleanup and Management Keeping your development environment clean:\n# Find and remove node_modules directories find . -name \u0026#34;node_modules\u0026#34; -type d -exec rm -rf {} + # Clean up old log files find . -name \u0026#34;*.log\u0026#34; -mtime +30 -delete # Find duplicate files by name find . -name \u0026#34;*.js\u0026#34; | sort | uniq -d # Check project size du -sh . \u0026amp;\u0026amp; du -sh */ | sort -rh Server Maintenance When managing Docker containers or web servers:\n# Monitor system resources top -u www-data # Check network connectivity ping -c 3 database.server.com # Verify SSL certificates (if using HTTPS setup) openssl s_client -connect domain.com:443 \u0026lt; /dev/null # Check service status systemctl status nginx # View recent system messages journalctl -n 50 Performance and Security Considerations Security Best Practices When working with these commands, especially on production servers:\nUse sudo carefully: Only when necessary, and always double-check commands Verify paths: Especially with rm -rf commands Check permissions: Before modifying files, understand their current permissions Monitor logs: Regularly check system and application logs for anomalies Backup before changes: Always backup important files before modifications Performance Tips Use specific paths: Instead of searching entire filesystem, limit searches to relevant directories Combine commands efficiently: Use pipes to avoid creating temporary files Monitor resource usage: Keep an eye on CPU and memory usage with top or htop Clean up regularly: Remove old logs, temporary files, and unused packages Conclusion Mastering these essential Linux commands will significantly boost your productivity as a developer in 2025. Whether you\u0026rsquo;re setting up secure HTTPS servers , managing containerized applications, or debugging complex systems, these commands form the foundation of effective Linux administration.\nThe key to becoming proficient is practice. Start incorporating these commands into your daily workflow, and soon they\u0026rsquo;ll become second nature. Remember, Linux command mastery isn\u0026rsquo;t about memorizing every flag and option - it\u0026rsquo;s about understanding the core functionality and knowing how to combine commands to solve real problems efficiently.\nAs development environments become increasingly complex with microservices, containers, and cloud infrastructure, these fundamental Linux skills become even more valuable. They\u0026rsquo;re the building blocks that will help you troubleshoot issues, automate tasks, and manage systems effectively throughout your development career.\nKeep this guide handy, practice regularly, and don\u0026rsquo;t hesitate to use the man command (e.g., man grep) to explore additional options and flags for each command. The Linux terminal is incredibly powerful, and these commands are your gateway to unlocking that power.\n","href":"/2025/08/essential-linux-commands-every-developer-must-know-2025.html","title":"Essential Linux Commands Every Developer Must Know in 2025"},{"content":"Building modern distributed systems is tricky business - you need services that can talk to each other quickly and reliably. That\u0026rsquo;s where gRPC comes in and absolutely crushes it. I\u0026rsquo;ve been building REST APIs for years, but when I first tried gRPC, it was like switching from a bicycle to a sports car. The speed difference is insane, plus you get type safety and can use it with practically any programming language.\nIf you\u0026rsquo;ve been building REST APIs in Go and wondering whether there\u0026rsquo;s a better approach for service-to-service communication, you\u0026rsquo;re in the right place. Today, we\u0026rsquo;ll explore gRPC from the ground up, building a complete user management service that you can actually use in production.\nWhat Makes gRPC Special? Before we dive into the code, let me tell you why I made the switch from REST to gRPC for service-to-service communication. Don\u0026rsquo;t get me wrong, REST APIs are fantastic for public APIs, but when you have a bunch of microservices that need to chat with each other all day long, REST starts showing its limitations.\ngRPC (Google Remote Procedure Call) runs on HTTP/2, so you automatically get all the cool stuff like multiplexing, server push, and binary serialization without any extra work. Instead of parsing JSON all the time (which gets expensive), gRPC uses Protocol Buffers for serialization. It\u0026rsquo;s way faster and takes up less space.\nBut here\u0026rsquo;s the real kicker - type safety. When you define your service contract with protobuf, it literally generates all your client and server code for you. Pretty sweet, right? No more of those annoying bugs where you spend 2 hours debugging only to find out someone changed a field name or used a string instead of an integer.\nSetting Up Your Go gRPC Environment First things first - let\u0026rsquo;s get everything set up. First, make sure you have Go installed (if not, check out our guide on installing Go on Linux ).\nYou\u0026rsquo;ll need to install the Protocol Buffer compiler and the Go plugins:\n# Install protoc compiler # On macOS brew install protobuf # On Ubuntu/Debian sudo apt update \u0026amp;\u0026amp; sudo apt install -y protobuf-compiler # Install Go plugins go install google.golang.org/protobuf/cmd/protoc-gen-go@latest go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest Create a new Go module for our project:\nmkdir grpc-user-service cd grpc-user-service go mod init grpc-user-service Install the required Go dependencies:\ngo get google.golang.org/grpc go get google.golang.org/protobuf/reflect/protoreflect go get google.golang.org/protobuf/runtime/protoimpl Defining Your Service Contract with Protocol Buffers Here\u0026rsquo;s where gRPC gets really cool - everything starts with defining your service contract. Create a proto directory and add our user service definition:\nmkdir proto Create proto/user.proto:\nsyntax = \u0026#34;proto3\u0026#34;; package user; option go_package = \u0026#34;./proto\u0026#34;; // User message definition message User { int32 id = 1; string name = 2; string email = 3; int32 age = 4; bool active = 5; } // Request messages message CreateUserRequest { string name = 1; string email = 2; int32 age = 3; } message GetUserRequest { int32 id = 1; } message UpdateUserRequest { int32 id = 1; string name = 2; string email = 3; int32 age = 4; bool active = 5; } message DeleteUserRequest { int32 id = 1; } message ListUsersRequest { int32 page = 1; int32 page_size = 2; } // Response messages message CreateUserResponse { User user = 1; string message = 2; } message GetUserResponse { User user = 1; } message UpdateUserResponse { User user = 1; string message = 2; } message DeleteUserResponse { string message = 1; } message ListUsersResponse { repeated User users = 1; int32 total = 2; } // UserService definition service UserService { rpc CreateUser(CreateUserRequest) returns (CreateUserResponse); rpc GetUser(GetUserRequest) returns (GetUserResponse); rpc UpdateUser(UpdateUserRequest) returns (UpdateUserResponse); rpc DeleteUser(DeleteUserRequest) returns (DeleteUserResponse); rpc ListUsers(ListUsersRequest) returns (ListUsersResponse); } Now generate the Go code from our protobuf definition:\nprotoc --go_out=. --go-grpc_out=. proto/user.proto This creates proto/user.pb.go and proto/user_grpc.pb.go with all the generated code we need.\nImportant: Make sure to add the generated files to your project structure. Your directory should look like this:\ngrpc-user-service/ ├── go.mod ├── go.sum ├── main.go ├── proto/ │ ├── user.proto │ ├── user.pb.go # Generated │ └── user_grpc.pb.go # Generated ├── server/ │ └── user_server.go └── client/ └── main.go Implementing the gRPC Server Now we\u0026rsquo;re getting to the fun part. Unlike handling HTTP requests manually , gRPC generates most of the boilerplate for us. We just need to implement the business logic.\nCreate server/user_server.go:\npackage server import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc/codes\u0026#34; \u0026#34;google.golang.org/grpc/status\u0026#34; ) type UserServer struct { pb.UnimplementedUserServiceServer users map[int32]*pb.User nextID int32 mu sync.RWMutex } func NewUserServer() *UserServer { return \u0026amp;UserServer{ users: make(map[int32]*pb.User), nextID: 1, } } func (s *UserServer) CreateUser(ctx context.Context, req *pb.CreateUserRequest) (*pb.CreateUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() // Basic validation if req.Name == \u0026#34;\u0026#34; { return nil, status.Error(codes.InvalidArgument, \u0026#34;name cannot be empty\u0026#34;) } if req.Email == \u0026#34;\u0026#34; { return nil, status.Error(codes.InvalidArgument, \u0026#34;email cannot be empty\u0026#34;) } if req.Age \u0026lt; 0 { return nil, status.Error(codes.InvalidArgument, \u0026#34;age must be positive\u0026#34;) } // Create new user user := \u0026amp;pb.User{ Id: s.nextID, Name: req.Name, Email: req.Email, Age: req.Age, Active: true, } s.users[s.nextID] = user s.nextID++ return \u0026amp;pb.CreateUserResponse{ User: user, Message: \u0026#34;User created successfully\u0026#34;, }, nil } func (s *UserServer) GetUser(ctx context.Context, req *pb.GetUserRequest) (*pb.GetUserResponse, error) { s.mu.RLock() defer s.mu.RUnlock() user, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } return \u0026amp;pb.GetUserResponse{User: user}, nil } func (s *UserServer) UpdateUser(ctx context.Context, req *pb.UpdateUserRequest) (*pb.UpdateUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() user, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } // Update fields if provided if req.Name != \u0026#34;\u0026#34; { user.Name = req.Name } if req.Email != \u0026#34;\u0026#34; { user.Email = req.Email } if req.Age \u0026gt; 0 { user.Age = req.Age } user.Active = req.Active s.users[req.Id] = user return \u0026amp;pb.UpdateUserResponse{ User: user, Message: \u0026#34;User updated successfully\u0026#34;, }, nil } func (s *UserServer) DeleteUser(ctx context.Context, req *pb.DeleteUserRequest) (*pb.DeleteUserResponse, error) { s.mu.Lock() defer s.mu.Unlock() _, exists := s.users[req.Id] if !exists { return nil, status.Error(codes.NotFound, \u0026#34;user not found\u0026#34;) } delete(s.users, req.Id) return \u0026amp;pb.DeleteUserResponse{ Message: \u0026#34;User deleted successfully\u0026#34;, }, nil } func (s *UserServer) ListUsers(ctx context.Context, req *pb.ListUsersRequest) (*pb.ListUsersResponse, error) { s.mu.RLock() defer s.mu.RUnlock() var users []*pb.User for _, user := range s.users { users = append(users, user) } // Simple pagination pageSize := req.PageSize if pageSize \u0026lt;= 0 { pageSize = 10 } page := req.Page if page \u0026lt;= 0 { page = 1 } start := (page - 1) * pageSize end := start + pageSize if start \u0026gt;= int32(len(users)) { users = []*pb.User{} } else if end \u0026gt; int32(len(users)) { users = users[start:] } else { users = users[start:end] } return \u0026amp;pb.ListUsersResponse{ Users: users, Total: int32(len(s.users)), }, nil } See that mutex stuff? That\u0026rsquo;s to keep things thread-safe when multiple requests come in at once. Obviously in real production apps, you\u0026rsquo;d swap out this in-memory storage for a proper database - but this keeps things simple for learning.\nRunning the gRPC Server Create main.go to start our server:\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net\u0026#34; \u0026#34;grpc-user-service/server\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/reflection\u0026#34; ) func main() { // Listen on port 50051 lis, err := net.Listen(\u0026#34;tcp\u0026#34;, \u0026#34;:50051\u0026#34;) if err != nil { log.Fatalf(\u0026#34;Failed to listen: %v\u0026#34;, err) } // Create gRPC server s := grpc.NewServer() // Register our service userServer := server.NewUserServer() pb.RegisterUserServiceServer(s, userServer) // Enable reflection for debugging with tools like grpcurl reflection.Register(s) log.Println(\u0026#34;gRPC server starting on :50051\u0026#34;) if err := s.Serve(lis); err != nil { log.Fatalf(\u0026#34;Failed to serve: %v\u0026#34;, err) } } Run the server:\ngo run main.go Building a gRPC Client Now let\u0026rsquo;s create a client to interact with our service. Create client/main.go:\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; pb \u0026#34;grpc-user-service/proto\u0026#34; \u0026#34;google.golang.org/grpc\u0026#34; \u0026#34;google.golang.org/grpc/credentials/insecure\u0026#34; ) func main() { // Connect to the gRPC server conn, err := grpc.Dial(\u0026#34;localhost:50051\u0026#34;, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil { log.Fatalf(\u0026#34;Failed to connect: %v\u0026#34;, err) } defer conn.Close() client := pb.NewUserServiceClient(conn) // Create a user ctx, cancel := context.WithTimeout(context.Background(), time.Second*10) defer cancel() createResp, err := client.CreateUser(ctx, \u0026amp;pb.CreateUserRequest{ Name: \u0026#34;John Doe\u0026#34;, Email: \u0026#34;john@example.com\u0026#34;, Age: 30, }) if err != nil { log.Fatalf(\u0026#34;Could not create user: %v\u0026#34;, err) } log.Printf(\u0026#34;Created user: %v\u0026#34;, createResp.User) // Get the user getResp, err := client.GetUser(ctx, \u0026amp;pb.GetUserRequest{ Id: createResp.User.Id, }) if err != nil { log.Fatalf(\u0026#34;Could not get user: %v\u0026#34;, err) } log.Printf(\u0026#34;Retrieved user: %v\u0026#34;, getResp.User) // Update the user updateResp, err := client.UpdateUser(ctx, \u0026amp;pb.UpdateUserRequest{ Id: createResp.User.Id, Name: \u0026#34;John Smith\u0026#34;, Email: \u0026#34;johnsmith@example.com\u0026#34;, Age: 31, Active: true, }) if err != nil { log.Fatalf(\u0026#34;Could not update user: %v\u0026#34;, err) } log.Printf(\u0026#34;Updated user: %v\u0026#34;, updateResp.User) // List users listResp, err := client.ListUsers(ctx, \u0026amp;pb.ListUsersRequest{ Page: 1, PageSize: 10, }) if err != nil { log.Fatalf(\u0026#34;Could not list users: %v\u0026#34;, err) } log.Printf(\u0026#34;Total users: %d\u0026#34;, listResp.Total) for _, user := range listResp.Users { log.Printf(\u0026#34;User: %v\u0026#34;, user) } // Delete the user deleteResp, err := client.DeleteUser(ctx, \u0026amp;pb.DeleteUserRequest{ Id: createResp.User.Id, }) if err != nil { log.Fatalf(\u0026#34;Could not delete user: %v\u0026#34;, err) } log.Printf(\u0026#34;Delete response: %s\u0026#34;, deleteResp.Message) } Test the client in a new terminal:\ngo run client/main.go Production Considerations Alright, so when you want to actually deploy this thing to production, there\u0026rsquo;s some stuff you need to think about. Unlike deploying a simple REST API , gRPC services need a bit more thought around load balancing and TLS setup.\nFirst off, make sure you\u0026rsquo;ve got solid error handling throughout your service. gRPC gives you a bunch of useful status codes so your clients know exactly what went wrong.\nFor auth, you\u0026rsquo;ll probably want JWT token validation or mutual TLS. Interceptors are your friend here - you can use them to handle auth, logging, and metrics for all your RPC methods in one place.\nObviously, you\u0026rsquo;ll need to hook up a real database for production. Swap out that in-memory storage for a real database connection. Check out our PostgreSQL guide if you\u0026rsquo;re going the SQL route, or look into NoSQL depending on what you\u0026rsquo;re building.\nPerformance Benefits and Testing What really blew my mind about gRPC was just how much faster it is compared to REST APIs. Protocol Buffers\u0026rsquo; binary serialization absolutely destroys JSON in terms of speed, and HTTP/2 lets you handle tons of requests over one connection without breaking a sweat.\nTesting gRPC services is actually pretty straightforward - you can write unit tests with mock clients and servers. All that generated code makes testing way easier than dealing with REST endpoints.\nWrapping Up gRPC is honestly a game changer for building fast, reliable distributed systems in Go. Sure, there\u0026rsquo;s a bit of a learning curve if you\u0026rsquo;re coming from REST, but trust me - once you see the performance gains and never have to deal with JSON parsing bugs again, you\u0026rsquo;ll wonder why you waited so long.\nWhat we built today is just basic CRUD stuff, but you can go crazy with streaming, fancy auth, and integrate it with your existing Go project setup .\nNext time you\u0026rsquo;re working on microservices, seriously give gRPC a shot. I guarantee you\u0026rsquo;ll be kicking yourself for not trying it sooner.\nGot questions about getting gRPC working in your Go projects? Hit me up in the comments - I\u0026rsquo;m always down to chat about different approaches and the weird edge cases you run into in production.\n","href":"/2025/08/grpc-in-go-complete-guide-from-basics-to-production.html","title":"gRPC in Go Complete Guide from Basics to Production Ready Services"},{"content":"Let\u0026rsquo;s be honest \u0026ndash; vanilla Visual Studio Code is good, but it\u0026rsquo;s not amazing. What makes this popular code editor truly shine are the extensions that turn it into a powerhouse IDE. After years of coding and trying countless extensions, I\u0026rsquo;ve narrowed down the absolute essentials that every developer should have installed in 2025.\nWhether you\u0026rsquo;re a seasoned developer or just starting your coding journey, these extensions will save you hours of work, catch bugs before they happen, and make your coding experience so much smoother. Let\u0026rsquo;s dive into the tools that have become indispensable in modern development.\n1. GitHub Copilot \u0026ndash; Your AI Coding Companion If you\u0026rsquo;re not using an AI coding assistant in 2025, you\u0026rsquo;re missing out on a massive productivity boost. GitHub Copilot has evolved into something truly remarkable \u0026ndash; it\u0026rsquo;s like having a senior developer sitting next to you, offering suggestions and writing boilerplate code.\nWhat makes it special:\nGenerates entire functions from comments Suggests code completions in real-time Understands context from your existing codebase Supports almost every programming language I\u0026rsquo;ve found Copilot particularly useful when working with APIs or writing repetitive code patterns. Instead of googling \u0026ldquo;how to make HTTP request in Python\u0026rdquo; for the hundredth time, Copilot just knows what you want to do.\nPro tip: Don\u0026rsquo;t just accept every suggestion blindly. Copilot is smart, but it\u0026rsquo;s not perfect. Always review the generated code and understand what it does.\nInstall: GitHub Copilot 2. Prettier \u0026ndash; Code Formatting Made Effortless Arguing about code formatting is so 2015. Prettier solves this problem once and for all by automatically formatting your code according to consistent rules. No more debates about tabs vs spaces or where to put your brackets.\nWhy it\u0026rsquo;s essential:\nFormats code on save automatically Maintains consistent style across your team Supports JavaScript, TypeScript, CSS, HTML, JSON, and more Reduces cognitive load during code reviews The beauty of Prettier is that you set it up once and forget about it. Your code always looks clean and professional, which makes it easier to read and maintain. When you\u0026rsquo;re building REST APIs or working on complex projects, consistent formatting becomes even more crucial.\nInstall: Prettier - Code formatter 3. ESLint \u0026ndash; Your JavaScript Guardian Angel ESLint is like having a vigilant code reviewer who never gets tired of pointing out potential issues. It catches common mistakes, enforces coding standards, and helps you write better JavaScript and TypeScript.\nKey benefits:\nCatches syntax errors before runtime Enforces coding best practices Customizable rules for your project needs Integrates perfectly with Prettier I can\u0026rsquo;t count how many times ESLint has saved me from shipping buggy code. It\u0026rsquo;s particularly helpful when working with frameworks like React or Vue, where certain patterns can lead to performance issues or bugs.\nInstall: ESLint 4. Live Server \u0026ndash; Instant Development Server Testing your web applications locally used to require setting up complex development servers. Live Server changes that by providing a one-click solution to run your HTML, CSS, and JavaScript projects with hot reload.\nWhat it offers:\nInstant local development server Auto-reload when files change Works with any static website Perfect for front-end development This extension is a lifesaver when you\u0026rsquo;re working on static sites or testing your front-end code. It\u0026rsquo;s especially useful if you\u0026rsquo;re following tutorials or building portfolio projects.\nInstall: Live Server 5. GitLens \u0026ndash; Git Supercharged Git is powerful, but it\u0026rsquo;s not always the most user-friendly. GitLens transforms your IDE\u0026rsquo;s Git integration into something intuitive and informative. You can see who changed what, when, and why \u0026ndash; all without leaving your editor.\nStandout features:\nBlame annotations show code authorship Rich commit information and history File and line history visualization Seamless GitHub/GitLab integration GitLens is particularly valuable when working on team projects or maintaining legacy code. Understanding the history and context of code changes becomes effortless.\nInstall: GitLens \u0026ndash; Git supercharged 6. Bracket Pair Colorizer \u0026ndash; Navigate Complex Code When you\u0026rsquo;re dealing with deeply nested code structures, matching brackets can become a nightmare. Bracket Pair Colorizer solves this by giving matching brackets the same color, making it easy to see code structure at a glance.\nWhy it\u0026rsquo;s helpful:\nColor-codes matching brackets Reduces syntax errors Makes complex nested structures readable Supports multiple bracket types This might seem like a small thing, but it makes a huge difference when you\u0026rsquo;re working with complex data structures or deeply nested functions. Your eyes will thank you.\nInstall: Bracket Pair Colorizer 2 7. Auto Rename Tag \u0026ndash; HTML/XML Editing Made Simple If you\u0026rsquo;ve ever spent time manually renaming HTML or XML tags, you know how tedious it can be. Auto Rename Tag automatically renames the paired tag when you change one, keeping your markup consistent.\nKey features:\nAutomatically renames paired HTML/XML tags Works with React JSX Prevents mismatched tag errors Saves time during refactoring This extension is a must-have if you\u0026rsquo;re doing any web development. It\u0026rsquo;s one of those tools that you don\u0026rsquo;t realize you need until you have it, and then you can\u0026rsquo;t live without it.\nInstall: Auto Rename Tag 8. Thunder Client \u0026ndash; API Testing Inside VS Code Postman is great, but do you really want to switch between applications just to test an API? Thunder Client brings API testing directly into your code editor, making it seamless to test your endpoints while you develop.\nWhat makes it awesome:\nTest REST APIs without leaving VS Code Clean, intuitive interface Environment variables support Request collections and organization This is particularly useful when you\u0026rsquo;re building FastAPI applications or working on backend services. You can write code, test it, and debug issues all in one place.\nInstall: Thunder Client 9. Error Lens \u0026ndash; Inline Error Highlighting Error Lens takes your programming environment\u0026rsquo;s error reporting and makes it impossible to ignore. Instead of having to hover over squiggly lines or check the problems panel, errors and warnings appear right in your editor as inline messages.\nBenefits:\nDisplays errors and warnings inline Highlights entire error lines Customizable styling and behavior Works with all language extensions This extension has completely changed how I deal with errors. Instead of missing warnings or ignoring small issues, they\u0026rsquo;re right there in your face, encouraging you to fix them immediately.\nInstall: Error Lens 10. Material Icon Theme \u0026ndash; Beautiful File Icons This might seem superficial, but good visual organization actually impacts productivity. Material Icon Theme provides beautiful, recognizable icons for different file types, making it easier to navigate your project structure.\nWhy it matters:\nInstantly recognizable file type icons Consistent, professional appearance Easier project navigation Customizable icon themes When you\u0026rsquo;re jumping between different files and folders constantly, having clear visual indicators for file types makes everything faster and more pleasant to work with.\nInstall: Material Icon Theme Setting Up Your Perfect Development Environment Installing these programming tools is just the first step. Here\u0026rsquo;s how to get the most out of your development setup:\nConfigure Prettier and ESLint together \u0026ndash; They work beautifully as a team when properly configured Set up keyboard shortcuts \u0026ndash; Learn the shortcuts for your most-used extensions Customize settings \u0026ndash; Each extension has settings that can be tuned to your preferences Keep them updated \u0026ndash; Extension updates often bring performance improvements and new features If you\u0026rsquo;re working with specific technologies, you might also want to explore language-specific developer tools. For example, if you\u0026rsquo;re into Go development , there are excellent Go extensions. Similarly, Laravel developers have their own set of must-have tools.\nThe Impact on Your Development Workflow These programming extensions don\u0026rsquo;t just add features \u0026ndash; they totally change how you code. With AI assistance from Copilot, automatic formatting from Prettier, and inline error detection from Error Lens, you spend less time on mundane tasks and more time solving interesting problems.\nThe combination of these coding tools creates an IDE where:\nCode quality improves automatically Common mistakes are caught early Repetitive tasks are automated Navigation and organization are effortless Avoiding Extension Bloat While developer extensions are powerful, it\u0026rsquo;s easy to go overboard. I recommend starting with these 10 programming essentials and only adding more if you have a specific need. Too many extensions can slow down your development environment and create conflicts.\nPro tip: Regularly review your installed extensions and disable or uninstall ones you\u0026rsquo;re not actively using. Your code editor performs better with fewer active programming tools.\nLooking Ahead: The Future of VS Code Extensions The Visual Studio Code extension ecosystem keeps getting better fast. AI-powered extensions like Copilot are just the beginning. We\u0026rsquo;re starting to see programming tools that can refactor code, generate tests, and even help with code reviews.\nAs we move further into 2025, I expect to see more developer tools that integrate machine learning, provide better collaborative features, and offer deeper integration with cloud services. The key is to stay updated with the programming ecosystem while not getting caught up in every new coding utility.\nConclusion These 10 developer tools have become indispensable in my daily coding workflow, and I\u0026rsquo;m confident they\u0026rsquo;ll improve yours too. They represent the best of what makes this IDE great \u0026ndash; a lightweight programming environment that can be transformed into exactly what you need.\nThe beauty of this setup is that it works whether you\u0026rsquo;re building secure web applications , working on deployment automation , or just learning the basics of programming. These tools scale with your needs and grow with your skills.\nStart with installing a few of these extensions and gradually add the rest as you see their value. Your future self will thank you for the time saved and bugs prevented. Happy coding!\nLooking for more development tips and tutorials? Check out our guides on FastAPI development and Linux server management to level up your backend skills.\n","href":"/2025/08/10-essential-vscode-extensions-developers-2025.html","title":"10 Essential VS Code Extensions Every Developer Must Have in 2025"},{"content":"Building APIs used to scare me when I first started programming. There\u0026rsquo;s so much to learn - databases, HTTP methods, authentication, error handling. But FastAPI changed everything for me. It\u0026rsquo;s like having training wheels that actually make you faster, not slower.\nWe\u0026rsquo;re going to build a real Book Library API from the ground up. No fluff, no complicated setups - just practical, working code that you can understand and expand on. By the end of this guide, you\u0026rsquo;ll have a fully functional REST API that can handle creating, reading, updating, and deleting books.\nOnce you master the basics here, you can take your FastAPI skills further with JWT authentication and OAuth2 security , or learn how to deploy your FastAPI application to production .\nI\u0026rsquo;m not going to throw a bunch of code at you and hope it sticks. We\u0026rsquo;ll walk through each piece together, and I\u0026rsquo;ll explain why we\u0026rsquo;re doing things a certain way. Think of it as pair programming through an article. Everything runs locally too - no cloud accounts or credit cards needed.\nWhat you\u0026rsquo;ll build:\nA complete Book Library REST API CRUD operations (Create, Read, Update, Delete) Data validation with Pydantic SQLite database integration Interactive API documentation Error handling and responses Testing with real HTTP requests Prerequisites Before we dive in, make sure you have:\nPython 3.8 or higher installed Basic Python knowledge (variables, functions, classes) A code editor (VS Code, PyCharm, or any text editor) Command line familiarity Don\u0026rsquo;t worry if you\u0026rsquo;re not an expert in any of these - we\u0026rsquo;ll explain everything as we go.\nWhy FastAPI? Why FastAPI and not Django or Flask? Good question. I\u0026rsquo;ve used all three in production, and here\u0026rsquo;s my take: Django feels like driving a truck when you need a motorcycle. Flask is that motorcycle, but you end up building the truck yourself anyway. FastAPI? It\u0026rsquo;s like a sports car that comes with GPS, heated seats, and a great sound system right out of the box.\nFastAPI automatically generates interactive documentation for your API, validates request data, and handles serialization. These features alone save hours of manual work. Plus, it\u0026rsquo;s built on modern Python features like type hints, making your code more readable and less bug-prone.\nSetting Up the Development Environment First things first - let\u0026rsquo;s set up a proper workspace. Open your terminal and create a new directory:\nmkdir book-library-api cd book-library-api Now create a virtual environment. This keeps our project dependencies separate from other Python projects on your system:\npython -m venv venv Activate the virtual environment:\nOn Windows:\nvenv\\Scripts\\activate On macOS/Linux:\nsource venv/bin/activate You should see (venv) at the beginning of your command prompt, indicating the virtual environment is active.\nInstalling Dependencies We need just a few packages to get started:\npip install fastapi uvicorn sqlalchemy Here\u0026rsquo;s what each package does:\nFastAPI: The web framework itself Uvicorn: ASGI server to run our application SQLAlchemy: Database ORM (Object-Relational Mapping) Let\u0026rsquo;s also create a requirements.txt file to track our dependencies:\npip freeze \u0026gt; requirements.txt Project Structure Good organization makes your code easier to understand and maintain. Create this folder structure:\nmkdir app mkdir app/models mkdir app/schemas mkdir app/database touch app/__init__.py touch app/main.py touch app/models/__init__.py touch app/models/book.py touch app/schemas/__init__.py touch app/schemas/book.py touch app/database/__init__.py touch app/database/database.py Your project should now look like this:\nbook-library-api/ ├── venv/ ├── app/ │ ├── __init__.py │ ├── main.py │ ├── models/ │ │ ├── __init__.py │ │ └── book.py │ ├── schemas/ │ │ ├── __init__.py │ │ └── book.py │ └── database/ │ ├── __init__.py │ └── database.py └── requirements.txt This structure separates different parts of our application, making it easier to find and modify code later.\nDatabase Setup Let\u0026rsquo;s start by setting up our database connection. We\u0026rsquo;ll use SQLite because it\u0026rsquo;s simple and doesn\u0026rsquo;t require a separate database server.\nCreate the database configuration:\n# app/database/database.py from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker # SQLite database file SQLALCHEMY_DATABASE_URL = \u0026#34;sqlite:///./books.db\u0026#34; # Create engine engine = create_engine( SQLALCHEMY_DATABASE_URL, connect_args={\u0026#34;check_same_thread\u0026#34;: False} ) # Create session SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) # Base class for models Base = declarative_base() # Dependency to get database session def get_db(): db = SessionLocal() try: yield db finally: db.close() This code sets up our database connection. The get_db() function is a dependency that FastAPI will use to provide database sessions to our API endpoints.\nCreating the Database Model Now let\u0026rsquo;s define what a book looks like in our database:\n# app/models/book.py from sqlalchemy import Column, Integer, String, Text from app.database.database import Base class Book(Base): __tablename__ = \u0026#34;books\u0026#34; id = Column(Integer, primary_key=True, index=True) title = Column(String(255), nullable=False) author = Column(String(255), nullable=False) description = Column(Text, nullable=True) published_year = Column(Integer, nullable=True) isbn = Column(String(20), unique=True, nullable=True) This model defines our book structure with fields for title, author, description, publication year, and ISBN. The __tablename__ tells SQLAlchemy what to name the table in the database.\nPydantic Schemas Pydantic schemas define how data should look when it comes into or goes out of our API. Think of them as contracts that ensure data consistency:\n# app/schemas/book.py from pydantic import BaseModel from typing import Optional, List, Any class BookBase(BaseModel): title: str author: str description: Optional[str] = None published_year: Optional[int] = None isbn: Optional[str] = None class BookCreate(BookBase): pass class BookUpdate(BaseModel): title: Optional[str] = None author: Optional[str] = None description: Optional[str] = None published_year: Optional[int] = None isbn: Optional[str] = None class BookResponse(BookBase): id: int class Config: from_attributes = True # Standard API Response Schemas class Meta(BaseModel): success: bool message: str total: Optional[int] = None page: Optional[int] = None limit: Optional[int] = None total_pages: Optional[int] = None class StandardResponse(BaseModel): meta: Meta data: Any class BookListResponse(BaseModel): meta: Meta data: List[BookResponse] class SingleBookResponse(BaseModel): meta: Meta data: BookResponse We have different schemas for different purposes:\nBookBase: Common fields for all book operations BookCreate: For creating new books (inherits from BookBase) BookUpdate: For updating existing books (all fields optional) BookResponse: For returning book data (includes the ID) Meta: Metadata for API responses (success status, pagination info) StandardResponse: Generic response wrapper with meta and data BookListResponse: Specific response for book lists SingleBookResponse: Specific response for single book operations This standard response format makes your API more consistent and easier to consume by frontend applications or other services.\nBuilding the FastAPI Application Now for the main event - creating our FastAPI application:\n# app/main.py from fastapi import FastAPI, HTTPException, Depends, status from sqlalchemy.orm import Session from typing import List import math from app.database.database import engine, get_db from app.models import book as book_models from app.schemas import book as book_schemas # Create database tables book_models.Base.metadata.create_all(bind=engine) # Initialize FastAPI app app = FastAPI( title=\u0026#34;Book Library API\u0026#34;, description=\u0026#34;A simple REST API for managing books with standardized responses\u0026#34;, version=\u0026#34;1.0.0\u0026#34; ) # Helper function to create standard responses def create_response(success: bool, message: str, data=None, total=None, page=None, limit=None): meta = book_schemas.Meta( success=success, message=message, total=total, page=page, limit=limit, total_pages=math.ceil(total / limit) if total and limit else None ) return book_schemas.StandardResponse(meta=meta, data=data) # Root endpoint @app.get(\u0026#34;/\u0026#34;) def read_root(): return create_response( success=True, message=\u0026#34;Welcome to Book Library API\u0026#34;, data={\u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34;} ) # Health check endpoint @app.get(\u0026#34;/health\u0026#34;) def health_check(): return create_response( success=True, message=\u0026#34;API is healthy\u0026#34;, data={\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2024-01-01T00:00:00Z\u0026#34;} ) # Get all books @app.get(\u0026#34;/books\u0026#34;, response_model=book_schemas.BookListResponse) def get_books(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)): # Get total count for pagination total_books = db.query(book_models.Book).count() # Get books with pagination books = db.query(book_models.Book).offset(skip).limit(limit).all() current_page = (skip // limit) + 1 meta = book_schemas.Meta( success=True, message=\u0026#34;Books retrieved successfully\u0026#34;, total=total_books, page=current_page, limit=limit, total_pages=math.ceil(total_books / limit) if total_books \u0026gt; 0 else 0 ) return book_schemas.BookListResponse(meta=meta, data=books) # Get single book by ID @app.get(\u0026#34;/books/{book_id}\u0026#34;, response_model=book_schemas.SingleBookResponse) def get_book(book_id: int, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) meta = book_schemas.Meta( success=True, message=\u0026#34;Book retrieved successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=book) # Create new book @app.post(\u0026#34;/books\u0026#34;, response_model=book_schemas.SingleBookResponse, status_code=status.HTTP_201_CREATED) def create_book(book: book_schemas.BookCreate, db: Session = Depends(get_db)): # Check if book with same ISBN already exists if book.isbn: existing_book = db.query(book_models.Book).filter(book_models.Book.isbn == book.isbn).first() if existing_book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with ISBN {book.isbn} already exists\u0026#34; ) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) db_book = book_models.Book(**book.dict()) db.add(db_book) db.commit() db.refresh(db_book) meta = book_schemas.Meta( success=True, message=\u0026#34;Book created successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=db_book) # Update existing book @app.put(\u0026#34;/books/{book_id}\u0026#34;, response_model=book_schemas.SingleBookResponse) def update_book(book_id: int, book_update: book_schemas.BookUpdate, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) # Update only provided fields update_data = book_update.dict(exclude_unset=True) if not update_data: meta = book_schemas.Meta( success=False, message=\u0026#34;No fields provided for update\u0026#34; ) raise HTTPException( status_code=status.HTTP_400_BAD_REQUEST, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) for field, value in update_data.items(): setattr(book, field, value) db.commit() db.refresh(book) meta = book_schemas.Meta( success=True, message=\u0026#34;Book updated successfully\u0026#34; ) return book_schemas.SingleBookResponse(meta=meta, data=book) # Delete book @app.delete(\u0026#34;/books/{book_id}\u0026#34;) def delete_book(book_id: int, db: Session = Depends(get_db)): book = db.query(book_models.Book).filter(book_models.Book.id == book_id).first() if not book: meta = book_schemas.Meta( success=False, message=f\u0026#34;Book with id {book_id} not found\u0026#34; ) raise HTTPException( status_code=status.HTTP_404_NOT_FOUND, detail={\u0026#34;meta\u0026#34;: meta.dict(), \u0026#34;data\u0026#34;: None} ) db.delete(book) db.commit() return create_response( success=True, message=f\u0026#34;Book with id {book_id} deleted successfully\u0026#34;, data={\u0026#34;deleted_book_id\u0026#34;: book_id} ) This is the heart of our API. Let\u0026rsquo;s break down what each endpoint does:\nGET /: Welcome message with standardized response GET /health: Simple health check with status information GET /books: Get all books with pagination and metadata GET /books/{book_id}: Get a specific book by ID POST /books: Create a new book with validation PUT /books/{book_id}: Update an existing book DELETE /books/{book_id}: Delete a book Standard Response Format Notice how all our responses now follow a consistent structure with meta and data fields:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Books retrieved successfully\u0026#34;, \u0026#34;total\u0026#34;: 25, \u0026#34;page\u0026#34;: 1, \u0026#34;limit\u0026#34;: 10, \u0026#34;total_pages\u0026#34;: 3 }, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } ] } This format provides several benefits:\nConsistent structure across all endpoints Success/failure indication in every response Helpful messages for debugging and user feedback Pagination metadata for list endpoints Easy parsing for frontend applications Running the Application Let\u0026rsquo;s see our API in action! Run this command from your project root:\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000 The --reload flag automatically restarts the server when you change code, making development much smoother.\nOpen your browser and go to http://127.0.0.1:8000. You should see:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Welcome to Book Library API\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;running\u0026#34; } } Interactive API Documentation Now for the coolest part. Navigate to http://127.0.0.1:8000/docs and prepare to be impressed. FastAPI automatically created interactive documentation for your entire API. You can test every endpoint right in the browser - no Postman needed.\nTry creating a book:\nClick on the POST /books endpoint Click \u0026ldquo;Try it out\u0026rdquo; Enter this sample data: { \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } Click \u0026ldquo;Execute\u0026rdquo; You should get a successful response with your newly created book in the standard format:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Book created successfully\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;The Python Guide\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Real Python\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;A comprehensive guide to Python programming\u0026#34;, \u0026#34;published_year\u0026#34;: 2023, \u0026#34;isbn\u0026#34;: \u0026#34;978-0123456789\u0026#34; } } Testing Your API Let\u0026rsquo;s test all our endpoints to make sure everything works. You can use the interactive docs at http://127.0.0.1:8000/docs, or test via command line with these examples:\nMethod 1: Copy-Paste Ready Commands Create a book (single line - just copy and paste):\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34;}\u0026#39; Get all books:\ncurl \u0026#34;http://127.0.0.1:8000/books\u0026#34; Get a specific book:\ncurl \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; Update a book:\ncurl -X PUT \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners - Updated Edition\u0026#34;}\u0026#39; Delete a book:\ncurl -X DELETE \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; Method 2: Using JSON Files (Recommended for Complex Data) For easier testing with complex data, create JSON files:\nCreate book.json:\ncat \u0026gt; book.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } EOF Then use the file:\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @book.json Create update.json:\ncat \u0026gt; update.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners - Updated Edition\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step with the latest updates\u0026#34; } EOF Update using file:\ncurl -X PUT \u0026#34;http://127.0.0.1:8000/books/1\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; --data-binary @update.json Expected Response Examples Successful book creation:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Book created successfully\u0026#34;, \u0026#34;total\u0026#34;: null, \u0026#34;page\u0026#34;: null, \u0026#34;limit\u0026#34;: null, \u0026#34;total_pages\u0026#34;: null }, \u0026#34;data\u0026#34;: { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } } Get all books response:\n{ \u0026#34;meta\u0026#34;: { \u0026#34;success\u0026#34;: true, \u0026#34;message\u0026#34;: \u0026#34;Books retrieved successfully\u0026#34;, \u0026#34;total\u0026#34;: 1, \u0026#34;page\u0026#34;: 1, \u0026#34;limit\u0026#34;: 100, \u0026#34;total_pages\u0026#34;: 1 }, \u0026#34;data\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;FastAPI for Beginners\u0026#34;, \u0026#34;author\u0026#34;: \u0026#34;Jane Developer\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Learn FastAPI step by step\u0026#34;, \u0026#34;published_year\u0026#34;: 2024, \u0026#34;isbn\u0026#34;: \u0026#34;978-0987654321\u0026#34; } ] } Testing with Pagination Get books with pagination:\ncurl \u0026#34;http://127.0.0.1:8000/books?skip=0\u0026amp;limit=5\u0026#34; Get second page:\ncurl \u0026#34;http://127.0.0.1:8000/books?skip=5\u0026amp;limit=5\u0026#34; Understanding the Code Before we move on, let me explain a few important concepts that might not be obvious:\nDependency Injection: The Depends(get_db) parameter in our endpoints is dependency injection. FastAPI automatically calls get_db() and provides the database session to your function.\nType Hints: Notice how we specify types like book_id: int and response_model=List[book_schemas.BookResponse]. This isn\u0026rsquo;t just for documentation - FastAPI uses these to validate data and provide better error messages.\nHTTP Status Codes: We use appropriate status codes like 201 for created resources and 404 for not found. This makes our API more professional and easier to integrate with.\nError Handling: When something goes wrong (like trying to access a non-existent book), we raise HTTPException with appropriate status codes and messages.\nAdding More Features Want to extend your API? Here are some ideas:\nSearch functionality:\n@app.get(\u0026#34;/books/search\u0026#34;, response_model=List[book_schemas.BookResponse]) def search_books(q: str, db: Session = Depends(get_db)): books = db.query(book_models.Book).filter( book_models.Book.title.contains(q) | book_models.Book.author.contains(q) ).all() return books Filtering by author:\n@app.get(\u0026#34;/books/by-author/{author}\u0026#34;, response_model=List[book_schemas.BookResponse]) def get_books_by_author(author: str, db: Session = Depends(get_db)): books = db.query(book_models.Book).filter(book_models.Book.author == author).all() return books Common Issues and Solutions Import Errors: Make sure all your __init__.py files exist and you\u0026rsquo;re running commands from the project root.\nDatabase Errors: If you get database-related errors, delete the books.db file and restart the application to recreate it.\nPort Already in Use: If port 8000 is busy, use a different port: uvicorn app.main:app --reload --port 8001\ncurl Command Issues: If you get \u0026ldquo;command not found\u0026rdquo; errors when copying multiline curl commands, use the single-line versions provided above, or create JSON files as shown in Method 2.\nJSON Parsing Errors: Make sure your JSON is valid. If you get \u0026ldquo;Field required\u0026rdquo; errors, check that your JSON structure matches the expected schema. Use the interactive docs at /docs to see the exact format needed.\nPermission Errors: On some systems, you might need to escape quotes differently. If single quotes don\u0026rsquo;t work, try double quotes with escaped inner quotes:\ncurl -X POST \u0026#34;http://127.0.0.1:8000/books\u0026#34; -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#34;{\\\u0026#34;title\\\u0026#34;: \\\u0026#34;FastAPI for Beginners\\\u0026#34;, \\\u0026#34;author\\\u0026#34;: \\\u0026#34;Jane Developer\\\u0026#34;}\u0026#34; Next Steps Congratulations! You\u0026rsquo;ve built a complete REST API from scratch. Here\u0026rsquo;s what you can do next:\nAdd Authentication: Protect your endpoints with JWT authentication and OAuth2 password flow Deploy to Production: Learn how to deploy FastAPI on Ubuntu 24.04 with Nginx and HTTPS Add Validation: Implement more complex validation rules with Pydantic Add Tests: Write unit tests for your endpoints Add a Frontend: Build a web interface to interact with your API Scale with Docker: Containerize your application for easier deployment Wrapping Up Look at what you just built - a real REST API that handles data, validates input, and documents itself. That\u0026rsquo;s not trivial stuff. You went from zero to having something that could actually power a web app or mobile app.\nWhat I love about this setup is how easy it is to extend. Need user accounts? Add a User model. Want to track book reviews? Create a Review endpoint. The foundation is solid, and FastAPI handles the boring stuff so you can focus on the interesting problems.\nKeep experimenting with different endpoints and features. The best way to learn API development is by building real projects and solving real problems. Your Book Library API is just the beginning - imagine what you\u0026rsquo;ll build next!\n","href":"/2025/08/fastapi-tutorial-build-rest-api-from-scratch-beginner-guide.html","title":"FastAPI Tutorial Build REST API from Scratch (Beginner Guide)"},{"content":"Deploying a Laravel application to a VPS (Virtual Private Server) with Nginx gives you complete control over your hosting environment and superior performance compared to shared hosting. This comprehensive guide will walk you through the entire process, from server setup to production optimization.\nWhat You\u0026rsquo;ll Learn Set up a VPS for Laravel deployment Configure Nginx for optimal Laravel performance Secure your application with SSL certificates Implement production best practices Set up automated deployments Monitor and maintain your application Prerequisites Before starting, ensure you have:\nA VPS running Ubuntu 20.04/22.04 LTS (DigitalOcean, Linode, AWS EC2, etc.) SSH access to your server A domain name pointing to your VPS IP Basic terminal/command line knowledge A Laravel application ready for deployment Step 1: Initial Server Setup Connect to Your VPS ssh root@your-server-ip Update System Packages apt update \u0026amp;\u0026amp; apt upgrade -y Create a Non-Root User # Create new user adduser deploy # Add to sudo group usermod -aG sudo deploy # Switch to new user su - deploy Configure SSH Key Authentication # On your local machine, copy your public key ssh-copy-id deploy@your-server-ip # Or manually add your key mkdir -p ~/.ssh chmod 700 ~/.ssh nano ~/.ssh/authorized_keys # Paste your public key and save chmod 600 ~/.ssh/authorized_keys Step 2: Install Required Software Install Nginx sudo apt install nginx -y sudo systemctl start nginx sudo systemctl enable nginx Install PHP 8.2 and Extensions # Add PHP repository sudo apt install software-properties-common -y sudo add-apt-repository ppa:ondrej/php -y sudo apt update # Install PHP and required extensions sudo apt install php8.2-fpm php8.2-common php8.2-mysql php8.2-xml php8.2-xmlrpc php8.2-curl php8.2-gd php8.2-imagick php8.2-cli php8.2-dev php8.2-imap php8.2-mbstring php8.2-opcache php8.2-soap php8.2-zip php8.2-intl php8.2-bcmath -y Install Composer cd ~ curl -sS https://getcomposer.org/installer -o composer-setup.php sudo php composer-setup.php --install-dir=/usr/local/bin --filename=composer rm composer-setup.php Install MySQL sudo apt install mysql-server -y sudo mysql_secure_installation Create Database and User sudo mysql -u root -p CREATE DATABASE laravel_app; CREATE USER \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;strong_password_here\u0026#39;; GRANT ALL PRIVILEGES ON laravel_app.* TO \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39;; FLUSH PRIVILEGES; EXIT; Install Git sudo apt install git -y Step 3: Deploy Your Laravel Application Clone Your Repository cd /var/www sudo git clone https://github.com/username/your-laravel-app.git sudo chown -R deploy:deploy your-laravel-app cd your-laravel-app Install Dependencies composer install --optimize-autoloader --no-dev Configure Environment cp .env.example .env nano .env Update your .env file:\nAPP_NAME=\u0026#34;Your Laravel App\u0026#34; APP_ENV=production APP_DEBUG=false APP_URL=https://yourdomain.com DB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=laravel_app DB_USERNAME=laravel_user DB_PASSWORD=strong_password_here CACHE_DRIVER=file QUEUE_CONNECTION=sync SESSION_DRIVER=file SESSION_LIFETIME=120 Generate Application Key php artisan key:generate Run Database Migrations php artisan migrate --force Optimize for Production php artisan config:cache php artisan route:cache php artisan view:cache php artisan storage:link Set File Permissions sudo chown -R www-data:www-data /var/www/your-laravel-app sudo chmod -R 755 /var/www/your-laravel-app sudo chmod -R 775 /var/www/your-laravel-app/storage sudo chmod -R 775 /var/www/your-laravel-app/bootstrap/cache Step 4: Configure Nginx Create Nginx Configuration sudo nano /etc/nginx/sites-available/your-laravel-app Add the following configuration:\nserver { listen 80; server_name yourdomain.com www.yourdomain.com; root /var/www/your-laravel-app/public; add_header X-Frame-Options \u0026#34;SAMEORIGIN\u0026#34;; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; add_header X-Content-Type-Options \u0026#34;nosniff\u0026#34;; index index.php; charset utf-8; # Security headers add_header Referrer-Policy \u0026#34;no-referrer-when-downgrade\u0026#34;; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains\u0026#34;; location / { try_files $uri $uri/ /index.php?$query_string; } location = /favicon.ico { access_log off; log_not_found off; } location = /robots.txt { access_log off; log_not_found off; } error_page 404 /index.php; location ~ \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; include fastcgi_params; fastcgi_hide_header X-Powered-By; } location ~ /\\.(?!well-known).* { deny all; } # Asset caching location ~* \\.(js|css|png|jpg|jpeg|gif|ico|svg)$ { expires 1y; add_header Cache-Control \u0026#34;public, immutable\u0026#34;; } # Deny access to sensitive files location ~ /\\.(htaccess|htpasswd|env) { deny all; } # Client max body size (for file uploads) client_max_body_size 20M; } Enable the Site sudo ln -s /etc/nginx/sites-available/your-laravel-app /etc/nginx/sites-enabled/ sudo nginx -t sudo systemctl reload nginx Remove Default Nginx Site sudo rm /etc/nginx/sites-enabled/default Step 5: Configure PHP-FPM Optimize PHP-FPM Settings sudo nano /etc/php/8.2/fpm/pool.d/www.conf Update these settings:\nuser = www-data group = www-data listen.owner = www-data listen.group = www-data pm = dynamic pm.max_children = 50 pm.start_servers = 5 pm.min_spare_servers = 5 pm.max_spare_servers = 35 pm.max_requests = 500 PHP Configuration sudo nano /etc/php/8.2/fpm/php.ini Update these settings:\nupload_max_filesize = 20M post_max_size = 25M memory_limit = 256M max_execution_time = 300 max_input_vars = 3000 opcache.enable=1 opcache.memory_consumption=128 opcache.max_accelerated_files=10000 Restart PHP-FPM sudo systemctl restart php8.2-fpm Step 6: SSL Certificate with Let\u0026rsquo;s Encrypt Install Certbot sudo apt install certbot python3-certbot-nginx -y Obtain SSL Certificate sudo certbot --nginx -d yourdomain.com -d www.yourdomain.com Auto-renewal Setup sudo crontab -e Add this line:\n0 12 * * * /usr/bin/certbot renew --quiet Step 7: Firewall Configuration Configure UFW sudo ufw allow ssh sudo ufw allow \u0026#39;Nginx Full\u0026#39; sudo ufw --force enable sudo ufw status Step 8: Production Optimization Configure Queue Processing Create a supervisor configuration:\nsudo apt install supervisor -y sudo nano /etc/supervisor/conf.d/laravel-worker.conf [program:laravel-worker] process_name=%(program_name)s_%(process_num)02d command=php /var/www/your-laravel-app/artisan queue:work autostart=true autorestart=true stopasgroup=true killasgroup=true user=www-data numprocs=2 redirect_stderr=true stdout_logfile=/var/www/your-laravel-app/storage/logs/worker.log stopwaitsecs=3600 sudo supervisorctl reread sudo supervisorctl update sudo supervisorctl start laravel-worker:* Setup Log Rotation sudo nano /etc/logrotate.d/laravel /var/www/your-laravel-app/storage/logs/*.log { daily rotate 14 missingok notifempty compress delaycompress copytruncate } Configure Redis (Optional) sudo apt install redis-server -y sudo systemctl enable redis-server Update .env:\nCACHE_DRIVER=redis SESSION_DRIVER=redis QUEUE_CONNECTION=redis Step 9: Automated Deployment Script Create a deployment script:\nnano ~/deploy.sh #!/bin/bash APP_DIR=\u0026#34;/var/www/your-laravel-app\u0026#34; BRANCH=\u0026#34;main\u0026#34; echo \u0026#34;Starting deployment...\u0026#34; # Navigate to app directory cd $APP_DIR # Enable maintenance mode sudo -u www-data php artisan down # Pull latest changes git pull origin $BRANCH # Install/update composer dependencies sudo -u www-data composer install --optimize-autoloader --no-dev # Clear and cache config sudo -u www-data php artisan config:clear sudo -u www-data php artisan config:cache # Clear and cache routes sudo -u www-data php artisan route:clear sudo -u www-data php artisan route:cache # Clear and cache views sudo -u www-data php artisan view:clear sudo -u www-data php artisan view:cache # Run database migrations sudo -u www-data php artisan migrate --force # Restart PHP-FPM and queue workers sudo systemctl reload php8.2-fpm sudo supervisorctl restart laravel-worker:* # Disable maintenance mode sudo -u www-data php artisan up echo \u0026#34;Deployment completed successfully!\u0026#34; Make it executable:\nchmod +x ~/deploy.sh Step 10: Monitoring and Security Install Fail2Ban sudo apt install fail2ban -y sudo systemctl enable fail2ban Configure Nginx Rate Limiting Add to your Nginx configuration:\n# Add to http block in /etc/nginx/nginx.conf limit_req_zone $binary_remote_addr zone=login:10m rate=5r/m; # Add to your server block location /login { limit_req zone=login burst=5 nodelay; try_files $uri $uri/ /index.php?$query_string; } Monitor Logs # View Nginx logs sudo tail -f /var/log/nginx/access.log sudo tail -f /var/log/nginx/error.log # View Laravel logs tail -f /var/www/your-laravel-app/storage/logs/laravel.log # View PHP-FPM logs sudo tail -f /var/log/php8.2-fpm.log Step 11: Backup Strategy Database Backup Script nano ~/backup-db.sh #!/bin/bash BACKUP_DIR=\u0026#34;/home/deploy/backups\u0026#34; DATE=$(date +%Y%m%d_%H%M%S) DB_NAME=\u0026#34;laravel_app\u0026#34; DB_USER=\u0026#34;laravel_user\u0026#34; DB_PASS=\u0026#34;your_password\u0026#34; mkdir -p $BACKUP_DIR # Create database backup mysqldump -u $DB_USER -p$DB_PASS $DB_NAME \u0026gt; $BACKUP_DIR/db_backup_$DATE.sql # Keep only last 7 days of backups find $BACKUP_DIR -name \u0026#34;db_backup_*.sql\u0026#34; -mtime +7 -delete echo \u0026#34;Database backup completed: $BACKUP_DIR/db_backup_$DATE.sql\u0026#34; Schedule Daily Backups crontab -e 0 2 * * * /home/deploy/backup-db.sh Troubleshooting Common Issues 1. 502 Bad Gateway # Check PHP-FPM status sudo systemctl status php8.2-fpm # Check PHP-FPM socket sudo ls -la /var/run/php/ # Restart services sudo systemctl restart php8.2-fpm nginx 2. Permission Issues sudo chown -R www-data:www-data /var/www/your-laravel-app sudo chmod -R 755 /var/www/your-laravel-app sudo chmod -R 775 /var/www/your-laravel-app/storage sudo chmod -R 775 /var/www/your-laravel-app/bootstrap/cache 3. Storage Link Issues php artisan storage:link sudo chown -R www-data:www-data /var/www/your-laravel-app/public/storage 4. Memory Issues # Increase PHP memory limit sudo nano /etc/php/8.2/fpm/php.ini # Set: memory_limit = 512M # Restart PHP-FPM sudo systemctl restart php8.2-fpm Performance Optimization Tips 1. Enable OPcache Ensure these settings in /etc/php/8.2/fpm/php.ini:\nopcache.enable=1 opcache.memory_consumption=256 opcache.max_accelerated_files=20000 opcache.validate_timestamps=0 opcache.save_comments=1 opcache.fast_shutdown=0 2. Optimize MySQL sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf [mysqld] innodb_buffer_pool_size = 256M innodb_log_file_size = 64M query_cache_type = 1 query_cache_size = 32M 3. Use CDN for Assets Consider using a CDN service like Cloudflare for static assets to improve loading times globally.\nSecurity Best Practices Regular Updates: Keep your server and applications updated Strong Passwords: Use complex passwords and consider key-based authentication Firewall: Configure UFW properly SSL/TLS: Always use HTTPS in production Hide Server Info: Remove server version information from headers Regular Backups: Implement automated backup strategies Monitor Logs: Regularly check access and error logs Rate Limiting: Implement rate limiting for sensitive endpoints Conclusion You now have a robust Laravel application running on a VPS with Nginx, complete with SSL certificates, optimization, and security measures. This setup provides excellent performance and gives you full control over your hosting environment.\nKey benefits of this setup:\nPerformance: Nginx + PHP-FPM provides excellent performance Security: SSL certificates and security headers protect your application Scalability: Easy to scale as your application grows Control: Full control over server configuration and optimization Cost-effective: VPS hosting is often more cost-effective than managed hosting Remember to:\nMonitor your application regularly Keep everything updated Implement proper backup strategies Test your deployment process in a staging environment first Happy deploying!\n","href":"/2025/08/deploy-laravel-to-vps-with-nginx-complete-guide.html","title":"Deploy Laravel to VPS with Nginx Complete Production Guide"},{"content":"Need to remove Docker from Ubuntu 24.04 (Noble) cleanly? This guide shows a safe, step‑by‑step removal that gets rid of the Engine, Compose v2 plugin, configs, and data \u0026ndash; plus optional rootless Docker cleanup. If you plan to reinstall after this, see: Install Docker on Ubuntu 24.04: Post‑Install, Rootless, and Compose v2 . For HTTPS and reverse proxy, see: Nginx + Certbot on Ubuntu 24.04: Free HTTPS with Let’s Encrypt .\nWarning: The steps below can remove containers, images, volumes, and networks. Back up anything important before continuing.\nWhat you’ll do\nStop and disable Docker services (Engine and containerd) Optionally remove all containers, images, volumes, and networks Purge Docker packages and the Compose v2 plugin Delete configuration and data directories (Engine and containerd) Optionally uninstall rootless Docker Verify that Docker is completely gone Prerequisites\nUbuntu 24.04 LTS (Noble) with sudo access Terminal access to the machine (SSH or local) (Optional) Remove containers, images, volumes, networks If you want a fully clean state, remove runtime data first. If you prefer to keep data, skip this step. # Remove all containers (running and stopped) sudo docker ps -aq | xargs -r sudo docker rm -f # Remove all images sudo docker image prune -a -f # Remove all volumes sudo docker volume prune -f # Remove unused networks sudo docker network prune -f Stop Docker services sudo systemctl disable --now docker docker.socket containerd || true Purge Docker packages Remove Engine, CLI, Buildx, and Compose v2 plugin (installed as apt plugins on Ubuntu 24.04 per official repo). Also cover legacy packages. sudo apt update sudo apt purge -y \\ docker-ce docker-ce-cli containerd.io \\ docker-buildx-plugin docker-compose-plugin \\ docker-ce-rootless-extras || true # In case older/alternative packages were installed sudo apt purge -y docker.io docker-doc podman-docker containerd runc || true sudo apt autoremove -y sudo apt clean Remove configuration, data, and repo files # Engine \u0026amp; containerd data/config sudo rm -rf /var/lib/docker /var/lib/containerd sudo rm -rf /etc/docker /etc/containerd 2\u0026gt;/dev/null || true sudo rm -rf /etc/systemd/system/docker.service.d 2\u0026gt;/dev/null || true # Socket leftovers sudo rm -f /var/run/docker.sock # Apt repository and key (official Docker repo) sudo rm -f /etc/apt/sources.list.d/docker.list sudo rm -f /etc/apt/keyrings/docker.gpg sudo apt update # Per-user Docker config (CLI) rm -rf ~/.docker (Optional) Uninstall rootless Docker (if you enabled it) Rootless Docker runs as a user service under systemd. If you used it, clean it up as well. # Stop/disable user service if present systemctl --user stop docker 2\u0026gt;/dev/null || true systemctl --user disable docker 2\u0026gt;/dev/null || true systemctl --user daemon-reload || true # If you installed via the helper tool, uninstall it command -v dockerd-rootless-setuptool.sh \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026amp; \\ dockerd-rootless-setuptool.sh uninstall || true # Remove user data/config rm -rf ~/.local/share/docker ~/.config/docker rm -f ~/.config/systemd/user/docker.service # Optional: disable lingering if you previously enabled it for rootless sudo loginctl disable-linger \u0026#34;$USER\u0026#34; 2\u0026gt;/dev/null || true Verify removal # Docker CLI should be missing if command -v docker; then echo \u0026#34;Docker still present\u0026#34;; else echo \u0026#34;Docker CLI not found ✔\u0026#34;; fi # No Docker or containerd packages dpkg -l | grep -E \u0026#34;^(ii|rc)\\s+(docker|containerd)\u0026#34; || echo \u0026#34;No docker/containerd packages found ✔\u0026#34; # Services should be inactive systemctl status docker 2\u0026gt;/dev/null | grep -q running \u0026amp;\u0026amp; echo \u0026#34;docker running\u0026#34; || echo \u0026#34;docker not running ✔\u0026#34; systemctl status containerd 2\u0026gt;/dev/null | grep -q running \u0026amp;\u0026amp; echo \u0026#34;containerd running\u0026#34; || echo \u0026#34;containerd not running ✔\u0026#34; Common troubleshooting\nStuck socket at /var/run/docker.sock: remove it with sudo rm -f /var/run/docker.sock and re‑check. Packages reappear after purge: run sudo apt purge ... again, then sudo apt autoremove -y \u0026amp;\u0026amp; sudo apt clean. Rootless processes still around: ps -u \u0026quot;$USER\u0026quot; | grep -E 'dockerd|containerd' then kill the PIDs, re‑run step 5. WSL2 on Windows: make sure you uninstall Docker Desktop WSL integration separately; this guide targets native Ubuntu 24.04. Reinstall later? When you’re ready to install again, follow the fresh 24.04 guide (official repo, Compose v2 plugin, optional rootless): Install Docker on Ubuntu 24.04: Post‑Install, Rootless, and Compose v2 .\n","href":"/2025/08/uninstall-docker-ubuntu-24-04-clean-removal.html","title":"Uninstall Docker on Ubuntu 24.04 Complete Clean Removal"},{"content":"Passkeys are increasingly supported across major platforms. They enable fast, convenient logins without passwords and are resistant to phishing. No more weak passwords or OTP codes hijacked via SIM swaps. This guide explains how passkeys work, compares them with legacy 2FA, and shows how to enable them on Google and Apple or use them with password managers like 1Password and Bitwarden.\nSummary: What Is a Passkey? A passkey is a passwordless credential based on FIDO2/WebAuthn. Instead of typing a shared secret, you prove possession of a private cryptographic key securely stored on your device (or in a compatible password manager). When you log in, the site/app sends a challenge that only your private key can sign. The server verifies the signature with the public key you registered. No shared secret travels over the network.\nIn practice:\nNo password transmission\u0026ndash;only per‑site cryptographic signatures. Phishing‑resistant\u0026ndash;signatures are bound to the real origin. More convenient\u0026ndash;use Face/Touch ID or your device PIN to approve. Why It’s Safer Than Passwords and SMS 2FA Phishing resistance: Traditional passwords/managers can be tricked by look‑alike domains; passkeys can’t. The challenge only works on the correct origin. No SIM‑swap risk: SMS codes can be intercepted or diverted; passkeys don’t rely on SMS. Not guessable/brute‑forceable: They’re cryptographic keys, not words. Lower breach impact: Sites store public keys, not secrets. A database leak alone won’t let attackers sign in. Caveat: Passkeys aren’t magic. A compromised device still puts you at risk. Keep your OS, browser, and extensions clean.\nHow to Enable Passkeys Menus vary by OS/browser version; the flow is similar everywhere.\nGoogle (Android/Chrome/Google Password Manager) Go to myaccount.google.com \u0026gt; Security \u0026gt; Passkeys. Click “Create a passkey” and approve with biometrics. On other websites that support passkeys, choose “Use passkey” to register/login. Sync: Your passkeys are stored in Google Password Manager and available on devices signed in to your Google account (protected by local biometrics/PIN). Apple (iCloud Keychain on iOS/macOS/Safari) Ensure iCloud Keychain is enabled: Settings \u0026gt; iCloud \u0026gt; Passwords and Keychain. When a site offers passkeys, Safari will prompt to save a passkey. Next logins are approved with Face ID/Touch ID. End‑to‑end encrypted sync via iCloud across your Apple devices. 1Password Update to the latest 1Password and enable passkey support in the app/extension. When a site offers passkeys, choose to save it in 1Password. Future logins can be approved via 1Password\u0026ndash;no password required. Benefits: cross‑platform, secure sharing for families/teams, admin policies for orgs. Bitwarden Update Bitwarden and enable passkey support in the extension/app. Save passkeys when registering/enabling them on supported sites. Approve future logins using Bitwarden with local biometrics/PIN. Benefits: open‑source, cost‑effective, organization features. Tip: If “Create/Use a passkey” doesn’t appear, check the site’s account security settings. Support is expanding\u0026ndash;banks, email providers, marketplaces, and developer platforms are rolling it out.\nHow It Works (The Short Version) On registration, your device creates a public/private key pair and registers the public key with the site. On login, the site sends a challenge that your device signs with the private key after local verification (biometrics/PIN). The browser enforces origin binding so signatures don’t work on fake domains. That property provides phishing resistance.\nLimitations and How to Mitigate Them Lost/replaced device: Ensure sync is enabled (iCloud/Google/manager) and keep recovery methods (backup codes) for critical accounts. Compatibility: Some sites don’t support passkeys yet\u0026ndash;keep a strong password + app‑based 2FA or a security key as fallback. Mixed ecosystems: If you use Apple + Windows + Android, a passkey‑capable manager (1Password/Bitwarden/Proton Pass) often provides the smoothest experience. Travel/emergency access: Keep at least one hardware security key as a break‑glass option for email, domain registrar, banking, and cloud. Migration Strategy: Practical Priorities Prioritize high‑value accounts first\u0026ndash;the ones attackers target most and the ones that would most harm your brand/SEO if compromised.\nSecure critical accounts first:\nPrimary email (Gmail/iCloud/Outlook) Cloud storage (Google Drive/iCloud/OneDrive) Banking/fintech Developer, domain/DNS, and hosting control panels Enable passkeys and keep app‑based 2FA (TOTP) as backup\nAvoid SMS where possible. Use a FIDO2 hardware key for mission‑critical accounts. Hygiene and audits\nRemove weak/duplicate passwords. Run your manager’s vault health check. Revoke unknown sessions/devices and retire risky recovery methods (old SMS). Team education (for orgs)\nStandardize on passkeys + authenticator + security keys. Teach staff to spot look‑alike domains, OAuth consent scams, and QR phishing. Quick FAQ Do I still need passwords? For many sites, yes\u0026ndash;as fallback. Increasingly, services allow passkey‑only. Keep a unique, strong fallback where required.\nAre passkeys safe if my phone is stolen? Passkeys are protected behind device biometrics/PIN. Enable remote wipe and rotate critical credentials if a device is lost.\nHow are passkeys different from TOTP? TOTP sits on top of passwords and can be entered on phishing sites. Passkeys remove passwords and bind authentication to the real domain.\nDo I need a hardware key? Highly recommended for critical accounts as a robust backup, but not mandatory for every account.\nGetting Started Enable passkeys on your Google/Apple account. Turn on passkey support in 1Password or Bitwarden (if you use them). Add passkeys to your primary email, domain registrar, and work platforms. Store recovery codes offline. Add one hardware key if possible. Phase out SMS 2FA where a stronger alternative exists (auth app/security key). Key Takeaways Passkeys provide a practical improvement: fast, convenient, and phishing‑resistant logins. Start with your most important accounts, enable trustworthy sync, set up recovery paths, and keep strong 2FA as backup. You get shorter logins, lower risk, and less password‑management overhead\u0026ndash;without the weak links of traditional passwords.\n","href":"/2025/08/what-are-passkeys-how-to-enable-google-apple-password-managers.html","title":"What Are Passkeys? How to Enable Them on Google, Apple, and Password Managers (2025 Guide)"},{"content":"Staying safe online is getting harder. Scammers use convincing emails, text messages, websites, and even mobile apps to trick people into giving away passwords, banking details, or installing malware. This plain-English guide explains the most common phishing signs, shows realistic (safe) examples, and gives you clear steps to protect yourself.\nWhat Is Phishing? Phishing is a social-engineering attack where criminals pretend to be a trusted brand, coworker, or service (bank, delivery company, marketplace, government agency) to make you click a link, open a file, or share sensitive information. Modern phishing blends good design with urgency (“Your account will be closed in 24 hours!”) so you act before thinking.\nQuick Warning: Dangerous Links and Apps Suspicious links can install malware or steal logins. Avoid clicking links from unexpected messages, even if they look official. Malicious apps (especially outside official stores) can steal SMS codes, read notifications, or take over your device. Shortened links (e.g., bit.ly), QR codes, and fake update pop-ups are common traps. Always verify the destination before proceeding. Common Signs of Phishing Emails Look for several red flags at the same time, not just one:\nMismatch sender and domain: The display name says “YourBank”, but the email is from notice@account-security.yourbank-support.example.com. Urgent or threatening tone: “Immediate action required”, “We detected unusual activity”, “Final warning”. Generic greeting: “Dear user” or “Dear customer” instead of your real name. Unexpected attachments: ZIP, PDF, HTML, or Office files asking to “enable content/macros”. Login links that don’t match the real domain: yourbank.secure-login.example.net instead of yourbank.com. Spelling or design inconsistencies: Wrong logo spacing, odd grammar, off-brand colors, or low-quality images. Requests for sensitive info: Passwords, OTP codes, card PIN, recovery codes\u0026ndash;legitimate companies won’t ask these by email/DM. Fake Email Examples (Safe Text-Only) Example 1 \u0026ndash; Delivery scam:\nSubject: Action required: Package on hold\n“We attempted to deliver your parcel. Confirm address and pay a small fee to release your package: hxxps://post-track-confirm[.]info/your-id”\nWhy it’s phishing: Delivery firms don’t ask for card details via generic links. The domain is unrelated to the real company.\nExample 2 \u0026ndash; Bank alert:\nSubject: Suspicious sign-in blocked\n“Your account will be suspended. Verify now: hxxps://yourbank-login[.]secure-check[.]net”\nWhy it’s phishing: Real banks use their exact domain (e.g., yourbank.com) and don’t threaten suspension via email links.\nExample 3 \u0026ndash; Workplace spear-phish:\nSubject: Updated payroll calendar Q3\n“See attached ‘Payroll_Q3.html’ and log in with your company email to view.”\nWhy it’s phishing: HTML attachments that ask you to log in are often credential harvesters.\nLink-Based Scams You’ll See Right Now Smishing (SMS) and messaging apps: Short texts with urgent links (“Your package fee is unpaid”) that open fake payment pages. QR phishing (QRishing): A QR code placed on posters or emails leading to a fake login portal. Treat QR codes like links\u0026ndash;verify before scanning. Link shorteners: Hide destinations. Use a URL expander or long-press/hover to preview before opening. Punycode lookalikes: Domains that visually mimic real brands (e.g., rn vs m, or accented characters) but are different under the hood. Fake invoice or payment request: “See invoice” buttons leading to a login capture page. OAuth consent scams: “This app wants access to your email/drive.” If approved, attackers don’t need your password. Only grant access to verified apps. Malicious Apps and Fake Updates Android sideloading (APK): Installing apps from links or unofficial stores can grant malware broad permissions (SMS, accessibility, overlay) to intercept OTP codes or control the screen. iOS test builds and profiles: Attackers may push TestFlight invites or configuration profiles that enable risky settings. Only install from known developers. Browser extensions: Fake “coupon”, “PDF”, or “security” extensions can read every page you visit. Only use well-reviewed, publisher-verified extensions. Fake update pop-ups: “Your browser/Flash needs an update” banners that download malware. Update via system settings or official stores only. How to Stay Safe (Practical Checklist) Verify the domain before you click. Manually type the website or use your saved bookmark. Check for subtle typos or extra words (e.g., -secure, -verify, or unusual subdomains). Use a password manager. It auto-fills only on the correct domain, acting as a built-in phishing detector. Turn on 2FA\u0026ndash;prefer authenticator apps or security keys over SMS. Security keys (FIDO2) block many phishing attempts by design. Never share OTP codes, recovery codes, or PINs\u0026ndash;no legitimate support will ask for them. Preview links. On desktop, hover to see the full URL. On mobile, long-press to preview. Expand shortened links before opening. Install apps only from official stores. Disable “install unknown apps”. Review requested permissions\u0026ndash;deny anything that looks excessive. Keep devices updated. Apply OS and app updates from official sources. Enable automatic updates. Use built-in protections: spam filters, Safe Browsing/SmartScreen, and device encryption. Consider enabling DNS filtering for families. Separate email addresses. Use one for banking/critical accounts, another for newsletters/shops to reduce exposure. Educate family and coworkers. Share examples, run quick simulations, and agree on a “call to verify” habit for money or data requests. What To Do If You Clicked Don’t panic\u0026ndash;act methodically. If you entered a password, change it immediately on the real site and any other site where you reused it. Then enable 2FA. If you approved a suspicious app/extension, remove it and revoke access: check your account’s “connected apps” or “security” page. Scan your device with a trusted security tool. On mobile, uninstall unknown apps and review permissions (Accessibility, Device Admin). Watch your accounts for unusual activity (login alerts, forwarding rules, payment changes). Set up alerts if available. Report the phish: mark as spam/phishing in your email app. If it impersonates your bank or employer, notify them through official channels. For financial or identity risk, contact your bank, freeze cards if needed, and consider credit monitoring. For Website and Email Owners (Quick Wins) Email authentication: Set up SPF, DKIM, and DMARC with a “quarantine/reject” policy to reduce spoofing of your domain. Enforce MFA for admin panels, hosting, and email accounts. Prefer security keys for critical roles. Use a WAF/CDN with bot and phishing page detection; enable rate limits for login endpoints. Educate staff about spear-phishing and CEO fraud. Use out-of-band verification for payment or credential requests. Key Takeaways Phishing is about pressure and imitation. Slow down and verify. Links and apps can be dangerous\u0026ndash;stick to official sources and check domains carefully. Password managers and security keys dramatically reduce risk. If you slip, reset credentials, revoke access, and monitor activity quickly. Stay cautious, share this guide with friends and family, and help others pause before they click.\n","href":"/2025/08/phishing-signs-fake-email-examples-how-to-avoid.html","title":"Phishing Signs, Fake Email Examples, and How to Avoid Them (2025 Guide)"},{"content":"Looking to add login to your FastAPI app without pulling in a full auth service? Here’s a small, production‑friendly setup. We’ll build username/password authentication with the OAuth2 Password flow and JSON Web Tokens (JWTs) for stateless access. It uses Pydantic v2 for validation and SQLAlchemy 2.0 for persistence. You’ll hash passwords properly, create/verify tokens, protect routes, and test everything end‑to‑end.\nIf you’re deploying the finished app on Ubuntu with HTTPS, check the deployment guide: Deploy FastAPI on Ubuntu 24.04: Gunicorn + Nginx + Certbot .\nWhat you’ll build A minimal user model backed by SQLAlchemy 2.0 Password hashing using passlib[bcrypt] JWT access token creation and verification with python-jose OAuth2 Password flow login endpoint (/token) Protected routes using OAuth2PasswordBearer A simple current‑user dependency that decodes JWTs Prerequisites Python 3.10+ Basic FastAPI experience SQLite for demo (swap with PostgreSQL/MySQL in production) Best‑practice project structure Use a small but clear layout so your imports stay tidy as the app grows:\napp/ main.py core/ security.py db/ base.py session.py models/ user.py schemas/ user.py api/ deps.py routes/ auth.py users.py health.py Install dependencies Create and activate a virtual environment, then install dependencies:\npython3 -m venv .venv source .venv/bin/activate pip install --upgrade pip pip install fastapi uvicorn sqlalchemy pydantic passlib[bcrypt] python-jose[cryptography] python-dotenv python-multipart Optional but recommended: manage secrets via a .env file during development.\nCreate a .env file in your project root:\ncat \u0026gt; .env \u0026lt;\u0026lt;\u0026#39;ENV\u0026#39; SECRET_KEY=$(openssl rand -hex 32) ACCESS_TOKEN_EXPIRE_MINUTES=30 ENV Where should .env live? Put .env in the project root (same level as app/). Run the app from the root so load_dotenv() finds it: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Add .env to .gitignore so it doesn’t get committed: .env If you sometimes run the app from a different working directory, you can load .env with an explicit path:\n# app/core/security.py (alternative) from pathlib import Path from dotenv import load_dotenv load_dotenv(Path(__file__).resolve().parents[2] / \u0026#34;.env\u0026#34;) .env.example and requirements.txt placement Keep both files at the project root for clarity and portability:\n. ├─ .env # not committed ├─ .env.example # committed, template for teammates/CI ├─ requirements.txt # pinned or curated dependencies └─ app/ ├─ core/ ├─ db/ ├─ models/ ├─ schemas/ ├─ api/ └─ main.py Suggested .env.example:\n# .env.example # Copy this file to .env and change the values as needed. SECRET_KEY=change-me-to-a-strong-random-value ACCESS_TOKEN_EXPIRE_MINUTES=30 # DATABASE_URL is optional here because the demo uses SQLite via app/db/session.py # For Postgres, uncomment and use your DSN: # DATABASE_URL=postgresql+psycopg://user:password@localhost:5432/mydb Pin dependencies with a requirements.txt (recommended):\nOption A \u0026ndash; write a curated requirements.txt with compatible ranges:\nfastapi\u0026gt;=0.110,\u0026lt;1 uvicorn[standard]\u0026gt;=0.29,\u0026lt;1 sqlalchemy\u0026gt;=2.0,\u0026lt;3 pydantic\u0026gt;=2.5,\u0026lt;3 passlib[bcrypt]\u0026gt;=1.7,\u0026lt;2 python-jose[cryptography]\u0026gt;=3.3,\u0026lt;4 python-dotenv\u0026gt;=1.0,\u0026lt;2 python-multipart\u0026gt;=0.0.9,\u0026lt;1 Option B \u0026ndash; pin exact versions from your current env:\npip freeze \u0026gt; requirements.txt Later, reproduce the env with:\npython3 -m venv .venv source .venv/bin/activate pip install -r requirements.txt Database setup (SQLAlchemy 2.0) Create two files for database plumbing.\n# app/db/base.py from sqlalchemy.orm import DeclarativeBase class Base(DeclarativeBase): pass # app/db/session.py from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker SQLALCHEMY_DATABASE_URL = \u0026#34;sqlite:///./app.db\u0026#34; engine = create_engine( SQLALCHEMY_DATABASE_URL, connect_args={\u0026#34;check_same_thread\u0026#34;: False} ) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) def get_db(): db = SessionLocal() try: yield db finally: db.close() Models and schemas We’ll store users with username and a hashed password (never store plain passwords).\n# app/models/user.py from sqlalchemy import Integer, String from sqlalchemy.orm import Mapped, mapped_column from app.db.base import Base class User(Base): __tablename__ = \u0026#34;users\u0026#34; id: Mapped[int] = mapped_column(Integer, primary_key=True, index=True) username: Mapped[str] = mapped_column(String, unique=True, index=True) hashed_password: Mapped[str] = mapped_column(String) Pydantic v2 schemas for reading/creating users:\n# app/schemas/user.py from pydantic import BaseModel class UserCreate(BaseModel): username: str password: str class UserRead(BaseModel): id: int username: str model_config = { \u0026#34;from_attributes\u0026#34;: True } class Token(BaseModel): access_token: str token_type: str = \u0026#34;bearer\u0026#34; Security helpers: hashing and JWT # app/core/security.py import os from datetime import datetime, timedelta, timezone from typing import Optional from jose import jwt from passlib.context import CryptContext from dotenv import load_dotenv load_dotenv() # load variables from .env if present pwd_context = CryptContext(schemes=[\u0026#34;bcrypt\u0026#34;], deprecated=\u0026#34;auto\u0026#34;) SECRET_KEY = os.getenv(\u0026#34;SECRET_KEY\u0026#34;, \u0026#34;change-this-in-env\u0026#34;) # override in production ALGORITHM = \u0026#34;HS256\u0026#34; ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv(\u0026#34;ACCESS_TOKEN_EXPIRE_MINUTES\u0026#34;, \u0026#34;30\u0026#34;)) def verify_password(plain_password: str, hashed_password: str) -\u0026gt; bool: return pwd_context.verify(plain_password, hashed_password) def hash_password(password: str) -\u0026gt; str: return pwd_context.hash(password) def create_access_token(subject: str, expires_delta: Optional[timedelta] = None) -\u0026gt; str: expire = datetime.now(timezone.utc) + (expires_delta or timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)) to_encode = {\u0026#34;sub\u0026#34;: subject, \u0026#34;exp\u0026#34;: expire} return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM) def decode_token(token: str) -\u0026gt; dict: return jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM]) API dependencies and routes Dependencies (current user) and small helper functions:\n# app/api/deps.py from fastapi import Depends, HTTPException, status from fastapi.security import OAuth2PasswordBearer from sqlalchemy.orm import Session from jose import JWTError from app.db.session import get_db from app.models.user import User from app.core.security import verify_password, decode_token oauth2_scheme = OAuth2PasswordBearer(tokenUrl=\u0026#34;token\u0026#34;) def get_user_by_username(db: Session, username: str) -\u0026gt; User | None: return db.query(User).filter(User.username == username).first() def authenticate_user(db: Session, username: str, password: str) -\u0026gt; User | None: user = get_user_by_username(db, username) if not user or not verify_password(password, user.hashed_password): return None return user def get_current_user(token: str = Depends(oauth2_scheme), db: Session = Depends(get_db)) -\u0026gt; User: try: payload = decode_token(token) username: str | None = payload.get(\u0026#34;sub\u0026#34;) if username is None: raise HTTPException(status_code=401, detail=\u0026#34;Invalid token payload\u0026#34;) except JWTError: raise HTTPException(status_code=401, detail=\u0026#34;Invalid or expired token\u0026#34;) user = get_user_by_username(db, username) if not user: raise HTTPException(status_code=404, detail=\u0026#34;User not found\u0026#34;) return user Auth and user routes:\n# app/api/routes/auth.py from datetime import timedelta from fastapi import APIRouter, Depends, HTTPException, status from fastapi.security import OAuth2PasswordRequestForm from sqlalchemy.orm import Session from app.api.deps import authenticate_user from app.db.session import get_db from app.schemas.user import Token from app.core.security import create_access_token, ACCESS_TOKEN_EXPIRE_MINUTES router = APIRouter() @router.post(\u0026#34;/token\u0026#34;, response_model=Token) def login(form_data: OAuth2PasswordRequestForm = Depends(), db: Session = Depends(get_db)): user = authenticate_user(db, form_data.username, form_data.password) if not user: raise HTTPException( status_code=status.HTTP_401_UNAUTHORIZED, detail=\u0026#34;Incorrect username or password\u0026#34;, headers={\u0026#34;WWW-Authenticate\u0026#34;: \u0026#34;Bearer\u0026#34;}, ) access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES) token = create_access_token(subject=user.username, expires_delta=access_token_expires) return {\u0026#34;access_token\u0026#34;: token, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;} # app/api/routes/users.py from fastapi import APIRouter, Depends, HTTPException from sqlalchemy.orm import Session from app.api.deps import get_current_user, get_user_by_username from app.db.session import get_db from app.models.user import User from app.schemas.user import UserCreate, UserRead from app.core.security import hash_password router = APIRouter() @router.post(\u0026#34;/users\u0026#34;, response_model=UserRead, status_code=201) def create_user(payload: UserCreate, db: Session = Depends(get_db)): exists = get_user_by_username(db, payload.username) if exists: raise HTTPException(status_code=400, detail=\u0026#34;Username already taken\u0026#34;) user = User(username=payload.username, hashed_password=hash_password(payload.password)) db.add(user) db.commit() db.refresh(user) return user @router.get(\u0026#34;/me\u0026#34;, response_model=UserRead) def read_me(current_user: User = Depends(get_current_user)): return current_user # app/api/routes/health.py from fastapi import APIRouter router = APIRouter() @router.get(\u0026#34;/healthz\u0026#34;) def healthz(): return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} FastAPI application entrypoint # app/main.py from fastapi import FastAPI from app.db.base import Base from app.db.session import engine from app.api.routes import auth, users, health app = FastAPI() # Create tables Base.metadata.create_all(bind=engine) # Mount routers app.include_router(auth.router, tags=[\u0026#34;auth\u0026#34;]) app.include_router(users.router, tags=[\u0026#34;users\u0026#34;]) app.include_router(health.router, tags=[\u0026#34;health\u0026#34;]) Try it out Run the app (from the project root):\nuvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Option A \u0026ndash; Swagger UI (easiest)\nOpen http://127.0.0.1:8000/docs POST /users to register a user (username + password) POST /token to get an access token Click “Authorize”, paste Bearer \u0026lt;the_token\u0026gt; GET /me to verify it returns your user Option B \u0026ndash; curl (robust, copy‑paste safe) To avoid shell quoting/wrapping issues, send JSON from a file and use urlencoded helpers:\n# 1) Register user echo \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;alice\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;S3curePass!\u0026#34;}\u0026#39; \u0026gt; user.json curl -sS -i -X POST http://127.0.0.1:8000/users \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ --data-binary @user.json # 2) Get token (form-url-encoded) TOKEN=$(curl -sS -X POST http://127.0.0.1:8000/token \\ -H \u0026#39;Content-Type: application/x-www-form-urlencoded\u0026#39; \\ --data-urlencode \u0026#39;username=alice\u0026#39; \\ --data-urlencode \u0026#39;password=S3curePass!\u0026#39; \\ | jq -r .access_token) # 3) Call protected route curl -sS -i http://127.0.0.1:8000/me -H \u0026#34;Authorization: Bearer $TOKEN\u0026#34; Notes\nIf you see “Invalid HTTP request received”, your curl command likely broke across lines or used smart quotes. Use the file + --data-binary approach above. If username is taken, register a different one (e.g., alice2). If you don’t have jq, you can copy the token manually from the JSON response, or extract it with Python: python -c \u0026quot;import sys,json;print(json.load(sys.stdin)['access_token'])\u0026quot;. Make sure python-multipart is installed; it’s required for the /token form endpoint. Option C \u0026ndash; Postman (GUI)\nRegister (POST /users): Body: raw -\u0026gt; JSON Content-Type: application/json Payload: { \u0026quot;username\u0026quot;: \u0026quot;alice\u0026quot;, \u0026quot;password\u0026quot;: \u0026quot;S3curePass!\u0026quot; } Login (POST /token): Body: x-www-form-urlencoded (not raw JSON) Keys: username=alice, password=S3curePass! Protected (GET /me): Authorization tab -\u0026gt; Type: Bearer Token -\u0026gt; paste the token (no quotes) Optional: import the Postman collection and use it directly: /postman/fastapi-jwt-auth.postman_collection.json.\nOption D \u0026ndash; HTTPie (nice DX)\n# Register http POST :8000/users username=alice password=S3curePass! # Login http -f POST :8000/token username=alice password=S3curePass! | jq # Me TOKEN=$(http -f POST :8000/token username=alice password=S3curePass! | jq -r .access_token) http GET :8000/me \u0026#34;Authorization:Bearer $TOKEN\u0026#34; Production notes Secrets: Never hardcode SECRET_KEY. Read it from environment variables or a secret manager. Token lifetime: Adjust ACCESS_TOKEN_EXPIRE_MINUTES based on risk. Consider short‑lived access tokens with refresh tokens. HTTPS and reverse proxy: Put FastAPI behind Nginx/Traefik and enforce HTTPS. See the deployment guide: /2025/08/deploy-fastapi-ubuntu-24-04-gunicorn-nginx-certbot.html. Password policy: Enforce minimum length and complexity. Consider rate‑limiting login attempts. Database: For PostgreSQL, change the SQLALCHEMY_DATABASE_URL (e.g., postgresql+psycopg://user:pass@host/db). Use Alembic for migrations. CORS/SPA: If used from a browser SPA, configure CORS properly and store tokens securely. For cookie‑based auth, consider OAuth2PasswordBearer alternatives with httponly cookies and CSRF protection. Scopes/roles: FastAPI supports OAuth2 scopes; add them to tokens and check in dependencies. Testing: Use httpx.AsyncClient and pytest to cover login and protected routes. Systemd tip (prod): set env vars in the unit file instead of .env:\n[Service] Environment=\u0026#34;SECRET_KEY=your-strong-secret\u0026#34; Environment=\u0026#34;ACCESS_TOKEN_EXPIRE_MINUTES=30\u0026#34; Wrap‑up You now have a working JWT‑based login using the OAuth2 Password flow in FastAPI with Pydantic v2 and SQLAlchemy 2.0. The example is deliberately small but production‑leaning: it hashes passwords, issues signed tokens, and protects endpoints with a simple dependency. From here, add what you need\u0026ndash;refresh tokens, roles/scopes, social logins, and migrations\u0026ndash;then deploy behind Nginx with HTTPS.\n","href":"/2025/08/fastapi-jwt-auth-oauth2-password-flow-pydantic-v2-sqlalchemy-2.html","title":"FastAPI JWT Auth with OAuth2 Password Flow (Pydantic v2 + SQLAlchemy 2.0)"},{"content":"If you want to run AI models locally on Ubuntu 24.04 with a clean web UI, this guide is for you. We’ll install Ollama , pull a model, and use Open WebUI for a modern chat interface. The steps cover CPU‑only and NVIDIA GPU notes, optional systemd services, and practical troubleshooting.\nWhat you\u0026rsquo;ll do\nInstall Ollama on Ubuntu 24.04 (Noble) Pull and run a starter model (e.g., llama3.1) Run Open WebUI (Docker) and connect to Ollama Optionally enable NVIDIA GPU acceleration (CUDA) Set up systemd services and basic hardening tips Prerequisites\nUbuntu 24.04 LTS (Noble), sudo user 4GB RAM minimum (8GB+ recommended) Optional: NVIDIA GPU with recent drivers for acceleration Step 1: Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh Start (or restart) the service:\nsudo systemctl enable --now ollama sudo systemctl status ollama --no-pager Step 2: Pull a model and test Examples:\nollama pull llama3.1 ollama run llama3.1 In the REPL, type a prompt and press Enter. Exit with Ctrl+C.\nStep 3 (optional): NVIDIA GPU acceleration If you have an NVIDIA GPU, ensure drivers and CUDA libraries are present. A common path is to install the official NVIDIA driver from Ubuntu’s Additional Drivers tool, then add CUDA if needed. Minimal CLI install:\nsudo apt update sudo apt install -y ubuntu-drivers-common ubuntu-drivers devices # see recommended driver sudo ubuntu-drivers install # installs the recommended driver sudo reboot After reboot, verify:\nnvidia-smi Ollama will detect CUDA automatically when available.\nStep 4: Run Open WebUI (Docker) Open WebUI connects to Ollama via its API (default http://127.0.0.1:11434).\ndocker run -d \\ --name open-webui \\ -p 3000:8080 \\ -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\ -v open-webui:/app/backend/data \\ --restart unless-stopped \\ ghcr.io/open-webui/open-webui:latest Notes:\nOn Linux, host.docker.internal works on recent Docker. If it doesn\u0026rsquo;t, you can either: Add host gateway mapping: --add-host=host.docker.internal:host-gateway, or Use host networking: --network host and set -e OLLAMA_BASE_URL=http://127.0.0.1:11434. Visit http://SERVER_IP:3000 to access the UI. Step 5 (optional): Make Ollama listen on LAN By default, Ollama binds to localhost. To make it reachable (e.g., from other machines or containers without host network), create an override:\nsudo systemctl edit ollama Paste the following (then save):\n[Service] Environment=\u0026#34;OLLAMA_HOST=0.0.0.0:11434\u0026#34; Apply the change:\nsudo systemctl daemon-reload sudo systemctl restart ollama Secure with a firewall (UFW) and reverse proxy auth if exposing publicly. For example, allow only your management IP and HTTPS:\nsudo ufw allow 22/tcp sudo ufw allow 443/tcp sudo ufw allow from YOUR_IP to any port 11434 proto tcp # optional, management only sudo ufw enable Step 6: Persist and manage with systemd (Open WebUI option) If you prefer systemd over docker run, create a simple unit that uses Docker Compose or a raw Docker command. Example raw Docker service:\nsudo tee /etc/systemd/system/open-webui.service \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; [Unit] Description=Open WebUI (Docker) After=network-online.target docker.service Wants=network-online.target [Service] Restart=always TimeoutStartSec=0 ExecStartPre=/usr/bin/docker rm -f open-webui || true ExecStart=/usr/bin/docker run --name open-webui \\ -p 3000:8080 \\ -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\ -v open-webui:/app/backend/data \\ --restart unless-stopped \\ ghcr.io/open-webui/open-webui:latest ExecStop=/usr/bin/docker stop open-webui [Install] WantedBy=multi-user.target EOF sudo systemctl daemon-reload sudo systemctl enable --now open-webui Step 7 (optional): Reverse proxy (Nginx) If you want https://ai.example.com, set up an Nginx proxy and a Let’s Encrypt cert. See this guide for TLS issuance and hardening: Nginx + Certbot on Ubuntu 24.04 Then proxy ai.example.com -\u0026gt; 127.0.0.1:3000.\nTroubleshooting\nPort 11434 in use: sudo lsof -i :11434 to find the process. Restart Ollama: sudo systemctl restart ollama. nvidia-smi missing or fails: ensure proper NVIDIA driver install; consider purging and reinstalling drivers. Open WebUI can’t reach Ollama: verify OLLAMA_BASE_URL, container networking, and that curl http://127.0.0.1:11434/api/tags returns JSON. Low RAM: try smaller models (e.g., phi3, qwen2:0.5b, or quantized variants) and keep a single model loaded. Uninstall Ollama:\nsudo systemctl disable --now ollama sudo rm -f /etc/systemd/system/ollama.service sudo rm -rf /usr/local/bin/ollama ~/.ollama sudo systemctl daemon-reload Open WebUI:\nsudo systemctl disable --now open-webui || true sudo rm -f /etc/systemd/system/open-webui.service sudo systemctl daemon-reload docker rm -f open-webui || true docker volume rm open-webui || true That’s it \u0026ndash; you now have a local AI stack on Ubuntu 24.04 with Ollama and Open WebUI. Start lightweight models first, then scale up as your hardware allows.\n","href":"/2025/08/install-ollama-openwebui-ubuntu-24-04.html","title":"Install Ollama and Open WebUI on Ubuntu 24.04 Local AI (CPU/GPU)"},{"content":"If you reuse passwords, the internet is quietly stacking odds against you. One small site gets breached, your email and password leak, and attackers try the same combo on your email, banking, cloud storage\u0026ndash;everywhere. That “I’ll remember it” system works right up until it doesn’t. The fix isn’t superhuman memory; it’s outsourcing the problem to a tool designed for it: a password manager.\nWhat a password manager actually does\nGenerates strong, unique passwords for every account Stores them encrypted, synced across your devices Auto‑fills only on the correct websites/apps Audits your vault for weak/reused/compromised passwords Holds secure notes, TOTP codes (in some apps), and sometimes passkeys The goal is simple: every account gets its own high‑entropy secret, and you never type or remember it again.\nRecommended apps (pick one that fits you)\nBitwarden (Free + Premium): Open‑source, great value, works on all platforms and browsers, supports organizations/families, and has excellent import/export. Paid tier adds TOTP, vault health, and more. A strong “default choice” for most people. 1Password (Paid): Polished UX, excellent security model (Secret Key + Master Password), great families features, best‑in‑class browser integration. If you want something that “just feels nice” and you’re okay paying, it’s hard to beat. Proton Pass (Free + Paid): From the Proton team (Mail/Drive/VPN). Simple, privacy‑centric, integrated with Proton ecosystem, passkey support. Good if you already live in Proton land. KeePassXC (Free, local): No cloud, full control. Great for people who want local files + their own sync (e.g., iCloud Drive, Syncthing). More hands‑on, but beloved by power users. Quick decision guide\nI want the best free cross‑platform option: Bitwarden I want the smoothest family experience: 1Password Families I want privacy + Proton ecosystem: Proton Pass I want local/no cloud: KeePassXC (plus a sync method you trust) How to migrate in a weekend (no overwhelm)\nChoose your manager and install across your devices Install the desktop app and browser extension (Chrome/Firefox/Safari/Edge). Install the mobile app. Enable biometrics for convenience (your face/fingerprint is only a local unlock\u0026ndash;your master password still matters). Create a strong master password Use a long passphrase (5\u0026ndash;6 random words, with separators). Length beats cleverness. Don’t reuse this anywhere else. Write it down once and store it in a safe or lockbox until you’ve memorized it. Turn on 2FA for your password manager Use an authenticator app (or hardware key) to protect your vault login. Import existing logins Export from your browser’s saved passwords (Chrome/Edge/Firefox/Safari) or your old manager. Import into the new vault. Then disable the browser’s built‑in password saving to avoid duplicates/confusion. Set your generator defaults 20+ characters, random, include symbols, avoid ambiguous characters. For sites that reject long passwords (it happens), drop to 16\u0026ndash;never reuse an old one. Fix the crown jewels first Email, primary phone account, banking, cloud storage, Apple/Google/Microsoft IDs, domain registrars, developer platforms (GitHub, GitLab). Rotate these passwords immediately and enable 2FA. Enable passkeys where available Many sites now support passkeys (phishing‑resistant, no password to steal). Your manager or platform (iCloud Keychain, Google Password Manager) can store them. Use passkeys when you can; keep a password fallback when you must. Clean up and audit Run the vault health check (Bitwarden/1Password/Proton Pass) to spot reused/weak/compromised passwords. Replace a handful each day until the list is clean. Back up recovery options Save recovery codes for critical accounts (email, cloud, banks). Store them offline. If your manager offers an emergency kit (1Password), print it and keep it safe. New habit: let the manager do the typing On sign‑up screens, use “Generate password” and save. On login, auto‑fill from the extension or app. If you ever type a password by hand, it’s a smell. Simple rules that keep you safe long‑term\nOne master password to rule them all\u0026ndash;never reuse it. 2FA everywhere it matters (email first, then banks, then social/dev tools). Unique passwords for every account, no exceptions. Don’t store 2FA codes in the same place as passwords for high‑value targets (email, banking). Split risk\u0026ndash;use a separate authenticator or a security key. Treat SMS 2FA as the last resort; prefer authenticator apps or hardware keys. Be picky about browser auto‑fill prompts. If your manager doesn’t light up on a page, double‑check the URL. Phishing relies on rushed clicks. What about “my browser already saves passwords”?\nBrowsers have improved, but dedicated managers still win on cross‑platform support, breach monitoring, secure sharing, granular vaults, and recovery workflows. If you’re deep in one platform (e.g., only Apple devices), iCloud Keychain + passkeys is fine\u0026ndash;but for most mixed setups, Bitwarden/1Password/Proton Pass give you fewer sharp edges.\nThreats this actually addresses\nCredential stuffing: Unique passwords stop attackers from reusing a leaked password elsewhere. Phishing: Managers auto‑fill only on the right domain; passkeys resist phishing by design. Weak/guessable passwords: Generators create high‑entropy secrets that aren’t in any wordlist. Things this does not solve (and what to do)\nMalware on your device: Keep OS and browser updated, don’t install sketchy extensions, and scan if anything feels off. Public Wi‑Fi interception: Use HTTPS (default) and a reputable VPN if you must use untrusted networks. Account recovery traps: Keep recovery emails/phones current; store backup codes offline. Quick Action Steps\nInstall a manager on desktop + phone Set a long master passphrase and enable 2FA on the vault Import your browser’s saved passwords Rotate the password on your email + cloud + bank Disable browser password saving, keep only the manager You don’t need to fix your entire digital life in one night\u0026ndash;just stop the worst risk: reuse. Move your important accounts now, chip away at the rest, and let the tool do the heavy lifting. In a week, you’ll wonder how you ever lived without the “Generate” button.\n","href":"/2025/08/stop-reusing-passwords-practical-password-manager-guide.html","title":"Stop Reusing Passwords A Practical Guide to Password Managers"},{"content":"Want to deploy FastAPI on Ubuntu 24.04 with a clean, secure, and maintainable setup? This guide walks you through running Gunicorn (ASGI server), Nginx (reverse proxy), and free HTTPS from Let’s Encrypt using Certbot. We’ll also use systemd so your service starts on boot and is easy to restart after updates.\nWhat you’ll build:\nA minimal FastAPI project structure Running the app with Gunicorn (Uvicorn worker) A systemd service for start/stop/restart Nginx reverse proxy to Gunicorn HTTPS (Certbot) with auto‑renewal UFW firewall (open 80/443), logs, and troubleshooting tips Prerequisites Ubuntu 24.04 server (sudo access) A domain pointing to the server (A/AAAA records) Python 3.10+ (Ubuntu 24.04 default is fine) Prepare the project structure on the server A tidy layout makes automation easier.\nsudo mkdir -p /opt/fastapi/app sudo adduser --system --group --home /opt/fastapi fastapi sudo chown -R fastapi:fastapi /opt/fastapi Create a virtualenv and install dependencies sudo apt update sudo apt install -y python3-venv sudo -u fastapi python3 -m venv /opt/fastapi/venv sudo -u fastapi /opt/fastapi/venv/bin/pip install --upgrade pip sudo -u fastapi /opt/fastapi/venv/bin/pip install fastapi uvicorn gunicorn Create a requirements.txt for easier dependency management:\nsudo -u fastapi tee /opt/fastapi/requirements.txt \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;REQS\u0026#39; fastapi==0.104.1 uvicorn[standard]==0.24.0 gunicorn==21.2.0 pydantic==2.5.0 REQS sudo -u fastapi /opt/fastapi/venv/bin/pip install -r /opt/fastapi/requirements.txt Create a minimal FastAPI app sudo -u fastapi tee /opt/fastapi/app/main.py \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;PY\u0026#39; from fastapi import FastAPI app = FastAPI() @app.get(\u0026#34;/healthz\u0026#34;) def healthz(): return {\u0026#34;status\u0026#34;: \u0026#34;ok\u0026#34;} @app.get(\u0026#34;/\u0026#34;) def root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello from FastAPI on Ubuntu 24.04!\u0026#34;} PY Optional: quick local test\n# IMPORTANT: Change to app directory first to avoid permission errors cd /opt/fastapi sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 Visit http://SERVER_IP:8000 to verify it works.\nCommon Error Fix: If you get PermissionError: Permission denied (os error 13) about [\u0026quot;/root\u0026quot;], it means uvicorn is trying to watch the wrong directory. Always cd /opt/fastapi first before running the command.\nRun with Gunicorn (ASGI) manually for testing sudo -u fastapi /opt/fastapi/venv/bin/gunicorn \\ -k uvicorn.workers.UvicornWorker \\ -w 2 \\ -b 0.0.0.0:8000 \\ app.main:app If logs look healthy and port 8000 serves requests (try curl http://SERVER_IP:8000/healthz or curl 127.0.0.1:8000/healthz), proceed to the service setup.\nChoose your process manager (pick one) You need to choose how to run your FastAPI app as a service. Pick either Option A (systemd) or Option B (PM2):\nOption A: Create a systemd service for Gunicorn (Recommended) sudo tee /etc/systemd/system/fastapi.service \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;SERVICE\u0026#39; [Unit] Description=FastAPI app with Gunicorn After=network.target [Service] User=fastapi Group=fastapi WorkingDirectory=/opt/fastapi Environment=\u0026#34;PATH=/opt/fastapi/venv/bin\u0026#34; ExecStart=/opt/fastapi/venv/bin/gunicorn -k uvicorn.workers.UvicornWorker -w 2 -b 0.0.0.0:8000 app.main:app Restart=always RestartSec=5 [Install] WantedBy=multi-user.target SERVICE sudo systemctl daemon-reload sudo systemctl enable --now fastapi sudo systemctl status fastapi --no-pager Option B: Using PM2 (Alternative Process Manager) PM2 is great for Node.js but also works excellently with Python apps. It provides easy clustering, monitoring, and log management.\nInstall PM2:\n# Install Node.js and PM2 curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - sudo apt-get install -y nodejs sudo npm install -g pm2 Create PM2 ecosystem config:\nsudo -u fastapi tee /opt/fastapi/ecosystem.config.js \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;JS\u0026#39; module.exports = { apps: [{ name: \u0026#39;fastapi-app\u0026#39;, script: \u0026#39;/opt/fastapi/venv/bin/gunicorn\u0026#39;, args: \u0026#39;-k uvicorn.workers.UvicornWorker -w 2 -b 127.0.0.1:8000 app.main:app\u0026#39;, cwd: \u0026#39;/opt/fastapi\u0026#39;, instances: 1, autorestart: true, watch: false, max_memory_restart: \u0026#39;1G\u0026#39;, env: { NODE_ENV: \u0026#39;production\u0026#39; }, error_file: \u0026#39;/opt/fastapi/logs/err.log\u0026#39;, out_file: \u0026#39;/opt/fastapi/logs/out.log\u0026#39;, log_file: \u0026#39;/opt/fastapi/logs/combined.log\u0026#39;, time: true }] } JS # Create logs directory sudo -u fastapi mkdir -p /opt/fastapi/logs Start with PM2:\n# Start the application sudo -u fastapi pm2 start /opt/fastapi/ecosystem.config.js # Save PM2 process list sudo -u fastapi pm2 save # Setup PM2 to start on boot sudo env PATH=$PATH:/usr/bin /usr/lib/node_modules/pm2/bin/pm2 startup systemd -u fastapi --hp /opt/fastapi # Check status sudo -u fastapi pm2 status sudo -u fastapi pm2 logs fastapi-app PM2 Management Commands:\n# Restart app sudo -u fastapi pm2 restart fastapi-app # Stop app sudo -u fastapi pm2 stop fastapi-app # Monitor in real-time sudo -u fastapi pm2 monit # View logs sudo -u fastapi pm2 logs fastapi-app --lines 50 Systemd vs PM2 Comparison:\nFeature Systemd PM2 Built-in Ubuntu Yes Requires Node.js Memory usage Lower Higher (Node.js overhead) Monitoring UI Command line only pm2 monit dashboard Log management journalctl Built-in log rotation Clustering Manual setup Easy clustering Learning curve Moderate Easier Production ready Enterprise grade Battle tested Choose systemd if: You want minimal overhead and native Ubuntu integration. Choose PM2 if: You want easier monitoring, log management, and plan to scale horizontally.\nIMPORTANT: You must complete either Option A or Option B above before proceeding to Nginx setup!\nInstall and configure Nginx (reverse proxy) sudo apt install -y nginx sudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 60s; } } NGINX sudo ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Open the firewall (UFW) for HTTP/HTTPS sudo ufw allow \u0026#39;Nginx Full\u0026#39; # opens 80/tcp and 443/tcp sudo ufw allow 8000 # allow direct access to FastAPI for testing sudo ufw status Issue free HTTPS with Certbot sudo apt install -y certbot python3-certbot-nginx sudo certbot --nginx -d example.com -d www.example.com Certbot will configure the 443 server block and set up auto‑renewal. You can test renewal with:\nsudo certbot renew --dry-run Checks and monitoring Try: curl -I https://example.com/healthz App logs: journalctl -u fastapi -f Nginx logs: /var/log/nginx/access.log and error.log Production optimizations Add some production-ready configurations:\nGunicorn production config:\nsudo -u fastapi tee /opt/fastapi/gunicorn.conf.py \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;GUNICORN\u0026#39; # Gunicorn configuration file bind = \u0026#34;0.0.0.0:8000\u0026#34; worker_class = \u0026#34;uvicorn.workers.UvicornWorker\u0026#34; workers = 2 worker_connections = 1000 max_requests = 1000 max_requests_jitter = 100 preload_app = True keepalive = 2 timeout = 30 graceful_timeout = 30 GUNICORN # Update systemd service to use config file sudo sed -i \u0026#39;s|ExecStart=.*|ExecStart=/opt/fastapi/venv/bin/gunicorn -c /opt/fastapi/gunicorn.conf.py app.main:app|\u0026#39; /etc/systemd/system/fastapi.service sudo systemctl daemon-reload sudo systemctl restart fastapi Enhanced Nginx config with security headers:\nsudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; return 301 https://$server_name$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name example.com www.example.com; # SSL configuration (handled by Certbot) # Security headers add_header X-Frame-Options DENY; add_header X-Content-Type-Options nosniff; add_header X-XSS-Protection \u0026#34;1; mode=block\u0026#34;; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains\u0026#34; always; # Gzip compression gzip on; gzip_vary on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml; location / { proxy_pass http://127.0.0.1:8000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 60s; proxy_connect_timeout 60s; proxy_send_timeout 60s; # Buffer settings proxy_buffering on; proxy_buffer_size 4k; proxy_buffers 8 4k; } # Health check endpoint (no logging) location /healthz { proxy_pass http://127.0.0.1:8000; access_log off; } } NGINX sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Update and deployment strategies For systemd deployments:\n# Create deployment script sudo tee /opt/fastapi/deploy.sh \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;DEPLOY\u0026#39; #!/bin/bash set -e echo \u0026#34;Starting deployment...\u0026#34; # Pull latest code (if using git) cd /opt/fastapi sudo -u fastapi git pull origin main # Update dependencies sudo -u fastapi /opt/fastapi/venv/bin/pip install -r requirements.txt # Run any migrations or setup scripts here # sudo -u fastapi /opt/fastapi/venv/bin/python manage.py migrate # Test the app syntax sudo -u fastapi /opt/fastapi/venv/bin/python -c \u0026#34;import app.main\u0026#34; # Restart the service sudo systemctl restart fastapi # Wait a moment and check if it\u0026#39;s running sleep 5 sudo systemctl is-active --quiet fastapi \u0026amp;\u0026amp; echo \u0026#34;Deployment successful!\u0026#34; || echo \u0026#34;Deployment failed!\u0026#34; echo \u0026#34;Checking app health...\u0026#34; curl -f http://127.0.0.1:8000/healthz || echo \u0026#34;Health check failed\u0026#34; DEPLOY sudo chmod +x /opt/fastapi/deploy.sh For PM2 deployments:\n# PM2 deployment sudo -u fastapi pm2 stop fastapi-app cd /opt/fastapi sudo -u fastapi git pull origin main sudo -u fastapi /opt/fastapi/venv/bin/pip install -r requirements.txt sudo -u fastapi pm2 restart fastapi-app sudo -u fastapi pm2 save Zero-downtime deployment with PM2:\n# Update ecosystem.config.js for zero-downtime sudo -u fastapi tee /opt/fastapi/ecosystem.config.js \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;JS\u0026#39; module.exports = { apps: [{ name: \u0026#39;fastapi-app\u0026#39;, script: \u0026#39;/opt/fastapi/venv/bin/gunicorn\u0026#39;, args: \u0026#39;-c /opt/fastapi/gunicorn.conf.py app.main:app\u0026#39;, cwd: \u0026#39;/opt/fastapi\u0026#39;, instances: 2, // Multiple instances for zero-downtime exec_mode: \u0026#39;fork\u0026#39;, autorestart: true, watch: false, max_memory_restart: \u0026#39;1G\u0026#39;, kill_timeout: 5000, wait_ready: true, listen_timeout: 10000, env: { NODE_ENV: \u0026#39;production\u0026#39; } }] } JS # Reload with zero downtime sudo -u fastapi pm2 reload fastapi-app Monitoring and logging Basic monitoring with systemd:\n# Check service status sudo systemctl status fastapi # View logs in real-time sudo journalctl -u fastapi -f # Check resource usage sudo systemctl show fastapi --property=MainPID ps aux | grep $(sudo systemctl show fastapi --property=MainPID --value) Basic monitoring with PM2:\n# Real-time monitoring dashboard sudo -u fastapi pm2 monit # Check memory and CPU usage sudo -u fastapi pm2 list # View detailed process info sudo -u fastapi pm2 describe fastapi-app Log rotation setup:\n# For systemd logs sudo tee /etc/logrotate.d/fastapi \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;LOGROTATE\u0026#39; /var/log/nginx/access.log { daily missingok rotate 30 compress delaycompress notifempty create 644 www-data www-data postrotate sudo systemctl reload nginx \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 endscript } LOGROTATE Tips \u0026amp; troubleshooting Common issues and solutions:\nPermission denied error when testing uvicorn:\n# Wrong: This will cause permission error if run from /root sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload # Correct: Always change directory first cd /opt/fastapi sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 # Or run without reload flag for testing sudo -u fastapi /opt/fastapi/venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 Root cause: Uvicorn with --reload tries to watch the current working directory. If you run from /root, the fastapi user cannot access it.\n502 Bad Gateway:\n# Check if FastAPI service is running sudo systemctl status fastapi # or for PM2 sudo -u fastapi pm2 status # Check application logs sudo journalctl -u fastapi -f --since \u0026#34;10 minutes ago\u0026#34; # or for PM2 sudo -u fastapi pm2 logs fastapi-app --lines 50 # Test direct connection to Gunicorn curl -I http://127.0.0.1:8000/healthz # Or test from outside curl -I http://SERVER_IP:8000/healthz High memory usage:\n# Check memory consumption sudo systemctl show fastapi --property=MemoryCurrent # Restart if memory is too high sudo systemctl restart fastapi # For PM2 - automatic restart on high memory # Already configured with max_memory_restart: \u0026#39;1G\u0026#39; Performance tuning:\n# Adjust workers based on CPU cores # Rule of thumb: (2 x CPU cores) + 1 nproc # Check CPU cores # Update gunicorn workers in config sudo sed -i \u0026#39;s/workers = 2/workers = 3/\u0026#39; /opt/fastapi/gunicorn.conf.py sudo systemctl restart fastapi SSL certificate issues:\n# Test certificate renewal sudo certbot renew --dry-run # Check certificate expiry sudo certbot certificates # Manual renewal if needed sudo certbot renew --force-renewal -d example.com Security Best Practices:\nNon-root user (fastapi) Firewall (UFW) configured SSL/TLS encryption Security headers in Nginx No direct access to Gunicorn port Consider: fail2ban, regular security updates Consider: database connection encryption Consider: rate limiting in Nginx Recommended next steps Monitoring: Set up Prometheus + Grafana for advanced metrics Backup: Database backups, SSL certificate backups CI/CD: GitHub Actions for automated testing and deployment Load balancing: Multiple app servers behind Nginx for high availability Caching: Redis for session storage and caching Database: PostgreSQL with connection pooling (SQLAlchemy + asyncpg) Related articles:\nNginx + Certbot on Ubuntu 24.04 - SSL setup guide Install Docker on Ubuntu 24.04 - Containerized deployment option That\u0026rsquo;s it! You now have a production-ready FastAPI deployment on Ubuntu 24.04 with multiple process management options (systemd vs PM2), HTTPS encryption, and comprehensive monitoring. Choose the approach that best fits your infrastructure and scaling needs. Happy coding!\n","href":"/2025/08/deploy-fastapi-ubuntu-24-04-gunicorn-nginx-certbot.html","title":"Deploy FastAPI on Ubuntu 24.04 Gunicorn + Nginx + Certbot (HTTPS)"},{"content":"Want a free, trusted HTTPS certificate for your site on Ubuntu 24.04? This guide walks you through installing Nginx, opening the right firewall ports, issuing a free Let’s Encrypt certificate with Certbot, enabling automatic renewal, forcing HTTP-\u0026gt;HTTPS redirects, and applying sane TLS settings. You’ll also see common troubleshooting steps and how to test your configuration. If you need to containerize your apps first, set up Docker here: Install Docker on Ubuntu 24.04: Post-Install, Rootless, and Compose v2 What you’ll do\nPoint your domain to your server via DNS (A/AAAA records) Install Nginx from Ubuntu repositories Allow HTTP/HTTPS through the firewall Install Certbot and issue a Let’s Encrypt certificate Auto-renew the certificate and verify renewal Redirect HTTP to HTTPS and harden TLS settings Test, troubleshoot, and (optionally) revoke/uninstall Prerequisites\nUbuntu 24.04 LTS (Noble) with sudo access A domain name (e.g., example.com) you control DNS A/AAAA records pointing to your server’s public IP Configure DNS Make sure your domain points to your server. At your DNS provider, set: A record: example.com -\u0026gt; YOUR_IPV4 AAAA record: example.com -\u0026gt; YOUR_IPV6 (optional) Optional: wildcard or subdomain records (e.g., www.example.com ) Propagation can take minutes to hours. You can check resolution with:\ndig +short example.com dig +short www.example.com Install Nginx sudo apt update sudo apt install -y nginx Validate Nginx is running:\nsystemctl status nginx --no-pager Open your server’s IP in a browser; you should see the default Nginx welcome page.\nOpen the firewall (UFW) If UFW is enabled, allow Nginx traffic: sudo ufw allow \u0026#39;Nginx Full\u0026#39; # opens 80/tcp and 443/tcp sudo ufw status If UFW is disabled, you can skip this step. For cloud providers, also ensure security groups allow ports 80 and 443.\nCreate a basic server block (optional but recommended) By default, Nginx serves the default site. Create a server block for your domain to keep things organized: sudo mkdir -p /var/www/example.com/html echo \u0026#39;\u0026lt;h1\u0026gt;It works!\u0026lt;/h1\u0026gt;\u0026#39; | sudo tee /var/www/example.com/html/index.html \u0026gt; /dev/null sudo tee /etc/nginx/sites-available/example.com \u0026gt;/dev/null \u0026lt;\u0026lt;\u0026#39;NGINX\u0026#39; server { listen 80; listen [::]:80; server_name example.com www.example.com; root /var/www/example.com/html; index index.html; location / { try_files $uri $uri/ =404; } } NGINX sudo ln -s /etc/nginx/sites-available/example.com /etc/nginx/sites-enabled/ sudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Visit http://example.com to confirm it serves your content.\nInstall Certbot (recommended via snap) The Certbot team recommends snap for the latest version. sudo apt install -y snapd sudo snap install core; sudo snap refresh core sudo snap install --classic certbot sudo ln -s /snap/bin/certbot /usr/bin/certbot || true Obtain and install a certificate (Nginx plugin) Use the Nginx plugin to edit config and reload automatically: sudo certbot --nginx -d example.com -d www.example.com Follow the prompts (email, ToS). Choose the redirect option when asked so HTTP automatically redirects to HTTPS.\nAlternative: Webroot method (if you prefer manual control)\nsudo certbot certonly --webroot -w /var/www/example.com/html -d example.com -d www.example.com If you used webroot, add SSL directives to your server block and reload Nginx (see step 8 for TLS settings).\nAuto-renewal Snap installs a systemd timer for Certbot. Verify it: systemctl list-timers | grep certbot sudo certbot renew --dry-run Dry-run should complete without errors. Certificates renew automatically ~30 days before expiry.\nForce HTTP-\u0026gt;HTTPS and apply TLS best practices If you didn’t choose the redirect option during Certbot run or you used webroot, update your Nginx config. A sane baseline (based on Mozilla’s “intermediate” profile) is: server { listen 80; listen [::]:80; server_name example.com www.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name example.com www.example.com; root /var/www/example.com/html; index index.html; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; ssl_session_timeout 1d; ssl_session_cache shared:SSL:10m; ssl_session_tickets off; ssl_protocols TLSv1.2 TLSv1.3; ssl_ciphers \u0026#39;ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305\u0026#39;; ssl_prefer_server_ciphers off; add_header Strict-Transport-Security \u0026#34;max-age=31536000; includeSubDomains; preload\u0026#34; always; add_header X-Content-Type-Options nosniff; add_header X-Frame-Options DENY; add_header Referrer-Policy no-referrer-when-downgrade; location / { try_files $uri $uri/ =404; } } Then test and reload:\nsudo nginx -t \u0026amp;\u0026amp; sudo systemctl reload nginx Use SSL Labs (Qualys) to analyze: https://www.ssllabs.com/ssltest/ Canonical redirect (optional) If you want to force a single hostname (e.g., redirect www-\u0026gt;apex), add a dedicated server block:\nserver { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.example.com; ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem; return 301 https://example.com$request_uri; } OCSP stapling (recommended) Reduce TLS handshake latency and improve scores with OCSP stapling:\nssl_stapling on; ssl_stapling_verify on; ssl_trusted_certificate /etc/letsencrypt/live/example.com/chain.pem; resolver 1.1.1.1 1.0.0.1 valid=300s; resolver_timeout 5s; Place these inside the TLS server block (port 443) after your ssl_certificate lines.\nCompression (performance) Enable gzip (widely available) for text assets:\ngzip on; gzip_comp_level 5; gzip_min_length 256; gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml; gzip_vary on; Note: Brotli offers better compression but may not be compiled by default in Ubuntu’s Nginx. If you install a Brotli-enabled build, you can use:\n# brotli on; # brotli_comp_level 5; # brotli_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript image/svg+xml; Test your HTTPS setup Browser: go to https://example.com and inspect the lock icon CLI: curl -I https://example.com should return HTTP/2 200 (or 301 -\u0026gt; 200 if redirecting from www) Check Nginx logs: /var/log/nginx/access.log and /var/log/nginx/error.log Troubleshooting\nDNS/Challenge failed: Ensure your A/AAAA records point to this server and port 80 is reachable from the internet. Temporarily disable any reverse proxy or CDN during issuance. Firewall blocks: Open ports 80 and 443 in UFW/security groups. nc -vz your.ip 80 from an external host can help verify reachability. Nginx conflicts: Run sudo nginx -t to find syntax errors or duplicated server_name blocks. Rate limits: Let’s Encrypt enforces rate limits. Use --dry-run for testing or wait before re-issuing. Webroot path mismatch: If using --webroot, ensure the -w path matches your server root and that Nginx serves /.well-known/acme-challenge/. Apt update/upgrade errors when installing snap/certbot Lihat: How to fix broken update error in Linux (Terminal) -\u0026gt; /2023/11/how-to-fix-broken-update-error-in-linux.html Multiple sites tip\nUntuk beberapa domain, buat satu file di sites-available/ per domain. Hindari overlap server_name agar Certbot dan Nginx bisa memilih blok yang tepat. Renewal and maintenance tips\nCertificates renew automatically; review logs in /var/log/letsencrypt/. After major Nginx changes, run sudo certbot renew --dry-run to confirm hooks still work. Consider enabling OCSP stapling and caching for further optimization if you terminate high traffic. Revoke or uninstall (if needed) Revoke a cert (compromised key or domain transfer):\nsudo certbot revoke --cert-path /etc/letsencrypt/live/example.com/fullchain.pem Remove cert files:\nsudo certbot delete --cert-name example.com Remove Certbot (snap) and Nginx:\nsudo snap remove certbot sudo apt purge -y nginx* \u0026amp;\u0026amp; sudo apt autoremove -y That’s it\u0026ndash;your site now serves a trusted HTTPS certificate with automatic renewal on Ubuntu 24.04. Enjoy the speed and security of Nginx + Let’s Encrypt!\n","href":"/2025/08/nginx-certbot-ubuntu-24-04-free-https.html","title":"Nginx + Certbot on Ubuntu 24.04 Free HTTPS with Let’s Encrypt"},{"content":"This guide shows how to install Docker Engine on Ubuntu 24.04 LTS (Noble Numbat), configure it for non-root use, enable optional rootless mode, and use Docker Compose v2. It also includes test commands, common troubleshooting tips, and how to uninstall cleanly. For securing your site with HTTPS, see: Nginx + Certbot on Ubuntu 24.04 What you’ll do\nAdd the official Docker repository for Ubuntu 24.04 (Noble) Install Docker Engine, Buildx, and Compose v2 plugins Run Docker as your regular user (without sudo) Optionally enable rootless Docker Verify with test containers and fix common errors Prerequisites\nFresh or updated Ubuntu 24.04 LTS (Noble) A user with sudo privileges Remove old Docker packages (if any) sudo apt remove -y docker docker-engine docker.io containerd runc || true Set up the Docker repository sudo apt update sudo apt install -y ca-certificates curl gnupg sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\ https://download.docker.com/linux/ubuntu $(. /etc/os-release; echo $VERSION_CODENAME) stable\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update Install Docker Engine, Buildx, and Compose v2 sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin sudo systemctl enable --now docker Test Docker (root) sudo docker run --rm hello-world You should see a confirmation message.\nPost-install: run Docker without sudo sudo usermod -aG docker $USER newgrp docker # reload group membership for current shell docker run --rm hello-world If the second command works without sudo, your user is set up correctly.\nDocker Compose v2 docker compose is included as a plugin. Check the version: docker compose version Example usage:\ncat \u0026gt; compose.yaml \u0026lt;\u0026lt;\u0026#39;YAML\u0026#39; services: web: image: nginx:alpine ports: - \u0026#34;8080:80\u0026#34; YAML docker compose up -d docker compose ps docker compose down 6a) Verify Buildx docker buildx is the modern builder with multi-platform support and advanced caching.\ndocker buildx version You should see a version string. Optionally, try a quick build to confirm the builder is healthy:\ndocker buildx bake --print 2\u0026gt;/dev/null || echo \u0026#34;Buildx is installed and ready.\u0026#34; Optional: Rootless Docker Rootless mode runs the Docker daemon and containers without root privileges. Good for tighter isolation (with some feature limitations). Install requirements and set up:\nsudo apt install -y uidmap dbus-user-session dockerd-rootless-setuptool.sh install Start and enable the user service:\nsystemctl --user start docker systemctl --user enable docker # Keep user services running after logout sudo loginctl enable-linger $USER Use the rootless daemon by pointing the client to your user socket (usually done automatically by the setup tool):\nexport DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock docker info | grep -i rootless Notes on rootless mode\nSome features (e.g., privileged containers, low ports \u0026lt;1024) are restricted. For Kubernetes-in-Docker or system-wide networking, classic (rootful) Docker is recommended. Troubleshooting Permission denied on /var/run/docker.sock Run: groups and ensure docker is listed. If not, run sudo usermod -aG docker $USER then re-login or newgrp docker. Network issues pulling images Check DNS and proxy settings. Try docker pull alpine and ping registry-1.docker.io (may be blocked by firewall). Cannot connect to the Docker daemon Check service: systemctl status docker (rootful) or systemctl --user status docker (rootless). Compose command not found Ensure you installed docker-compose-plugin and run docker compose (space), not docker-compose. Apt update/upgrade errors during install Lihat: How to fix broken update error in Linux (Terminal) -\u0026gt; /2023/11/how-to-fix-broken-update-error-in-linux.html 8a) Maintenance \u0026amp; Cleanup (disk usage) Over time, images/layers can consume disk space. Inspect usage and prune carefully:\ndocker system df docker image prune -f # remove unused images (dangling) docker container prune -f # remove stopped containers docker volume prune -f # remove unused volumes docker builder prune -f # remove unused build cache Tip: omit -f to get a prompt before deleting. Review before pruning on production hosts.\nUninstall Docker completely sudo apt purge -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras sudo rm -rf /var/lib/docker /var/lib/containerd sudo rm -f /etc/apt/sources.list.d/docker.list /etc/apt/keyrings/docker.gpg sudo apt autoremove -y Security note\nMembers of the docker group can effectively escalate privileges on the host (they can start containers with access to the filesystem). Only add trusted users to the docker group. That’s it! You now have Docker Engine, Compose v2, and (optionally) rootless mode on Ubuntu 24.04.\n","href":"/2025/08/install-docker-on-ubuntu-24-04-with-compose-v2-and-rootless.html","title":"Install Docker on Ubuntu 24.04 Post-Install, Rootless, and Compose v2"},{"content":"In modern web applications, storing and retrieving data from a database is a fundamental requirement. Go provides a low-level database/sql package, but using it directly can be verbose and repetitive. Thankfully, sqlx extends database/sql by adding useful features like struct scanning and named queries, making database operations in Go much easier.\nIn this article, we’ll walk through how to connect a Go application to a PostgreSQL database using sqlx, and how to perform basic CRUD operations.\nWhat is sqlx? sqlx is a Go library that enhances the standard database/sql by making it easier to work with structs and common query patterns. It\u0026rsquo;s widely used for developers who want more control and performance without jumping into full ORMs.\nInstall sqlx with:\ngo get github.com/jmoiron/sqlx You also need the PostgreSQL driver:\ngo get github.com/lib/pq Connect to PostgreSQL To connect to a PostgreSQL database, you need to provide a connection string that includes the database name, user, password, host, and port. Here’s how to set up a basic connection using sqlx:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) var db *sqlx.DB func main() { dsn := \u0026#34;user=postgres password=yourpassword dbname=mydb sslmode=disable\u0026#34; var err error db, err = sqlx.Connect(\u0026#34;postgres\u0026#34;, dsn) if err != nil { log.Fatalln(err) } fmt.Println(\u0026#34;Connected to PostgreSQL!\u0026#34;) } Make sure to replace yourpassword and mydb with your actual PostgreSQL credentials and database name.\nCreate a Struct and Table Create a table in PostgreSQL:\nCREATE TABLE users ( id SERIAL PRIMARY KEY, name TEXT NOT NULL, age INT NOT NULL ); Next, define a Go struct that matches the table schema:\ntype User struct { ID int `db:\u0026#34;id\u0026#34;` Name string `db:\u0026#34;name\u0026#34;` Age int `db:\u0026#34;age\u0026#34;` } Insert Data To insert data into the users table, you can use the NamedExec method provided by sqlx, which allows you to use named parameters in your SQL queries:\nfunc createUser(name string, age int) error { user := User{Name: name, Age: age} query := `INSERT INTO users (name, age) VALUES (:name, :age)` _, err := db.NamedExec(query, user) return err } Query Data To retrieve data from the users table, you can use the Select method, which scans the results into a slice of structs:\nfunc getUsers() ([]User, error) { var users []User query := `SELECT * FROM users` err := db.Select(\u0026amp;users, query) return users, err } Update Data To update a user\u0026rsquo;s information, you can use the NamedExec method again:\nfunc updateUser(id int, name string, age int) error { user := User{ID: id, Name: name, Age: age} query := `UPDATE users SET name = :name, age = :age WHERE id = :id` _, err := db.NamedExec(query, user) return err } Delete Data To delete a user from the users table, you can use the Exec method:\nfunc deleteUser(id int) error { query := `DELETE FROM users WHERE id = $1` _, err := db.Exec(query, id) return err } Putting It All Together Here’s a complete example that includes connecting to the database, creating a user, retrieving users, updating a user, and deleting a user:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/jmoiron/sqlx\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) type User struct { ID int `db:\u0026#34;id\u0026#34;` Name string `db:\u0026#34;name\u0026#34;` Age int `db:\u0026#34;age\u0026#34;` } var db *sqlx.DB func main() { dsn := \u0026#34;user=postgres password=yourpassword dbname=mydb sslmode=disable\u0026#34; var err error db, err = sqlx.Connect(\u0026#34;postgres\u0026#34;, dsn) if err != nil { log.Fatalln(err) } fmt.Println(\u0026#34;Connected to PostgreSQL!\u0026#34;) // Create a user if err := createUser(\u0026#34;Alice\u0026#34;, 30); err != nil { log.Println(\u0026#34;Error creating user:\u0026#34;, err) } // Get users users, err := getUsers() if err != nil { log.Println(\u0026#34;Error getting users:\u0026#34;, err) } else { fmt.Println(\u0026#34;Users:\u0026#34;, users) } // Update a user if err := updateUser(1, \u0026#34;Alice Smith\u0026#34;, 31); err != nil { log.Println(\u0026#34;Error updating user:\u0026#34;, err) } // Delete a user if err := deleteUser(1); err != nil { log.Println(\u0026#34;Error deleting user:\u0026#34;, err) } } Best Practices Use Named Parameters: Named parameters make your queries more readable and maintainable. Error Handling: Always check for errors after executing queries to handle any issues gracefully. Connection Pooling: sqlx uses the database/sql package under the hood, which supports connection pooling. Make sure to configure the pool size according to your application\u0026rsquo;s needs. Migrations: Use a migration tool like golang-migrate to manage your database schema changes. Environment Variables: Store sensitive information like database credentials in environment variables or a configuration file, not hard-coded in your source code. Close the Database Connection Gracefully: Ensure you close the database connection when your application exits to avoid resource leaks. Conclusion sqlx is a powerful tool for interacting with PostgreSQL in Go. It keeps your code clean while avoiding the overhead of a full ORM. You’ve now seen how to connect to PostgreSQL, run basic CRUD operations, and structure your DB code using sqlx.\nIn the next article, we’ll go further by integrating this into a REST API and later explore GORM for higher-level abstraction.\nHappy coding!\n","href":"/2025/05/connecting-postgresql-in-go-using-sqlx.html","title":"Connecting to PostgreSQL in Go using sqlx"},{"content":"When you start building larger applications in Go, having a clean and maintainable project structure is essential. Unlike some other languages or frameworks that enforce certain patterns, Go gives you a lot of freedom in how you organize your code. While this is powerful, it can also lead to messy projects if not handled carefully.\nIn this guide, we\u0026rsquo;ll explore how to structure Go projects following clean architecture principles and best practices that many professional Go developers use.\nWhy Project Structure Matters in Go A good project structure will help you:\nMake your code easier to read and navigate. Make testing and maintenance easier. Separate concerns cleanly (API, service, data access, domain logic). Prepare your code for scaling and collaboration. Go doesn\u0026rsquo;t have a strict convention, but the community has adopted patterns that work well, especially for building web APIs, microservices, or CLI tools.\nBasic Go Project Structure Let\u0026rsquo;s start with a simple example of a Go project structure:\nmy-go-project/ ├── cmd/ │ └── myapp/ │ └── main.go ├── internal/ │ ├── ... ├── pkg/ │ ├── ... ├── go.mod ├── go.sum └── README.md Directory Breakdown cmd/\nThis directory contains the entry points for your application. Each subdirectory under cmd/ represents a different executable. For example, myapp/ could be the main application, while myapp-cli/ could be a command-line interface for the same application.\ninternal/\nThis directory contains application code that is not meant to be used by external applications. It can include business logic, data access, and other components that are specific to your application.\npkg/\nThis directory contains code that can be used by other applications. It can include libraries, utilities, and shared components that are reusable across different projects.\ngo.mod\nThis file defines the module and its dependencies. It is created when you run go mod init.\ngo.sum\nThis file contains the checksums of the dependencies listed in go.mod. It ensures that the same versions of dependencies are used across different environments.\nREADME.md\nThis file provides documentation for your project, including how to install, run, and use it.\nClean Architecture Approach (Recommended for Medium/Large Apps) For larger applications, it\u0026rsquo;s beneficial to adopt a clean architecture approach. This means organizing your code into layers that separate concerns and make it easier to test and maintain.\nSuggested structure:\nmy-go-project/ ├── cmd/ │ └── myapp/ │ └── main.go ├── internal/ │ ├── app/ │ │ ├── service/ │ │ ├── handler/ │ │ └── repository/ │ ├── domain/ │ │ ├── model/ │ │ └── service/ │ └── infrastructure/ │ ├── db/ │ ├── api/ │ └── config/ ├── pkg/ │ ├── utils/ │ └── middleware/ ├── go.mod ├── go.sum └── README.md Directory Breakdown app/\nContains the application logic, including services, handlers, and repositories. This is where the core of your application lives.\nservice/ Contains business logic and service implementations.\nhandler/ Contains HTTP handlers or gRPC handlers that interact with the outside world.\nrepository/ Contains data access code, such as database queries or API calls.\ndomain/ Contains domain models and services. This is where you define your core business entities and their behaviors.\nmodel/ Contains the domain models, which represent the core entities of your application.\nservice/ Contains domain services that encapsulate business logic related to the domain models.\ninfrastructure/\nContains code related to external systems, such as databases, APIs, and configuration.\ndb/ Contains database-related code, such as migrations and connection management.\napi/ Contains code related to external APIs, such as clients or adapters.\nconfig/ Contains configuration files and code for loading configurations.\npkg/\nContains reusable code that can be shared across different projects. This can include utility functions, middleware, and other shared components.\nutils/\nContains utility functions and helpers that can be used throughout the project.\nmiddleware/\nContains middleware functions for HTTP servers, such as logging, authentication, and error handling.\ngo.mod\nDefines the module and its dependencies.\ngo.sum\nContains the checksums of the dependencies listed in go.mod.\nREADME.md\nProvides documentation for your project.\nThis approach makes it easier to swap your database, refactor your API layer, or even reuse your business logic in different contexts.\nConclusion Structuring your Go projects effectively is crucial for maintainability and scalability. By following clean architecture principles and best practices, you can create a project structure that is easy to navigate, test, and extend.\nThis guide provides a solid foundation for structuring your Go projects, whether you\u0026rsquo;re building a simple CLI tool or a complex web application. Remember that the best structure is one that fits your specific needs and team preferences, so feel free to adapt these suggestions as necessary.\nBy following these guidelines, you\u0026rsquo;ll be well on your way to creating clean, maintainable, and scalable Go projects that are easy to work with and understand.\nHappy coding!\n","href":"/2025/05/structuring-go-projects-clean-project-structure-and-best-practices.html","title":"Structuring Go Projects Clean Project Structure and Best Practices"},{"content":"Building a REST API in Go is one of the most practical ways to learn how Go handles HTTP servers, JSON , and struct-based logic. In this tutorial, you’ll learn how to create a simple RESTful API using the standard net/http package\u0026ndash;without using any third-party frameworks. This is a great starting point before moving to more complex architectures.\nIn this guide, we’ll create a simple API for managing books. Each book will have an ID, title, and author.\nWhat You’ll Learn How to create HTTP server routes in Go How to handle GET, POST, PUT, and DELETE requests How to encode and decode JSON data How to organize handlers and write clean code Step 1: Define a Book Struct package main type Book struct { ID string `json:\u0026#34;id\u0026#34;` Title string `json:\u0026#34;title\u0026#34;` Author string `json:\u0026#34;author\u0026#34;` } We’ll use this struct to store data in memory.\nStep 2: Step 2: Create a Global Book Slice var books = []Book{ {ID: \u0026#34;1\u0026#34;, Title: \u0026#34;Go Basics\u0026#34;, Author: \u0026#34;John Doe\u0026#34;}, {ID: \u0026#34;2\u0026#34;, Title: \u0026#34;Mastering Go\u0026#34;, Author: \u0026#34;Jane Smith\u0026#34;}, } Step 3: Create Handlers Get All Books func getBooks(w http.ResponseWriter, r *http.Request) { w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) json.NewEncoder(w).Encode(books) } Get a Single Book func getBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for _, book := range books { if book.ID == id { json.NewEncoder(w).Encode(book) return } } http.NotFound(w, r) } Create a New Book func createBook(w http.ResponseWriter, r *http.Request) { var book Book json.NewDecoder(r.Body).Decode(\u0026amp;book) books = append(books, book) w.WriteHeader(http.StatusCreated) json.NewEncoder(w).Encode(book) } Update a Book func updateBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for i, book := range books { if book.ID == id { json.NewDecoder(r.Body).Decode(\u0026amp;books[i]) w.WriteHeader(http.StatusOK) json.NewEncoder(w).Encode(books[i]) return } } http.NotFound(w, r) } Delete a Book func deleteBook(w http.ResponseWriter, r *http.Request) { id := strings.TrimPrefix(r.URL.Path, \u0026#34;/books/\u0026#34;) for i, book := range books { if book.ID == id { books = append(books[:i], books[i+1:]...) w.WriteHeader(http.StatusNoContent) return } } http.NotFound(w, r) } Step 4: Set Up Routes func main() { http.HandleFunc(\u0026#34;/books\u0026#34;, getBooks) http.HandleFunc(\u0026#34;/books/\u0026#34;, func(w http.ResponseWriter, r *http.Request) { switch r.Method { case http.MethodGet: getBook(w, r) case http.MethodPost: createBook(w, r) case http.MethodPut: updateBook(w, r) case http.MethodDelete: deleteBook(w, r) default: http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) } }) fmt.Println(\u0026#34;Server running on http://localhost:8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil)) } Step 5: Run the Server To run the server, save your code in a file named main.go and execute the following command in your terminal:\ngo run main.go You should see the message Server running on http://localhost:8080. You can now test your API using tools like Postman or curl.\nStep 6: Test the API Get All Books curl -X GET http://localhost:8080/books Get a Single Book curl -X GET http://localhost:8080/books/1 Create a New Book curl -X POST http://localhost:8080/books \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;3\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Learning Go\u0026#34;, \u0026#34;author\u0026#34;:\u0026#34;Alice Johnson\u0026#34;}\u0026#39; Update a Book curl -X PUT http://localhost:8080/books/1 \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;title\u0026#34;:\u0026#34;Go Basics Updated\u0026#34;, \u0026#34;author\u0026#34;:\u0026#34;John Doe\u0026#34;}\u0026#39; Delete a Book curl -X DELETE http://localhost:8080/books/1 Conclusion Congratulations! You’ve built a simple REST API in Go using the net/http package. This is just the beginning; you can extend this API by adding features like authentication, database integration, and more. Feel free to explore the Go documentation and other resources to deepen your understanding of Go and RESTful APIs.\nIf you have any questions or need further assistance, don’t hesitate to ask. Happy coding!\nAdditional Resources Go Documentation Go by Example Building Web Applications in Go Repository ","href":"/2025/05/how-to-build-a-rest-api-in-go-using-net-http.html","title":"How to Build a REST API in Go using net/http"},{"content":"JSON (JavaScript Object Notation) is a widely used data format in APIs and web applications. Go provides strong support for JSON through the standard encoding/json package. In this article, you’ll learn how to parse JSON into structs, generate JSON from Go data, use struct tags, and work with nested or dynamic structures.\nIn this article, you’ll learn:\nHow to encode Go structs to JSON How to decode JSON into Go structs Using JSON tags to customize field names Working with maps and dynamic JSON Handling nested JSON structures Best practices and error handling Encoding Structs to JSON Use json.Marshal to convert Go structs into JSON strings:\ntype User struct { Name string `json:\u0026#34;name\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Age int `json:\u0026#34;age\u0026#34;` } func main() { user := User{\u0026#34;Alice\u0026#34;, \u0026#34;alice@example.com\u0026#34;, 30} jsonData, err := json.Marshal(user) if err != nil { log.Fatal(err) } fmt.Println(string(jsonData)) } Decoding JSON into Structs Use json.Unmarshal to parse JSON into a struct:\nvar jsonInput = []byte(`{\u0026#34;name\u0026#34;:\u0026#34;Bob\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;bob@example.com\u0026#34;,\u0026#34;age\u0026#34;:25}`) var user User err := json.Unmarshal(jsonInput, \u0026amp;user) if err != nil { log.Fatal(err) } fmt.Println(user.Name, user.Email, user.Age) Using Struct Tags By default, Go uses struct field names as JSON keys. Use tags to customize:\ntype Product struct { ID int `json:\u0026#34;id\u0026#34;` Name string `json:\u0026#34;name\u0026#34;` Price float64 `json:\u0026#34;price\u0026#34;` } Working with Maps and Dynamic JSON Use map[string]interface{} when the structure is not fixed:\nvar data = []byte(`{\u0026#34;status\u0026#34;:\u0026#34;ok\u0026#34;,\u0026#34;code\u0026#34;:200}`) var result map[string]interface{} err := json.Unmarshal(data, \u0026amp;result) if err != nil { log.Fatal(err) } fmt.Println(result[\u0026#34;status\u0026#34;], result[\u0026#34;code\u0026#34;]) Nested JSON Example type Address struct { City string `json:\u0026#34;city\u0026#34;` Country string `json:\u0026#34;country\u0026#34;` } type Employee struct { Name string `json:\u0026#34;name\u0026#34;` Address Address `json:\u0026#34;address\u0026#34;` } JSON:\n{ \u0026#34;name\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;address\u0026#34;: { \u0026#34;city\u0026#34;: \u0026#34;Jakarta\u0026#34;, \u0026#34;country\u0026#34;: \u0026#34;Indonesia\u0026#34; } } Encode JSON to File f, err := os.Create(\u0026#34;data.json\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() json.NewEncoder(f).Encode(user) Decode JSON from File f, err := os.Open(\u0026#34;data.json\u0026#34;) if err != nil { log.Fatal(err) } defer f.Close() json.NewDecoder(f).Decode(\u0026amp;user) Best Practices Always handle encoding/decoding errors Use struct tags for clean JSON output Validate incoming JSON before using Use omitempty tag to skip empty fields Conclusion Working with JSON in Go is simple, powerful, and type-safe. Whether you\u0026rsquo;re building APIs, reading config files, or exchanging data between systems, the encoding/json package gives you everything you need.\nNext, we’ll dive into building a REST API in Go using net/http.\nHappy coding!\n","href":"/2025/04/working-with-json-in-go-encode-decode.html","title":"Working with JSON in Go - Encode and Decode"},{"content":"In Go, file handling is straightforward and powerful. You can create, read, write, and manage files using standard packages like os, io, and ioutil (deprecated but still common). Understanding how to work with files is essential when building CLI tools, web servers, or any application that deals with local data.\nIn this article, you’ll learn:\nHow to create and write to a file How to read a file Appending data to files Working with directories Checking if a file exists Best practices and error handling Creating and Writing to a File To create and write content to a file:\nfunc main() { content := []byte(\u0026#34;Hello, file!\u0026#34;) err := os.WriteFile(\u0026#34;example.txt\u0026#34;, content, 0644) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;File written successfully\u0026#34;) } os.WriteFile creates the file if it doesn\u0026rsquo;t exist and replaces it if it does.\nReading a File To read the entire content of a file:\nfunc main() { data, err := os.ReadFile(\u0026#34;example.txt\u0026#34;) if err != nil { log.Fatal(err) } fmt.Println(\u0026#34;File content:\u0026#34;, string(data)) } Appending to a File If you want to add content to an existing file without overwriting it:\nfunc main() { f, err := os.OpenFile(\u0026#34;example.txt\u0026#34;, os.O_APPEND|os.O_WRONLY, 0644) if err != nil { log.Fatal(err) } defer f.Close() if _, err := f.WriteString(\u0026#34;\\nThis is appended.\u0026#34;); err != nil { log.Fatal(err) } fmt.Println(\u0026#34;Appended successfully\u0026#34;) } Working with Directories Create a new folder: err := os.Mkdir(\u0026#34;myfolder\u0026#34;, 0755) Create nested folders: err := os.MkdirAll(\u0026#34;path/to/folder\u0026#34;, 0755) List files in a folder: files, err := os.ReadDir(\u0026#34;.\u0026#34;) for _, file := range files { fmt.Println(file.Name()) } Check if a File Exists func fileExists(filename string) bool { _, err := os.Stat(filename) return !os.IsNotExist(err) } Deleting a File or Folder err := os.Remove(\u0026#34;example.txt\u0026#34;) // delete file err := os.RemoveAll(\u0026#34;path/to/folder\u0026#34;) // delete folder and contents Best Practices Always handle file errors (file not found, permissions) Use defer f.Close() after opening files Use os.ReadFile and os.WriteFile for simple tasks Use buffered I/O (like bufio) for large files Conclusion File handling in Go is clean and efficient. Whether you\u0026rsquo;re reading logs, saving data, or managing folders, the standard library provides everything you need. Understanding how to work with files opens the door to building robust and real-world applications in Go.\nNext, we’ll look into working with JSON in Go \u0026ndash; another essential skill for building APIs and storing structured data.\nHappy coding!\n","href":"/2025/04/file-handling-in-go-read-write-and.html","title":"Read, Write, and Manage Files"},{"content":"When you write concurrent programs in Go, multiple goroutines may try to access and modify the same data at the same time. Without proper synchronization, this leads to race conditions, bugs, or crashes. Go provides tools like sync.Mutex, sync.RWMutex, and sync.Once to safely share data across goroutines.\nIn this article, you’ll learn:\nWhat race conditions are and how to avoid them How to use sync.Mutex to protect data Using sync.RWMutex for read-write access How sync.Once ensures code runs only once Real-world examples and best practices What Is a Race Condition? A race condition happens when two or more goroutines access the same variable at the same time, and at least one of them is modifying it. This can cause unexpected behavior or corrupted data.\nYou can detect race conditions using:\ngo run -race main.go Using sync.Mutex sync.Mutex is a mutual exclusion lock. Only one goroutine can hold the lock at a time. Use Lock() before accessing shared data, and Unlock() after.\ntype Counter struct { mu sync.Mutex value int } func (c *Counter) Increment() { c.mu.Lock() defer c.mu.Unlock() c.value++ } func (c *Counter) Value() int { c.mu.Lock() defer c.mu.Unlock() return c.value } Using sync.RWMutex sync.RWMutex allows multiple readers or one writer. It\u0026rsquo;s useful when reads are frequent but writes are rare.\ntype SafeMap struct { mu sync.RWMutex m map[string]string } func (s *SafeMap) Get(key string) string { s.mu.RLock() defer s.mu.RUnlock() return s.m[key] } func (s *SafeMap) Set(key, value string) { s.mu.Lock() defer s.mu.Unlock() s.m[key] = value } Using sync.Once sync.Once guarantees that a piece of code is only executed once, even if called from multiple goroutines. This is commonly used to initialize shared resources.\nvar once sync.Once func initialize() { fmt.Println(\u0026#34;Initialization done\u0026#34;) } func main() { for i := 0; i \u0026lt; 5; i++ { go func() { once.Do(initialize) }() } time.Sleep(time.Second) } Real-World Example: Safe Counter type SafeCounter struct { mu sync.Mutex val int } func (sc *SafeCounter) Add() { sc.mu.Lock() sc.val++ sc.mu.Unlock() } func main() { var sc SafeCounter var wg sync.WaitGroup for i := 0; i \u0026lt; 1000; i++ { wg.Add(1) go func() { sc.Add() wg.Done() }() } wg.Wait() fmt.Println(\u0026#34;Final count:\u0026#34;, sc.val) } Best Practices Always use defer Unlock() right after Lock() Keep the locked section as short as possible Use RWMutex when many goroutines only need to read Use sync.Once to initialize global/shared data Test with go run -race to catch race conditions Conclusion Synchronization is key to building correct concurrent programs. By using sync.Mutex, sync.RWMutex, and sync.Once, you can ensure that your goroutines work together safely without corrupting shared data.\nHappy coding!\n","href":"/2025/04/synchronizing-goroutines-in-go-using.html","title":"Using sync.Mutex and sync.Once"},{"content":"As your Go applications become more concurrent and complex, you\u0026rsquo;ll need a way to manage the lifecycle of your goroutines\u0026ndash;especially when you want to cancel them, set timeouts, or propagate deadlines. This is where the context package comes in. It\u0026rsquo;s the idiomatic way in Go to control concurrent processes gracefully and reliably.\nIn this article, you’ll learn:\nWhat context is and why it’s important Using context.Background() and context.TODO() How to cancel a goroutine with context.WithCancel() How to set a timeout or deadline How to check if a context is done Real-world examples and best practices What Is Context? The context package provides a way to carry deadlines, cancellation signals, and other request-scoped values across function boundaries and between goroutines.\nIt helps you:\nCancel long-running tasks Set deadlines or timeouts Propagate cancellation across multiple goroutines Starting Point: Background and TODO ctx := context.Background() // root context, no cancel/timeout ctx := context.TODO() // use when unsure (placeholder) Cancelling a Goroutine: WithCancel You can use context.WithCancel to manually stop a goroutine:\nfunc doWork(ctx context.Context) { for { select { case \u0026lt;-ctx .done=\u0026#34;\u0026#34; :=\u0026#34;context.WithCancel(context.Background())\u0026#34; cancel=\u0026#34;\u0026#34; canceled=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; context=\u0026#34;\u0026#34; ctx=\u0026#34;\u0026#34; default:=\u0026#34;\u0026#34; dowork=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; main=\u0026#34;\u0026#34; orking...=\u0026#34;\u0026#34; oroutine=\u0026#34;\u0026#34; return=\u0026#34;\u0026#34; the=\u0026#34;\u0026#34; time.millisecond=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34;\u0026gt; When cancel() is called, the goroutine receives a signal via ctx.Done().\nSetting a Timeout: WithTimeout ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second) defer cancel() select { case \u0026lt;-time .after=\u0026#34;\u0026#34; case=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; completed=\u0026#34;\u0026#34; ctx.done=\u0026#34;\u0026#34; ctx.err=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; ontext=\u0026#34;\u0026#34; peration=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; timeout:=\u0026#34;\u0026#34;\u0026gt; WithDeadline works the same way, but with a fixed time:\ndeadline := time.Now().Add(2 * time.Second) ctx, cancel := context.WithDeadline(context.Background(), deadline) How to Use ctx.Done() The ctx.Done() channel is closed when the context is canceled or times out. Use it in select blocks to exit early.\nReal-World Example: HTTP Request Timeout func fetch(ctx context.Context, url string) error { req, err := http.NewRequestWithContext(ctx, \u0026#34;GET\u0026#34;, url, nil) if err != nil { return err } client := http.Client{} resp, err := client.Do(req) if err != nil { return err } defer resp.Body.Close() fmt.Println(\u0026#34;Status:\u0026#34;, resp.Status) return nil } func main() { ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second) defer cancel() err := fetch(ctx, \u0026#34;https://httpbin.org/delay/2\u0026#34;) if err != nil { fmt.Println(\u0026#34;Request failed:\u0026#34;, err) } } Best Practices Always call cancel() to release resources Pass context.Context as the first argument in your functions Use context.WithTimeout for operations with time limits Use context.WithCancel for manual control Common Mistakes Not deferring cancel() -\u0026gt; memory leak Ignoring ctx.Err() -\u0026gt; silent failure Passing nil context or using context.TODO() in production Conclusion Understanding context is essential for writing responsive, well-behaved concurrent programs in Go. Whether you\u0026rsquo;re managing goroutines, dealing with timeouts, or handling request chains in a web server, context gives you the tools to do it cleanly and safely.\nNext, we\u0026rsquo;ll cover sync.Mutex and other tools for synchronizing data between goroutines.\nHappy coding!\n","href":"/2025/04/using-context-in-go-cancellation.html","title":"Using Context in Go - Cancellation"},{"content":"One of the most powerful features of Go is its built-in support for concurrency. Go makes it easy to write programs that perform multiple tasks at the same time, thanks to goroutines and channels. Unlike traditional multithreading, Go provides a lightweight and clean way to build concurrent systems with minimal overhead and boilerplate.\nIn this article, you’ll learn:\nThe difference between concurrency and parallelism What goroutines are and how to use them How channels allow communication between goroutines Buffered vs unbuffered channels The select statement Common concurrency problems and how to avoid them Real-world examples and best practices Concurrency vs Parallelism Concurrency means doing multiple things at once (interleaved), while parallelism means running them simultaneously on different processors. Go’s concurrency model allows you to write code that is concurrent, and Go’s runtime handles whether it is executed in parallel depending on available CPU cores.\nIntroducing Goroutines A goroutine is a function that runs concurrently with other functions. You start one by using the go keyword:\nfunc sayHello() { fmt.Println(\u0026#34;Hello from goroutine!\u0026#34;) } func main() { go sayHello() fmt.Println(\u0026#34;Main function\u0026#34;) } Goroutines are lightweight and managed by the Go runtime, not the OS. You can spawn thousands of them without major performance issues.\nWhy You Need to Wait The above example might not print the goroutine output if main() exits first. You can fix this using time.Sleep or better, sync.WaitGroup:\nvar wg sync.WaitGroup func sayHi() { defer wg.Done() fmt.Println(\u0026#34;Hi!\u0026#34;) } func main() { wg.Add(1) go sayHi() wg.Wait() } Using Channels Channels are used to send and receive values between goroutines. They are typed and provide safe communication.\nfunc main() { ch := make(chan string) go func() { ch \u0026lt;- :=\u0026#34;\u0026lt;-ch\u0026#34; code=\u0026#34;\u0026#34; essage=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; from=\u0026#34;\u0026#34; goroutine=\u0026#34;\u0026#34; msg=\u0026#34;\u0026#34;\u0026gt; Buffered Channels A buffered channel allows sending without blocking, up to its capacity:\nch := make(chan int, 2) ch \u0026lt;- 1=\u0026#34;\u0026#34; 2=\u0026#34;\u0026#34; 3=\u0026#34;\u0026#34; block=\u0026#34;\u0026#34; buffer=\u0026#34;\u0026#34; ch=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; full=\u0026#34;\u0026#34; if=\u0026#34;\u0026#34; is=\u0026#34;\u0026#34; this=\u0026#34;\u0026#34; will=\u0026#34;\u0026#34;\u0026gt; Select Statement select lets you wait on multiple channel operations:\nfunc main() { ch1 := make(chan string) ch2 := make(chan string) go func() { time.Sleep(1 * time.Second) ch1 \u0026lt;- :=\u0026#34;\u0026lt;-ch2:\u0026#34; case=\u0026#34;\u0026#34; ch1=\u0026#34;\u0026#34; ch2=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; from=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; msg1=\u0026#34;\u0026#34; msg2=\u0026#34;\u0026#34; select=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34;\u0026gt; Common Problems Deadlocks: when goroutines wait forever Race conditions: two goroutines access the same variable concurrently Use go run -race to detect race conditions.\nReal-World Example: Worker Pool func worker(id int, jobs \u0026lt;-chan 2=\u0026#34;\u0026#34; 3=\u0026#34;\u0026#34; 5=\u0026#34;\u0026#34; :=\u0026#34;1;\u0026#34; chan=\u0026#34;\u0026#34; close=\u0026#34;\u0026#34; code=\u0026#34;\u0026#34; d=\u0026#34;\u0026#34; finished=\u0026#34;\u0026#34; fmt.printf=\u0026#34;\u0026#34; fmt.println=\u0026#34;\u0026#34; for=\u0026#34;\u0026#34; func=\u0026#34;\u0026#34; go=\u0026#34;\u0026#34; id=\u0026#34;\u0026#34; int=\u0026#34;\u0026#34; j=\u0026#34;\u0026#34; job=\u0026#34;\u0026#34; jobs=\u0026#34;\u0026#34; main=\u0026#34;\u0026#34; n=\u0026#34;\u0026#34; orker=\u0026#34;\u0026#34; r=\u0026#34;\u0026#34; results=\u0026#34;\u0026#34; started=\u0026#34;\u0026#34; time.second=\u0026#34;\u0026#34; time.sleep=\u0026#34;\u0026#34; w=\u0026#34;\u0026#34; worker=\u0026#34;\u0026#34;\u0026gt; Best Practices Close channels only when you’re done sending Use sync.WaitGroup to wait for goroutines Don’t create unbounded goroutines \u0026ndash; may cause memory leaks Use buffered channels to avoid blocking when needed Conclusion Goroutines and channels are the foundation of concurrency in Go. With them, you can build scalable and efficient programs without the complexity of traditional multithreading. Start small, experiment with simple patterns, and scale your knowledge step by step.\nNext, we\u0026rsquo;ll explore advanced concurrency control using sync.Mutex, sync.Once, and context for cancellation and timeouts.\nHappy coding!\n","href":"/2025/04/concurrency-in-go-goroutines-and.html","title":"Goroutines and Channels Explained"},{"content":"Generics were introduced in Go 1.18, marking a significant evolution of the language. They allow you to write flexible, reusable code without sacrificing type safety. With generics, you can define functions, types, and data structures that work with different types, all while maintaining strong compile-time checks.\nIn this article, you’ll learn:\nWhat generics are and why they matter How to define generic functions and types Type parameters and constraints Real-world examples of generics Best practices when using generics in Go What Are Generics? Generics let you write code that works with different data types while keeping the benefits of static typing. Before generics, developers often used interface{} and type assertions to achieve flexibility, but that meant losing compile-time type safety.\nDefining a Generic Function A generic function introduces a type parameter list using square brackets [] before the function parameters.\nfunc Print[T any](value T) { fmt.Println(value) } Here, T is a type parameter, and any is a constraint (alias for interface{}). This function works with any type, like:\nPrint(10) Print(\u0026#34;Hello\u0026#34;) Print(true) Using Type Constraints You can limit what types can be passed by using constraints:\ntype Number interface { ~int | ~float64 } func Sum[T Number](a, b T) T { return a + b } Now Sum can only be called with numeric types.\nGeneric Types You can also define structs or custom types with generics:\ntype Pair[T any] struct { First T Second T } func main() { p := Pair[string]{\u0026#34;Go\u0026#34;, \u0026#34;Lang\u0026#34;} fmt.Println(p.First, p.Second) } Multiple Type Parameters You can define more than one type parameter:\ntype Map[K comparable, V any] struct { data map[K]V } The comparable constraint is required for keys in a map (they must support ==).\nReal-World Example: Generic Filter Function func Filter[T any](items []T, predicate func(T) bool) []T { var result []T for _, item := range items { if predicate(item) { result = append(result, item) } } return result } Usage:\nevens := Filter([]int{1, 2, 3, 4}, func(n int) bool { return n%2 == 0 }) Generics vs Interface Before generics, we often used interface{} and did type assertion:\nfunc PrintAny(val interface{}) { fmt.Println(val) } This works, but doesn’t give compile-time safety or clarity. With generics, you avoid runtime type errors.\nBest Practices Use generics when you write reusable logic (e.g. map, reduce, filter) Don’t overuse \u0026ndash; avoid generics when concrete types are simpler Name type parameters clearly (T, K, V, etc.) Use type constraints to enforce correctness Conclusion Generics are a powerful addition to Go that let you write cleaner, more reusable code without giving up type safety. Whether you\u0026rsquo;re building data structures, utility functions, or abstractions, generics help reduce duplication and improve flexibility.\nNow that you understand generics, you\u0026rsquo;re ready to explore Go\u0026rsquo;s concurrency model and build high-performance programs using goroutines and channels.\nHappy coding!\n","href":"/2025/04/generics-in-go-writing-reusable-and-type-safe-code.html","title":"Writing Reusable and Type-Safe Code"},{"content":"Benchmarking is the process of measuring the performance of code. In Go, benchmarking is built into the standard testing package, making it easy to test how fast your functions run. Whether you\u0026rsquo;re comparing two algorithms, optimizing critical sections of code, or experimenting with concurrency, benchmarking helps you make informed decisions.\nThis article will walk you through:\nWhat is benchmarking and why it matters How to write benchmark functions in Go Interpreting benchmark results Using b.ResetTimer(), b.StopTimer(), and b.StartTimer() Common use cases for benchmarking Best practices for writing meaningful benchmarks Why Benchmarking is Important Benchmarking allows you to evaluate performance based on data, not assumptions. You can compare the execution time of different code versions, measure improvements, and catch performance regressions early. This is crucial for optimizing critical parts of applications such as sorting, searching, or processing large datasets.\nWriting Your First Benchmark Just like test functions in Go, benchmark functions are placed in a file ending with _test.go. Benchmark functions must start with Benchmark and have this signature:\nfunc BenchmarkXxx(b *testing.B) Example:\nfunc BenchmarkAdd(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = 1 + 2 } } Go runs this loop repeatedly to get a stable measurement. The b.N is automatically adjusted to get an accurate average runtime.\nRunning Benchmarks To run all benchmarks in a package, use:\ngo test -bench=. To run a specific benchmark:\ngo test -bench=BenchmarkAdd You’ll see output like this:\nBenchmarkAdd-8 1000000000 0.25 ns/op -8 means 8 logical CPUs used 1000000000 is how many times it ran 0.25 ns/op is time per operation Controlling Timers You can use b.StopTimer() and b.StartTimer() to exclude setup code:\nfunc BenchmarkWithSetup(b *testing.B) { data := make([]int, 1000) b.ResetTimer() for i := 0; i \u0026lt; b.N; i++ { _ = process(data) } } Comparing Implementations Let’s say you want to compare two ways to concatenate strings:\nfunc BenchmarkConcatPlus(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = \u0026#34;hello\u0026#34; + \u0026#34; \u0026#34; + \u0026#34;world\u0026#34; } } func BenchmarkConcatSprintf(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { _ = fmt.Sprintf(\u0026#34;%s %s\u0026#34;, \u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;) } } This helps you choose the faster approach in performance-critical sections.\nBest Practices Keep benchmarks small and focused on a single operation Avoid external dependencies (e.g., file I/O, network) Isolate logic you\u0026rsquo;re testing to avoid side effects Use go test -bench with -count for averaging over multiple runs Conclusion Benchmarking in Go is simple but powerful. It helps you write better-performing programs by providing real measurements instead of guesses. Combined with testing, it becomes a critical part of writing production-ready software.\nHappy benchmarking!\n","href":"/2025/04/benchmarking-in-go-measuring.html","title":"Measuring Performance with testing.B"},{"content":"Testing is one of the most important parts of software development, yet often overlooked. In Go, testing is not an afterthought \u0026ndash; it\u0026rsquo;s built into the language itself through the powerful and easy-to-use testing package. Whether you\u0026rsquo;re building a web app, API, or CLI tool, writing tests will help you catch bugs early, document your code, and refactor safely.\nThis article will help you understand:\nWhy testing matters in software development The basics of writing tests in Go Using t.Error, t.Fail, and t.Fatal Table-driven tests Running and understanding test results Measuring code coverage Best practices for writing useful tests Why Testing is Important Testing helps you ensure that your code works as expected \u0026ndash; not just today, but as it evolves. Without tests, it\u0026rsquo;s risky to make changes because you can\u0026rsquo;t be confident you haven\u0026rsquo;t broken something.\nBenefits of testing include:\nPreventing bugs before reaching production Providing documentation for your code\u0026rsquo;s behavior Making code easier to refactor Enabling safe collaboration within teams Getting Started: Writing Your First Test In Go, a test file must end with _test.go and be in the same package as the code you want to test.\nLet’s say you have a simple math function:\npackage calculator func Add(a, b int) int { return a + b } Your test file could look like this:\npackage calculator import \u0026#34;testing\u0026#34; func TestAdd(t *testing.T) { result := Add(2, 3) expected := 5 if result != expected { t.Errorf(\u0026#34;Add(2, 3) = %d; want %d\u0026#34;, result, expected) } } Understanding t.Error, t.Fail, and t.Fatal t.Error: reports an error but continues running the test t.Fatal: reports an error and immediately stops the test t.Fail: marks the test as failed but doesn’t log a message Table-Driven Tests This is a common Go pattern for testing multiple cases in a clean way:\nfunc TestAddMultipleCases(t *testing.T) { tests := []struct { a, b int expected int }{ {1, 2, 3}, {0, 0, 0}, {-1, -1, -2}, } for _, tt := range tests { result := Add(tt.a, tt.b) if result != tt.expected { t.Errorf(\u0026#34;Add(%d, %d) = %d; want %d\u0026#34;, tt.a, tt.b, result, tt.expected) } } } Running Tests To run all tests in a package, use:\ngo test To see detailed output:\ngo test -v Code Coverage Want to know how much of your code is tested?\ngo test -cover You can even generate an HTML report:\ngo test -coverprofile=coverage.out go tool cover -html=coverage.out Where to Put Tests It’s a good practice to place tests right next to the code they are testing. This makes them easy to find and maintain. Use the same package name unless you’re doing black-box testing.\nBest Practices Write tests as you write code, not after Use table-driven tests to cover edge cases Make your test failures readable (clear messages) Group related logic into subtests using t.Run Keep test functions short and focused Conclusion Testing is not just a formality \u0026ndash; it’s a mindset. Go makes it easy to write fast, reliable tests without third-party tools. By integrating testing into your daily development flow, you’ll gain confidence, spot bugs earlier, and create better software.\nIn the next topic, we\u0026rsquo;ll explore how to benchmark Go code and write performance tests.\nKeep testing and happy coding!\n","href":"/2025/04/testing-in-go-writing-unit-tests-with.html","title":"Writing Unit Tests with the Testing Package"},{"content":"Error handling is a core part of Go programming. Unlike many languages that use exceptions, Go takes a more straightforward and explicit approach. In Go, functions often return an error as the last return value, and it\u0026rsquo;s the developer’s job to check and handle it. This method may seem verbose at first, but it leads to more robust and predictable code.\nIn this article, you\u0026rsquo;ll learn:\nWhat an error is in Go How to handle errors using if err != nil Creating custom errors Error wrapping with Go 1.13+ Custom error types Using panic and recover (when and why) Best practices for error handling What is an Error in Go? In Go, the error type is a built-in interface:\ntype error interface { Error() string } Any type that implements the Error() method satisfies the error interface. Most standard functions return an error as a way to indicate that something went wrong.\nBasic Error Handling The standard way to handle errors in Go is with if err != nil blocks:\npackage main import ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) func divide(a, b int) (int, error) { if b == 0 { return 0, errors.New(\u0026#34;cannot divide by zero\u0026#34;) } return a / b, nil } func main() { result, err := divide(10, 0) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } fmt.Println(\u0026#34;Result:\u0026#34;, result) } Creating Custom Errors You can create custom errors using the errors.New or fmt.Errorf functions:\nerr := errors.New(\u0026#34;something went wrong\u0026#34;) err := fmt.Errorf(\u0026#34;error occurred: %v\u0026#34;, err) Error Wrapping (Go 1.13+) Go 1.13 introduced error wrapping, which lets you keep the original error while adding context:\noriginal := errors.New(\u0026#34;file not found\u0026#34;) wrapped := fmt.Errorf(\u0026#34;cannot load config: %w\u0026#34;, original) You can later use errors.Is and errors.As to inspect wrapped errors:\nif errors.Is(wrapped, original) { fmt.Println(\u0026#34;Original error matched\u0026#34;) } Custom Error Types To add more detail or behavior, you can define your own error types:\ntype MyError struct { Code int Msg string } func (e MyError) Error() string { return fmt.Sprintf(\u0026#34;Code %d: %s\u0026#34;, e.Code, e.Msg) } Now you can return MyError from functions and check its fields with type assertions.\nPanic and Recover panic is used when your program cannot continue. It\u0026rsquo;s similar to throwing an exception but should be avoided for expected errors.\nfunc risky() { panic(\u0026#34;something went really wrong\u0026#34;) } To handle panic safely, use recover inside a deferred function:\nfunc safe() { defer func() { if r := recover(); r != nil { fmt.Println(\u0026#34;Recovered from panic:\u0026#34;, r) } }() risky() } Best Practices Always check and handle errors returned from functions Wrap errors with context using fmt.Errorf and %w Use custom error types for more control Avoid panic unless absolutely necessary (e.g., for programming errors) Log errors with enough context to debug later Conclusion Go’s error handling may be explicit and repetitive, but it leads to clear and predictable code. By following best practices and understanding how to create, return, and wrap errors, you’ll build programs that are easier to maintain and debug.\nIn the next topic, we\u0026rsquo;ll explore how to write tests in Go to verify the correctness of your code using go test and the testing package.\nHappy coding!\n","href":"/2025/04/error-handling-in-go-managing-errors.html","title":"Managing Errors the Right Way"},{"content":"Interfaces are one of the most important features in Go. They allow you to write flexible, reusable, and loosely coupled code. In Go, an interface defines a set of method signatures, and any type that implements those methods satisfies the interface \u0026ndash; without needing to explicitly declare that it does so. This is a powerful concept that supports polymorphism and clean architecture in Go applications.\nIn this article, you\u0026rsquo;ll learn:\nWhat an interface is in Go How to define and implement interfaces Implicit interface implementation Using interface as function parameters The empty interface and type assertions Real-world examples of interfaces Best practices when working with interfaces What is an Interface? An interface is a type that defines a set of method signatures. Any type that provides implementations for those methods is said to satisfy the interface.\ntype Speaker interface { Speak() string } This interface requires a method Speak that returns a string.\nImplementing an Interface Unlike other languages, Go uses implicit implementation. You don’t need to explicitly say “this struct implements an interface.” You just define the required methods.\ntype Dog struct {} func (d Dog) Speak() string { return \u0026#34;Woof!\u0026#34; } type Cat struct {} func (c Cat) Speak() string { return \u0026#34;Meow!\u0026#34; } Both Dog and Cat now satisfy the Speaker interface because they implement the Speak method.\nUsing Interface as Function Parameter Interfaces allow you to write functions that work with any type that satisfies the interface.\nfunc makeItSpeak(s Speaker) { fmt.Println(s.Speak()) } func main() { makeItSpeak(Dog{}) makeItSpeak(Cat{}) } This is very powerful for building reusable code, such as in logging, HTTP handling, and I/O.\nInterface with Multiple Methods type Reader interface { Read(p []byte) (n int, err error) } type Writer interface { Write(p []byte) (n int, err error) } type ReadWriter interface { Reader Writer } Interfaces can be composed from other interfaces, helping you build powerful abstractions.\nThe Empty Interface The empty interface interface{} can represent any type. It is often used in situations where you don’t know the exact type at compile time (e.g., in JSON decoding, generic containers).\nfunc describe(i interface{}) { fmt.Printf(\u0026#34;Value: %v, Type: %T \u0026#34;, i, i) } Type Assertion You can convert an empty interface back to a concrete type using type assertion.\nvar i interface{} = \u0026#34;hello\u0026#34; s := i.(string) fmt.Println(s) Or safely:\nif s, ok := i.(string); ok { fmt.Println(\u0026#34;String value:\u0026#34;, s) } else { fmt.Println(\u0026#34;Not a string\u0026#34;) } Type Switch Type switches are like regular switches, but for handling multiple possible types.\nfunc printType(i interface{}) { switch v := i.(type) { case string: fmt.Println(\u0026#34;It\u0026#39;s a string:\u0026#34;, v) case int: fmt.Println(\u0026#34;It\u0026#39;s an int:\u0026#34;, v) default: fmt.Println(\u0026#34;Unknown type\u0026#34;) } } Real-World Example: Logger Interface Let’s create a logger interface and different implementations:\ntype Logger interface { Log(message string) } type ConsoleLogger struct {} func (c ConsoleLogger) Log(message string) { fmt.Println(\u0026#34;[Console]\u0026#34;, message) } type FileLogger struct { File *os.File } func (f FileLogger) Log(message string) { fmt.Fprintln(f.File, \u0026#34;[File]\u0026#34;, message) } This allows you to use either logger with the same code:\nfunc logMessage(logger Logger, message string) { logger.Log(message) } Best Practices Name interfaces based on behavior (e.g., Reader, Formatter) Prefer small interfaces with one or two methods Use interface embedding for composition Only expose interfaces when they are needed (don’t over-abstract) Conclusion Interfaces are a core feature in Go that allow you to write flexible, reusable, and testable code. They help you define behavior and decouple implementation from abstraction. By understanding how to define and work with interfaces, you\u0026rsquo;ll be ready to create clean and modular Go programs.\nTry writing your own interfaces, build functions that accept them, and explore the built-in interfaces in Go’s standard library.\nHappy coding!\n","href":"/2025/04/interfaces-in-go-building-flexible-and.html","title":"Building Flexible and Reusable Code"},{"content":"In Go, understanding pointers is essential if you want to work effectively with functions, methods, and memory-efficient code. Unlike some other languages, Go’s approach to pointers is clean and straightforward\u0026ndash;there’s no pointer arithmetic, and most things can be done without overly complex syntax.\nThis article will help you understand:\nWhat pointers are in Go and how they work Using pointers in functions Method receivers: value vs pointer Choosing between value or pointer receiver Common mistakes with pointers Best practices for using pointers effectively What is a Pointer? A pointer is a variable that stores the memory address of another variable. You use the \u0026amp; operator to get the address and * to access the value at that address.\nfunc main() { x := 10 p := \u0026amp;x fmt.Println(*p) // 10 } Here, p is a pointer to x. *p accesses the value stored at the address.\nPointers and Functions When passing variables to functions, Go uses value semantics\u0026ndash;meaning it passes a copy. If you want the function to modify the original variable, pass a pointer.\nfunc update(val *int) { *val = 100 } func main() { x := 10 update(\u0026amp;x) fmt.Println(x) // 100 } This is useful when working with large structs or when you need to update the caller\u0026rsquo;s data.\nPointer Receivers in Methods In Go, methods can be defined with either value receivers or pointer receivers. Pointer receivers allow methods to modify the actual object.\ntype Person struct { Name string Age int } func (p *Person) GrowUp() { p.Age++ } func main() { person := Person{\u0026#34;Alice\u0026#34;, 20} person.GrowUp() fmt.Println(person.Age) // 21 } If GrowUp() used a value receiver (i.e., func (p Person)), the change would not persist outside the method.\nValue vs Pointer Receiver Go allows both styles, but here\u0026rsquo;s when to choose each:\nValue receiver: small structs, method does not modify data Pointer receiver: large structs, method needs to modify state func (p Person) ValueGreet() { fmt.Println(\u0026#34;Hello,\u0026#34;, p.Name) } func (p *Person) PointerUpdate(name string) { p.Name = name } Go is Smart: Automatic Conversion Go is smart enough to let you call pointer receiver methods on value types and vice versa\u0026ndash;it will automatically add or remove the \u0026amp; for you:\nperson := Person{\u0026#34;Bob\u0026#34;, 30} person.GrowUp() // Works even though GrowUp has a pointer receiver Common Mistakes Forgetting to pass \u0026amp;x when a function expects *int Trying to use *x when x is not a pointer Not understanding that value receiver methods work on copies Best Practices Use pointer receivers when your method modifies the struct or for performance Keep your struct small when using value receivers Avoid unnecessary pointer complexity\u0026ndash;Go is designed to make things simple Conclusion Pointers in Go are powerful, but not difficult. They let you control memory usage, update values across scopes, and create efficient, flexible methods. Understanding pointers will make you a better Go developer\u0026ndash;especially when working with structs, interfaces, and large systems.\nNow that you understand pointers, you\u0026rsquo;re ready to dive deeper into Go\u0026rsquo;s concurrency model and start using goroutines and channels. But don’t forget \u0026ndash; great power comes with great responsibility, even in Go!\nHappy coding!\n","href":"/2025/04/understanding-pointers-in-go-reference.html","title":"Reference Types and Receivers Explained"},{"content":"In Go, a struct is a powerful way to group related data together. It allows you to define your own custom types by combining variables (also called fields). Structs are often used to model real-world entities like users, products, or messages. When combined with methods, structs become the foundation for writing clean and reusable code in Go.\nIn this article, you\u0026rsquo;ll learn:\nHow to define and use structs in Go How to attach methods to a struct The difference between value and pointer receivers Best practices for using structs and methods effectively Defining a Struct To define a struct, you use the type keyword followed by the name of the struct and the struct keyword:\ntype User struct { Name string Email string Age int } This defines a struct called User with three fields. To create a value of that struct, you can do the following:\nfunc main() { user := User{ Name: \u0026#34;Alice\u0026#34;, Email: \u0026#34;alice@example.com\u0026#34;, Age: 30, } fmt.Println(user) } You can also declare an empty struct and assign fields later:\nvar u User u.Name = \u0026#34;Bob\u0026#34; u.Email = \u0026#34;bob@example.com\u0026#34; u.Age = 25 Accessing and Updating Struct Fields To access a field, use the dot . operator:\nfmt.Println(user.Name) To update a field:\nuser.Age = 31 Structs with Functions You can write a function that accepts a struct as an argument:\nfunc printUser(u User) { fmt.Println(\u0026#34;Name:\u0026#34;, u.Name) fmt.Println(\u0026#34;Email:\u0026#34;, u.Email) fmt.Println(\u0026#34;Age:\u0026#34;, u.Age) } Methods in Go In Go, you can define a function that is associated with a struct. This is called a method.\nfunc (u User) Greet() { fmt.Println(\u0026#34;Hi, my name is\u0026#34;, u.Name) } Here, (u User) means this function is a method that can be called on a User value.\nPointer Receivers vs Value Receivers You can define methods using either a value receiver or a pointer receiver:\n// Value receiver func (u User) Info() { fmt.Println(\u0026#34;User info:\u0026#34;, u.Name, u.Email) } // Pointer receiver func (u *User) UpdateEmail(newEmail string) { u.Email = newEmail } Use a pointer receiver if the method needs to modify the original struct or if copying the struct would be expensive.\nEmbedding Structs Go allows embedding one struct into another. This can be used to extend functionality:\ntype Address struct { City string State string } type Employee struct { User Address Position string } You can now access fields from both User and Address in an Employee instance directly.\nAnonymous Structs Go also supports defining structs without giving them a name. These are used for quick data grouping:\nperson := struct { Name string Age int }{ Name: \u0026#34;Charlie\u0026#34;, Age: 22, } Best Practices Group related data using structs for better organization Use methods to define behavior related to a struct Use pointer receivers when modifying struct data Use struct embedding to promote code reuse Conclusion Structs and methods are a core part of writing structured and maintainable code in Go. By learning how to define and work with them, you\u0026rsquo;ll be better equipped to build complex systems that are easy to manage. Practice creating your own structs and adding behavior with methods to solidify your understanding.\nHappy coding!\n","href":"/2025/04/structs-and-methods-in-go-defining-and.html","title":"Defining and Using Custom Types"},{"content":"Functions are an essential part of programming in any language, and Go is no exception. A function lets you organize code into reusable blocks, which helps reduce duplication and improve readability. In this article, you’ll learn how functions work in Go, how to define them, use them, and apply best practices.\nThis guide covers:\nHow to define and call a function in Go Function parameters and return values Multiple return values Named return values Variadic functions Functions as values and arguments Best practices for clean function design Defining and Calling a Function To define a function in Go, use the func keyword, followed by the function name, parameters, and return type (if any). Here\u0026rsquo;s a simple example:\npackage main import \u0026#34;fmt\u0026#34; func greet(name string) { fmt.Println(\u0026#34;Hello,\u0026#34;, name) } func main() { greet(\u0026#34;Alice\u0026#34;) } This function takes a string parameter and prints a greeting message. It is called from the main function.\nFunction Parameters and Return Values Functions can accept multiple parameters and return values. You need to specify the type for each parameter.\nfunc add(a int, b int) int { return a + b } func main() { result := add(3, 5) fmt.Println(\u0026#34;Sum:\u0026#34;, result) } Go also allows you to declare multiple parameters of the same type together, like this:\nfunc multiply(a, b int) int { return a * b } Multiple Return Values One of Go’s unique features is that a function can return more than one value.\nfunc divide(a, b int) (int, int) { quotient := a / b remainder := a % b return quotient, remainder } func main() { q, r := divide(10, 3) fmt.Println(\u0026#34;Quotient:\u0026#34;, q, \u0026#34;Remainder:\u0026#34;, r) } This is commonly used in Go for returning both result and error values.\nNamed Return Values You can also name return values in the function signature. This makes your code more readable and enables implicit return.\nfunc compute(a, b int) (sum int, product int) { sum = a + b product = a * b return } This is useful when the function logic is a bit more complex and you want to keep track of return values easily.\nVariadic Functions Sometimes, you may want to pass an arbitrary number of arguments to a function. Go supports this with variadic functions.\nfunc total(numbers ...int) int { sum := 0 for _, number := range numbers { sum += number } return sum } func main() { fmt.Println(total(1, 2, 3, 4, 5)) } The ...int means the function accepts any number of int values. Inside the function, numbers behaves like a slice.\nFunctions as Values and Arguments In Go, functions are first-class citizens. You can assign them to variables, pass them as arguments, and return them from other functions.\nfunc square(x int) int { return x * x } func apply(op func(int) int, value int) int { return op(value) } func main() { result := apply(square, 4) fmt.Println(result) } This opens up many possibilities such as writing flexible and composable code, especially when used with closures or higher-order functions.\nBest Practices Here are some general tips when writing functions in Go:\nKeep your functions short and focused on one task Use descriptive names for function and parameter names Avoid too many parameters (consider grouping them in structs) Document the purpose and behavior of your functions Conclusion Functions are a fundamental concept in Go programming. They allow you to organize your logic, make your code reusable, and improve structure. Go’s support for multiple return values, variadic functions, and treating functions as first-class values gives you powerful tools to build real-world applications.\nPractice writing your own functions, try combining features like variadic parameters with multiple returns, and use functions to structure your Go projects cleanly.\nHappy coding!\n","href":"/2025/04/understanding-functions-in-go-beginners.html","title":"A Beginner's Guide"},{"content":"When building applications in Go, it\u0026rsquo;s common to work with groups of data. For example, you might want to store a list of user names, or map names to scores. In Go, you can use collections like arrays, slices, and maps to do that.\nIn this article, we’ll explore:\nWhat arrays are and how they work How slices offer more flexibility What maps are and how to use them Common operations with collections Practical examples to understand the difference between them Let’s dive in and learn how Go helps us manage grouped data efficiently.\nArrays in Go An array is a fixed-size collection of elements of the same type. Once an array is created, its size cannot change.\npackage main import \u0026#34;fmt\u0026#34; func main() { var numbers [3]int numbers[0] = 10 numbers[1] = 20 numbers[2] = 30 fmt.Println(numbers) } You can also initialize an array directly:\nnames := [3]string{\u0026#34;Alice\u0026#34;, \u0026#34;Bob\u0026#34;, \u0026#34;Charlie\u0026#34;} Arrays have a fixed size. All elements must be of the same type, and you can access items using their index (starting from 0).\nArrays are not commonly used in large Go applications, but understanding them is key to learning slices.\nSlices in Go Slices are more flexible than arrays. They are built on top of arrays but allow dynamic resizing.\nnumbers := []int{10, 20, 30} fmt.Println(numbers) Adding elements to a slice:\nnumbers = append(numbers, 40) fmt.Println(numbers) Creating slices from existing arrays:\narr := [5]int{1, 2, 3, 4, 5} slice := arr[1:4] // includes index 1 to 3 fmt.Println(slice) Useful slice operations include append, len (length), and cap (capacity). Slices are widely used in Go because they are flexible and efficient.\nAnother great thing about slices is that they can share the same underlying array. This allows for memory-efficient manipulation of data. However, you should be cautious when modifying shared slices as changes might affect other parts of your code.\nMaps in Go Maps are key-value pairs. You can use them to store and retrieve data by key.\nscores := map[string]int{ \u0026#34;Alice\u0026#34;: 90, \u0026#34;Bob\u0026#34;: 85, } fmt.Println(scores[\u0026#34;Alice\u0026#34;]) Adding and updating values:\nscores[\u0026#34;Charlie\u0026#34;] = 88 scores[\u0026#34;Bob\u0026#34;] = 95 Deleting a value:\ndelete(scores, \u0026#34;Alice\u0026#34;) Looping through a map:\nfor name, score := range scores { fmt.Println(name, \u0026#34;has score\u0026#34;, score) } Checking if a key exists:\nvalue, exists := scores[\u0026#34;David\u0026#34;] if exists { fmt.Println(\u0026#34;Score:\u0026#34;, value) } else { fmt.Println(\u0026#34;David not found\u0026#34;) } Maps are extremely useful when you need fast lookups or need to associate labels with values. For example, they’re great for storing configuration options, lookup tables, or grouped statistics.\nChoosing Between Arrays, Slices, and Maps Use arrays when the size is known and fixed. Use slices when you need a dynamic list. Use maps when you need to associate keys to values (like name to score).\nEach data structure has its own strengths. As a Go developer, you’ll likely use slices and maps much more often than arrays, especially when working with APIs, databases, or handling JSON.\nPractical Example: Student Grades grades := map[string][]int{ \u0026#34;Alice\u0026#34;: {90, 85, 88}, \u0026#34;Bob\u0026#34;: {78, 82, 80}, } for name, gradeList := range grades { total := 0 for _, grade := range gradeList { total += grade } average := total / len(gradeList) fmt.Println(name, \u0026#34;average grade:\u0026#34;, average) } This example combines maps and slices to store multiple grades for each student and calculates the average.\nSummary Collections in Go help you group and organize data. Arrays are useful but limited by their fixed size. Slices are flexible and the most commonly used collection in Go. Maps let you link one value to another using keys.\nBy understanding and practicing with these three types of collections, you’ll be ready to write real-world programs that work with lists of data, settings, or records.\nAs you continue learning Go, try building small programs that use slices and maps. Practice manipulating data, looping through collections, and performing operations like sorting or searching. These are real-world tasks you\u0026rsquo;ll encounter as a developer.\nKeep exploring and happy coding!\n","href":"/2025/04/working-with-collections-in-go-arrays.html","title":"Arrays, Slices, and Maps Explained"},{"content":"Loops are a key part of programming. They let us run the same piece of code multiple times without repeating ourselves. In Go, loops are simple but powerful \u0026ndash; and they\u0026rsquo;re built using just one keyword: for.\nIn this article, we’ll explore:\nThe basic for loop in Go Using for as a while loop Looping with range Breaking or skipping parts of loops with break and continue Real-world examples to help you understand how loops work What is a Loop? A loop is a way to repeat a block of code as long as a condition remains true. Instead of writing similar code many times, we can put it in a loop and let the program handle the repetition. This makes our code shorter, cleaner, and easier to manage. Go uses the keyword for for all loop types, which makes it both simple and flexible.\nThe Basic for Loop The most common way to write a loop in Go is with the standard for loop structure. It includes three parts: an initializer, a condition, and a post statement.\npackage main import \u0026#34;fmt\u0026#34; func main() { for i := 0; i \u0026lt; 5; i++ { fmt.Println(\u0026#34;Count:\u0026#34;, i) } } This loop will print numbers from 0 to 4. First, it starts with i = 0. Then it checks the condition i \u0026lt; 5. If true, it runs the code inside the loop. After each loop, i is increased by 1. When the condition is false, the loop stops.\nUsing for as a while Loop Go doesn’t have a while keyword. But you can use for in the same way by just writing the condition.\nfunc main() { i := 0 for i \u0026lt; 3 { fmt.Println(\u0026#34;i is:\u0026#34;, i) i++ } } This loop works exactly like a while loop. It continues running as long as the condition i \u0026lt; 3 is true. This format is useful when you don’t need a counter setup like in the basic for loop.\nInfinite Loops Sometimes you want a loop to run forever, such as when building servers or listening to user input. You can do this by writing for without a condition.\nfunc main() { for { fmt.Println(\u0026#34;This runs forever until we break it.\u0026#34;) break } } This is an infinite loop, and you control when to stop it using a break statement inside the loop.\nLooping with range Go provides a very handy way to loop over arrays, slices, strings, and maps using range. It simplifies working with collections.\nExample with a slice: func main() { fruits := []string{\u0026#34;apple\u0026#34;, \u0026#34;banana\u0026#34;, \u0026#34;cherry\u0026#34;} for index, fruit := range fruits { fmt.Println(index, fruit) } } Here, range gives both the index and the value of each item. If you don’t need the index, you can ignore it using an underscore:\nfor _, fruit := range fruits { fmt.Println(fruit) } Looping through a map: You can use range to loop through key-value pairs in a map:\nfunc main() { scores := map[string]int{\u0026#34;Alice\u0026#34;: 90, \u0026#34;Bob\u0026#34;: 85} for name, score := range scores { fmt.Println(name, \u0026#34;scored\u0026#34;, score) } } Looping over a string: Strings in Go are UTF-8 encoded. Using range lets you loop through each character:\nfunc main() { word := \u0026#34;go\u0026#34; for _, char := range word { fmt.Println(char) } } Note: This prints the Unicode code points (runes) for each character. If you want the actual character, you can use fmt.Printf(\u0026quot;%c\u0026quot;, char).\nUsing break and continue To control your loop more precisely, you can use break to stop the loop early, or continue to skip the current iteration and move to the next one.\nExample with break: func main() { for i := 0; i \u0026lt; 10; i++ { if i == 5 { break } fmt.Println(i) } } Example with continue: func main() { for i := 0; i \u0026lt; 5; i++ { if i == 2 { continue } fmt.Println(i) } } In this example, when i equals 2, the loop skips that iteration and continues with the next one.\nWhy Loops Matter Loops allow you to handle tasks like processing data, creating repeated outputs, checking conditions, or iterating through user input efficiently. Whether you’re building a calculator, a file reader, or a game, you’ll probably use loops often.\nSummary Loops in Go are powerful but simple. You can use for in different styles: the traditional counter-based loop, while-like loops, infinite loops, and range-based loops for collections. You can even control the flow inside the loop with break and continue.\nWith just one keyword, Go gives you all the looping tools you need. Try writing your own loops, experiment with slices and maps, and see how you can apply them in your real projects.\nKeep learning and happy coding!\n","href":"/2025/04/understanding-loops-in-go-for-range.html","title":"for, range, break, and continue Explained"},{"content":"Conditional statements are one of the essential building blocks in any programming language, including Go. They allow us to make decisions in our code \u0026ndash; telling the program to do something only if a certain condition is true.\nIn this article, we will explore:\nThe if, else, and else if statements The switch statement Best practices for using conditionals in Go Real examples to help you practice What is a Conditional Statement? A conditional statement evaluates whether a condition is true or false. Based on that, your Go program can choose which block of code to execute.\nLet’s say you want your app to greet users differently depending on the time of day. That’s where conditional logic comes in!\nif, else if, and else The most common conditional structure is if.\nBasic if syntax: package main import \u0026#34;fmt\u0026#34; func main() { age := 20 if age \u0026gt;= 18 { fmt.Println(\u0026#34;You are an adult.\u0026#34;) } } With else: func main() { age := 15 if age \u0026gt;= 18 { fmt.Println(\u0026#34;You are an adult.\u0026#34;) } else { fmt.Println(\u0026#34;You are underage.\u0026#34;) } } With else if: func main() { hour := 14 if hour \u0026lt; 12 { fmt.Println(\u0026#34;Good morning!\u0026#34;) } else if hour \u0026lt; 18 { fmt.Println(\u0026#34;Good afternoon!\u0026#34;) } else { fmt.Println(\u0026#34;Good evening!\u0026#34;) } } You can use multiple else if statements to check different conditions.\nShort if Statement Go supports a shorter form to declare variables inside the if block:\nfunc main() { if num := 10; num%2 == 0 { fmt.Println(\u0026#34;Even number\u0026#34;) } } This is useful if you only need the variable inside the if scope.\nswitch Statement The switch statement lets you compare a value against multiple conditions. It\u0026rsquo;s a cleaner alternative to many else if blocks.\nExample: func main() { day := \u0026#34;Friday\u0026#34; switch day { case \u0026#34;Monday\u0026#34;: fmt.Println(\u0026#34;Start of the week!\u0026#34;) case \u0026#34;Friday\u0026#34;: fmt.Println(\u0026#34;Almost weekend!\u0026#34;) case \u0026#34;Saturday\u0026#34;, \u0026#34;Sunday\u0026#34;: fmt.Println(\u0026#34;Weekend time!\u0026#34;) default: fmt.Println(\u0026#34;Another day!\u0026#34;) } } You can also group cases like Saturday and Sunday above.\nBest Practices for Beginners Keep your condition logic simple. Prefer switch when comparing one variable to multiple values. Don\u0026rsquo;t forget the default case in switch. Avoid deep nesting (e.g. if-inside-if-inside-if). More Practice Examples 1. Check if a number is positive, negative, or zero: func main() { num := 0 if num \u0026gt; 0 { fmt.Println(\u0026#34;Positive\u0026#34;) } else if num \u0026lt; 0 { fmt.Println(\u0026#34;Negative\u0026#34;) } else { fmt.Println(\u0026#34;Zero\u0026#34;) } } 2. Simple login simulation: func main() { username := \u0026#34;admin\u0026#34; password := \u0026#34;1234\u0026#34; if username == \u0026#34;admin\u0026#34; \u0026amp;\u0026amp; password == \u0026#34;1234\u0026#34; { fmt.Println(\u0026#34;Login successful\u0026#34;) } else { fmt.Println(\u0026#34;Invalid credentials\u0026#34;) } } Conclusion Understanding how conditionals work in Go helps you control the flow of your programs. Start with if and else, and move on to switch when you need to compare multiple options. Use these tools to build dynamic and interactive applications.\nNext Step: Learn about loops in Go \u0026ndash; another powerful way to control program flow!\nHappy coding!\n","href":"/2025/04/understanding-conditional-statements-in.html","title":"Understanding Conditional Statements in Go (if, switch, etc.)"},{"content":"In our series on understanding data types in the Go programming language, after discussing numeric and boolean types, we will now explore strings. Strings are one of the most frequently used data types in programming due to their ubiquitous use in handling text. In Go, strings have several unique characteristics that we will explore in this article.\nIntroduction to Strings In Go, a string is a sequence of immutable bytes. This means that once a string value is set, it cannot be changed without creating a new string.\npackage main import \u0026#34;fmt\u0026#34; func main() { s := \u0026#34;hello world\u0026#34; // s[0] = \u0026#39;H\u0026#39; // this will result in an error because strings are immutable s = \u0026#34;Hello World\u0026#34; // this is valid, creates a new string fmt.Println(s) } Output\nHello World Basic Operations Basic operations on strings include concatenation and substring extraction. Concatenation can be done using the + operator, and substrings can be obtained by slicing.\npackage main func main() { firstName := \u0026#34;John\u0026#34; lastName := \u0026#34;Doe\u0026#34; fullName := firstName + \u0026#34; \u0026#34; + lastName // String concatenation println(fullName) hello := \u0026#34;Hello, world!\u0026#34; sub := hello[7:] // Extracting a substring println(sub) } Output\nJohn Doe world! String Manipulation The strings package in Go provides many functions for string manipulation. Here are a few examples:\npackage main import \u0026#34;fmt\u0026#34; import \u0026#34;strings\u0026#34; func main() { var str = \u0026#34;Hello, World\u0026#34; fmt.Println(strings.ToLower(str)) // convert all letters to lowercase fmt.Println(strings.ToUpper(str)) // convert all letters to uppercase fmt.Println(strings.TrimSpace(\u0026#34; space remover \u0026#34;)) // trim spaces from both ends } Output\nhello, world HELLO, WORLD space remover Iteration and Transformation We can iterate over strings with a for loop, and convert strings to byte slices or rune arrays.\npackage main import \u0026#34;fmt\u0026#34; func main() { str := \u0026#34;Hello, 世界\u0026#34; for i, runeValue := range str { fmt.Printf(\u0026#34;%#U starts at byte position %d\\n\u0026#34;, runeValue, i) } // Convert string to byte slice byteSlice := []byte(str) fmt.Println(byteSlice) // Convert string to rune slice runeSlice := []rune(str) fmt.Println(runeSlice) } Output\nU+0048 \u0026#39;H\u0026#39; starts at byte position 0 U+0065 \u0026#39;e\u0026#39; starts at byte position 1 U+006C \u0026#39;l\u0026#39; starts at byte position 2 U+006C \u0026#39;l\u0026#39; starts at byte position 3 U+006F \u0026#39;o\u0026#39; starts at byte position 4 U+002C \u0026#39;,\u0026#39; starts at byte position 5 U+0020 \u0026#39; \u0026#39; starts at byte position 6 U+4E16 \u0026#39;世\u0026#39; starts at byte position 7 U+754C \u0026#39;界\u0026#39; starts at byte position 10 [72 101 108 108 111 44 32 228 184 150 231 149 140] [72 101 108 108 111 44 32 19990 30028]` Strings and Unicode Go supports Unicode characters, which means that strings can contain characters from any language. This is because Go uses UTF-8 encoding for strings, which can represent all Unicode characters.\npackage main import \u0026#34;fmt\u0026#34; func main() { const nihongo = \u0026#34;日本語\u0026#34; for index, runeValue := range nihongo { fmt.Printf(\u0026#34;%#U starts at byte position %d\\n\u0026#34;, runeValue, index) } } Output\nU+65E5 \u0026#39;日\u0026#39; starts at byte position 0 U+672C \u0026#39;本\u0026#39; starts at byte position 3 U+8A9E \u0026#39;語\u0026#39; starts at byte position 6 Conclusion Strings are a fundamental data type in Go, and understanding how to work with them is essential for any Go programmer. In this article, we explored the basics of strings in Go, including their immutability, basic operations, manipulation, iteration, and Unicode support. Armed with this knowledge, you should be well-equipped to handle strings in your Go programs.\nFor more information on strings and other data types in Go, check out the official strings package documentation.\nHappy coding!\n","href":"/2024/07/understanding-string-data-type-in-go.html","title":"Basics and Practical Examples"},{"content":"Go, also known as Golang, is a statically typed language developed by Google. It\u0026rsquo;s known for its simplicity and efficiency, especially when it comes to systems and concurrent programming. In this article, we\u0026rsquo;ll explore the numeric types in Go and provide practical examples to illustrate their usage.\nBasic Numeric Types Go offers several basic numeric types categorized into integers, floating point numbers, and complex numbers. Here’s a quick overview:\nInteger Integer types are divided into two categories, signed and unsigned. The signed integers int8, int16, int32, int64 can hold both negative and positive values, whereas unsigned integers int8, int16, int32, int64 can only hold positive values and zero.\nHere’s an example of how you can declare and initialize an integer variable in Go:\n`package main import \u0026#34;fmt\u0026#34; func main() { var a int8 = 127 // a := int8(127) var b uint8 = 255 // b := uint8(255) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, b, b) }` Output\nType: int8 Value: 127 Type: uint8 Value: 255 Floating Point go has two floating point types: float32 and float64. The numbers represent single and double precision floating point numbers respectively.\nHere’s an example of how you can declare and initialize a floating point variable in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { var pi float64 = 3.14159 fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, pi, pi) } Output\nType: float64 Value: 3.14159` Complex Numbers Go has two complex number types: complex64 and complex128. The numbers represent complex numbers with float32 and float64 real and imaginary parts respectively.\nHere’s an example of how you can declare and initialize a complex number variable in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { c := complex(3, 4) fmt.Printf(\u0026#34;Type: %T Value: %v\\n\u0026#34;, c, c) } Output\nType: complex128 Value: (3+4i) Numeric Literals Go supports several numeric literals, including decimal, binary, octal, and hexadecimal. Here’s an example of how you can declare and initialize numeric literals in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { a := 42 b := 0b101010 // binary literal c := 0o52 // octal literal d := 0x2a // hexadecimal literal fmt.Println(a, b, c, d) } Output\n42 42 42 42 Numeric Operations Go supports several arithmetic operations on numeric types, including addition, subtraction, multiplication, division, and modulus. Here’s an example of how you can perform arithmetic operations in Go:\npackage main import \u0026#34;fmt\u0026#34; func main() { a := 10 b := 20 sum := a + b diff := a - b product := a * b quotient := a / b remainder := a % b fmt.Println(sum, diff, product, quotient, remainder) } Output\n30 -10 200 0 10 Conclusion Go provides a rich set of numeric types and operations that make it easy to work with numbers in your programs. By understanding the different numeric types and their usage, you can write efficient and reliable code that performs well in a variety of scenarios.\nFor more information on Go’s numeric types, you can refer to the official Go documentation .\nHappy coding!\n","href":"/2024/07/understanding-numeric-data-type-in-go.html","title":"Basics and Practical Examples"},{"content":"In the Go programming language, as in many other programming languages, the boolean data type is fundamental. It represents truth values, either true or false. Booleans are crucial in software development for decision-making, allowing developers to control the flow of execution through conditional statements like if, else, and looping constructs such as for.\nDeclaration and Initialization\nTo declare a boolean in Go, you use the keyword bool. Here\u0026rsquo;s how you can declare and initialize a boolean variable:\nvar myBool bool = true This code snippet shows how to initialize a boolean variable named myBool with the value true.\nIn this line, isOnline is a boolean variable that is initialized to true . Alternatively, Go supports type inference where the compiler automatically detects the type based on the initial value:\nisOnline := true This shorthand method is preferred in Go for its simplicity and readability.\nBoolean in conditional statement Booleans are extensively used in conditional statements. Here\u0026rsquo;s an example of how to use a boolean in an if and else statement:\npackage main import \u0026#34;fmt\u0026#34; func main() { isOnline := true if isOnline { fmt.Println(\u0026#34;User is online\u0026#34;) } else { fmt.Println(\u0026#34;User is offline\u0026#34;) } } Output\nUser is online Practical example: User Authentication Let\u0026rsquo;s create a practical example where booleans are used to check whether a user\u0026rsquo;s username and password match the expected values:\npackage main import \u0026#34;fmt\u0026#34; func main() { username := \u0026#34;admin\u0026#34; password := \u0026#34;password\u0026#34; inputUsername := \u0026#34;admin\u0026#34; inputPassword := \u0026#34;password\u0026#34; if username == inputUsername \u0026amp;\u0026amp; password == inputPassword { fmt.Println(\u0026#34;User authenticated\u0026#34;) } else { fmt.Println(\u0026#34;Invalid credentials\u0026#34;) } } Output\nUser authenticated in this example, isAuthenticated is a boolean that becomes true if both the username and password match the expected values. This boolean is then used to determine the message to display to the user.\nUsing Booleans with Loops Booleans are also useful in loops to determine when the loop should end. Here\u0026rsquo;s a simple for loop that uses a boolean condition:\npackage main import \u0026#34;fmt\u0026#34; func main() { isRunning := true count := 0 for isRunning { fmt.Println(\u0026#34;Count:\u0026#34;, count) count++ if count == 5 { isRunning = false } } } Output\nCount: 0 Count: 1 Count: 2 Count: 3 Count: 4 In this loop, the boolean expression count \u0026lt; 5 determines whether the loop should continue running.\nConclusion Booleans in Go provide a simple yet powerful way to handle decision-making in your programs. They are essential for executing different code paths under different conditions, handling user authentication, controlling loops, and more.\nAs you continue to develop in Go, you\u0026rsquo;ll find that booleans ar an indispensable part of many common programming task.\nNow that you have a good understanding of booleans in Go, you can start using them in your programs to make them more dynamic and responsive to different conditions.\nFor more information on booleans and other data types in Go, check out the official builtin package documentation.\nHappy coding!\n","href":"/2024/07/understanding-booleans-in-go-basics.html","title":"Basics and Practical Examples"},{"content":"If you\u0026rsquo;re just getting started with Laravel or even if you\u0026rsquo;ve been working with it for a while, using the right tools can make a big difference. Visual Studio Code (VS Code) is one of the most popular code editors among web developers, and thankfully, it has a great ecosystem of extensions that can help boost your productivity when working with Laravel.\nIn this article, we\u0026rsquo;ll go through five essential VS Code extensions that you should install if you\u0026rsquo;re working with Laravel. These tools will help you write code faster, reduce bugs, and improve your workflow overall.\n1. Laravel Blade Snippets This extension provides syntax highlighting and snippets for Laravel Blade. It makes writing Blade templates much easier by auto-completing common directives like @if, @foreach, @csrf, and more.\nWhy it\u0026rsquo;s helpful:\nSpeeds up writing Blade views Reduces typos in directives Supports auto-complete and syntax colors Install: You can find it on the VS Code marketplace by searching Laravel Blade Snippets by Winnie Lin.\n2. Laravel Artisan The Laravel Artisan extension allows you to run Artisan commands directly from VS Code without having to switch to the terminal. You can quickly create controllers, models, migrations, and more with just a few clicks.\nWhy it\u0026rsquo;s helpful:\nAccess Artisan commands via command palette Fast scaffolding for common tasks Works well in any Laravel version Install: Look for Artisan by Ryan Naddy in the VS Code marketplace.\n3. Laravel Extra Intellisense This extension adds improved IntelliSense support for Laravel projects, giving you better autocompletion for facades, routes, models, and other Laravel features.\nWhy it\u0026rsquo;s helpful:\nBetter code suggestions and navigation Works seamlessly with Laravel\u0026rsquo;s facades Saves time looking up class names Install: Search Laravel Extra Intellisense by amiralizadeh9480.\n4. PHP Intelephense While not Laravel-specific, this extension is a must-have for PHP developers. It provides advanced PHP IntelliSense, diagnostics, and more. Combined with Laravel Extra Intellisense, it gives a robust development experience.\nWhy it\u0026rsquo;s helpful:\nFaster autocompletion Real-time error checking Supports namespaces, classes, and functions Install: Search for PHP Intelephense by Ben Mewburn.\n5. Laravel goto Controller This extension allows you to quickly navigate from a route or Blade file to the corresponding controller method. It\u0026rsquo;s great when you\u0026rsquo;re working on medium to large Laravel projects and want to jump between files quickly.\nWhy it\u0026rsquo;s helpful:\nQuickly locate controller methods Jump between route, view, and controller Increases navigation speed Install: Look for Laravel goto Controller by codingyu.\nFinal Thoughts Using the right extensions can make your Laravel development process much smoother and more enjoyable. These five extensions cover the essentials: writing Blade templates, navigating controllers, running Artisan commands, and getting smarter IntelliSense.\nIf you\u0026rsquo;re learning Laravel, these tools can help you focus on writing code instead of memorizing every command or directive. And if you\u0026rsquo;re working on a big project, they\u0026rsquo;ll save you time and energy.\nGive them a try and see how much better your coding experience becomes. Happy coding!\n","href":"/2024/04/5-laravel-extensions-that-you-must-install-on-your-visual-studio-code.html","title":"5 Laravel extensions that you must install on your Visual Studio Code"},{"content":"Ketika kita pertama kali melangkah ke dalam dunia pengembangan web, rasanya seperti memasuki sebuah labirin yang penuh dengan kode dan logika yang rumit. Namun, ada sesuatu yang menarik tentang proses belajar bagaimana segala sesuatu terhubung dan bekerja bersama untuk membentuk sebuah aplikasi web.\nApakah Anda sedang mencari hobi baru atau ingin mengejar karier sebagai pengembang web, membangun aplikasi pertama Anda adalah pengalaman yang sangat berharga. Dengan memahami dasar-dasar pengembangan web, Anda akan memiliki dasar yang kuat untuk mempelajari teknologi-teknologi baru dan membangun aplikasi yang lebih kompleks di masa depan.\nDalam blog kali ini, saya akan membawa Anda melalui proses pembuatan aplikasi web pertama Anda dengan Laravel, sebuah framework PHP yang akan memudahkan kita mengatur dan menulis kode. Dengan Laravel, tugas-tugas yang dulu tampak rumit sekarang bisa kita lakukan dengan lebih terorganisir dan efisien.\nSaya akan menunjukkan kepada Anda bahwa siapa pun bisa mulai membuat aplikasi, dan dengan sedikit kesabaran serta ketekunan, Anda akan bisa membuat sesuatu yang bisa Anda banggakan. Jadi, mari kita mulai petualangan ini bersama-sama dan lihat apa yang bisa kita ciptakan!\nLangkah 1: Persiapan dan Instalasi Sebelum kita mulai, ada beberapa alat yang perlu Anda siapkan dan install di komputer Anda:\nPHP: Versi 7.3 atau lebih tinggi diperlukan. Unduh dari situs resmi PHP . Composer: Manajemen dependensi untuk PHP. Unduh dari situs resmi Composer . Server Web: Gunakan XAMPP atau MAMP untuk pengembangan lokal. Text Editor: Visual Studio Code atau Sublime Text disarankan. Terminal atau Command Prompt: Untuk menjalankan perintah Laravel. Node.js (Opsional): Untuk menjalankan npm atau development mode. Langkah 2: Instalasi Laravel Buka terminal atau command prompt dan jalankan perintah berikut:\ncomposer create-project laravel/laravel example-app **namaAplikasi** Sesuaikan namaAplikasi dengan nama yang Anda inginkan. Proses ini akan mengunduh dan menginstal Laravel serta dependensinya.\nLangkah 3: Menjelajahi Struktur Laravel Setelah instalasi, Anda akan memiliki struktur folder yang dapat dijelajahi sebagai berikut:\napp/: Berisi kode inti aplikasi Anda seperti controllers dan models. bootstrap/: Mengandung file app.php yang melakukan bootstrap framework dan konfigurasi autoloading. config/: Berisi semua file konfigurasi aplikasi Anda. database/: Tempat untuk migrasi database, seeders, dan factories. public/: Root publik aplikasi Anda dengan index.php yang mengarahkan semua permintaan. resources/: Berisi file view Blade, file sumber (LESS, SASS, JS), dan file bahasa. routes/: Berisi semua file rute untuk aplikasi Anda termasuk web, api, console, dan channels. storage/: Direktori untuk menyimpan file yang diunggah, cache, view dikompilasi, dan logs. tests/: Berisi tes otomatis Anda termasuk PHPUnit tests. vendor/: Berisi pustaka Composer dependensi aplikasi Anda. .env: File konfigurasi lingkungan untuk aplikasi Anda. .env.example: Template file .env. .gitignore: Menentukan file apa yang tidak akan ditrack oleh Git. artisan: Command-line interface untuk Laravel. composer.json: File konfigurasi untuk Composer. composer.lock: File kunci untuk dependensi yang diinstal oleh Composer. package.json: Menentukan dependensi Node.js. phpunit.xml: File konfigurasi untuk PHPUnit. README.md: File markdown yang berisi informasi tentang aplikasi. vite.config.js: File konfigurasi untuk Vite yang digunakan dalam pengembangan front-end. Langkah 4: Menjalankan Web Pertama Anda Jalankan perintah berikut di terminal vscode ataupun terminal kesayangan anda:\nphp artisan serve Perintah ini akan menjalankan server pengembangan lokal dan memberikan Anda URL untuk mengakses aplikasi web Anda, seperti link dibawah ini.\nhttp://127.0.0.1:8000 http://localhost:8000 Secara default Laravel akan berjalan di port 8000, jika port tersebut sudah digunakan, maka Laravel akan berjalan di port 8001, 8002, dan seterusnya, namun port tersebut bisa diubah sesuai dengan keinginan anda dengan cara seperti di bawah ini:\nphp artisan serve --port=8080 Buka browser dan kunjungi URL yang diberikan. Anda akan melihat halaman selamat datang Laravel.\n","href":"/2024/04/belajar-membuat-aplikasi-pertama-anda-dengan-laravel.html","title":"Belajar Membuat Aplikasi Pertama Anda dengan Laravel"},{"content":"Learning Golang recently opened up new perspectives for me in software development. One of the best ways to solidify your understanding is by teaching others. That’s why in this article, I’m sharing my experience installing Go on Linux\u0026ndash;using both Snap and manual source installation.\nWriting this guide not only helps others get started, but also helps reinforce the steps in my own memory.\nInstalling Golang Using Snap Snap is a universal package manager developed by Canonical (Ubuntu’s creator). It simplifies app installation by bundling dependencies, ensuring compatibility across most Linux distributions.\nEnsure Snap is Installed\nOn many modern Linux distros, Snap is pre-installed. If not, you can install it via terminal:\nsudo apt update sudo apt install snapd Install Go via Snap\nsudo snap install go --classic Verify the Installation\ngo version That’s it! You’ve successfully installed Go using Snap.\n🛠️ Installing Golang from Official Source If you want more control over your Go installation or prefer not to use Snap, manual installation is the way to go.\nDownload the Official Go Tarball\nVisit the official Go downloads page and download the latest version. Example:\nwget https://go.dev/dl/go1.16.3.linux-amd64.tar.gz Extract the Archive to /usr/local\nsudo tar -C /usr/local -xzf go1.16.3.linux-amd64.tar.gz Update Your PATH\nAdd Go’s binary path to your environment variable:\nexport PATH=$PATH:/usr/local/go/bin Add that line to ~/.bashrc or ~/.zshrc, then apply:\nsource ~/.bashrc Verify the Installation\ngo version Snap vs Manual Installation \u0026ndash; Which One is Better? Method Pros Cons Snap Quick, easy, auto-updates Slightly slower start-up time Source Full control, latest versions Manual setup \u0026amp; maintenance Conclusion Whether you choose Snap or manual installation, both methods are solid and effective. Snap is faster for beginners, while manual installation is great for advanced users or multi-version management.\nNow that Go is installed, you\u0026rsquo;re ready to build high-performance APIs, CLI tools, or even web servers. Happy coding with Golang!\n","href":"/2024/04/easiest-way-to-install-golang-on-linux.html","title":"Easiest Way to Install Golang on Linux Snap or Manual Source?"},{"content":"Linux is a robust operating system, but occasionally you might encounter a \u0026lsquo;broken update error\u0026rsquo; when trying to update your system through the terminal. This issue can halt your system updates and potentially affect system stability. Here’s a comprehensive guide on how to resolve this error, ensuring your Linux system remains up-to-date and secure.\nUnderstanding the Error\nA broken update error in Linux typically occurs when package dependencies are unsatisfied, when there are conflicts between packages, or when the package repositories are not correctly configured. This can lead to a partial or failed update, rendering your system\u0026rsquo;s package manager unable to proceed with updates.\nStep 1: Check Internet Connection\nBefore proceeding, ensure your internet connection is stable. An interrupted or weak connection can cause update processes to fail. Use ping command to check your connectivity, for example:\nping google.com Step 2: Update Repository Lists\nStart by refreshing your repository lists. This ensures that your package manager has the latest information about available packages and their dependencies:\nsudo apt-get update For non-Debian based distributions, replace apt-get with the package manager relevant to your distribution (like yum for Fedora or pacman for Arch Linux).\nStep 3: Upgrade Packages\nAttempt to upgrade all your system packages with:\nsudo apt-get upgrade This might resolve dependency issues that were causing the update process to break.\nStep 4: Fix Broken Packages\nIf the upgrade doesn’t resolve the issue, you can specifically target and fix broken packages:\nsudo apt-get install -f The -f flag stands for “fix broken”. It repairs broken dependencies, helping the package manager to recover.\nStep 5: Clean Up\nClear out the local repository of retrieved package files. It\u0026rsquo;s a good practice to clean up the cache to free space and remove potentially corrupted files:\nsudo apt-get clean Step 6: Remove Unnecessary Packages\nRemove packages that were automatically installed to satisfy dependencies for other packages and are now no longer needed:\nsudo apt-get autoremove Step 7: Configure Package Manager\nIf the error persists, reconfigure the package manager. This can help resolve any corrupt configurations:\nsudo dpkg --configure -a Step 8: Manually Resolve Dependencies\nSometimes, you may need to manually fix dependencies. Look at the error messages carefully. They often indicate which package is causing the problem. You can then either remove, reinstall, or update that specific package.\nStep 9: Check for Repository Issues\nEnsure that your system’s repositories are correctly set up. Incorrect or outdated sources can cause update errors. The repository configuration files are typically located in /etc/apt/sources.list and /etc/apt/sources.list.d/. Make sure they contain the correct URLs and distribution names.\nStep 10: Seek Community Support\nIf you’ve tried all the above and still face issues, seek support from the Linux community. Linux has a vibrant community on forums like Ask Ubuntu, Linux Mint forums, or Fedora forums, depending on your distribution.\nIf the method above has not made any changes and is still experiencing errors, try the method below:\nStep 1: Identify and Stop the Conflicting Process\nYou can find out what process is holding the lock by using the process ID (PID) given in the error message. In your case, the PID is 1582.\nRun\nps -f -p 1582 ```in the terminal to see details about the process. If it\u0026#39;s a process that can be safely stopped, use sudo kill -9 1582\n**Step 2: Remove the Lock Files** If you are certain no other apt processes are running, you can manually remove the lock files. Use ```bash sudo rm /var/lib/apt/lists/lock Additionally, you might need to remove the lock file in the cache directory:\nsudo rm /var/cache/apt/archives/lock And the lock file in the dpkg directory:\nsudo rm /var/lib/dpkg/lock Note: This is generally not recommended unless you\u0026rsquo;re sure that no apt processes are running, as it can potentially corrupt your package database.\nStep 4 : Restart your computer\nConclusion\nResolving broken update errors in Linux involves a systematic approach to identify and fix package dependencies, configuration issues, and repository errors. By following these steps, most update issues can be resolved directly from the terminal, restoring the smooth functioning of your Linux system. Remember, regular updates are crucial for security and stability, so resolving these errors promptly is important.\n","href":"/2023/11/how-to-fix-broken-update-error-in-linux.html","title":"How to fix broken update error in linux (Terminal)"},{"content":"In the realm of modern web development, providing a seamless user experience and enhancing the overall performance of your web applications is paramount. One essential aspect that plays a pivotal role in achieving these goals is efficient data presentation and manipulation. This is where Yajra DataTables comes into the picture.\nYajra DataTables is a powerful and versatile jQuery-based plugin for Laravel, designed to simplify the process of displaying data in tabular form with advanced features such as filtering, sorting, pagination, and more. It empowers developers to create interactive and dynamic data tables effortlessly, significantly improving how data is showcased to end users.\nThis article will delve into the step-by-step process of installing and configuring Yajra DataTables in Laravel. Whether you are a seasoned Laravel developer or just starting with the framework, this guide will walk you through the necessary setup, providing you with the knowledge to harness the full potential of Yajra DataTables in your Laravel projects.\nSo, if you\u0026rsquo;re ready to elevate your data presentation game and unlock a world of possibilities in your Laravel applications, let\u0026rsquo;s dive in and get started with Yajra DataTables!\nSo let\u0026rsquo;s get started on how to install and configure Yajra Datatable in Laravel.\nThe first step you must be to visit the official website of Yajra Datatable , if you want to follow my way please follow the guide below.\n`composer require yajra/laravel-datatables-oracle:\u0026#34;^10.3.1\u0026#34;` If you want to change the version of Yajra Datatable you must change the value \u0026ldquo;^10.3.1\u0026rdquo; to an old version or if you want to get the new version you can use the script below.\ncomposer require yajra/laravel-datatables-oracle By default, you will download the latest version from Yajra Datatable.\nSo, in the next step, we will configure the provider in Laravel so that you go to the file in the path folder, Config/app.php, and then add the script below to your code.\nproviders\u0026#39; \\=\u0026gt; \\[ // ... Yajra\\\\DataTables\\\\DataTablesServiceProvider::class, \\], If you have put your code into the file app.php, now you can follow this step to publish assets and vendors from Yajra Datatable so that you can use Yajra Datatable on your project.\nphp artisan vendor:publish --tag=datatables Now you can use Datatable on your projects yeah, now if you want to call the Datatable in your blade or view you must add style and script from Datatable because Datatable is a package from jquery.\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.datatables.net/1.13.4/css/dataTables.jqueryui.min.css\u0026#34; /\u0026gt; If you have been adding the following script on the top now you add the script below to call the data table from javascript below. ```js @push(\u0026#39;after-script\u0026#39;) \u0026lt;script\u0026gt; $(\u0026#39;#tb_user\u0026#39;).DataTable({ processing: true, serverSide: true, ajax: { url: \u0026#34;{!! url()-\u0026gt;current() !!}\u0026#34;, }, columns: [ { data: \u0026#39;DT_RowIndex\u0026#39;, name: \u0026#39;id\u0026#39; }, { data: \u0026#39;photo\u0026#39;, name: \u0026#39;photo\u0026#39; }, { data: \u0026#39;email\u0026#39;, name: \u0026#39;email\u0026#39; }, { data: \u0026#39;username\u0026#39;, name: \u0026#39;username\u0026#39; }, { data: \u0026#39;action\u0026#39;, name: \u0026#39;action\u0026#39;, orderable: false, searchable: false }, ], }); \u0026lt;/script\u0026gt; @endpush And then, you must be sent data from the controller to view with script below.\nif (request()-\u0026gt;ajax()) { $query = Layanan::where(\u0026#39;users\\_id\u0026#39;, Auth::user()-\u0026gt;id)-\u0026gt;get(); return datatables()-\u0026gt;of($query) -\u0026gt;addIndexColumn() -\u0026gt;editColumn(\u0026#39;photo\u0026#39;, function ($item) { return $item-\u0026gt;photo ? \u0026#39;\u0026lt;img src=\u0026#34;\u0026#39; . url(\u0026#39;storage/\u0026#39; . $item-\u0026gt;photo) . \u0026#39;\u0026#34; style=\u0026#34;max-height: 50px;\u0026#34; /\u0026gt;\u0026#39; : \u0026#39;-\u0026#39;; }) -\u0026gt;editColumn(\u0026#39;action\u0026#39;, function ($item) { return \u0026#39; \u0026lt;a href=\u0026#34;\u0026#39; . route(\u0026#39;user.edit\u0026#39;, $item-\u0026gt;id) . \u0026#39;\u0026#34; class=\u0026#34;btn btn-sm btn-primary\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-pencil-alt\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;form action=\u0026#34;\u0026#39; . route(\u0026#39;user.destroy\u0026#39;, $item-\u0026gt;id) . \u0026#39;\u0026#34; method=\u0026#34;POST\u0026#34; style=\u0026#34;display: inline-block;\u0026#34;\u0026gt; \u0026#39; . method\\_field(\u0026#39;delete\u0026#39;) . csrf\\_field() . \u0026#39; \u0026lt;button type=\u0026#34;submit\u0026#34; class=\u0026#34;btn btn-sm btn-danger\u0026#34;\u0026gt; \u0026lt;i class=\u0026#34;fa fa-trash\u0026#34;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/form\u0026gt; \u0026#39;; }) -\u0026gt;rawColumns(\\[\u0026#39;photo\u0026#39;, \u0026#39;action\u0026#39;\\]) -\u0026gt;make(true); } return view(\u0026#39;user.index\u0026#39;); Okay, the data table installation and configuration are complete, now you can use and display data using the data table on Laravel, if you have any stuck or questions, you can contact me or add your comment below, Thank you.\n","href":"/2023/08/how-to-install-and-configure-yajra.html","title":"how to install and configure yajra datatable in Laravel "},{"content":"Dapat project Laravel dari GitHub tapi bingung cara jalankannya? Atau malah error melulu saat setup? Tenang, kamu tidak sendirian. Banyak developer pemula (bahkan yang udah agak senior) sering stuck di tahap ini.\nClone project Laravel dari GitHub itu gampang kalau tau step-by-step yang benar. Tapi kalau asal comot dan langsung jalankan, siap-siap ketemu segudang error dari missing dependencies, database connection failed, sampai permission issues.\nPanduan ini akan ngajarin kamu cara yang benar - mulai dari persiapan tools, proses cloning, konfigurasi environment, setup database, sampai troubleshooting error-error umum yang sering muncul. Saya jelaskan juga kenapa setiap step penting, jadi kamu paham beneran, bukan cuma ikutin command buta.\nPersiapan: Tools yang Wajib Ada Sebelum mulai clone project Laravel, pastikan kamu sudah install tools-tools ini. Tanpa mereka, project Laravel tidak akan bisa jalan sama sekali.\nGit - Version Control System Git dipakai untuk clone repository dari GitHub. Ini tools wajib pertama yang harus ada.\nInstall Git:\nWindows - Download dari git-scm.com dan install seperti biasa.\nmacOS - Biasanya sudah ada, kalau belum:\nbrew install git Linux (Ubuntu/Debian):\nsudo apt-get update sudo apt-get install git Cek instalasi Git:\ngit --version # git version 2.40.0 Composer - PHP Dependency Manager Composer itu kayak npm-nya PHP. Laravel butuh Composer untuk install semua package dan dependency yang diperlukan.\nInstall Composer:\nDownload installer dari getcomposer.org dan ikuti instruksi untuk sistem operasi kamu.\nVerifikasi instalasi:\ncomposer --version # Composer version 2.6.5 Composer harus terinstall global supaya bisa dipanggil dari mana aja di terminal.\nPHP - Minimal Versi 8.1 Laravel 10 butuh PHP minimal 8.1. Laravel 11 butuh PHP 8.2. Cek versi PHP kamu:\nphp -v # PHP 8.2.0 Kalau versi PHP kamu masih lama, upgrade dulu. Di Ubuntu:\nsudo add-apt-repository ppa:ondrej/php sudo apt-get update sudo apt-get install php8.2 Di macOS pakai Homebrew:\nbrew install php@8.2 Ekstensi PHP yang Dibutuhkan Laravel butuh beberapa ekstensi PHP. Cek dengan php -m dan pastikan ada:\nOpenSSL PDO Mbstring Tokenizer XML Ctype JSON BCMath Kalau ada yang kurang, install sesuai package manager OS kamu. Di Ubuntu contohnya:\nsudo apt-get install php8.2-mbstring php8.2-xml php8.2-bcmath Database - MySQL atau PostgreSQL Laravel support berbagai database, tapi yang paling umum:\nMySQL:\n# Ubuntu sudo apt-get install mysql-server # macOS brew install mysql PostgreSQL:\n# Ubuntu sudo apt-get install postgresql # macOS brew install postgresql Start service database setelah install:\n# MySQL sudo systemctl start mysql # PostgreSQL sudo systemctl start postgresql Pastikan kamu punya akses root atau buat user database baru untuk Laravel.\nNode.js dan NPM (Opsional tapi Sering Perlu) Kalau project Laravel pakai frontend assets (Vue, React, atau Tailwind), kamu butuh Node.js untuk compile assets.\n# Ubuntu curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash - sudo apt-get install -y nodejs # macOS brew install node Verifikasi:\nnode -v npm -v Sekarang semua tools sudah ready. Lanjut ke proses cloning.\nCara 1: Clone Menggunakan Git (Recommended) Ini cara yang paling proper dan flexible. Kamu dapat full repository dengan git history, bisa pull update, dan push changes kalau punya akses.\nStep 1: Dapatkan URL Repository Buka repository Laravel di GitHub. Klik tombol hijau Code, copy URL-nya. Ada dua pilihan:\nHTTPS - https://github.com/username/project-laravel.git Gampang, tapi harus login terus kalau push.\nSSH - git@github.com:username/project-laravel.git Perlu setup SSH key dulu, tapi lebih aman dan ga perlu login berkali-kali.\nUntuk pemula, pakai HTTPS dulu aja.\nStep 2: Clone Repository Buka terminal, masuk ke folder tempat kamu mau simpan project:\ncd ~/Documents/projects Clone repository:\ngit clone https://github.com/username/project-laravel.git Kalau mau ganti nama folder:\ngit clone https://github.com/username/project-laravel.git nama-project-baru Git akan download semua file dari repository ke komputer kamu. Tunggu sampai selesai.\nStep 3: Masuk ke Folder Project cd project-laravel Cek isi folder:\nls -la Kamu harusnya lihat struktur Laravel standar: app/, config/, database/, public/, dll.\nStep 4: Install Dependencies dengan Composer Ini step paling penting. Folder vendor/ yang berisi semua package Laravel tidak di-commit ke Git (karena terlalu besar). Jadi kamu harus install manual.\ncomposer install Command ini baca file composer.json dan composer.lock, lalu download semua dependency yang dibutuhkan ke folder vendor/.\nProsesnya bisa 1-5 menit tergantung koneksi internet dan jumlah package. Kalau sukses, kamu lihat output:\nPackage operations: 108 installs, 0 updates, 0 removals - Installing doctrine/inflector (2.0.8) - Installing doctrine/lexer (3.0.0) ... Generating optimized autoload files Jangan pakai composer update! Itu akan upgrade package ke versi terbaru yang mungkin incompatible dengan project. Selalu pakai composer install untuk install dependency sesuai versi di composer.lock.\nStep 5: Setup File Environment (.env) Laravel pakai file .env untuk konfigurasi environment-specific seperti database credentials, APP_KEY, debug mode, dll.\nFile .env tidak di-commit ke Git karena berisi data sensitif. Yang di-commit itu .env.example sebagai template.\nCopy .env.example jadi .env:\ncp .env.example .env Di Windows pakai:\ncopy .env.example .env Sekarang kamu punya file .env yang bisa dikonfigurasi.\nStep 6: Generate Application Key Laravel butuh APP_KEY untuk enkripsi session, cookies, dan data sensitif lainnya. Generate dengan:\nphp artisan key:generate Output:\nApplication key set successfully. Command ini otomatis update APP_KEY di file .env dengan random string 32 karakter.\nKalau kamu skip step ini, nanti error:\nRuntimeException: No application encryption key has been specified. Step 7: Konfigurasi Database Buka file .env dengan text editor favorit:\nnano .env # atau code .env # atau editor lain Cari bagian database configuration:\nDB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=nama_database DB_USERNAME=root DB_PASSWORD= Sesuaikan dengan setup database kamu:\nDB_DATABASE - Nama database yang akan dipakai. Pastikan database ini sudah dibuat di MySQL/PostgreSQL.\nDB_USERNAME - Username database (biasanya root untuk MySQL lokal).\nDB_PASSWORD - Password database (kosongkan kalau ga pakai password di lokal).\nContoh untuk MySQL lokal:\nDB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=laravel_project DB_USERNAME=root DB_PASSWORD=secret123 Contoh untuk PostgreSQL:\nDB_CONNECTION=pgsql DB_HOST=127.0.0.1 DB_PORT=5432 DB_DATABASE=laravel_project DB_USERNAME=postgres DB_PASSWORD=password Save file .env setelah edit.\nStep 8: Buat Database Sebelum migrate, pastikan database sudah ada. Masuk ke MySQL atau PostgreSQL dan buat database:\nMySQL:\nmysql -u root -p Di MySQL console:\nCREATE DATABASE laravel_project; EXIT; PostgreSQL:\npsql -U postgres Di PostgreSQL console:\nCREATE DATABASE laravel_project; \\q Nama database harus sama dengan DB_DATABASE di file .env.\nStep 9: Jalankan Migration Migration itu file-file PHP yang define struktur database (tabel, kolom, index, dll). Laravel pakai migration untuk bikin tabel secara programmatic.\nJalankan migration:\nphp artisan migrate Laravel akan baca semua file di database/migrations/ dan execute query untuk bikin tabel. Output:\nMigration table created successfully. Migrating: 2014_10_12_000000_create_users_table Migrated: 2014_10_12_000000_create_users_table (85.32ms) Migrating: 2014_10_12_100000_create_password_reset_tokens_table Migrated: 2014_10_12_100000_create_password_reset_tokens_table (48.21ms) ... Kalau error \u0026ldquo;Access denied for user\u0026rdquo;, cek lagi credential database di .env.\nKalau error \u0026ldquo;Unknown database\u0026rdquo;, pastikan database sudah dibuat di step sebelumnya.\nStep 10: Seed Database (Opsional) Kalau project punya seeder untuk data dummy atau data awal, jalankan:\nphp artisan db:seed Atau seed specific seeder:\nphp artisan db:seed --class=UserSeeder Seeder populate database dengan data sample buat testing atau development. Tidak semua project punya seeder, jadi kalau error \u0026ldquo;Class DatabaseSeeder does not exist\u0026rdquo;, skip aja.\nStep 11: Set Permissions untuk Storage dan Cache Laravel butuh write permission di folder storage/ dan bootstrap/cache/ untuk simpan log, cache, compiled views, uploaded files, dll.\nDi Linux/macOS:\nchmod -R 775 storage bootstrap/cache Kalau pakai web server seperti Nginx atau Apache:\nsudo chown -R www-data:www-data storage bootstrap/cache Ganti www-data dengan user web server kamu (bisa nginx, apache, atau lainnya).\nDi Windows:\nKlik kanan folder storage dan bootstrap/cache, masuk Properties \u0026gt; Security, pastikan user kamu punya Full Control. Uncheck \u0026ldquo;Read-only\u0026rdquo; kalau aktif.\nStep 12: Install NPM Dependencies (Kalau Ada) Kalau project pakai frontend framework atau CSS preprocessor, kamu perlu install NPM packages:\nnpm install Lalu compile assets:\n# Development npm run dev # Production npm run build Kalau ga ada package.json, skip step ini.\nStep 13: Clear Cache (Opsional tapi Disarankan) Kadang Laravel nyimpen cache yang ga sesuai dengan environment baru. Clear semua cache:\nphp artisan config:clear php artisan cache:clear php artisan route:clear php artisan view:clear Ini ensure Laravel mulai fresh tanpa cache lama yang bikin bingung.\nStep 14: Jalankan Development Server Sekarang project siap dijalankan. Start Laravel development server:\nphp artisan serve Output:\nINFO Server running on [http://127.0.0.1:8000]. Press Ctrl+C to stop the server Buka browser dan akses http://127.0.0.1:8000. Kalau semua step benar, kamu lihat homepage Laravel.\nKalau mau ganti port:\nphp artisan serve --port=8080 Atau bind ke IP tertentu:\nphp artisan serve --host=0.0.0.0 --port=8000 Ini bikin Laravel bisa diakses dari komputer lain di network yang sama.\nCara 2: Download ZIP dari GitHub Kalau ga mau pake Git (entah kenapa), kamu bisa download project sebagai ZIP file.\nStep 1: Download ZIP Di halaman GitHub repository, klik tombol Code \u0026gt; Download ZIP.\nSave file ZIP ke komputer, lalu extract.\nStep 2: Masuk ke Folder Project Buka terminal, navigate ke folder hasil extract:\ncd ~/Downloads/project-laravel-main Nama folder biasanya nama-repo-main atau nama-repo-master tergantung branch default.\nStep 3: Lanjutkan Step Install Seperti Cara Git Mulai dari Step 4 di metode Git di atas:\ncomposer install cp .env.example .env php artisan key:generate Konfigurasi database di .env Buat database php artisan migrate Set permissions php artisan serve Semua step sama persis, cuma beda di awal aja (download ZIP vs git clone).\nKekurangan Metode ZIP Download ZIP berarti kamu ga punya Git history. Kamu ga bisa:\nPull update terbaru dari repository Push perubahan kamu ke GitHub Lihat commit history Switch branch Jadi metode ini cuma cocok buat sekedar coba-coba atau belajar. Untuk development serius, pakai Git clone.\nSetup Database: Lebih Detail Database configuration sering jadi sumber error paling banyak. Mari bahas lebih dalam.\nMemilih Database Driver Laravel support beberapa database. Pilih sesuai kebutuhan project:\nMySQL - Paling populer, banyak hosting support, performa bagus untuk aplikasi medium.\nPostgreSQL - Lebih advanced features, strict data integrity, bagus untuk aplikasi enterprise.\nSQLite - File-based, gampang setup, cocok untuk testing atau aplikasi kecil.\nSQL Server - Untuk environment Windows Server dan .NET integration.\nDi .env, set DB_CONNECTION sesuai database yang kamu pakai:\n# MySQL DB_CONNECTION=mysql # PostgreSQL DB_CONNECTION=pgsql # SQLite DB_CONNECTION=sqlite # SQL Server DB_CONNECTION=sqlsrv Konfigurasi MySQL Edit .env:\nDB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=laravel_app DB_USERNAME=laravel_user DB_PASSWORD=secure_password Best practice: Jangan pakai user root untuk aplikasi. Buat user khusus dengan privilege terbatas:\nCREATE DATABASE laravel_app; CREATE USER \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;secure_password\u0026#39;; GRANT ALL PRIVILEGES ON laravel_app.* TO \u0026#39;laravel_user\u0026#39;@\u0026#39;localhost\u0026#39;; FLUSH PRIVILEGES; Konfigurasi PostgreSQL Edit .env:\nDB_CONNECTION=pgsql DB_HOST=127.0.0.1 DB_PORT=5432 DB_DATABASE=laravel_app DB_USERNAME=laravel_user DB_PASSWORD=secure_password Di PostgreSQL:\nCREATE DATABASE laravel_app; CREATE USER laravel_user WITH PASSWORD \u0026#39;secure_password\u0026#39;; GRANT ALL PRIVILEGES ON DATABASE laravel_app TO laravel_user; Konfigurasi SQLite SQLite simple, ga butuh server terpisah. Database-nya cuma file.\nEdit .env:\nDB_CONNECTION=sqlite # Hapus atau comment line DB_HOST, DB_PORT, DB_DATABASE, dll Buat file database:\ntouch database/database.sqlite Jalankan migration langsung, Laravel otomatis pakai file itu.\nTest Koneksi Database Sebelum migrate, test dulu koneksi database:\nphp artisan tinker Di Tinker console:\nDB::connection()-\u0026gt;getPdo(); Kalau sukses, output object PDO. Kalau error, cek lagi credential di .env.\nExit Tinker:\nexit Running Migrations: Yang Perlu Kamu Tau Migration itu version control untuk database schema. Setiap perubahan struktur database (bikin tabel, tambah kolom, dll) dibuat sebagai migration file.\nStruktur Migration File Migration file ada di database/migrations/. Contoh:\n2014_10_12_000000_create_users_table.php 2014_10_12_100000_create_password_reset_tokens_table.php 2023_05_15_143022_create_posts_table.php Format: YYYY_MM_DD_HHMMSS_description.php\nTimestamp ensure migration jalan sesuai urutan.\nIsi Migration File Contoh create_users_table.php:\nuse Illuminate\\Database\\Migrations\\Migration; use Illuminate\\Database\\Schema\\Blueprint; use Illuminate\\Support\\Facades\\Schema; return new class extends Migration { public function up(): void { Schema::create(\u0026#39;users\u0026#39;, function (Blueprint $table) { $table-\u0026gt;id(); $table-\u0026gt;string(\u0026#39;name\u0026#39;); $table-\u0026gt;string(\u0026#39;email\u0026#39;)-\u0026gt;unique(); $table-\u0026gt;timestamp(\u0026#39;email_verified_at\u0026#39;)-\u0026gt;nullable(); $table-\u0026gt;string(\u0026#39;password\u0026#39;); $table-\u0026gt;rememberToken(); $table-\u0026gt;timestamps(); }); } public function down(): void { Schema::dropIfExists(\u0026#39;users\u0026#39;); } }; Method up() - Dijalankan saat migrate, define struktur tabel.\nMethod down() - Dijalankan saat rollback, revert perubahan.\nJalankan Migration Command dasar:\n# Jalankan semua migration yang belum dijalankan php artisan migrate # Rollback migration terakhir php artisan migrate:rollback # Rollback semua migration php artisan migrate:reset # Rollback semua, lalu migrate lagi (fresh start) php artisan migrate:refresh # Drop semua tabel, lalu migrate lagi php artisan migrate:fresh # Migrate dengan seeder php artisan migrate --seed Hati-hati dengan migrate:fresh dan migrate:refresh! Kedua command ini drop semua data di database. Jangan pakai di production.\nMigration Status Cek migration mana yang sudah jalan:\nphp artisan migrate:status Output:\n+------+---------------------------------------------+-------+ | Ran? | Migration | Batch | +------+---------------------------------------------+-------+ | Yes | 2014_10_12_000000_create_users_table | 1 | | Yes | 2014_10_12_100000_create_password_resets | 1 | | No | 2023_05_15_143022_create_posts_table | - | +------+---------------------------------------------+-------+ Ini bantu kamu tau migration mana yang pending.\nTroubleshooting Migration Error Error: SQLSTATE[42S01]: Base table or view already exists\nTabel sudah ada. Rollback dulu atau drop manual:\nphp artisan migrate:rollback Error: Syntax error or access violation\nBiasanya typo di migration file atau privilege database kurang. Cek query di migration file dan privilege user database.\nError: Class \u0026lsquo;Database\u0026rsquo; not found\nPastikan composer install sudah jalan dan autoload di-generate:\ncomposer dump-autoload Menangani Seeders dan Dummy Data Seeder populate database dengan data awal atau sample. Berguna untuk development dan testing.\nCheck Seeder yang Tersedia Lihat folder database/seeders/. File utama biasanya DatabaseSeeder.php:\nnamespace Database\\Seeders; use Illuminate\\Database\\Seeder; class DatabaseSeeder extends Seeder { public function run(): void { $this-\u0026gt;call([ UserSeeder::class, PostSeeder::class, CategorySeeder::class, ]); } } Seeder lain dipanggil dari DatabaseSeeder.\nJalankan Seeder Jalankan semua seeder:\nphp artisan db:seed Atau seeder tertentu:\nphp artisan db:seed --class=UserSeeder Migrate + Seed Sekaligus php artisan migrate --seed Atau fresh migrate dengan seed:\nphp artisan migrate:fresh --seed Ini drop semua tabel, migrate lagi, lalu seed. Cocok buat reset database ke state awal.\nSeeder untuk Production Jangan jalankan seeder sembarangan di production. Seeder biasanya cuma untuk development data.\nKalau butuh data awal di production (misal: roles, permissions, default settings), buat seeder khusus dan document dengan jelas.\nPermission dan Storage: Kenapa Penting Laravel butuh write access ke beberapa folder untuk nyimpen file temporary, cache, log, dan uploaded files.\nFolder yang Butuh Write Permission storage/app/ - File upload, temporary files\nstorage/framework/ - Cache, sessions, compiled views\nstorage/logs/ - Application logs\nbootstrap/cache/ - Compiled services dan packages\nTanpa write permission, Laravel error kayak:\nThe stream or file \u0026#34;storage/logs/laravel.log\u0026#34; could not be opened: failed to open stream: Permission denied Set Permission di Linux/macOS chmod -R 775 storage bootstrap/cache Kalau pakai web server:\nsudo chown -R www-data:www-data storage bootstrap/cache User www-data harus punya akses write. Ganti sesuai user web server kamu (nginx, apache, dll).\nTroubleshooting Permission Issues Error: failed to open stream: Permission denied\nFolder atau file ga punya permission. Set ulang:\nchmod -R 775 storage Error: The stream or file could not be opened in append mode\nLog file ga bisa di-write. Fix:\nchmod -R 775 storage/logs Atau delete log file lama:\nrm storage/logs/laravel.log Laravel auto-create file baru dengan permission yang benar.\nPermission di Windows Windows jarang ada masalah permission. Tapi kalau error:\nKlik kanan folder storage dan bootstrap/cache Properties \u0026gt; Security Edit \u0026gt; pilih user kamu Check \u0026ldquo;Full control\u0026rdquo; Apply Pastikan juga folder tidak Read-only:\nProperties \u0026gt; General Uncheck \u0026ldquo;Read-only\u0026rdquo; Apply to all subfolders Compile Frontend Assets Banyak project Laravel modern pakai frontend framework atau CSS framework yang butuh compilation.\nCheck Package.json Lihat file package.json di root project. Kalau ada, berarti project butuh npm:\n{ \u0026#34;private\u0026#34;: true, \u0026#34;type\u0026#34;: \u0026#34;module\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;dev\u0026#34;: \u0026#34;vite\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;vite build\u0026#34; }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;axios\u0026#34;: \u0026#34;^1.6.0\u0026#34;, \u0026#34;laravel-vite-plugin\u0026#34;: \u0026#34;^1.0\u0026#34;, \u0026#34;vite\u0026#34;: \u0026#34;^5.0\u0026#34; } } Install NPM Dependencies npm install Ini download semua packages di node_modules/. Folder ini juga ga di-commit ke Git karena besar.\nCompile Assets untuk Development npm run dev Vite (atau Laravel Mix untuk project lama) compile assets dan watch perubahan. Output:\nVITE v5.0.0 ready in 543 ms ➜ Local: http://localhost:5173/ ➜ Network: use --host to expose ➜ press h + enter to show help Biarkan running di terminal terpisah. Setiap kali kamu edit JS/CSS, auto recompile.\nBuild untuk Production npm run build Ini compile assets dengan optimisasi penuh (minify, tree shaking, dll). File hasil ada di public/build/.\nKalau Ga Ada package.json Berarti project ga pakai build tools. Assets langsung di public/ dan siap pakai.\nTroubleshooting: Error Umum dan Solusinya Setelah bantu ratusan developer setup Laravel, ini error yang paling sering muncul.\nError: \u0026ldquo;No application encryption key has been specified\u0026rdquo; Penyebab: APP_KEY di .env kosong atau belum di-generate.\nSolusi:\nphp artisan key:generate Refresh browser, error hilang.\nError: \u0026ldquo;SQLSTATE[HY000] [1045] Access denied for user\u0026rdquo; Penyebab: Username atau password database salah di .env.\nSolusi:\nCek credential database di .env Test login manual ke database: mysql -u username -p Kalau ga bisa login, reset password atau buat user baru Error: \u0026ldquo;SQLSTATE[HY000] [1049] Unknown database\u0026rdquo; Penyebab: Database belum dibuat.\nSolusi:\nmysql -u root -p CREATE DATABASE nama_database; EXIT; Sesuaikan nama_database dengan DB_DATABASE di .env.\nError: \u0026ldquo;Class \u0026lsquo;XXX\u0026rsquo; not found\u0026rdquo; Penyebab: Autoload belum di-generate atau ada class yang missing.\nSolusi:\ncomposer dump-autoload Kalau masih error, cek apakah ada typo di namespace atau class name.\nError: \u0026ldquo;The stream or file could not be opened: failed to open stream: Permission denied\u0026rdquo; Penyebab: Folder storage/ atau bootstrap/cache/ ga punya write permission.\nSolusi:\nchmod -R 775 storage bootstrap/cache Error: \u0026ldquo;419 Page Expired\u0026rdquo; saat Submit Form Penyebab: CSRF token expired atau missing.\nSolusi:\nClear cache:\nphp artisan cache:clear Pastikan form punya @csrf:\n\u0026lt;form method=\u0026#34;POST\u0026#34;\u0026gt; @csrf \u0026lt;!-- form fields --\u0026gt; \u0026lt;/form\u0026gt; Set SESSION_DRIVER di .env:\nSESSION_DRIVER=file Error: \u0026ldquo;Vite manifest not found\u0026rdquo; Penyebab: Frontend assets belum di-compile.\nSolusi:\nnpm install npm run dev Atau untuk production:\nnpm run build Error: \u0026ldquo;Target class [XXXController] does not exist\u0026rdquo; Penyebab: Route reference controller yang ga ada atau namespace salah.\nSolusi:\nCek file controller ada di app/Http/Controllers/ Pastikan namespace benar: namespace App\\Http\\Controllers; Clear route cache: php artisan route:clear Error: \u0026ldquo;The Mix manifest does not exist\u0026rdquo; Penyebab: Project pakai Laravel Mix tapi assets belum di-compile.\nSolusi:\nnpm install npm run dev Laravel Mix itu predecessor Vite, masih dipakai di project Laravel lama.\nEnvironment Configuration: Deep Dive File .env itu jantung konfigurasi Laravel. Setiap environment (local, staging, production) punya .env berbeda.\nStruktur File .env Contoh .env lengkap:\nAPP_NAME=Laravel APP_ENV=local APP_KEY=base64:xxx APP_DEBUG=true APP_TIMEZONE=Asia/Jakarta APP_URL=http://localhost:8000 LOG_CHANNEL=stack LOG_LEVEL=debug DB_CONNECTION=mysql DB_HOST=127.0.0.1 DB_PORT=3306 DB_DATABASE=laravel_app DB_USERNAME=root DB_PASSWORD= BROADCAST_DRIVER=log CACHE_DRIVER=file FILESYSTEM_DISK=local QUEUE_CONNECTION=sync SESSION_DRIVER=file SESSION_LIFETIME=120 MAIL_MAILER=smtp MAIL_HOST=smtp.gmail.com MAIL_PORT=587 MAIL_USERNAME=your@email.com MAIL_PASSWORD=your-app-password MAIL_ENCRYPTION=tls MAIL_FROM_ADDRESS=noreply@yourdomain.com MAIL_FROM_NAME=\u0026#34;${APP_NAME}\u0026#34; APP_ENV: Environment Type local - Development di komputer lokal. Debug mode aktif, error ditampilkan lengkap.\nstaging - Testing environment sebelum production. Setup mirip production tapi data dummy.\nproduction - Live server. Debug mode off, error ga detail ke user.\nSet sesuai environment:\nAPP_ENV=local APP_DEBUG: Debug Mode true - Tampilkan error detail dengan stack trace. Jangan pakai di production!\nfalse - Error generic, detail hanya di log file. Wajib di production.\nAPP_DEBUG=true # local APP_DEBUG=false # production APP_URL: Base URL URL dasar aplikasi. Penting untuk generate link dan asset path yang benar.\nAPP_URL=http://localhost:8000 # local APP_URL=https://staging.yourdomain.com # staging APP_URL=https://yourdomain.com # production Mail Configuration Kalau project kirim email (reset password, notification, dll), configure mail:\nMAIL_MAILER=smtp MAIL_HOST=smtp.gmail.com MAIL_PORT=587 MAIL_USERNAME=your-email@gmail.com MAIL_PASSWORD=your-app-password MAIL_ENCRYPTION=tls Untuk Gmail, pakai App Password , bukan password akun.\nCache dan Session CACHE_DRIVER - Tempat simpan cache. Pilihan: file, redis, memcached, database.\nSESSION_DRIVER - Tempat simpan session. Pilihan: file, cookie, database, redis.\nUntuk local development, file sudah cukup:\nCACHE_DRIVER=file SESSION_DRIVER=file Production biasanya pakai redis untuk performa lebih baik.\nQueue Configuration QUEUE_CONNECTION - Driver untuk job queue.\nDevelopment pakai sync (langsung execute):\nQUEUE_CONNECTION=sync Production pakai redis atau database:\nQUEUE_CONNECTION=redis Jangan lupa jalankan queue worker:\nphp artisan queue:work Testing Project: Memastikan Semuanya Jalan Setelah setup, test apakah project bener-bener jalan.\nTest Homepage Akses http://127.0.0.1:8000 di browser. Kalau muncul homepage Laravel atau custom homepage project, berarti routing basic works.\nTest Database Connection Buka route yang query database (misal: /users, /posts). Kalau data muncul tanpa error, database connection sukses.\nAtau test via Tinker:\nphp artisan tinker User::count(); // Harus return number, bukan error Test Authentication Kalau project punya login/register, test:\nRegister user baru Login dengan user tersebut Akses halaman yang butuh auth Kalau sukses tanpa error 419 atau redirect loop, auth system works.\nTest File Upload Kalau ada fitur upload (avatar, post image, dll), test upload file. Pastikan:\nFile tersimpan di storage/app/ File bisa diakses via URL kalau public Ga ada error permission Run Automated Tests Kalau project punya unit test atau feature test:\nphp artisan test Atau pakai PHPUnit langsung:\n./vendor/bin/phpunit Test yang pass berarti core functionality project ga rusak.\nCheck Logs Buka storage/logs/laravel.log. Lihat apakah ada error atau warning yang mencurigakan.\nKalau ada error meski aplikasi jalan, fix sekarang sebelum jadi masalah besar.\nGit Workflow Setelah Clone Setelah clone dan setup, kamu biasanya mau mulai development. Ini workflow Git yang proper.\nCreate Development Branch Jangan langsung coding di main atau master. Buat branch baru:\ngit checkout -b feature/user-profile Atau untuk bugfix:\ngit checkout -b fix/login-error Commit Changes Setelah coding:\ngit add . git commit -m \u0026#34;Add user profile page with avatar upload\u0026#34; Commit message harus descriptive, jelasin apa yang kamu tambah/ubah/fix.\nPush ke GitHub git push origin feature/user-profile Kalau belum punya akses push, fork dulu repository-nya, clone fork kamu, baru push.\nPull Request Di GitHub, buat Pull Request dari branch kamu ke main. Tunggu code review dari team.\nPull Latest Changes Sebelum mulai coding, selalu pull changes terbaru:\ngit checkout main git pull origin main git checkout -b feature/new-feature Ini ensure kamu coding di codebase terbaru, ga conflict dengan perubahan orang lain.\nSync Fork Kalau kamu pake fork, sync dengan upstream repository:\ngit remote add upstream https://github.com/original-owner/repo.git git fetch upstream git checkout main git merge upstream/main git push origin main Deployment Considerations Project udah jalan di lokal? Saatnya mikir deployment ke production.\nEnvironment Variables Jangan commit file .env ke Git! File ini berisi credentials dan secret keys.\nDi server production:\nCopy .env.example jadi .env Edit sesuai production environment Set APP_ENV=production dan APP_DEBUG=false Optimize untuk Production Jalankan optimization commands:\n# Cache configuration php artisan config:cache # Cache routes php artisan route:cache # Cache views php artisan view:cache # Optimize Composer autoload composer install --optimize-autoloader --no-dev Ini bikin Laravel load lebih cepat dengan cache semua configuration dan routes.\nDatabase Migration di Production Hati-hati dengan migration di production! Bisa bikin downtime atau data loss.\nBest practice:\nBackup database dulu Test migration di staging dengan production data Run migration saat traffic rendah Monitor error logs # Backup mysqldump -u user -p database \u0026gt; backup.sql # Migrate php artisan migrate --force Flag --force diperlukan di production karena Laravel confirm dulu.\nWeb Server Configuration Kalau pakai Nginx atau Apache, pastikan document root point ke public/ folder, bukan root project.\nNginx contoh:\nserver { listen 80; server_name yourdomain.com; root /var/www/laravel-app/public; index index.php; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_pass unix:/var/run/php/php8.2-fpm.sock; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $realpath_root$fastcgi_script_name; include fastcgi_params; } } Apache contoh (.htaccess sudah ada di public/):\n\u0026lt;VirtualHost *:80\u0026gt; ServerName yourdomain.com DocumentRoot /var/www/laravel-app/public \u0026lt;Directory /var/www/laravel-app/public\u0026gt; AllowOverride All Require all granted \u0026lt;/Directory\u0026gt; \u0026lt;/VirtualHost\u0026gt; Security Checklist Sebelum go live:\nAPP_DEBUG=false di .env APP_ENV=production di .env HTTPS aktif (SSL certificate installed) File .env ga accessible via web Folder storage/ dan bootstrap/cache/ ga accessible via web Database user punya privilege minimal (bukan root) Firewall configured (hanya port 80, 443, dan SSH) Regular backup database dan files Working with Teams Clone project dari team? Ikuti konvensi dan workflow mereka.\nREADME.md Baca README.md di root project. Biasanya ada instruksi khusus untuk setup:\nVersi PHP dan extensions yang dibutuhkan Database yang harus dipakai External services (Redis, ElasticSearch, dll) Custom setup steps Environment variables yang perlu di-set Ikuti README itu. Developer sebelumnya udah documentasikan hal-hal penting.\nContributing Guidelines File CONTRIBUTING.md menjelaskan:\nBranch naming convention Commit message format Code style guide Pull request process Testing requirements Ikuti guidelines ini supaya PR kamu ga direject.\nProject-Specific Commands Cek composer.json bagian scripts. Sering ada custom commands:\n\u0026#34;scripts\u0026#34;: { \u0026#34;post-install-cmd\u0026#34;: [ \u0026#34;@php artisan key:generate --ansi\u0026#34; ], \u0026#34;setup\u0026#34;: [ \u0026#34;composer install\u0026#34;, \u0026#34;npm install\u0026#34;, \u0026#34;cp .env.example .env\u0026#34;, \u0026#34;@php artisan key:generate\u0026#34;, \u0026#34;@php artisan migrate --seed\u0026#34; ] } Kalau ada script setup, kamu bisa jalankan:\ncomposer run setup Ini auto-execute semua command setup sekaligus.\nAsk Questions Ga ngerti sesuatu? Tanya team lead atau developer lain. Lebih baik tanya daripada asal asumsi dan bikin bug.\nAdvanced: Docker dan Laravel Sail Banyak project Laravel modern pakai Docker untuk development environment yang consistent.\nLaravel Sail Laravel Sail itu Docker wrapper buat Laravel. Tinggal satu command, environment siap.\nKalau project punya file docker-compose.yml dan folder vendor/laravel/sail, kemungkinan pakai Sail.\nSetup dengan Sail # Install dependencies via Docker (ga perlu PHP atau Composer di local) docker run --rm \\ -u \u0026#34;$(id -u):$(id -g)\u0026#34; \\ -v \u0026#34;$(pwd):/var/www/html\u0026#34; \\ -w /var/www/html \\ laravelsail/php82-composer:latest \\ composer install --ignore-platform-reqs # Copy .env cp .env.example .env # Start Sail ./vendor/bin/sail up -d # Generate key ./vendor/bin/sail artisan key:generate # Migrate ./vendor/bin/sail artisan migrate Sail Commands Setelah Sail running, semua command Laravel pakai prefix sail:\n# Artisan ./vendor/bin/sail artisan migrate # Composer ./vendor/bin/sail composer require package/name # NPM ./vendor/bin/sail npm install # Tinker ./vendor/bin/sail tinker # Bash ke container ./vendor/bin/sail bash Alias untuk Sail Biar ga cape ngetik ./vendor/bin/sail terus, buat alias:\nalias sail=\u0026#39;./vendor/bin/sail\u0026#39; Add ke ~/.bashrc atau ~/.zshrc supaya permanent.\nSekarang tinggal:\nsail up sail artisan migrate sail composer install Benefits Pakai Sail Consistent environment semua developer (same PHP, MySQL, Redis version) Ga perlu install PHP, MySQL, dll di local machine Easy switch antar project dengan setup berbeda Mirip production environment Common Mistakes dan Cara Menghindarinya Dari pengalaman ngajarin Laravel, ini kesalahan umum pemula.\nMistake 1: Langsung composer update Salah: Habis clone, langsung composer update.\nKenapa salah: composer update upgrade packages ke versi terbaru, bisa breaking compatibility.\nBenar: Selalu composer install. Ini install sesuai versi di composer.lock.\nMistake 2: Lupa Generate APP_KEY Salah: Langsung akses aplikasi tanpa generate key.\nKenapa salah: Error \u0026ldquo;No application encryption key\u0026rdquo; dan session ga jalan.\nBenar: Selalu php artisan key:generate setelah copy .env.\nMistake 3: Pakai Database Root User Salah: Set DB_USERNAME=root di .env production.\nKenapa salah: Security risk. Root user punya akses penuh ke semua database.\nBenar: Buat user khusus dengan privilege minimal untuk aplikasi.\nMistake 4: Debug Mode di Production Salah: APP_DEBUG=true di production.\nKenapa salah: Expose sensitive info (database queries, file paths, environment variables) ke user.\nBenar: Selalu APP_DEBUG=false di production.\nMistake 5: Ignore Storage Permissions Salah: Ga set permission di storage/ dan bootstrap/cache/.\nKenapa salah: Laravel ga bisa write logs, cache, sessions. Aplikasi error atau lambat.\nBenar: Always chmod -R 775 storage bootstrap/cache setelah clone.\nMistake 6: Commit File .env Salah: Add file .env ke Git.\nKenapa salah: Credentials dan secrets ke-expose di repository.\nBenar: .env harus ada di .gitignore. Commit hanya .env.example.\nMistake 7: Skip Migration Salah: Langsung akses halaman tanpa migrate.\nKenapa salah: Tabel belum ada, query error.\nBenar: Selalu php artisan migrate setelah setup database.\nLaravel Versions dan Compatibility Project Laravel beda versi punya requirement beda.\nLaravel 11 (Latest) PHP 8.2 minimum New application structure Streamlined configuration Better performance Laravel 10 PHP 8.1 minimum Long Term Support (LTS) until Feb 2025 Native type hints Improved processes Laravel 9 PHP 8.0 minimum LTS until Feb 2024 (already ended) Controller route groups Improved Eloquent Laravel 8 PHP 7.3 minimum Jetstream and Fortify Job batching Migration squashing Check Laravel Version Lihat di composer.json:\n\u0026#34;require\u0026#34;: { \u0026#34;php\u0026#34;: \u0026#34;^8.1\u0026#34;, \u0026#34;laravel/framework\u0026#34;: \u0026#34;^10.0\u0026#34; } Atau via Artisan:\nphp artisan --version # Laravel Framework 10.48.4 Pastikan versi PHP kamu sesuai dengan minimum requirement Laravel yang dipakai project.\nResources dan Learning Path Habis berhasil clone dan jalanin project Laravel, lanjut belajar apa?\nOfficial Documentation Laravel Documentation - Selalu refer ke docs versi yang sesuai project kamu.\nTutorial Lanjutan Kalau kamu tertarik lebih dalam dengan Laravel, cek artikel-artikel ini:\nCara Install dan Configure Yajra DataTable di Laravel - Buat tabel data interaktif Integrasi Laravel dengan API external Authentication dan authorization advanced Queue dan background jobs Real-time dengan Laravel Echo dan Pusher Backend Development Path Laravel itu bagian dari ekosistem backend development. Perluas skill dengan:\nREST API development - Build dan consume APIs Database management - Optimization dan scaling Docker deployment - Containerization dan orchestration Community Join komunitas Laravel:\nLaracasts - Video tutorials berkualitas Laravel News - Update terbaru Laravel Laravel Indonesia Telegram/Discord - Diskusi dengan developer lokal ","href":"/2023/04/cara-menjalankan-project-laravel-clone.html","title":"Cara Menjalankan Project Laravel Clone dari GitHub"},{"content":"Hi, I\u0026rsquo;m Wiku Karno! 👋 Welcome to BuanaCoding – where I share my journey as a software developer and help others build better applications through practical tutorials and real-world insights.\nWhat I Do I\u0026rsquo;m a passionate software developer who loves diving deep into modern programming languages and frameworks. My expertise spans across several key areas:\nGo Programming Go is my primary language of choice. I\u0026rsquo;ve written extensively about Go fundamentals, advanced concepts like goroutines and channels, building REST APIs, working with databases, and following Go best practices. Whether you\u0026rsquo;re just starting with Go or looking to level up your skills, you\u0026rsquo;ll find comprehensive guides here.\nWeb Development I specialize in full-stack web development using:\nLaravel/PHP for robust web applications Python/FastAPI for high-performance APIs Modern development practices and clean architecture DevOps \u0026amp; Linux System administration and deployment are crucial skills for any developer. I share tutorials on:\nLinux server management and troubleshooting Docker containerization Nginx configuration and SSL setup Application deployment strategies Security \u0026amp; Best Practices Security isn\u0026rsquo;t an afterthought – it\u0026rsquo;s built into everything I do. I cover topics like:\nWeb application security Password management and authentication Modern authentication methods (Passkeys, WebAuthn) Protecting against common security threats Developer Productivity I\u0026rsquo;m always exploring tools and techniques that make developers more productive:\nCode editors and essential extensions Development environment setup Automation and workflow optimization My Mission BuanaCoding exists to bridge the gap between complex technical concepts and practical, actionable knowledge. I believe that:\nLearning should be accessible – I write for developers at all levels, from beginners to experienced professionals Real-world examples matter – Every tutorial includes practical examples you can actually use in your projects Quality over quantity – I focus on creating comprehensive, well-researched content rather than quick tips Community drives growth – The best learning happens when we share knowledge and learn from each other Why Trust My Content? I don\u0026rsquo;t just write about technologies – I use them in real projects. Every tutorial and guide is based on hands-on experience, tested solutions, and lessons learned from actual development work.\nMy content has helped thousands of developers:\nLearn Go programming from scratch to advanced concepts Build secure web applications with Laravel and FastAPI Deploy applications to production servers Implement modern security practices Optimize their development workflows Let\u0026rsquo;s Connect Whether you\u0026rsquo;re just starting your programming journey or you\u0026rsquo;re an experienced developer looking to expand your skills, I\u0026rsquo;m here to help. Feel free to reach out through the comments on any article – I read and respond to every message.\nHappy coding, and welcome to the BuanaCoding community! 🚀\nP.S. All tutorials and code examples on this site are thoroughly tested and regularly updated to reflect the latest best practices and framework versions.\n","href":"/about/","title":"About"},{"content":"If you would like to get in touch or collaborate with me — including freelance work — feel free to reach out via the contact information below.\nEmail: buanacoding@gmail.com ","href":"/contact/","title":"Contact"},{"content":"If you require any more information or have any questions about our site\u0026rsquo;s disclaimer, please feel free to contact us by email at buanacoding@gmail.com All the information on this website - https://www.buanacoding.com - is published in good faith and for general information purpose only. buanacoding does not make any warranties about the completeness, reliability and accuracy of this information. Any action you take upon the information you find on this website (buanacoding), is strictly at your own risk. buanacoding will not be liable for any losses and/or damages in connection with the use of our website. Our disclaimer was generated with the help of the Disclaimer Generator.\nFrom our website, you can visit other websites by following hyperlinks to such external sites. While we strive to provide only quality links to useful and ethical websites, we have no control over the content and nature of these sites. These links to other websites do not imply a recommendation for all the content found on these sites. Site owners and content may change without notice and may occur before we have the opportunity to remove a link which may have gone \u0026lsquo;bad\u0026rsquo;.\nPlease be also aware that when you leave our website, other sites may have different privacy policies and terms which are beyond our control. Please be sure to check the Privacy Policies of these sites as well as their \u0026ldquo;Terms of Service\u0026rdquo; before engaging in any business or uploading any information.\nConsent By using our website, you hereby consent to our disclaimer and agree to its terms.\nUpdates Should we update, amend or make any changes to this document, those changes will be prominently posted here.\n","href":"/disclaimer/","title":"Disclaimer"},{"content":"At BuanaCoding, accessible from https://www.buanacoding.com , one of our main priorities is the privacy of our visitors. This Privacy Policy document contains the types of information that are collected and recorded by BuanaCoding and how we use it.\nIf you have additional questions or require more information about our Privacy Policy, do not hesitate to contact us.\nLog Files BuanaCoding follows a standard procedure of using log files. These files log visitors when they visit websites. All hosting companies do this as part of hosting services\u0026rsquo; analytics. The information collected by log files includes Internet Protocol (IP) addresses, browser type, Internet Service Provider (ISP), date and time stamps, referring/exit pages, and possibly the number of clicks. These are not linked to any information that is personally identifiable. The purpose of the information is to analyze trends, administer the site, track users’ movement around the website, and gather demographic information.\nOur Privacy Policy was created with the help of the Privacy Policy Generator .\nGoogle DoubleClick DART Cookie Google is one of the third-party vendors on our site. It also uses cookies, known as DART cookies, to serve ads to our site visitors based on their visit to www.website.com and other sites on the internet. However, visitors may choose to decline the use of DART cookies by visiting the Google Ad and Content Network Privacy Policy .\nPrivacy Policies of Advertising Partners You may refer to this section to find the Privacy Policy for each of the advertising partners of BuanaCoding.\nThird-party ad servers or ad networks use technologies like cookies, JavaScript, or Web Beacons in their respective advertisements and links that appear on BuanaCoding, which are sent directly to users’ browsers. They automatically receive your IP address when this occurs. These technologies are used to measure the effectiveness of their advertising campaigns and/or to personalize the advertising content that you see on websites you visit.\nPlease note that BuanaCoding has no access to or control over these cookies that are used by third-party advertisers.\nThird-Party Privacy Policies BuanaCoding’s Privacy Policy does not apply to other advertisers or websites. Thus, we advise you to consult the respective Privacy Policies of these third-party ad servers for more detailed information. This may include their practices and instructions about how to opt out of certain options.\nYou can choose to disable cookies through your individual browser options. More detailed information about cookie management with specific web browsers can be found at the respective websites of those browsers.\nChildren’s Information Another part of our priority is adding protection for children while using the internet. We encourage parents and guardians to observe, participate in, and/or guide and advise their children’s online activity.\nBuanaCoding does not knowingly collect any personally identifiable information from children under the age of 13. If you think your child has provided such information on our website, we strongly encourage you to contact us immediately and we will make our best efforts to promptly remove such information from our records.\nOnline Privacy Policy Only This Privacy Policy applies only to our online activities and is valid for visitors to our website with regard to the information that they shared and/or collect in BuanaCoding. This policy does not apply to any information collected offline or via channels other than this website.\n","href":"/privacy-policy/","title":"Privacy Policy"}]